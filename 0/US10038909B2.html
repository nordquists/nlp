
    <html>
        <body>
            <search-app>
                <article class="result" itemscope="" itemtype="http://schema.org/ScholarlyArticle">
    <h1 itemprop="pageTitle">US10038909B2 - Compression of light field images 
        - Google Patents</h1><section itemprop="abstract" itemscope="">
<h2>Abstract</h2>
<div html="" itemprop="content"><abstract lang="EN" load-source="patent-office" mxw-id="PA234256539">
<div class="abstract" id="p-0001" num="0000">RAW images and/or light field images may be compressed through the use of specialized techniques. The color depth of a light field image may be reduced through the use of a bit reduction algorithm such as a K-means algorithm. The image may then be retiled to group pixels of similar intensities and/or colors. The retiled image may be padded with extra pixel rows and/or pixel columns as needed, and compressed through the use of an image compression algorithm. The compressed image may be assembled with metadata pertinent to the manner in which compression was done to form a compressed image file. The compressed image file may be decompressed by following the compression method in reverse.</div>
</abstract>
</div>
</section><section itemprop="description" itemscope="">
<h2>Description</h2>
<div html="" itemprop="content"><div class="description" lang="EN" load-source="patent-office" mxw-id="PDES139430158">
<heading id="h-0001">CROSS-REFERENCE TO RELATED APPLICATION</heading>
<div class="description-paragraph" id="p-0002" num="0001">The present application is a divisional of U.S. application Ser. No. 14/261,144 for “Compression of Light Field Images”, filed Apr. 24, 2014, the disclosure of which is incorporated herein by reference in its entirety.</div>
<heading id="h-0002">FIELD OF THE INVENTION</heading>
<div class="description-paragraph" id="p-0003" num="0002">The present invention relates to systems and methods for compressing and storing digital media such as two-dimensional images projected from light field data.</div>
<heading id="h-0003">BACKGROUND</heading>
<div class="description-paragraph" id="p-0004" num="0003">The advent of digital image capture technologies has revolutionized the photography industry. Digital cameras can now take pictures in high-resolution formats in which each image has several million pixels. The size of such images, along with the limited storage capacity typically available, has prompted the development of many different image compression algorithms. Many of these algorithms function based on grouping pixels of similar colors together and removing variations that are not generally perceptible to the human eye.</div>
<div class="description-paragraph" id="p-0005" num="0004">Light field cameras capture not just a two-dimensional image, but also light field data related to the angle of incidence of light received at various locations within the image. Such data is captured by causing the light to pass through a microlens array (MLA) positioned between the main lens and the image capture sensor. The resulting image may have a plurality of pixel clusters, each of which represents light that passed through a single microlens of the microlens array. Capture of directional information in this manner facilitates various operations, such as refocusing and other forms of image manipulation that are not possible with conventional images.</div>
<div class="description-paragraph" id="p-0006" num="0005">Because they include significant amounts of information not present in conventional images, light field image files may be very large. Unfortunately, the presence of a microlens pattern superimposed on the representation of the scene can result in rapidly varying content within the light field image file; this high-frequency spatial variation in pixel intensity can result in poor performance when applying conventional image compression techniques.</div>
<heading id="h-0004">SUMMARY</heading>
<div class="description-paragraph" id="p-0007" num="0006">According to various embodiments, the system and method of the present invention provide mechanisms for compressing two-dimensional images projected from light field data, while avoiding the above-described problems that result from the high-frequency spatial variation in pixel intensity inherent in light field image data.</div>
<div class="description-paragraph" id="p-0008" num="0007">A light field image may be received, for example, from a light field camera. According to various embodiments, the light field image may be compressed in a number of steps that may include any or all of the following steps: (1) reducing a color depth of the pixels of the image, (2) retiling the image, (3) padding the image so that it is sized for optimal performance of an image compression algorithm, (4) applying the image compression algorithm to the image, and/or (5) assembling the resulting compressed image with metadata. These steps need not all be performed in every embodiment.</div>
<div class="description-paragraph" id="p-0009" num="0008">If desired, the color depth reduction step may include the use of a K-means bit reduction algorithm. Such an algorithm may be used, for example, to reduce the color depth of the pixels of the image from 12 bits to 8 bits, which may help to enhance performance of the image compression algorithm. Color depth reduction parameters may be included in the metadata of the compressed image to indicate how color depth reduction was performed on the image.</div>
<div class="description-paragraph" id="p-0010" num="0009">The retiling step may be used further enhance the performance of the image compression algorithm by grouping pixels of similar colors and/or similar intensities together for image compression purposes. The image may be a RAW image, and may also be Bayer filtered image or the like. Thus, the image may have groups of 2×2 pixels, each of which has an intensity obtained from light filtered through red, green, or blue filters. The retiling step may be used to group pixels of each color together, thereby reducing the spatial intensity variation and improving compression performance.</div>
<div class="description-paragraph" id="p-0011" num="0010">A light field image may include a plurality of pixel clusters, each of which encodes a portion of the light field data corresponding to a microlens of the microlens array of the light field camera. Each pixel cluster may have pixels horizontally and vertically arranged in a grid pattern. In at least one embodiment, the image is retiled according to the period of the microlens array; this results in, retiled pixel clusters wherein adjacent pixels are relatively similar in intensity. The arrangement of retiled pixels ensures that there is a one-to-one correspondence between the pixels of each pixel cluster and the retiled pixel clusters of the retiled image, while reducing spatial intensity variation and thereby improving compression performance. In at least one embodiment, the retiled pixel clusters may be arranged in a grid pattern that corresponds to the relative positions of each pixel within each pixel cluster were arranged. In at least one embodiment, a mapping of pixels from the pixel clusters to the retiled pixel clusters may be included in the metadata of the compressed image.</div>
<div class="description-paragraph" id="p-0012" num="0011">Padding the image may involve adding one or more pixel rows and/or pixel columns to the image so that it is the appropriate size for application of the image compression algorithm. The added pixel rows and/or pixel columns may be duplicates of the adjacent edge a pixel column and/or edge pixel row. In at least one embodiment, padding parameters that indicate how pixels were added to the retiled image may be included in the compressed image.</div>
<div class="description-paragraph" id="p-0013" num="0012">Applying the image compression algorithm to the image may entail applying a known lossless or lossy image compression algorithm such as that developed by the Joint Photographic experts Group, known as “JPEG” compression. Additionally or alternatively, application of the image compression algorithm may entail application of one or more novel techniques for compressing an image, as described herein. The image compression algorithm may be applied to the retiled image; the performance of the image compression algorithm may be enhanced by the color depth reduction, retiling, and/or padding steps performed previously.</div>
<div class="description-paragraph" id="p-0014" num="0013">The compressed image may be processed through the use of a method with steps that are, generally, the reverse of the steps applied to compress the image. Thus, the image compression algorithm may be applied in reverse. The padding may be reversed by removing the extra pixel columns and/or rows. The compressed image may be retiled again such that the pixels originally in each pixel cluster are again grouped together, and the color depth of the pixels may again be increased. The metadata of the compressed image, which may contain any of the components set forth above, may be used to facilitate any of the foregoing steps.</div>
<description-of-drawings>
<heading id="h-0005">BRIEF DESCRIPTION OF THE DRAWINGS</heading>
<div class="description-paragraph" id="p-0015" num="0014">The accompanying drawings illustrate several embodiments of the invention and, together with the description, serve to explain the principles of the invention according to the embodiments. One skilled in the art will recognize that the particular embodiments illustrated in the drawings are merely exemplary, and are not intended to limit the scope of the present invention.</div>
<div class="description-paragraph" id="p-0016" num="0015"> <figref idrefs="DRAWINGS">FIG. 1A</figref> depicts an example of an architecture for implementing the present invention in a light field capture device, according to one embodiment.</div>
<div class="description-paragraph" id="p-0017" num="0016"> <figref idrefs="DRAWINGS">FIG. 1B</figref> depicts an example of an architecture for implementing the present invention in a post-processing system communicatively coupled to a light field capture device, according to one embodiment.</div>
<div class="description-paragraph" id="p-0018" num="0017"> <figref idrefs="DRAWINGS">FIG. 2</figref> depicts an example of an architecture for a light field camera for implementing the present invention according to one embodiment.</div>
<div class="description-paragraph" id="p-0019" num="0018"> <figref idrefs="DRAWINGS">FIG. 3</figref> depicts a portion of a light field image.</div>
<div class="description-paragraph" id="p-0020" num="0019"> <figref idrefs="DRAWINGS">FIG. 4</figref> depicts transmission of light rays through a microlens to illuminate pixels in a digital sensor.</div>
<div class="description-paragraph" id="p-0021" num="0020"> <figref idrefs="DRAWINGS">FIG. 5</figref> depicts an arrangement of a light field capture device wherein a microlens array is positioned such that images of a main-lens aperture, as projected onto the digital sensor, do not overlap.</div>
<div class="description-paragraph" id="p-0022" num="0021"> <figref idrefs="DRAWINGS">FIG. 6</figref> depicts an example of projection and reconstruction to reduce a four-dimensional light field representation to a two-dimensional image.</div>
<div class="description-paragraph" id="p-0023" num="0022"> <figref idrefs="DRAWINGS">FIGS. 7A and 7B</figref> depict two exemplary lookup tables that map three-bit raw pixel values to two-bit encoded, lower precision values.</div>
<div class="description-paragraph" id="p-0024" num="0023"> <figref idrefs="DRAWINGS">FIG. 8</figref> depicts an exemplary image histogram with a corresponding cumulative histogram.</div>
<div class="description-paragraph" id="p-0025" num="0024"> <figref idrefs="DRAWINGS">FIG. 9A</figref> depicts a histogram for an initial state, in which a single span encompasses the entire histogram.</div>
<div class="description-paragraph" id="p-0026" num="0025"> <figref idrefs="DRAWINGS">FIG. 9B</figref> depicts the histogram of <figref idrefs="DRAWINGS">FIG. 9A</figref> after one step, showing the final metric values for each of two spans.</div>
<div class="description-paragraph" id="p-0027" num="0026"> <figref idrefs="DRAWINGS">FIG. 9C</figref> depicts the histogram of <figref idrefs="DRAWINGS">FIG. 9A</figref> after two steps, showing the final metric values for each of three spans.</div>
<div class="description-paragraph" id="p-0028" num="0027"> <figref idrefs="DRAWINGS">FIG. 9D</figref> depicts the selection of four pixel values spaced across the full range of pixel vales of the histogram of <figref idrefs="DRAWINGS">FIG. 9A</figref>.</div>
<div class="description-paragraph" id="p-0029" num="0028"> <figref idrefs="DRAWINGS">FIGS. 10A and 10B</figref> depict the use of four seed values in an exemplary four-bit to two-bit conversion lookup table to populate the remainder of the lookup table.</div>
<div class="description-paragraph" id="p-0030" num="0029"> <figref idrefs="DRAWINGS">FIG. 11A</figref> depicts a 4×3 grid of pixels of a full-color YUV image.</div>
<div class="description-paragraph" id="p-0031" num="0030"> <figref idrefs="DRAWINGS">FIG. 11B</figref> depicts a 4×3 grid of pixels of a full-color 422-sub-sampled YUV image.</div>
<div class="description-paragraph" id="p-0032" num="0031"> <figref idrefs="DRAWINGS">FIG. 12</figref> depicts the mapping between a 2×2 grid of RAW image pixels and a 2×1 grid of YUV422 full-color image pixels.</div>
<div class="description-paragraph" id="p-0033" num="0032"> <figref idrefs="DRAWINGS">FIG. 13</figref> depicts the mapping between a 4×4 grid of RAW image pixels and a 4×2 grid of YUV422 full-color image pixels.</div>
<div class="description-paragraph" id="p-0034" num="0033"> <figref idrefs="DRAWINGS">FIG. 14A</figref> depicts compression of a YUV422 image buffer to a JPEG data stream.</div>
<div class="description-paragraph" id="p-0035" num="0034"> <figref idrefs="DRAWINGS">FIG. 14B</figref> depicts decompression of a YUV422 image buffer from a JPEG data stream.</div>
<div class="description-paragraph" id="p-0036" num="0035"> <figref idrefs="DRAWINGS">FIG. 15</figref> depicts JPEG encoding and decoding in schematic form.</div>
<div class="description-paragraph" id="p-0037" num="0036"> <figref idrefs="DRAWINGS">FIG. 16</figref> depicts an exemplary inverse lookup table mapping two-bit encoded pixel values to 3-bit raw values.</div>
<div class="description-paragraph" id="p-0038" num="0037"> <figref idrefs="DRAWINGS">FIG. 17A</figref> depicts a RAW light field image.</div>
<div class="description-paragraph" id="p-0039" num="0038"> <figref idrefs="DRAWINGS">FIG. 17B</figref> depicts an enlarged portion of the RAW light field image of <figref idrefs="DRAWINGS">FIG. 17A</figref>, illustrating the pixel clusters of the RAW light field image.</div>
<div class="description-paragraph" id="p-0040" num="0039"> <figref idrefs="DRAWINGS">FIG. 18</figref> depicts a method for compressing an image projected from light field data, according to one embodiment of the invention.</div>
<div class="description-paragraph" id="p-0041" num="0040"> <figref idrefs="DRAWINGS">FIG. 19</figref> depicts a histogram of a type that may be used to facilitate pixel color depth reduction in the form of K-means bit reduction.</div>
<div class="description-paragraph" id="p-0042" num="0041"> <figref idrefs="DRAWINGS">FIG. 20</figref> depicts a mapping of pixels from pixel clusters of the image to retiled pixel clusters to form a retiled image.</div>
<div class="description-paragraph" id="p-0043" num="0042"> <figref idrefs="DRAWINGS">FIG. 21A</figref> depicts a retiled light field image.</div>
<div class="description-paragraph" id="p-0044" num="0043"> <figref idrefs="DRAWINGS">FIG. 21B</figref> depicts an enlarged portion of the retiled light field image of <figref idrefs="DRAWINGS">FIG. 21A</figref>.</div>
<div class="description-paragraph" id="p-0045" num="0044"> <figref idrefs="DRAWINGS">FIG. 22</figref> depicts a padding process for adding one or more pixel rows and/or columns to the retiled image.</div>
<div class="description-paragraph" id="p-0046" num="0045"> <figref idrefs="DRAWINGS">FIG. 23</figref> depicts an image file that may be stored after assembly of the compressed light field image and metadata.</div>
<div class="description-paragraph" id="p-0047" num="0046"> <figref idrefs="DRAWINGS">FIG. 24</figref> depicts a method for processing a compressed image for use, wherein the compressed image has been compressed with a method such as that of <figref idrefs="DRAWINGS">FIG. 18</figref>.</div>
</description-of-drawings>
<heading id="h-0006">DETAILED DESCRIPTION</heading>
<heading id="h-0007">Definitions</heading>
<div class="description-paragraph" id="p-0048" num="0047">For purposes of the description provided herein, the following definitions are used:
</div> <ul> <li id="ul0001-0001" num="0048">Bayer Pattern: a particular 2×2 pattern of different color filters above pixels on a digital sensor. The filter pattern is 50% green, 25% red and 25% blue.</li> <li id="ul0001-0002" num="0049">Disk: a region in a light field image that is illuminated by light passing through a single microlens; may be circular or any other suitable shape.</li> <li id="ul0001-0003" num="0050">Image: a two-dimensional array of pixel values, or pixels, each specifying a color.</li> <li id="ul0001-0004" num="0051">Light Field: a collection of rays. A ray's direction specifies a path taken by light, and its color specifies the radiance of light following that path.</li> <li id="ul0001-0005" num="0052">Light Field Image: a two-dimensional image that spatially encodes a four-dimensional light field. The sensor image from a light field camera is a light field image.</li> <li id="ul0001-0006" num="0053">Microlens: a small lens, typically one in an array of similar microlenses.</li> <li id="ul0001-0007" num="0054">MLA: abbreviation for microlens array.</li> <li id="ul0001-0008" num="0055">Pixel: an n-tuple of intensity values, with an implied meaning for each value. A typical 3-tuple pixel format is RGB, wherein the first value is red intensity, the second green intensity, and the third blue intensity. Also refers to an individual sensor element for capturing data for a pixel.</li> <li id="ul0001-0009" num="0056">Representative Ray: a single ray that represents all the rays that reach a pixel.</li> <li id="ul0001-0010" num="0057">Two-Dimensional Image (or Image): a two-dimensional array of pixels, each specifying a color. The pixels are typically arranged in a square or rectangular Cartesian pattern, but other patterns are possible.</li> <li id="ul0001-0011" num="0058">Two-Dimensional Image Processing: any type of changes that may be performed on a two-dimensional image.</li> <li id="ul0001-0012" num="0059">Vignetting: a phenomenon, related to modulation, in which an image's brightness or saturation is reduced at the periphery as compared to the image center.</li> </ul>
<div class="description-paragraph" id="p-0049" num="0060">In addition, for ease of nomenclature, the term “camera” is used herein to refer to an image capture device or other data acquisition device. Such a data acquisition device can be any device or system for acquiring, recording, measuring, estimating, determining and/or computing data representative of a scene, including but not limited to two-dimensional image data, three-dimensional image data, and/or light field data. Such a data acquisition device may include optics, sensors, and image processing electronics for acquiring data representative of a scene, using techniques that are well known in the art, are disclosed herein, or could be conceived by a person of skill in the art with the aid of the present disclosure.</div>
<div class="description-paragraph" id="p-0050" num="0061">One skilled in the art will recognize that many types of data acquisition devices can be used in connection with the present invention, and that the invention is not limited to cameras. Thus, the use of the term “camera” herein is intended to be illustrative and exemplary, but should not be considered to limit the scope of the invention. Specifically, any use of such term herein should be considered to refer to any suitable device for acquiring image data.</div>
<div class="description-paragraph" id="p-0051" num="0062">In the following description, several techniques and methods for processing light field images are described. One skilled in the art will recognize that these various techniques and methods can be performed singly and/or in any suitable combination with one another.</div>
<heading id="h-0008">Architecture</heading>
<div class="description-paragraph" id="p-0052" num="0063">In at least one embodiment, the system and method described herein can be implemented in connection with light field images captured by light field capture devices including but not limited to those described in Ng et al., Light field photography with a hand-held plenoptic capture device, Technical Report CSTR 2005-02, Stanford Computer Science.</div>
<div class="description-paragraph" id="p-0053" num="0064">Referring now to <figref idrefs="DRAWINGS">FIG. 1A</figref>, there is shown a block diagram depicting an architecture for implementing the present invention in a light field capture device such as a camera <b>100</b>. Referring now also to <figref idrefs="DRAWINGS">FIG. 1B</figref>, there is shown a block diagram depicting an architecture for implementing the present invention in a post-processing system communicatively coupled to a light field capture device such as a camera <b>100</b>, according to one embodiment. One skilled in the art will recognize that the particular configurations shown in <figref idrefs="DRAWINGS">FIGS. 1A and 1B</figref> are merely exemplary, and that other architectures are possible for camera <b>100</b>. One skilled in the art will further recognize that several of the components shown in the configurations of <figref idrefs="DRAWINGS">FIGS. 1A and 1B</figref> are optional, and may be omitted or reconfigured. Other components as known in the art may additionally or alternatively be added.</div>
<div class="description-paragraph" id="p-0054" num="0065">In at least one embodiment, camera <b>100</b> may be a light field camera that includes light field image data acquisition device <b>109</b> having optics <b>101</b>, image sensor or sensor <b>103</b> (including a plurality of individual sensors for capturing pixels), and microlens array <b>102</b>. Optics <b>101</b> may include, for example, aperture <b>112</b> for allowing a selectable amount of light into camera <b>100</b>, and main lens <b>113</b> for focusing light toward microlens array <b>102</b>. In at least one embodiment, microlens array <b>102</b> may be disposed and/or incorporated in the optical path of camera <b>100</b> (between main lens <b>113</b> and sensor <b>103</b>) so as to facilitate acquisition, capture, sampling of, recording, and/or obtaining light field image data via sensor <b>103</b>.</div>
<div class="description-paragraph" id="p-0055" num="0066">Referring now also to <figref idrefs="DRAWINGS">FIG. 2</figref>, there is shown an example of an architecture for a light field camera, or a camera <b>100</b>, for implementing the present invention according to one embodiment. <figref idrefs="DRAWINGS">FIG. 2</figref> is not shown to scale. <figref idrefs="DRAWINGS">FIG. 2</figref> shows, in conceptual form, the relationship between aperture <b>112</b>, main lens <b>113</b>, microlens array <b>102</b>, and sensor <b>103</b>, as such components interact to capture light field data for subject <b>201</b>.</div>
<div class="description-paragraph" id="p-0056" num="0067">In at least one embodiment, camera <b>100</b> may also include a user interface <b>105</b> for allowing a user to provide input for controlling the operation of camera <b>100</b> for capturing, acquiring, storing, and/or processing image data.</div>
<div class="description-paragraph" id="p-0057" num="0068">In at least one embodiment, camera <b>100</b> may also include control circuitry <b>110</b> for facilitating acquisition, sampling, recording, and/or obtaining light field image data. For example, control circuitry <b>110</b> may manage and/or control (automatically or in response to user input) the acquisition timing, rate of acquisition, sampling, capturing, recording, and/or obtaining of light field image data.</div>
<div class="description-paragraph" id="p-0058" num="0069">In at least one embodiment, camera <b>100</b> may include memory <b>111</b> for storing image data, such as output by sensor <b>103</b>. The memory <b>111</b> can include external and/or internal memory. In at least one embodiment, memory <b>111</b> can be provided at a separate device and/or location from camera <b>100</b>. For example, camera <b>100</b> may store raw light field image data, as output by sensor <b>103</b>, and/or a representation thereof, such as a compressed image data file. In addition, as described in related U.S. Utility application Ser. No. 12/703,367 for “Light field Camera Image, File and Configuration Data, and Method of Using, Storing and Communicating Same,” filed Feb. 10, 2010, memory <b>111</b> can also store data representing the characteristics, parameters, and/or configurations (collectively “configuration data”) of field image data acquisition device <b>109</b>.</div>
<div class="description-paragraph" id="p-0059" num="0070">In at least one embodiment, captured image data is provided to post-processing circuitry <b>104</b>. Such processing circuitry <b>104</b> may be disposed in or integrated into light field image data acquisition device <b>109</b>, as shown in <figref idrefs="DRAWINGS">FIG. 1A</figref>, or it may be in a separate component external to light field image data acquisition device <b>109</b>, as shown in <figref idrefs="DRAWINGS">FIG. 1B</figref>. Such separate component may be local or remote with respect to light field image data acquisition device <b>109</b>. The post-processing circuitry <b>104</b> may include a processor of any known configuration, including microprocessors, ASICS, and the like. Any suitable wired or wireless protocol can be used for transmitting image data <b>121</b> to processing circuitry <b>104</b>; for example, the camera <b>100</b> can transmit image data <b>121</b> and/or other data via the Internet, a cellular data network, a Wi-Fi network, a Bluetooth communication protocol, and/or any other suitable means.</div>
<heading id="h-0009">Overview of Light Field Image Capture</heading>
<div class="description-paragraph" id="p-0060" num="0071">Light field images often include a plurality of projections (which may be circular or of other shapes) of aperture <b>112</b> of camera <b>100</b>, each projection taken from a different vantage point on the camera's focal plane. The light field image may be captured on sensor <b>103</b>. The interposition of microlens array <b>102</b> between main lens <b>113</b> and sensor <b>103</b> causes images of aperture <b>112</b> to be formed on sensor <b>103</b>, each microlens in the microlens array <b>102</b> projecting a small image of main-lens aperture <b>112</b> onto sensor <b>103</b>. These aperture-shaped projections are referred to herein as disks, although they need not be circular in shape.</div>
<div class="description-paragraph" id="p-0061" num="0072">Light field images include four dimensions of information describing light rays impinging on the focal plane of camera <b>100</b> (or other capture device). Two spatial dimensions (herein referred to as x and y) are represented by the disks themselves. For example, the spatial resolution of a light field image with 120,000 disks, arranged in a Cartesian pattern 400 wide and 300 high, is 400×300. Two angular dimensions (herein referred to as u and v) are represented as the pixels within an individual disk. For example, the angular resolution of a light field image with 100 pixels within each disk, arranged as a 10×10 Cartesian pattern, is 10×10. This light field image has a four-dimensional (x,y,u,v) resolution of (400,300,10,10).</div>
<div class="description-paragraph" id="p-0062" num="0073">Referring now to <figref idrefs="DRAWINGS">FIG. 3</figref>, there is shown an example of a 2-disk by 2-disk portion <b>300</b> of such a light field image, including depictions of disks <b>302</b> and individual pixels <b>403</b>; for illustrative purposes, each disk <b>302</b> is ten pixels <b>403</b> across. Many light rays in the light field within a light field camera contribute to the illumination of a single pixel <b>403</b>.</div>
<div class="description-paragraph" id="p-0063" num="0074">Referring now to <figref idrefs="DRAWINGS">FIG. 4</figref>, there is shown an example of transmission of light rays <b>402</b>, including representative rays <b>402</b>A, <b>402</b>D, through microlens <b>401</b>B of the microlens array <b>102</b>, to illuminate sensor pixels <b>403</b>A, <b>403</b>B in sensor <b>103</b>. In the example of <figref idrefs="DRAWINGS">FIG. 4</figref>, rays <b>402</b>A, <b>402</b>B, <b>402</b>C (represented by solid lines) illuminate sensor pixel <b>403</b>A, while dashed rays <b>402</b>D, <b>402</b>E, <b>402</b>F illuminate sensor pixel <b>403</b>B. The value at each sensor pixel <b>403</b> is determined by the sum of the irradiance of all rays <b>402</b> that illuminate it. For illustrative and descriptive purposes, however, it may be useful to identify a single geometric ray <b>402</b> with each sensor pixel <b>403</b>. That ray <b>402</b> may be chosen to be representative of all the rays <b>402</b> that illuminate that sensor pixel <b>403</b>, and is therefore referred to herein as a representative ray <b>402</b>. Such representative rays <b>402</b> may be chosen as those that pass through the center of a particular microlens <b>401</b>, and that illuminate the center of a particular sensor pixel <b>403</b>. In the example of <figref idrefs="DRAWINGS">FIG. 4</figref>, rays <b>402</b>A and <b>402</b>D are depicted as representative rays; both rays <b>402</b>A, <b>402</b>D pass through the center of microlens <b>401</b>B, with ray <b>402</b>A representing all rays <b>402</b> that illuminate sensor pixel <b>403</b>A and ray <b>402</b>D representing all rays <b>402</b> that illuminate sensor pixel <b>403</b>B.</div>
<div class="description-paragraph" id="p-0064" num="0075">There may be a one-to-one relationship between sensor pixels <b>403</b> and their representative rays <b>402</b>. This relationship may be enforced by arranging the (apparent) size and position of main-lens aperture <b>112</b>, relative to microlens array <b>102</b>, such that images of aperture <b>112</b>, as projected onto sensor <b>103</b>, do not overlap.</div>
<div class="description-paragraph" id="p-0065" num="0076">Referring now to <figref idrefs="DRAWINGS">FIG. 5</figref>, there is shown an example of an arrangement of a light field capture device, such as camera <b>100</b>, wherein microlens array <b>102</b> is positioned such that images of a main-lens aperture <b>112</b>, as projected onto sensor <b>103</b>, do not overlap. All rays <b>402</b> depicted in <figref idrefs="DRAWINGS">FIG. 5</figref> are representative rays <b>402</b>, as they all pass through the center of one of microlenses <b>401</b> to the center of a pixel <b>403</b> of sensor <b>103</b>.</div>
<div class="description-paragraph" id="p-0066" num="0077">In at least one embodiment, the four-dimensional light field representation may be reduced to a two-dimensional image through a process of projection and reconstruction.</div>
<div class="description-paragraph" id="p-0067" num="0078">Referring now to <figref idrefs="DRAWINGS">FIG. 6</figref>, there is shown an example of such a process. A virtual surface of projection <b>601</b> may be introduced, and the intersection of each representative ray <b>402</b> with surface <b>601</b> may be computed. Surface <b>601</b> may be planar or non-planar. If planar, it may be parallel to microlens array <b>102</b> and sensor <b>103</b>, or it may not be parallel. In general, surface <b>601</b> may be positioned at any arbitrary location with respect to microlens array <b>102</b> and sensor <b>103</b>. The color of each representative ray <b>402</b> may be taken to be equal to the color of its corresponding pixel. In at least one embodiment, pixels <b>403</b> of sensor <b>103</b> may include filters arranged in a regular pattern, such as a Bayer pattern, and converted to full-color pixels. Such conversion can take place prior to projection, so that projected rays <b>402</b> can be reconstructed without differentiation. Alternatively, separate reconstruction can be performed for each color channel.</div>
<div class="description-paragraph" id="p-0068" num="0079">The color of an image pixel <b>602</b> on projection surface <b>601</b> may be computed by summing the colors of representative rays <b>402</b> that intersect projection surface <b>601</b> within the domain of that image pixel <b>602</b>. The domain may be within the boundary of the image pixel <b>602</b>, or may extend beyond the boundary of the image pixel <b>602</b>. The summation may be weighted, such that different representative rays <b>402</b> contribute different fractions to the sum. Ray weights may be assigned, for example, as a function of the location of the intersection between ray <b>402</b> and surface <b>601</b>, relative to the center of a particular pixel <b>602</b>. Any suitable weighting algorithm can be used, including for example a bilinear weighting algorithm, a bicubic weighting algorithm and/or a Gaussian weighting algorithm.</div>
<heading id="h-0010">RAW Image Processing and Compression</heading>
<div class="description-paragraph" id="p-0069" num="0080">This section describes exemplary systems and methods for compressing and decompressing RAW images that may achieve high compression ratios and also leverage commonplace, widely deployed image compression/decompression hardware that is designed to operate on full-color images.</div>
<div class="description-paragraph" id="p-0070" num="0081">RAW images may be characterized as being one or more of the following:
</div> <ul> <li id="ul0002-0001" num="0082">Images captured by a camera or other imaging device that features a color filter array (for example, color filter array with a Bayer mosaic pattern);</li> <li id="ul0002-0002" num="0083">Images that contain a single value per pixel that corresponds to a single color (for example, red, green or blue)</li> </ul>
<div class="description-paragraph" id="p-0071" num="0084">The most common type of RAW image includes a repeating 2×2 grid pattern of pixels in which two are green, one is red, and one is blue; however, other RAW image layouts and/or formats exist, and are intended to also fall within the scope of the present invention.</div>
<div class="description-paragraph" id="p-0072" num="0085">Full-color images, such as RGB or YUV images, contain multiple (for example, 3) values per pixel, each value corresponding to a different color. Standard and/or commonplace image compression/decompression hardware is designed to compress full-color images by taking advantage of the fact that the human eye is more sensitive to certain aspects of full-color images than other aspects, and encoding those different aspects with greater or less precision accordingly. In at least one embodiment, RAW images are processed in such a way as to make them suitable for compression and decompression using such hardware, achieving higher compression ratios than are typically observed for RAW image compression while enabling highly efficient and fast compression and decompression implementations.</div>
<div class="description-paragraph" id="p-0073" num="0086">In certain embodiments, the inventions described and/or illustrated herein may be characterized as including one or more of the following components or aspects:
</div> <ul> <li id="ul0003-0001" num="0087">RAW image compression, comprising one or more of:
    <ul> <li id="ul0004-0001" num="0088">A technique for converting the RAW image pixel values, which are often represented by 10- to 16-bit numbers, to lower levels of precision, for example 8-bit values. This conversion may be performed in such a way as to minimize the overall precision loss in the RAW image, and to make use of the image data (for example, via an image histogram) to determine the mapping in a data-dependent fashion;</li> <li id="ul0004-0002" num="0089">A rearrangement of the image data to map each RAW pixel, each of which has a single value, to a position in a full-color image, in which each pixel has multiple (for example, 3) values. Full-color images may be subsampled, resulting in a subsampled image in which each pixel only has a subset of the color channels of the full-color image, for example YUV422 images; and/or</li> <li id="ul0004-0003" num="0090">The compression of the rearranged data using a method that is designed to be employed on full-color images, for example JPEG compression or any other full-color image compression algorithm.</li> </ul>
</li> <li id="ul0003-0002" num="0091">Storing and/or transferring the image while in this compressed form.
    <ul> <li id="ul0005-0001" num="0092">Associated metadata that describes the compression process may be stored and/or transmitted along with the compressed image data, for example tables that describe the mapping between the original-precision and the low-precision pixel encodings.</li> </ul>
</li> <li id="ul0003-0003" num="0093">RAW image decompression, comprising one or more of:
    <ul> <li id="ul0006-0001" num="0094">The decompression of the compressed image data using a respective complementary full-color decompressor, for example a JPEG decompressor if a JPEG compression algorithm was used to compress the image;</li> <li id="ul0006-0002" num="0095">The rearrangement of the image data to invert the mapping performed during compression, that is, to map the pixel values from their positions in the full-color image back to their positions in the RAW image; and/or</li> <li id="ul0006-0003" num="0096">The conversion of each pixel value from its low-precision encoded form (for example, 8 bits per pixel) back to its original precision (for example, 10 to 16 bits per pixel). The conversion may include the dithering of each pixel value by adding a randomly selected amount from within a range that is chosen to take into account some or all of the following information, or any other information:
        <ul> <li id="ul0007-0001" num="0097">The magnitude of the pixel value; or</li> <li id="ul0007-0002" num="0098">The mapping used to convert between the original precision and the low-precision encoded pixel values.</li> </ul>
</li> </ul>
</li> </ul>
<div class="description-paragraph" id="p-0074" num="0099">Existing image compression algorithms are mostly related to full-color image compression, and achieve high compression ratios by lossy encodings that exploit the human eye's varying sensitivity to different image aspects. The inventions described and/or illustrated herein may enable such full-color image compression techniques to be brought to bear on RAW images and in particular RAW light field images, thus enabling highly efficient and fast hardware that implements such full-color image compression algorithms to be used to compress RAW images. Some or all of the techniques described herein can be applied to light field images in RAW format or in other formats.</div>
<heading id="h-0011">Encoding RAW Pixel Values Using Less Precision</heading>
<div class="description-paragraph" id="p-0075" num="0100">In certain aspects, the invention described herein relates to a method of encoding pixels in RAW images using less per-pixel precision. For example, RAW image pixels are often represented by values that span a 10-bit to 16-bit range (corresponding to numbers in the ranges [0,1023] to [0,65535], respectively), and the method described herein may transform the RAW pixel values into an encoding that requires less precision, for example an 8-bit value (corresponding to a number in the range [0,255]). Specific embodiments may transform pixel values between any RAW precision and any lesser encoded precision; the description is not intended to be limited to the exemplary precisions described and/or illustrated herein.</div>
<div class="description-paragraph" id="p-0076" num="0101">Various alternative methods may be employed to implement this conversion, for example:
</div> <ul> <li id="ul0008-0001" num="0102">Dividing or “right-shifting” each pixel value by the appropriate amount, for example right-shifting the values by 2 bits when converting from a 10-bit RAW pixel value to an 8-bit encoded value. A variation of this method may involve first adding an amount to the RAW value prior to dividing or shifting it, and in general, adding an amount roughly equal to half of the divisor will prevent this method from introducing an intensity bias in the resultant image, for example slightly darkening it.</li> <li id="ul0008-0002" num="0103">Creating one or more lookup tables that are indexed by RAW pixel values and map them to encoded values. For example, when converting from 10-bit RAW pixel values to 8-bit encoded values, such a lookup-table may have 1024 entries, each containing a value in the range [0,255]. There may be a single lookup table that is used for all RAW pixel values, or there may be multiple lookup tables, for example a separate lookup table dedicated to each of the different colors that a RAW pixel value may correspond to (for example red, green, and blue).</li> </ul>
<div class="description-paragraph" id="p-0077" num="0104">Various alternative methods may in turn be used to create such lookup tables, for example:
</div> <ul> <li id="ul0009-0001" num="0105">Evenly spacing the encoded values across the entire RAW pixel value range (for example as depicted in <figref idrefs="DRAWINGS">FIG. 7A</figref> for an example 3-bit to 2-bit conversion).</li> <li id="ul0009-0002" num="0106">Spacing the encoded values in a non-evenly-spaced fashion (for example, as depicted in <figref idrefs="DRAWINGS">FIG. 7B</figref>). In this case, there is great latitude in the decision of which spacing to use, and the choice of spacing may play a large role in the final quality of the overall compression process.</li> </ul>
<div class="description-paragraph" id="p-0078" num="0107">One approach for generating a non-evenly-spaced lookup-table may be to examine the RAW image data itself. Based on an analysis of the RAW image data, the system can determine the spacing that produces the best output, measured (for example) with respect to some overall quantitative error metric (for example, RMS error), or with respect to a qualitative estimate of image quality degradation resulting from the compression and decompression processes.</div>
<div class="description-paragraph" id="p-0079" num="0108">One exemplary implementation of generating a lookup-table based on an analysis of the RAW image data is as follows:
</div> <ul> <li id="ul0010-0001" num="0109">1. Build a separate histogram of the RAW pixel values for each RAW color; for example, for a RAW image in which pixel values could be red, green, or blue, 3 histograms would be built, one per color. Each histogram has as many entries as there are possible RAW pixel values for its color. For example, if red RAW pixel values are 10-bit binary values, then the red histogram has 2<sup>10</sup>=1024 entries.</li> <li id="ul0010-0002" num="0110">2. For each histogram, build a corresponding cumulative histogram, as illustrated in <figref idrefs="DRAWINGS">FIG. 8</figref>.</li> <li id="ul0010-0003" num="0111">3. For each histogram, compute a single “span” for the entire histogram. For each of these spans, assign to it a metric value corresponding to the product of the total sum of pixel values within it and the span's width as follows (where a is the minimum pixel value (0), and b is the maximum pixel value, for example 255 if 8-bit encoded pixel values are being used):
<br/>
metric[span <i>a:b</i>]=(SUM[<i>i=a </i>to <i>b</i>]histogram[<i>i</i>])*(<i>b−a</i>)
</li> </ul>
<div class="description-paragraph" id="p-0080" num="0112">This may be computed efficiently by using the cumulative histogram:
<br/>
metric[span <i>a:b</i>]=(cumulativeHistogram[<i>b</i>]−cumulativeHistogram[<i>a</i>])*(<i>b−a</i>)
</div> <ul> <li id="ul0011-0001" num="0113">4. For an encoded value precision of N-bits, there are 2<sup>N </sup>lookup values; for example, for an 8-bit encoded value range, the lookup table will map RAW pixel values to one of 2<sup>8</sup>=256 numbers (that is, numbers in the range [0,255]). Iteratively perform the following 2<sup>N</sup>−2 times:
    <ul> <li id="ul0012-0001" num="0114">a. Choose the span with the largest metric value.</li> <li id="ul0012-0002" num="0115">b. Split that span in half, by removing it and creating two new spans, each covering half of the RAW pixel values. This step is illustrated in the transitions between <figref idrefs="DRAWINGS">FIGS. 9A-9B</figref>, and <figref idrefs="DRAWINGS">FIGS. 9B-9C</figref>. Each new span is assigned a new metric, using the same formula listed above.</li> </ul>
</li> <li id="ul0011-0002" num="0116">5. For all 2<sup>N</sup>−1 spans, use the “start” value of the span as one of the RAW pixel values to “seed” the lookup table: mark each table entry as unmapped, and then fill in the 2<sup>N</sup>−1 table entries that are indexed by the 2<sup>N</sup>−1 span start values with an increasing sequence of numbers that covers the numeric range of the target encoded precision (for example) the numbers 0 through 254 in the case of encoding to an 8-bit precision. The “end” value of the last span is also used to seed the lookup table by filling in the last table entry with the highest number representable in the target precision (for example) 255 in the case of 8-bit precision. For the example 4-bit to 2-bit conversion illustrated in <figref idrefs="DRAWINGS">FIGS. 9A-9D</figref>, <figref idrefs="DRAWINGS">FIG. 9D</figref> depicts the span endpoints used to seed the lookup table, and <figref idrefs="DRAWINGS">FIG. 10A</figref> shows the resultant table.</li> <li id="ul0011-0003" num="0117">6. Populate the remaining entries of the lookup table according to the nearest seed value, for example as illustrated in <figref idrefs="DRAWINGS">FIG. 10B</figref>.</li> </ul>
<div class="description-paragraph" id="p-0081" num="0118">Once this process is complete, the per-color lookup tables may be used to convert the RAW pixel values to encoded pixel values by using the RAW pixel value to index the table corresponding to its color (for example, red, green, or blue):
<br/>
encodedValue[<i>x,y</i>]=lookupTable[rawValue[<i>x,y]]</i>
</div>
<div class="description-paragraph" id="p-0082" num="0119">Other algorithms for determining the lookup tables from the raw image data are possible, for example annealing-based approaches to choosing the histogram spans.</div>
<heading id="h-0012">Rearranging RAW Image Data into a Full-Color Image Layout</heading>
<div class="description-paragraph" id="p-0083" num="0120">In at least one embodiment, image data is rearranged from a RAW image layout to a full-color image layout. For example, RAW images may include pixels consisting of a single value corresponding to a single color, while full-color images may include pixels consisting of multiple values corresponding to multiple colors.</div>
<div class="description-paragraph" id="p-0084" num="0121"> <figref idrefs="DRAWINGS">FIGS. 11A and 11B</figref> illustrate two different full-color image layouts: YUV and YUV422. In the case of <figref idrefs="DRAWINGS">FIG. 11A</figref>, each pixel consists of three values (Y, U, and V), while in the case of <figref idrefs="DRAWINGS">FIG. 11B</figref>, each pixel consists of two values, either (Y, U) or (Y, V), where Y is the luma and U,V are the chroma channels. Note that YUV422 images are sub-sampled versions of YUV images: both are used to describe full-color images, but the YUV422 image stores half as many chroma values as the YUV images do.</div>
<div class="description-paragraph" id="p-0085" num="0122"> <figref idrefs="DRAWINGS">FIGS. 12 and 13</figref> are an illustrative example of mapping from an exemplary Bayer-like RAW image layout to a YUV422 full-color image layout. Each RAW pixel consists of a single color value (either R, G0, G1, or B, where G0 and G1 are both green). In this mapping, a 2×2 grid of four RAW pixels is mapped to a 2×1 grid of YUV422 full-color pixels. Note that 4 RAW pixels, each consisting of 1 value, are mapped to 2 YUV422 pixels, each consisting of 2 values, so the net effect is that 4 RAW pixel values are mapped to 4 YUV422 image values. Note also that the 2×2 grid of values is mapped to a 4×1 grid of values, in the YUV422 image layout.</div>
<div class="description-paragraph" id="p-0086" num="0123">This exemplary rearrangement is chosen due to the fact that there is a mapping from a 2×2 grid of pixels in this particular RAW layout (comprising one red, one blue, and two green values) to a 2×1 grid of YUV422 pixels (comprising one U, one V, and two Y values) that satisfies the following two constraints:
</div> <ul> <li id="ul0013-0001" num="0124">1. Pixels that are nearby in one layout are also nearby in the alternate layout, preserving local image properties such as features/edges that the image compression and decompression processes may take into account.</li> <li id="ul0013-0002" num="0125">2. Pixel value color relationships are preserved, in that by mapping R→U, B→V, and G→Y, the property that the two G pixels in the 2×2 RAW pixel grid are of the same color channel is preserved as they are mapped to two Y values in the YUV422 full-color image layout.</li> </ul>
<div class="description-paragraph" id="p-0087" num="0126">This exemplary rearrangement transforms a RAW image of dimensions width W by height H into a YUV422 image of dimensions W by H/2, where each YUV422 image pixel consists of 2 values. Thus, each layout represents the image using a total of W*H pixel values.</div>
<div class="description-paragraph" id="p-0088" num="0127">Note that a mapping and/or rearrangement may be defined between any RAW layout and any full-color image layout; the description herein is not intended to be limited to Bayer-like RAW image layouts and YUV422 full-color image layouts.</div>
<heading id="h-0013">Compressing RAW Image Data in a Full-Color Image Layout</heading>
<div class="description-paragraph" id="p-0089" num="0128">In at least one embodiment, the present invention includes a method of compressing a RAW image that has been represented as a full-color image. In one embodiment, as illustrated in <figref idrefs="DRAWINGS">FIG. 14A</figref>, the RAW image is represented as a YUV422 image as described herein, a standard JPEG encoder is used to compress the YUV422 image data into a compressed data stream. In at least one embodiment, the method of the present invention may utilize existing highly efficient and fast hardware implementations of such JPEG encoders. Decompression of the data stream into YUV422 image data is shown in <figref idrefs="DRAWINGS">FIG. 14B</figref>.</div>
<heading id="h-0014">Storing and/or Transmitting Compressed RAW Image Data</heading>
<div class="description-paragraph" id="p-0090" num="0129">In at least one embodiment, the present invention includes a method of storing and/or transmitting data that has been encoded and/or compressed via the techniques described herein. <figref idrefs="DRAWINGS">FIG. 15</figref> depicts one exemplary embodiment, in which some associated metadata that describes the encoding and/or compression is stored and/or transmitted alongside the compressed image data. Such associated metadata may include, for example, one or more of:
</div> <ul> <li id="ul0014-0001" num="0130">The lookup table(s) used to convert between the original RAW precision and the lower-precision RAW pixel values;</li> <li id="ul0014-0002" num="0131">Information describing any pixel data rearrangement that was performed; and/or</li> <li id="ul0014-0003" num="0132">Information describing the compression that was performed, including any parameters that were used.</li> </ul>
<div class="description-paragraph" id="p-0091" num="0133">Any associated metadata may be used when the compressed image data is loaded and/or received; such metadata can be used to decompress, decode, and otherwise process the image data.</div>
<heading id="h-0015">Decompressing RAW Image Data in a Full-Color Image Layout</heading>
<div class="description-paragraph" id="p-0092" num="0134">In at least one embodiment, the present invention includes a method of decompressing image data that has been compressed by the techniques described herein. <figref idrefs="DRAWINGS">FIG. 14B</figref> depicts an example of image decompression wherein JPEG decoding is used. In general, the method of decompressing compressed image data uses a complementary method to that which was used to compress the image data.</div>
<div class="description-paragraph" id="p-0093" num="0135">Just as there exist highly fast and efficient hardware image compressors that are designed to operate on full-color images, there also exist complementary hardware decompressors for compressed full-color images. Various embodiments of the present invention permit the exploitation of such existing hardware decompressors to rapidly decompress raw image data that has been encoded and compressed as described herein.</div>
<heading id="h-0016">Rearranging RAW Image Data from a Full-Color Image Layout</heading>
<div class="description-paragraph" id="p-0094" num="0136">In at least one embodiment, the present invention includes a method of rearranging the pixel values from a full-color image layout back to the RAW image layout corresponding to the original RAW image, as illustrated in <figref idrefs="DRAWINGS">FIGS. 12 and 13</figref> for the example of a YUV422 full-color image layout and a Bayer RAW image layout. This rearrangement is complementary to the rearrangement done when the image was compressed, as described herein.</div>
<heading id="h-0017">Converting Low-Precision-Encoded RAW Pixel Values to Higher-Precision Values</heading>
<div class="description-paragraph" id="p-0095" num="0137">In at least one embodiment, the present invention includes a method of converting low-precision-encoded pixel values in a decompressed image to higher precision pixel values, for example to the original precision of the uncompressed RAW image. Given the lookup tables used to encode each RAW pixel value (for example, one lookup table per RAW image color channel), an inverse lookup table may be created that maps the encoded values to their original values, for example as illustrated in <figref idrefs="DRAWINGS">FIG. 16</figref>. Such an inverse lookup table may be created from the original lookup table by averaging all of the RAW image pixel values (i.e. lookup table indices) that map to the same encoded pixel value.</div>
<div class="description-paragraph" id="p-0096" num="0138">An equation that may be used to apply the inverse lookup table to the lower-precision-encoded RAW image to derive the higher-precision RAW image is as follows:
<br/>
rawValue[<i>x,y</i>]=inverseLookupTable[encodedValue[<i>x,y]]</i>
</div>
<div class="description-paragraph" id="p-0097" num="0139">An alternate equation may incorporate dithering during the decoding process, as follows:
<br/>
rawValue[<i>x,y</i>]=CLAMP[inverseLookupTable[encodedValue[<i>x,y</i>]]+randomValue]
<br/>
where the random value is selected randomly from a range of possible values which may be predetermined, may be dependent on the RAW pixel value, and/or may be dependent on the lookup tables in use. An exemplary method of selecting the random value for dithering is to randomly select it from the RAW pixel value range between adjacent entries in the inverse lookup table; such a technique will dither the decoded pixel values by an amount that is related to the precision that was lost when converting the RAW pixel value to its lower-precision value during the compression procedure described herein. In at least one embodiment, after adding the random value, the method clamps the resultant RAW pixel value to be within some specified RAW pixel value range.
</div>
<div class="description-paragraph" id="p-0098" num="0140">By way of explanation using the example of <figref idrefs="DRAWINGS">FIG. 16</figref>, supposing the 2-bit encoded value was “0”, the resultant RAW value would be determined as “1”, and a dithering amount randomly selected from the range [−2,+2] could be added, since 2 is the largest value that can be added without the pixel value becoming an adjacent value in the inverse lookup table (i.e. “4” in this example).</div>
<heading id="h-0018">Light Field Image Compression</heading>
<div class="description-paragraph" id="p-0099" num="0141">Light field images present unique challenges for image compression. In addition to the challenges mentioned above in connection with RAW images, RAW light field images tend to have a high degree of spatial variation in pixel intensity. This will be shown and described in connection with <figref idrefs="DRAWINGS">FIGS. 17A and 17B</figref>, as follows.</div>
<div class="description-paragraph" id="p-0100" num="0142"> <figref idrefs="DRAWINGS">FIG. 17A</figref> depicts a RAW light field image <b>1700</b>. The RAW light field image may be formed of a plurality of pixel clusters, each of which encodes a portion of the light field data corresponding to a microlens of the microlens array. More precisely, each pixel cluster may correspond to the portion of the sensor <b>103</b> that receives light from a single microlens <b>401</b> of the microlens array <b>102</b>. Thus, each pixel cluster may be comparable to the disks <b>302</b> of <figref idrefs="DRAWINGS">FIG. 3</figref>. The pixel clusters are too small to be visible in the RAW light field image in the scale of <figref idrefs="DRAWINGS">FIG. 17A</figref>.</div>
<div class="description-paragraph" id="p-0101" num="0143"> <figref idrefs="DRAWINGS">FIG. 17B</figref> depicts an enlarged portion <b>1750</b> of the RAW light field image of <figref idrefs="DRAWINGS">FIG. 17A</figref>, illustrating the pixel clusters <b>1760</b> of the RAW light field image. Each pixel cluster <b>1760</b> may be generally circular, and may have a plurality of pixel rows and pixel columns that define a grid pattern. The pixels of each pixel cluster <b>1760</b> may be color-specific since they may embody the mosaic pattern of raw output from the microlens array <b>102</b>, such as a Bayer pattern. Thus, the pixels of each pixel cluster may include, for example, a plurality of 2×2 mosaics, each of which includes red, green, and blue pixels.</div>
<div class="description-paragraph" id="p-0102" num="0144">One of the challenges of compressing a light field image may be the high degree of pixel intensity spatial variation present in the image. As shown in <figref idrefs="DRAWINGS">FIG. 17B</figref>, each of the pixel clusters <b>1760</b> may display vignetting, which is a phenomenon in which an image's brightness or saturation is reduced at the periphery as compared to the image center. Accordingly, each of the pixel clusters <b>1760</b> may represent a transition from low intensity (at, for example, the left edge of the pixel cluster <b>1760</b>) to high intensity (in the center of the pixel cluster <b>1760</b>) and back to low intensity again (at, for example, the right edge of the pixel cluster <b>1760</b>). Many image compression algorithms may effectively group adjacent pixels of similar intensity and/or hue together to compress the image; thus, the vignetting present in the light field image may interfere with and/or prevent effective compression of the image with known image compression techniques, when applied alone.</div>
<div class="description-paragraph" id="p-0103" num="0145">The present invention may provide mechanisms for compressing two-dimensional images projected from light field data that overcome these challenges. In some embodiments, this may be done through the performance of additional image processing steps in addition to application of an image compression algorithm. These steps will be shown and described in connection with <figref idrefs="DRAWINGS">FIG. 18</figref>.</div>
<div class="description-paragraph" id="p-0104" num="0146"> <figref idrefs="DRAWINGS">FIG. 18</figref> depicts a method <b>1800</b> for compressing an image projected from light field data, according to one embodiment of the invention. The method <b>1800</b> may be carried out by the camera <b>100</b>, a separate post-processing system connected to the camera <b>100</b>, by a computing device independent of the camera <b>100</b>, or the like. Such a computing device (not shown) may have components such as one or more processors, memory blocks, user interface elements, and/or other computing device components known in the art.</div>
<div class="description-paragraph" id="p-0105" num="0147">The light field image may be a RAW image as described previously, or may be encoded in a different format. If the light field image is a RAW image, it may be a Bayer pattern mosaic image (RGGB), and may thus have groups of 2×2 pixels, each of which has an intensity obtained from light filtered through red, green, or blue filters. Alternatively, the light field image may have other color mosaic patterns such as an RGBE, CYYM, CYGM, or RGBW pattern.</div>
<div class="description-paragraph" id="p-0106" num="0148">The method <b>1800</b> may start <b>1810</b> with a step <b>1820</b> in which the light field image is retrieved, for example, from the camera <b>100</b>, the memory <b>111</b>, and/or any other data storage system. Once the light field image has been retrieved, the method <b>1800</b> may proceed to a step <b>1830</b> in which the color depth of the pixels of the light field image is reduced. Color depth reduction may entail reduction of the number of bits required to encode each pixel. Some image compression algorithms may operate more effectively on images with a certain color depth, such as 8 bits. Thus, if desired, the step <b>1830</b> may reduce the color depth of the pixels of the light field image from their native color depth, which may be 12 or 16 bits, to 8 bits. Exemplary color depth reduction methods will be shown and described in connection with <figref idrefs="DRAWINGS">FIG. 19</figref>.</div>
<div class="description-paragraph" id="p-0107" num="0149">Once the pixel color depth of the light field image has been reduced, the method <b>1800</b> may proceed to a step <b>1840</b> in which the light field image is retiled. Retiling may entail moving pixels from pixel clusters, each of which pertains to one microlens <b>401</b> of the microlens array <b>102</b>, to a retiled pixel cluster in which the pixels are generally grouped by color and/or intensity. The step <b>1840</b> may thus help overcome the challenges posed by the relatively high pixel intensity spatial variation of the light field image, as described above. Accordingly, the step <b>1840</b> may also enhance performance of the image compression algorithm to be applied subsequently. Exemplary retiling methods will be shown and described in connection with <figref idrefs="DRAWINGS">FIG. 20</figref>, with exemplary results shown in <figref idrefs="DRAWINGS">FIGS. 21A and 21B</figref>.</div>
<div class="description-paragraph" id="p-0108" num="0150">Once the step <b>1840</b> is complete, the method <b>1800</b> may proceed to a step <b>1850</b> in which the retiled image is padded. Padding the image may involve adding one or more pixel rows and/or pixel columns to the image so that it is the appropriate size for application of the image compression algorithm. Some image compression algorithms break up the subject image into tiles for processing. Such image compression algorithms may function more effectively if the subject image already has a number of pixel rows and pixel columns that are both integer multiples of the number of pixel rows and pixel columns of each tile the image compression algorithm is designed to process. For example, JPEG image compression algorithms may process tiles of 16×8 pixels or 8×8 pixels.</div>
<div class="description-paragraph" id="p-0109" num="0151">The added pixel rows and/or pixel columns may be duplicates of the adjacent edge a pixel column and/or edge pixel row. Exemplary padding methods will be shown and described in connection with <figref idrefs="DRAWINGS">FIG. 22</figref>.</div>
<div class="description-paragraph" id="p-0110" num="0152">After completion of the step <b>1850</b>, the method <b>1800</b> may proceed to a step <b>1860</b> in which an image compression algorithm is applied to the padded image. Applying the image compression algorithm to the image may entail applying a known lossless or lossy image compression algorithm such as that developed by the Joint Photographic experts Group, known as “JPEG” compression. Other formats may be used, including but not limited to JPEG 2000, EXIF, TIFF, GIF, WEBP, and the like. Additionally or alternatively, application of the image compression algorithm may entail application of one or more novel techniques for compressing an image, as described above in connection with <figref idrefs="DRAWINGS">FIGS. 7A through 16</figref>. The image compression algorithm may be applied to the retiled image. The performance of the image compression algorithm may be enhanced by the color depth reduction, retiling, and/or padding steps performed previously.</div>
<div class="description-paragraph" id="p-0111" num="0153">After the step <b>1860</b> has been performed, the method <b>1800</b> may proceed to a step <b>1870</b> in which the compressed image file is assembled. This may be done by combining the compressed image with metadata. The metadata may include a variety of items such as camera parameters and information regarding the steps performed in the method <b>1800</b>, which may help in further processing and/or decompression of the compressed image file. The step <b>1870</b> will be shown and described in greater detail in connection with <figref idrefs="DRAWINGS">FIG. 23</figref>. After completion of the step <b>1870</b>, the method <b>1800</b> may end <b>1890</b>.</div>
<heading id="h-0019">Bit Reduction</heading>
<div class="description-paragraph" id="p-0112" num="0154"> <figref idrefs="DRAWINGS">FIG. 19</figref> depicts a histogram <b>1900</b> of a type that may be used to facilitate pixel color depth reduction in the form of K-means bit reduction. Any of a variety of pixel color depth reduction techniques, or “bit reduction” techniques, may be employed as part of the step <b>1830</b>. Such bit reduction techniques may be used to reduce the number of bits required to encode each pixel of the light field image from N bits to M bits, where M&lt;N. One exemplary technique is the K-means bit reduction algorithm.</div>
<div class="description-paragraph" id="p-0113" num="0155">As shown in <figref idrefs="DRAWINGS">FIG. 19</figref>, the histogram <b>1900</b> may have a horizontal axis <b>1910</b> of pixel values present within the light field image (prior to bit reduction). The horizontal axis <b>1910</b> may extend from 0 to 2<sup>N</sup>−1 (for example, 0-4095 in the case of a 12-bit color depth prior to reduction). The histogram <b>1900</b> may also have a vertical axis <b>1920</b> of pixel populations, and may extend from 0 to the largest number of pixels at any single value. The histogram <b>1900</b> may have a line <b>1930</b> indicating the population (i.e., the number of pixels) in the light field image with each value. The shape of the line <b>1930</b> in <figref idrefs="DRAWINGS">FIG. 19</figref> is merely exemplary.</div>
<div class="description-paragraph" id="p-0114" num="0156">The K-means bit reduction algorithm may commence with the division of the histogram <b>1900</b> into 2<sup>M </sup>boundaries <b>1940</b>, which may be evenly spaced apart as shown, or may be unevenly spaced apart. Then, the method may determine which values on the horizontal axis <b>1910</b> are closest to each boundary. This is shown in <figref idrefs="DRAWINGS">FIG. 19</figref> by a shaded region <b>1950</b>, which is the set of pixels closest to the boundary <b>1940</b> that is centered within the shaded region <b>1950</b>.</div>
<div class="description-paragraph" id="p-0115" num="0157">The horizontal center of mass of the pixels closest to each boundary <b>1940</b> may then be calculated. For the example of the boundary <b>1940</b> within the shaded region <b>1950</b>, a dashed line adjacent the boundary <b>1940</b> may represent the horizontal center of mass <b>1960</b> of the shaded region <b>1950</b>. Note that, due to the slope of the line <b>1930</b> where it defines the top boundary of the shaded region <b>1950</b>, the center of mass <b>1960</b> may be positioned just to the right of the boundary <b>1940</b>. The horizontal center of mass of the pixels nearest to each of the boundaries <b>1940</b> may similarly be calculated.</div>
<div class="description-paragraph" id="p-0116" num="0158">Once the horizontal center of mass of each region has been determined, the boundaries <b>1940</b> may each be moved to the corresponding horizontal center of mass <b>1960</b>. This may complete the first iteration of the K-means bit reduction method. The K-means bit reduction algorithm may be iterative, and may thus continue with another determination of which values on the horizontal axis <b>1910</b> are closest to each new boundary <b>1940</b>, calculation of the corresponding horizontal center of mass, and motion of each boundary <b>1940</b> to the corresponding horizontal center of mass.</div>
<div class="description-paragraph" id="p-0117" num="0159">The method may iterate for as long as desired. In some embodiments, the K-means algorithm may iterate until the boundaries <b>1940</b> are no longer shifting. In other embodiments, the K-means algorithm may iterate for a predetermined number of cycles, such as 30.</div>
<div class="description-paragraph" id="p-0118" num="0160">Once the iteration is complete, the pixel values of all pixels closest to each boundary <b>1940</b> may be changed to the pixel value of the corresponding boundary <b>1940</b>. In this way, the number of bits required to encode each pixel may be reduced from N bits to M bits. The K-means algorithm may tend to place boundaries <b>1940</b> closer together where the slope of the histogram <b>1900</b> is relatively large (positive or negative); this may beneficially result in a higher density of pixel values applied where the pixel values change most rapidly. Thus, the resulting reduced color depth may have better resolution at the pixel values that are most in need of higher resolution, i.e., those that exhibit relatively high-frequency change.</div>
<div class="description-paragraph" id="p-0119" num="0161">If desired, color depth reduction parameters may be stored for later use in processing and/or decoding the image. Such color depth reduction parameters may include a mapping of which of the N pixel values were changed to each of the M pixel values. Such information may facilitate subsequent decompression of the image.</div>
<heading id="h-0020">Retiling</heading>
<div class="description-paragraph" id="p-0120" num="0162"> <figref idrefs="DRAWINGS">FIG. 20</figref> depicts a mapping of pixels from pixel clusters <b>2000</b> of the light field image to retiled pixel clusters <b>2010</b> to form a retiled image <b>2012</b>. As mentioned previously, the light field image may be retiled so that pixels of different pixel clusters are grouped together in a manner that groups pixels of similar colors and/or pixels of similar intensities. <figref idrefs="DRAWINGS">FIG. 20</figref> depicts one manner in which this may be accomplished.</div>
<div class="description-paragraph" id="p-0121" num="0163">In the example of <figref idrefs="DRAWINGS">FIG. 20</figref>, each of the pixel clusters <b>2000</b> may have a grid pattern with two rows of four columns, i.e., a 2×4 grid. A first pixel cluster <b>2020</b>, shown in dashed lines, is exemplary. The pixel clusters <b>2000</b> shown in <figref idrefs="DRAWINGS">FIG. 20</figref> may include four pixel clusters, each of which may have the same grid pattern as the first pixel cluster <b>2020</b>.</div>
<div class="description-paragraph" id="p-0122" num="0164">The first pixel cluster <b>2020</b> is shown with pixels numbered 1 through 8. Thus, each of the pixel clusters <b>2000</b> may have a top row including a first pixel <b>2030</b>, a second pixel <b>2032</b>, a third pixel <b>2034</b>, and fourth pixel <b>2036</b>. Similarly, each of the pixel clusters may have a bottom row including a fifth pixel <b>2040</b>, a sixth pixel <b>2042</b>, a seventh pixel <b>2044</b>, and an eighth pixel <b>2046</b>.</div>
<div class="description-paragraph" id="p-0123" num="0165">As illustrated in <figref idrefs="DRAWINGS">FIG. 20</figref>, the first pixel cluster <b>2020</b> has a Bayer mosaic pattern; thus, the first pixel cluster <b>2020</b> has eight pixels including two groups of four (2×2) pixels. Each 2×2 grouping may have the RGGB Bayer pattern, as seen in pixels 1, 2, 5, and 6 of the first pixel cluster <b>220</b>. The same pattern may be repeated in pixels 3, 4, 7, and 8 of the first pixel cluster <b>220</b>.</div>
<div class="description-paragraph" id="p-0124" num="0166">The grouping of pixels in the Bayer mosaic pattern (or other raw mosaic pattern, as applicable) may advantageously be rearranged to group pixels of the same color next to each other. Additionally, it may be helpful to group pixels at the same position within the various pixel clusters <b>2000</b> together because, due to the vignetting that may be present in each of the pixel clusters <b>2000</b>, the intensity of a given pixel may be determined, at least in part, by its location within the pixel cluster <b>2000</b>.</div>
<div class="description-paragraph" id="p-0125" num="0167">The retiled pixel clusters <b>2010</b> may accomplish this regrouping by grouping the first pixel <b>2030</b> of each of the pixel clusters <b>2000</b> together, grouping the second pixel <b>2032</b> of each of the pixel clusters <b>2000</b> together, etc. Thus, the retiled pixel clusters <b>2010</b> may include eight retiled pixel clusters, which may include a first retiled pixel cluster <b>2050</b>, a second retiled pixel cluster <b>2052</b>, a third retiled pixel cluster <b>2054</b>, a fourth retiled pixel cluster <b>2056</b>, a fifth retiled pixel cluster <b>2060</b>, a sixth retiled pixel cluster <b>2062</b>, a seventh retiled pixel cluster <b>2064</b>, and an eighth retiled pixel cluster <b>2066</b>. The retiled pixel clusters <b>2010</b> may be arranged in the same 2×4 grid pattern as the pixels of each of the pixel clusters <b>2000</b> to form the retiled image <b>2012</b>. Thus, there may be a one-to-one correspondence between the pixels of each of the pixel cluster <b>2000</b> and the retiled pixel clusters <b>2010</b> of the retiled image <b>2012</b>.</div>
<div class="description-paragraph" id="p-0126" num="0168">Rather than duplicating the mosaic pattern of the pixels of each of the pixel clusters <b>2000</b>, the retiled pixel clusters <b>2010</b> may be arranged such that pixels of the same color are, to the extent possible, grouped next to each other. Hence, the first pixels <b>2030</b> of each of the pixel clusters <b>2000</b> may be placed in the first retiled pixel cluster <b>2050</b>, and the third pixels <b>2034</b> of each of the pixel clusters <b>2000</b> may be placed together in the second retiled pixel cluster <b>2010</b>, which may be adjacent to the first retiled pixel cluster <b>2050</b>. Similarly, the second pixels <b>2032</b> may be placed in the third retiled pixel cluster <b>2054</b> and the fourth pixels <b>2036</b> may be placed in the fourth retiled pixel cluster <b>2056</b>. The fifth pixels <b>2040</b> may be placed in the fifth retiled pixel cluster <b>2060</b>, the seventh pixels <b>2044</b> may be placed in the sixth retiled pixel cluster <b>2062</b>, the sixth pixels <b>2042</b> may be placed in the seventh retiled pixel cluster <b>2064</b>, and the eighth pixels <b>2046</b> may be placed in the eighth retiled pixel cluster <b>2066</b>.</div>
<div class="description-paragraph" id="p-0127" num="0169">Within each of the retiled pixel clusters <b>2010</b>, the pixels from different pixel clusters <b>2000</b> may be spatially arranged according to the placement of the pixel clusters <b>2000</b> relative to each other. Thus, the pixels of the first pixel cluster <b>2020</b> may be placed at the upper left of each of the retiled pixel clusters <b>2010</b>. The pixels of the pixel cluster <b>2000</b> to the right of the first pixel cluster <b>2020</b> may be placed to the right of the pixels of the first pixel cluster <b>2020</b>, and the pixels of the pixel cluster <b>2000</b> below the first pixel cluster <b>2020</b> may be placed below the pixels of the first pixel cluster <b>2020</b>, and so on.</div>
<div class="description-paragraph" id="p-0128" num="0170">More generally, with microlens pitch that is approximately N<sub>x </sub>pixels in one direction, and N<sub>y </sub>pixels in the orthogonal direction, the light field image can be retiled into N<sub>x</sub>*N<sub>y </sub>sub images. The first row of the first sub image can be formed by selecting the corner pixel of the image and every N<sub>x </sub>pixel along the row. Similarly, the columns may be formed by starting at the same corner pixel and selecting every N<sub>y </sub>pixel in the Y-direction. The next sub image can be formed by repeating the procedure for the pixel next to the corner pixel.</div>
<div class="description-paragraph" id="p-0129" num="0171">In the example of <figref idrefs="DRAWINGS">FIG. 20</figref>, the retiled image <b>2012</b> may have only eight retiled pixel clusters <b>2010</b> shown. The pixel clusters <b>2000</b> may include more pixel clusters (not shown) than the four shown in <figref idrefs="DRAWINGS">FIG. 20</figref>; the pixels of such pixel clusters may be arranged in the eight retiled pixel clusters <b>2010</b> in the manner indicated above. The use of eight pixel clusters <b>2010</b> is merely exemplary; in alternative embodiments (not shown) in which each pixel cluster includes more or less than eight pixels, the retiled image may have similarly have more or fewer than eight pixels, which may be arranged in the same grid pattern as that of the pixel clusters.</div>
<div class="description-paragraph" id="p-0130" num="0172">Returning to the example of <figref idrefs="DRAWINGS">FIG. 20</figref>, in the manner indicated above, red pixels may be grouped together in the first retiled pixel cluster <b>2050</b> and the second retiled pixel cluster <b>2052</b>. Green pixels may be grouped together in the third retiled pixel cluster <b>2054</b>, the fourth retiled pixel cluster <b>2056</b>, the fifth retiled pixel cluster <b>2060</b>, and the sixth retiled pixel cluster <b>2062</b>. Red pixels may be grouped together in the seventh retiled pixel cluster <b>2064</b> and the eighth retiled pixel cluster <b>2066</b>. Thus, in the retiled image <b>2012</b>, pixels from the pixel clusters <b>2000</b> may be grouped so that pixels of the same color are grouped together, with pixels of similar probable intensities also grouped relatively close together. Such grouping may enhance the operation of the image compression algorithm that may be applied subsequently by facilitating the ability of such an image compression algorithm to break up the retiled image into tiles with similar pixel colors and intensities. Additionally, the manner in which pixel intensities are grouped together may further enhance the performance of the image compression by reducing the spatial variation in pixel intensity.</div>
<div class="description-paragraph" id="p-0131" num="0173">If desired, retiling parameters may be stored for later use in processing and/or decoding the image. Such retiling parameters may include a mapping of the original location of each pixel of the retiled pixel clusters <b>2010</b>. Such information may facilitate subsequent decompression of the image.</div>
<div class="description-paragraph" id="p-0132" num="0174"> <figref idrefs="DRAWINGS">FIG. 21A</figref> depicts a retiled light field image <b>2100</b> generated by a process such as that described in connection with <figref idrefs="DRAWINGS">FIG. 20</figref>. The retiled light field image <b>2100</b> may generally be divided into four quadrants, which may have different colors. Each quadrant may have multiple miniature copies of the raw light field image <b>1700</b>.</div>
<div class="description-paragraph" id="p-0133" num="0175"> <figref idrefs="DRAWINGS">FIG. 21B</figref> depicts an enlarged portion <b>2150</b> of the retiled light field image of <figref idrefs="DRAWINGS">FIG. 21A</figref>. As shown, the high-frequency artifacts, or high-frequency pixel intensity variations, of <figref idrefs="DRAWINGS">FIG. 17B</figref> have been effectively removed from the enlarged portion <b>2150</b>. Thus, an image compression algorithm is likely to achieve better results with the retiled light field image <b>2100</b>.</div>
<heading id="h-0021">Padding</heading>
<div class="description-paragraph" id="p-0134" num="0176"> <figref idrefs="DRAWINGS">FIG. 22</figref> depicts a padding process for adding one or more pixel rows and/or columns to a retiled image <b>2200</b>. As described above, image compression algorithms may divide an image into tiles of a certain size, and may perform best when used with images that are sized as multiples of the tile size. As mentioned previously, JPEG image compression algorithms may process tiles of 16×8 pixels or 8×8 pixels.</div>
<div class="description-paragraph" id="p-0135" num="0177">The retiled image <b>2200</b> of <figref idrefs="DRAWINGS">FIG. 22</figref> is a 14×14 image. Thus, without modification, it may not be the optimal size for application of a JPEG compression algorithm. The retiled image <b>2200</b> may thus be padded with additional rows and/or columns in order cause it to have a number of pixel rows that is an integer multiple of the number of pixel rows in the JPEG tile size, and a number of pixel columns that is an integer multiple of the number of pixel columns in the JPEG tile size. Adding two pixel rows and two pixel columns may cause the retiled image <b>2200</b> to conform to both 16×8 and 8×8 tile sizes.</div>
<div class="description-paragraph" id="p-0136" num="0178">This may be done, for example, by duplicating the adjacent edge pixel column and the adjacent edge pixel row, as applicable. For example, the retiled image <b>2200</b> may have a pixel edge column <b>2210</b> at the right-hand side of the retiled image <b>2200</b> and a pixel edge row <b>2220</b> at the bottom of the retiled image <b>2200</b>. The retiled image <b>2200</b> may be padded by adding two pixel columns <b>2230</b> to the right-hand side of the retiled image <b>2200</b>, adjacent to the pixel edge column <b>2210</b>, and by adding two pixel rows <b>2240</b> to the bottom of the retiled image <b>2200</b>, adjacent to the pixel edge row <b>2220</b>.</div>
<div class="description-paragraph" id="p-0137" num="0179">The content of the pixel columns <b>2230</b> and the pixel rows <b>2240</b> may be unimportant, since they may subsequently be removed as part of the decompression process. In order to facilitate image compression, each of the pixel columns <b>2230</b> may be a copy of the pixel edge column <b>2210</b>, and each of the pixel rows <b>2240</b> may be a copy of the pixel edge row <b>2220</b>.</div>
<div class="description-paragraph" id="p-0138" num="0180">If desired, image padding parameters may be stored for later use in processing and/or decoding the image. Such padding parameters may include an indication of the pixel columns (for example, the pixel columns <b>2230</b> of <figref idrefs="DRAWINGS">FIG. 22</figref>) and/or pixel rows (for example, the pixel rows <b>2240</b> of <figref idrefs="DRAWINGS">FIG. 22</figref>) that were added. Such information may facilitate subsequent decompression of the image.</div>
<heading id="h-0022">Compression</heading>
<div class="description-paragraph" id="p-0139" num="0181">As mentioned previously, the padded image may be ready for compression, which may be performed through the use of known image compression algorithms and/or through the use of the RAW image compression techniques set forth in connection with <figref idrefs="DRAWINGS">FIGS. 7A through 16</figref> herein. After the image has been compressed via the image compression algorithm, it may be assembled into an image file.</div>
<heading id="h-0023">Image File</heading>
<div class="description-paragraph" id="p-0140" num="0182"> <figref idrefs="DRAWINGS">FIG. 23</figref> depicts an image file <b>2300</b> that may be stored after assembly of the compressed image <b>2310</b> and metadata <b>2320</b>. The metadata <b>2320</b> may include any of a number of pieces of information that may be helpful in further processing, decompression, and/or use of the compressed image <b>2310</b>.</div>
<div class="description-paragraph" id="p-0141" num="0183">For example, the metadata <b>2320</b> may include camera parameters <b>2330</b> that indicate the state of one or more camera settings when the light field image, from which the compressed image <b>2310</b> is derived, was captured. Such parameters may facilitate a wide variety of image processing techniques.</div>
<div class="description-paragraph" id="p-0142" num="0184">Additionally or alternatively, the metadata <b>2320</b> may include color depth reduction parameters <b>2340</b>, a pixel mapping <b>2350</b>, and/or padding parameters <b>2360</b>. As set forth above, the color depth reduction parameters <b>2340</b>, the pixel mapping <b>2350</b>, and the padding parameters <b>2360</b> may provide information regarding how the step <b>1830</b>, the step <b>1840</b>, and the step <b>1850</b>, respectively, were carried out.</div>
<div class="description-paragraph" id="p-0143" num="0185">Such information may facilitate image processing and in particular, decompression of the compressed image <b>2310</b> to enable the compressed image <b>2310</b> to be refocused and/or otherwise manipulated according to light field image usage techniques. More specifically, the color depth reduction parameters <b>2340</b>, the pixel mapping <b>2350</b>, and the padding parameters <b>2360</b> may be used to facilitate reversal of the step <b>1830</b>, the step <b>1840</b>, and the step <b>1850</b>, respectively.</div>
<div class="description-paragraph" id="p-0144" num="0186">The metadata <b>2320</b> is merely exemplary; in alternative embodiments, metadata may not have some of the components illustrated in <figref idrefs="DRAWINGS">FIG. 23</figref>. Additionally or alternatively, such metadata may have additional components not shown in <figref idrefs="DRAWINGS">FIG. 23</figref>. If desired, the metadata <b>2320</b> may have image compression parameters that indicate the type of image compression algorithm used in the step <b>1860</b>, the applicable parameters, or the like.</div>
<div class="description-paragraph" id="p-0145" num="0187">Through the use of the present invention, light field images may be compressed to a fraction of their original size. In some embodiments, the size of the compressed image may be approximately ⅓ the size of the RAW light field image. The compression may be a lossy compression; accordingly, the compressed image may lack the data needed to restore the exact original RAW light field image. However, through the use of the present invention, the decompressed image may retain the full functionality of the original RAW light field image and may sufficiently similar to the original RAW light field image that the user may not notice any difference.</div>
<div class="description-paragraph" id="p-0146" num="0188">Those of skill in the art will recognize that other compression ratios are possible with modification of the steps set forth previously. It is to be expected that additional compression will result in additional data loss, and therefore may increase the likelihood that the user will perceive a difference in image quality between the decompressed image and the original RAW light field image.</div>
<heading id="h-0024">Processing of Compressed Image</heading>
<div class="description-paragraph" id="p-0147" num="0189"> <figref idrefs="DRAWINGS">FIG. 24</figref> depicts a method <b>2400</b> for processing a compressed image for use, wherein the compressed image has been compressed with a method such as the method <b>1800</b> of <figref idrefs="DRAWINGS">FIG. 18</figref>. The method <b>2400</b> may be substantially the reverse of the method <b>1800</b>. Thus, the steps of the method <b>2400</b> may generally be the inverse of those of <figref idrefs="DRAWINGS">FIG. 18</figref>, performed in the opposite order.</div>
<div class="description-paragraph" id="p-0148" num="0190">More specifically, the method <b>2400</b> may start <b>2410</b> with a step <b>2420</b> in which the compressed light field image file is retrieved, for example, from the camera <b>100</b>, the memory <b>111</b>, and/or any other data storage system. Once the compressed light field image has been retrieved, the method <b>2400</b> may proceed to a step <b>2430</b> in which the compressed image <b>2310</b> and the metadata <b>2320</b> are extracted from the compressed image file.</div>
<div class="description-paragraph" id="p-0149" num="0191">The method <b>2400</b> may then proceed to a step <b>2440</b> in which the compressed image is decompressed. This may entail application of an image decompression algorithm of the image compression algorithm used to compress the light field image. Additionally or alternatively, the step <b>2440</b> may entail application of the image compression algorithm in reverse. Notably, the compression of the light field image may be lossy, so application of the image decompression algorithm or application of the image compression algorithm in reverse may not restore all data present in the original RAW light field image.</div>
<div class="description-paragraph" id="p-0150" num="0192">Once the step <b>2440</b> has been carried out, the method <b>2400</b> may proceed to a step <b>2450</b> in which any padding process applied to derive the compressed image is reversed. This may entail deletion of any added pixel rows and/or pixel columns, such as the pixel columns <b>2230</b> and/or the pixel rows <b>2240</b> of <figref idrefs="DRAWINGS">FIG. 22</figref>. Data from the metadata <b>2320</b>, such as the padding parameters <b>2360</b>, may be used to facilitate this process.</div>
<div class="description-paragraph" id="p-0151" num="0193">Once any padding has been removed, the method <b>2400</b> may proceed to a step <b>2460</b> in which the image is again retiled. This may entail restoring the structure of the original pixel clusters, i.e., the pixel clusters <b>2000</b> of <figref idrefs="DRAWINGS">FIG. 20</figref>. The retiled image may then have pixel clusters that correspond to individual microlenses <b>401</b> of the microlens array <b>102</b>, like those of the original RAW light field image. Thus, the step <b>2460</b> may again enable refocusing and other processes unique to light field images. Data from the metadata <b>2320</b>, such as the pixel mapping <b>2350</b>, may be used to facilitate this process.</div>
<div class="description-paragraph" id="p-0152" num="0194">Once the image has been retiled, the method <b>2400</b> may proceed to a step <b>2470</b> in which the color depth of the pixels of the retiled image is again increased. If desired, the original color depth may be used, so that the color depth of each pixel may be increased from M bits to N bits, where N&gt;M. Notably, the data to restore each pixel to its original intensity value may not be present. However, increasing the pixel color depth may help to facilitate further processes that may be dependent upon a higher bit depth.</div>
<div class="description-paragraph" id="p-0153" num="0195">Once the step <b>2470</b> is complete, the decompressed image may be ready for use, and may be viewed, processed, converted, and/or otherwise used in a manner comparable to that of the original RAW light field image. The method <b>2400</b> may then end <b>2490</b>.</div>
<div class="description-paragraph" id="p-0154" num="0196">The present invention has been described in particular detail with respect to possible embodiments. Those of skill in the art will appreciate that the invention may be practiced in other embodiments. First, the particular naming of the components, capitalization of terms, the attributes, data structures, or any other programming or structural aspect is not mandatory or significant, and the mechanisms that implement the invention or its features may have different names, formats, or protocols. Further, the system may be implemented via a combination of hardware and software, as described, or entirely in hardware elements, or entirely in software elements. Also, the particular division of functionality between the various system components described herein is merely exemplary, and not mandatory; functions performed by a single system component may instead be performed by multiple components, and functions performed by multiple components may instead be performed by a single component.</div>
<div class="description-paragraph" id="p-0155" num="0197">In various embodiments, the present invention can be implemented as a system or a method for performing the above-described techniques, either singly or in any combination. In another embodiment, the present invention can be implemented as a computer program product comprising a nontransitory computer-readable storage medium and computer program code, encoded on the medium, for causing a processor in a computing device or other electronic device to perform the above-described techniques.</div>
<div class="description-paragraph" id="p-0156" num="0198">Reference in the specification to “one embodiment” or to “an embodiment” means that a particular feature, structure, or characteristic described in connection with the embodiments is included in at least one embodiment of the invention. The appearances of the phrase “in at least one embodiment” in various places in the specification are not necessarily all referring to the same embodiment.</div>
<div class="description-paragraph" id="p-0157" num="0199">Some portions of the above are presented in terms of algorithms and symbolic representations of operations on data bits within a memory of a computing device. These algorithmic descriptions and representations are the means used by those skilled in the data processing arts to most effectively convey the substance of their work to others skilled in the art. An algorithm is here, and generally, conceived to be a self-consistent sequence of steps (instructions) leading to a desired result. The steps are those requiring physical manipulations of physical quantities. Usually, though not necessarily, these quantities take the form of electrical, magnetic or optical signals capable of being stored, transferred, combined, compared and otherwise manipulated. It is convenient at times, principally for reasons of common usage, to refer to these signals as bits, values, elements, symbols, characters, terms, numbers, or the like. Furthermore, it is also convenient at times, to refer to certain arrangements of steps requiring physical manipulations of physical quantities as modules or code devices, without loss of generality.</div>
<div class="description-paragraph" id="p-0158" num="0200">It should be borne in mind, however, that all of these and similar terms are to be associated with the appropriate physical quantities and are merely convenient labels applied to these quantities. Unless specifically stated otherwise as apparent from the following discussion, it is appreciated that throughout the description, discussions utilizing terms such as “processing” or “computing” or “calculating” or “displaying” or “determining” or the like, refer to the action and processes of a computer system, or similar electronic computing module and/or device, that manipulates and transforms data represented as physical (electronic) quantities within the computer system memories or registers or other such information storage, transmission or display devices.</div>
<div class="description-paragraph" id="p-0159" num="0201">Certain aspects of the present invention include process steps and instructions described herein in the form of an algorithm. It should be noted that the process steps and instructions of the present invention can be embodied in software, firmware and/or hardware, and when embodied in software, can be downloaded to reside on and be operated from different platforms used by a variety of operating systems.</div>
<div class="description-paragraph" id="p-0160" num="0202">The present invention also relates to an apparatus for performing the operations herein. This apparatus may be specially constructed for the required purposes, or it may comprise a general-purpose computing device selectively activated or reconfigured by a computer program stored in the computing device. Such a computer program may be stored in a computer readable storage medium, such as, but is not limited to, any type of disk including floppy disks, optical disks, CD-ROMs, magnetic-optical disks, read-only memories (ROMs), random access memories (RAMs), EPROMs, EEPROMs, flash memory, solid state drives, magnetic or optical cards, application specific integrated circuits (ASICs), or any type of media suitable for storing electronic instructions, and each coupled to a computer system bus. Further, the computing devices referred to herein may include a single processor or may be architectures employing multiple processor designs for increased computing capability.</div>
<div class="description-paragraph" id="p-0161" num="0203">The algorithms and displays presented herein are not inherently related to any particular computing device, virtualized system, or other apparatus. Various general-purpose systems may also be used with programs in accordance with the teachings herein, or it may prove convenient to construct more specialized apparatus to perform the required method steps. The required structure for a variety of these systems will be apparent from the description provided herein. In addition, the present invention is not described with reference to any particular programming language. It will be appreciated that a variety of programming languages may be used to implement the teachings of the present invention as described herein, and any references above to specific languages are provided for disclosure of enablement and best mode of the present invention.</div>
<div class="description-paragraph" id="p-0162" num="0204">Accordingly, in various embodiments, the present invention can be implemented as software, hardware, and/or other elements for controlling a computer system, computing device, or other electronic device, or any combination or plurality thereof. Such an electronic device can include, for example, a processor, an input device (such as a keyboard, mouse, touchpad, trackpad, joystick, trackball, microphone, and/or any combination thereof), an output device (such as a screen, speaker, and/or the like), memory, long-term storage (such as magnetic storage, optical storage, and/or the like), and/or network connectivity, according to techniques that are well known in the art. Such an electronic device may be portable or nonportable. Examples of electronic devices that may be used for implementing the invention include: a mobile phone, personal digital assistant, smartphone, kiosk, server computer, enterprise computing device, desktop computer, laptop computer, tablet computer, consumer electronic device, television, set-top box, or the like. An electronic device for implementing the present invention may use any operating system such as, for example: Linux; Microsoft Windows, available from Microsoft Corporation of Redmond, Wash.; Mac OS X, available from Apple Inc. of Cupertino, Calif.; iOS, available from Apple Inc. of Cupertino, Calif.; and/or any other operating system that is adapted for use on the device.</div>
<div class="description-paragraph" id="p-0163" num="0205">While the invention has been described with respect to a limited number of embodiments, those skilled in the art, having benefit of the above description, will appreciate that other embodiments may be devised which do not depart from the scope of the present invention as described herein. In addition, it should be noted that the language used in the specification has been principally selected for readability and instructional purposes, and may not have been selected to delineate or circumscribe the inventive subject matter. Accordingly, the disclosure of the present invention is intended to be illustrative, but not limiting, of the scope of the invention, which is set forth in the claims.</div>
</div>
</div>
</section><section itemprop="claims" itemscope="">
<h2>Claims (<span itemprop="count">23</span>)</h2>
<div html="" itemprop="content"><div class="claims" lang="EN" load-source="patent-office" mxw-id="PCLM132985786">
<claim-statement>What is claimed is:</claim-statement>
<div class="claim"> <div class="claim" id="CLM-00001" num="00001">
<div class="claim-text">1. A method for processing a compressed image derived from a raw image projected from light field data acquired through a microlens array, the method comprising:
<div class="claim-text">retrieving a compressed image derived from a raw image projected from light field data, wherein the compressed image comprises a plurality of retiled pixel clusters; and</div>
<div class="claim-text">at a processor, retiling the compressed image to generate an image comprising a plurality of pixel clusters, each of which comprises a pixel from each of the retiled pixel clusters;</div>
<div class="claim-text">wherein each of the pixel clusters in the generated image encodes a portion of the light field data corresponding to a microlens of the microlens array.</div>
</div>
</div>
</div> <div class="claim-dependent"> <div class="claim" id="CLM-00002" num="00002">
<div class="claim-text">2. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein retiling the compressed image comprises substantially reversing a retiling process previously carried out in a compression process by which the compressed image was derived from the raw image.</div>
</div>
</div> <div class="claim-dependent"> <div class="claim" id="CLM-00003" num="00003">
<div class="claim-text">3. The method of <claim-ref idref="CLM-00002">claim 2</claim-ref>, wherein the reified pixel clusters are arranged in rows and columns to define a grid pattern, wherein railing the compressed image comprises generating the image such that there is a one-to-one correspondence between the retiled pixel clusters and the pixels of each pixel cluster in the generated image, so that each pixel cluster in the generated image has the grid pattern.</div>
</div>
</div> <div class="claim-dependent"> <div class="claim" id="CLM-00004" num="00004">
<div class="claim-text">4. The method of <claim-ref idref="CLM-00002">claim 2</claim-ref>, further comprising decompressing a selection from the group consisting of the compressed image and the generated image, wherein decompressing the selection comprises substantially reversing application of an image compression algorithm previously applied as part of the compression process.</div>
</div>
</div> <div class="claim-dependent"> <div class="claim" id="CLM-00005" num="00005">
<div class="claim-text">5. The method of <claim-ref idref="CLM-00002">claim 2</claim-ref>, further comprising increasing a color depth of pixels of a selection from the group consisting of the compressed image and the generated image, wherein increasing the color depth of pixels of the selection comprises substantially reversing a color depth reduction process previously applied as part of the compression process.</div>
</div>
</div> <div class="claim-dependent"> <div class="claim" id="CLM-00006" num="00006">
<div class="claim-text">6. The method of <claim-ref idref="CLM-00005">claim 5</claim-ref>, wherein the color depth reduction process comprises application of a K-means bit reduction algorithm, wherein the compressed image comprises metadata comprising color depth reduction parameters indicating how the K-means bit reduction algorithm was performed, wherein substantially reversing the color depth reduction process comprises using the color depth reduction parameters to map color values of pixels of the selection to corresponding color values of the pixels prior to application of the K-means bit reduction algorithm.</div>
</div>
</div> <div class="claim-dependent"> <div class="claim" id="CLM-00007" num="00007">
<div class="claim-text">7. The method of <claim-ref idref="CLM-00002">claim 2</claim-ref>, further comprising substantially reversing a padding process previously carried out as part of the compression process, wherein substantially reversing the padding process comprises removing, from the compressed image or the generated image, a selection from the group consisting of:
<div class="claim-text">a pixel edge column substantially identical to a pixel column adjacent to it; and</div>
<div class="claim-text">a pixel edge row substantially identical to a pixel row adjacent to it.</div>
</div>
</div>
</div> <div class="claim"> <div class="claim" id="CLM-00008" num="00008">
<div class="claim-text">8. A non-transitory computer-readable medium for processing a compressed image derived from a raw image projected from light field data acquired through a microlens array, comprising instructions stored thereon, that when executed by a processor, perform the steps of:
<div class="claim-text">retrieving a compressed image derived from a raw image projected from light field data, wherein the compressed image comprises a plurality of retiled pixel clusters; and</div>
<div class="claim-text">retiling the compressed image to generate an image comprising a plurality of pixel clusters, each of which comprises a pixel from each of the retiled pixel clusters;</div>
<div class="claim-text">wherein each of the pixel clusters in the generated image encodes a portion of the light field data corresponding to a microlens of the microlens array.</div>
</div>
</div>
</div> <div class="claim-dependent"> <div class="claim" id="CLM-00009" num="00009">
<div class="claim-text">9. The non-transitory computer-readable medium of <claim-ref idref="CLM-00008">claim 8</claim-ref>, wherein retiling the compressed image comprises substantially reversing a retiling process previously carried out in a compression process by which the compressed image was derived from the raw image.</div>
</div>
</div> <div class="claim-dependent"> <div class="claim" id="CLM-00010" num="00010">
<div class="claim-text">10. The non-transitory computer-readable medium of <claim-ref idref="CLM-00009">claim 9</claim-ref>, further comprising instructions stored thereon, that when executed by a processor, perform the step of:
<div class="claim-text">decompressing a selection from the group consisting of the compressed image and the generated image, by substantially reversing application of an image compression algorithm previously applied as part of the compression process.</div>
</div>
</div>
</div> <div class="claim-dependent"> <div class="claim" id="CLM-00011" num="00011">
<div class="claim-text">11. The non-transitory computer-readable medium of <claim-ref idref="CLM-00009">claim 9</claim-ref>, further comprising instructions stored thereon, that when executed by a processor, perform the step of:
<div class="claim-text">increasing a color depth of pixels of a selection from the group consisting of the compressed image and the generated image by substantially reversing a color depth reduction process previously applied as part of the compression process.</div>
</div>
</div>
</div> <div class="claim"> <div class="claim" id="CLM-00012" num="00012">
<div class="claim-text">12. A system for processing a compressed image derived from a raw image projected from light field data acquired through a microlens array, the system comprising:
<div class="claim-text">a storage device, configured to store an image; and</div>
<div class="claim-text">a processor, communicatively coupled to the storage device, configured to:
<div class="claim-text">retrieve, from the storage device, a compressed image derived from a raw image projected from light field data, wherein the compressed image comprises a plurality of retiled pixel clusters; and</div>
<div class="claim-text">retile the compressed image to generate an image comprising a plurality of pixel clusters, each of which comprises a pixel from each of the retiled pixel clusters;</div>
</div>
<div class="claim-text">wherein each of the pixel clusters in the generated image encodes a portion of the light field data corresponding to a microlens of the microlens array.</div>
</div>
</div>
</div> <div class="claim-dependent"> <div class="claim" id="CLM-00013" num="00013">
<div class="claim-text">13. The system of <claim-ref idref="CLM-00012">claim 12</claim-ref>, wherein the processor is configured to retile the compressed image by substantially reversing a retiling process previously carried out in a compression process by which the compressed image was derived from the raw image.</div>
</div>
</div> <div class="claim-dependent"> <div class="claim" id="CLM-00014" num="00014">
<div class="claim-text">14. The system of <claim-ref idref="CLM-00013">claim 13</claim-ref>, wherein the processor is further configured to:
<div class="claim-text">decompress a selection from the group consisting of the compressed image and the generated image by substantially reversing application of an image compression algorithm previously applied as part of the compression process.</div>
</div>
</div>
</div> <div class="claim-dependent"> <div class="claim" id="CLM-00015" num="00015">
<div class="claim-text">15. The system of <claim-ref idref="CLM-00013">claim 13</claim-ref>, wherein the processor is further configured to:
<div class="claim-text">increase a color depth of pixels of a selection from the group consisting of the compressed image and the generated image by substantially reversing a color depth reduction process previously applied as part of the compression process.</div>
</div>
</div>
</div> <div class="claim-dependent"> <div class="claim" id="CLM-00016" num="00016">
<div class="claim-text">16. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein retrieving a compressing image comprises extracting a compressed image from a compressed image file.</div>
</div>
</div> <div class="claim-dependent"> <div class="claim" id="CLM-00017" num="00017">
<div class="claim-text">17. The method of <claim-ref idref="CLM-00016">claim 16</claim-ref>, further comprising extracting metadata from the compressed image file;
<div class="claim-text">and wherein retiling the compressed image is performed using the extracted metadata.</div>
</div>
</div>
</div> <div class="claim-dependent"> <div class="claim" id="CLM-00018" num="00018">
<div class="claim-text">18. The non-transitory computer-readable medium of <claim-ref idref="CLM-00009">claim 9</claim-ref>, further comprising instructions stored thereon, that when executed by a processor, perform the step of substantially reversing a padding process previously carried out as part of the compression process, wherein substantially reversing the padding process comprises removing, from the compressed image or the generated image, a selection from the group consisting of:
<div class="claim-text">a pixel edge column substantially identical to a pixel column adjacent to it; and</div>
<div class="claim-text">a pixel edge row substantially identical to a pixel row adjacent to it.</div>
</div>
</div>
</div> <div class="claim-dependent"> <div class="claim" id="CLM-00019" num="00019">
<div class="claim-text">19. The non-transitory computer-readable medium of <claim-ref idref="CLM-00008">claim 8</claim-ref>, wherein retrieving a compressing image comprises extracting a compressed image from a compressed image file.</div>
</div>
</div> <div class="claim-dependent"> <div class="claim" id="CLM-00020" num="00020">
<div class="claim-text">20. The non-transitory computer-readable medium of <claim-ref idref="CLM-00019">claim 19</claim-ref>, further comprising instructions stored thereon, that when executed by a processor, perform the step of extracting metadata from the compressed image file;
<div class="claim-text">and wherein retiling the compressed image is performed using the extracted metadata.</div>
</div>
</div>
</div> <div class="claim-dependent"> <div class="claim" id="CLM-00021" num="00021">
<div class="claim-text">21. The system of <claim-ref idref="CLM-00013">claim 13</claim-ref>, wherein the processor is further configured to perform the step of substantially reversing a padding process previously carried out as part of the compression process, wherein substantially reversing the padding process comprises removing, from the compressed image or the generated image, a selection from the group consisting of:
<div class="claim-text">a pixel edge column substantially identical to a pixel column adjacent to it; and</div>
<div class="claim-text">a pixel edge row substantially identical to a pixel row adjacent to it.</div>
</div>
</div>
</div> <div class="claim-dependent"> <div class="claim" id="CLM-00022" num="00022">
<div class="claim-text">22. The system of <claim-ref idref="CLM-00012">claim 12</claim-ref>, wherein the processor retrieving a compressing image comprises the processor extracting a compressed image from a compressed image file.</div>
</div>
</div> <div class="claim-dependent"> <div class="claim" id="CLM-00023" num="00023">
<div class="claim-text">23. The system of <claim-ref idref="CLM-00022">claim 22</claim-ref>, wherein the processor is further configured to perform the step of extracting metadata from the compressed image file;
<div class="claim-text">and wherein retiling the compressed image is performed using the extracted metadata.</div>
</div>
</div>
</div> </div>
</div>
</section>
                </article>
            </search-app>
        </body>
    </html>
    