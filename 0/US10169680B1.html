
    <html>
        <body>
            <search-app>
                <article class="result" itemscope="" itemtype="http://schema.org/ScholarlyArticle">
    <h1 itemprop="pageTitle">US10169680B1 - Object identification and labeling tool for training autonomous vehicle controllers 
        - Google Patents</h1><section itemprop="abstract" itemscope="">
<h2>Abstract</h2>
<div html="" itemprop="content"><abstract lang="EN" load-source="docdb" mxw-id="PA281117785" source="national office">
<div class="abstract">Techniques for identifying and labeling distinct objects within 3-D images of environments in which vehicles operate, to thereby generate training data used to train models that autonomously control and/or operate vehicles, are disclosed. A 3-D image may be presented from various perspective views (in some cases, dynamically), and/or may be presented with a corresponding 2-D environment image in a side-by-side and/or a layered manner, thereby allowing a user to more accurately identify groups/clusters of data points within the 3-D image that represent distinct objects. Automatic identification/delineation of various types of objects depicted within 3-D images, automatic labeling of identified/delineated objects, and automatic tracking of objects across various frames of a 3-D video are disclosed. A user may modify and/or refine any automatically generated information. Further, at least some of the techniques described herein are equally applicable to 2-D images.</div>
</abstract>
</div>
</section><section itemprop="description" itemscope="">
<h2>Description</h2>
<div html="" itemprop="content"><div class="description" lang="EN" load-source="patent-office" mxw-id="PDES181568653">
<heading id="h-0001">CROSS-REFERENCE TO RELATED APPLICATIONS</heading>
<div class="description-paragraph" id="p-0002" num="0001">This application claims priority to and the benefit of U.S. Provisional Patent Application No. 62/609,015 filed on Dec. 21, 2017 and entitled “Object Identification and Labeling Tool for Training Autonomous Vehicle Controllers,” the entire disclosure of which is hereby incorporated by reference herein in its entirety for all purposes.</div>
<heading id="h-0002">FIELD OF THE DISCLOSURE</heading>
<div class="description-paragraph" id="p-0003" num="0002">This disclosure generally relates to autonomous vehicles and, more particularly, to software-based techniques, tools, and/or graphical user interfaces for labeling distinct objects depicted within source images to generate training data for models that control or operate autonomous vehicles.</div>
<heading id="h-0003">BACKGROUND</heading>
<div class="description-paragraph" id="p-0004" num="0003">A vehicle (such as a fully- or partially-autonomous or self-driving vehicle) typically includes one or more sensors to detect and sense an environment in which the vehicle is located and through which the vehicle may be moving. The sensed data (along with other data, in some cases) may be utilized to control vehicle operations and maneuvers. The sensors may be any type or types of sensors which are capable of sensing various objects and/or conditions within the vehicle's environment, such as lidar, radar, cameras, and/or other types of sensors. The vehicle may also include other sensor devices, such as inertial measurement units (IMUs), and/or include other types of devices that provide information on the current position of the vehicle (e.g., a GPS unit).</div>
<div class="description-paragraph" id="p-0005" num="0004">The data generated by the sensors (and possibly other data) may be processed by a perception component of the autonomous vehicle, which outputs signals indicative of the current state of the vehicle's environment. The output signals generated by the perception component of the vehicle may be utilized to control various driving operations and maneuvers of the vehicle (e.g., steering direction, speed, braking force, etc.). In an example implementation, the perception component may identify (and possibly classify and/or track) objects within the vehicle's environment. As a more specific example implementation, the perception component may include (1) a segmentation module that partitions or distinguishes various objects within images that have been obtained via the various sensors to correspond to probable objects, (2) a classification module that determines labels/classes for the segmented objects, and (3) a tracking module that tracks segmented and/or classified objects over time (e.g., across image frames). For example, based on data provided by one or more of the vehicle sensors, the perception component may discern, identify, classify, and/or track the presence and positions of objects or particular types thereof within the vehicle's environment, and/or may track the configuration of the road (and any objects thereon) ahead of the vehicle. As such, one or more autonomous or self-driving behaviors of the vehicle may be controlled based on the objects that are segmented, classified, and/or tracked by the perception component of the vehicle over time.</div>
<div class="description-paragraph" id="p-0006" num="0005">In some embodiments, one or more machine-learning based models are trained and utilized by the autonomous vehicle to control the perception component to identify, classify, and/or track objects within the vehicle's environment, and as such, are referred to herein as one or more “perception models.” The perception models may be trained using any of various suitable types of learning, such as supervised learning, and may be trained using real-world image data and/or image data generated in a simulated environment that have been labeled according to “correct” outputs of one or more perception functions (e.g., segmentation, classification, and/or tracking). In some configurations, different models are utilized by each of the segmentation module, the classification module, and the tracking module. In some configurations, the segmentation, classification, and/or tracking module utilize one or more common models.</div>
<div class="description-paragraph" id="p-0007" num="0006">Currently known techniques for identifying and labeling objects for the purposes of generating training data for training autonomous vehicle control models require a human using a computer tool to indicate and label objects within conventional, two-dimensional (“2-D”) visual images of vehicle environments, e.g., images that have been generated by a passive imaging device or system, for example, an optical system that uses a lens and a diaphragm and/or filter or other sensors to passively sense, detect, and capture the colors, intensities, etc. of incoming rays of light that are visible to the human eye and that have reflected off of objects within the vehicle environments. For example, conventional, 2-D visual images may be stored in data file formats such as JPEG, Exif, PNG, etc., and conventional dynamic 2-D visual images or videos may be stored in data file formats such as AVI, QuickTime, GIF, etc. Typically, to indicate and label objects that are depicted in conventional, 2-D visual images, a frame of a 2-D image is presented on a user interface. A human may utilize controls provided by the user interface to place a box around an object within the image (e.g., to “bound” the object, or to “place a bounding box around” the object), thereby distinguishing the object from other objects within the image, and provide a respective label for the bounded object (e.g., “car”, “person,” “bicycle,” etc.). A conventional, 2-D visual image may be manipulated by the user in two dimensions, such as by zooming in, zooming out, or translating, to aid the user in bounding the object.</div>
<div class="description-paragraph" id="p-0008" num="0007">As is commonly known, thousands, if not millions, of labeled image data frames are needed to sufficiently train a vehicle's perception component to be able to identify, classify, and/or track objects within the vehicle's environment with enough accuracy and within a short enough time window to allow for safe control and operation of the vehicle during a variety of driving conditions. Thus, each object depicted within these thousands and millions of training image data frames must be bounded and labeled, one bounding box at a time, by a human using a conventional labeling tool, which is not only time consuming and inefficient, but may also suffer from human errors, inaccuracies, and inconsistencies. Further, two-dimensional images are limited in their accuracy in portraying three-dimensional objects in the respective locations in space. As such, techniques are needed to decrease the time that is needed to label training image data (e.g., over multiple frames and multiple images) as well as to increase the efficiency and accuracy of the labeling itself, thereby increasing both the amount and quality of labeled data used to train the perception component, and ultimately increasing the safety of autonomous operation of a vehicle whose operations and maneuvers are controlled by the trained perception component.</div>
<heading id="h-0004">SUMMARY</heading>
<div class="description-paragraph" id="p-0009" num="0008">The present disclosure includes systems, methods, tools, and techniques for efficiently and accurately identifying and labeling objects (e.g., objects that are depicted in images of environments in which vehicles operate) for training machine-learning based models that are used to operate and control vehicles in a fully-autonomous or partially-autonomous manner. In an example usage scenario, the system or tool is used to identify and label distinct objects that are depicted in three-dimensional (3-D) source images, such as point cloud datasets or images generated by active sensing devices and/or systems, e.g., lidar devices, radar devices, sonar devices, infrared devices, etc. Generally speaking, active sensing devices and systems include respective sources of illumination and/or of other types of electromagnetic energy. Active sensing devices and/or systems typically generate and emit, by using their electromagnetic energy sources, one or more electromagnetic waves into the surrounding environment, and measure the backscatter of the emitted waves that is reflected back to the device or system to thereby generate a 3-D image of the environment. In another example usage scenario, the system or tool is used to identify and label distinct objects that are depicted in two-dimensional (2-D) source images, such as those generated by passive sensing devices and systems, e.g., RGB (red-green-blue) cameras, heat sensors, light sensors, etc. Passive sensing devices typically measure electromagnetic energy that is emitted by one or more other, third party sources, such as the sun and/or other light sources, heat sources, etc., and that has been reflected off of various objects located within the environment.</div>
<div class="description-paragraph" id="p-0010" num="0009">When used to identity and label objects depicted in three-dimensional images, the system or tool may present, on a user interface, a source image that is a 3-D image of an environment in which vehicles operate, and may allow a user to dynamically change and vary the perspective view of the environment depicted in the 3-D source image, e.g., in three dimensions, thereby enabling the user to more accurately visually ascertain, discern and/or identify the boundaries of a particular, distinct object depicted within the 3-D image, and in particular as compared to doing so from only an arbitrary 2-D perspective. That is, a user may dynamically change and vary the virtual, 3-D camera angle from which the 3-D image of the environment is presented on the user interface. In an embodiment, the system or tool may provide the user with a virtual reality representation and interactive experience of the environment depicted by the 3-D image, so that the user/viewer may virtually move or navigate, in three dimensions, through the 3-D spatial environment presented on the user interface.</div>
<div class="description-paragraph" id="p-0011" num="0010">While viewing the environment that is depicted by the 3-D image from a first perspective view via the system or tool, the user may indicate, define, or identify the boundaries of a particular, distinct object within the image by adding or including a graphical representation (such as a bounding box, a fill treatment, one or more lines, and/or other types of visual properties) thereof to the 3-D source image. Additionally, via the system or tool, the user may associate the identified boundaries of the particular object (e.g., as denoted by the graphical representation) with a respective label that describes, categorizes, and/or classifies the particular, distinct object that is distinguished from other portions of the image via the graphical boundary representation. In some embodiments, the system or tool may provide an initial or interim draft graphical representation of the object's boundaries, which the user may accept, approve, and/or save without any changes thereto, or which the user may modify/adjust and accept, approve, and/or save with the applied modifications/changes. When the user subsequently changes the presentation of the 3-D image to be from a second perspective view (e.g., changes the virtual 3-D camera angle from which the 3-D image is presented on the user interface), the system/tool automatically maintains, carries forward, and/or presents, within the second perspective view, the graphical representation of the particular object's boundaries that were previously identified or defined in the first perspective view (and its associated label). In some embodiments, the system or tool automatically modifies the graphical representation from the first perspective view to account for the view from the second perspective prior to displaying the graphical boundary representation of the particular object on the second perspective view. The user then may accept the proposed graphical representation of the second perspective view without any changes thereto, or may refine the boundaries of the particular object within the image shown from the second perspective view, e.g., by modifying or adjusting the graphical boundary representation of the particular object within the second perspective view, prior to accepting the modified graphical boundary representation. As such, the system/tool enables a user to define or identify and label the boundaries of a particular object depicted within a 3-D image from multiple perspective views, thereby providing a more accurate identification of the overall boundaries of the particular, distinct object within the 3-D image. Bounding an object as depicted within a 3-D image may be performed as granularly as desired using the techniques disclosed herein. For example, each data point or pixel of the 3-D image may be identified and/or labeled as being included in the image of the particular, distinct object, or as being excluded from the image of the object.</div>
<div class="description-paragraph" id="p-0012" num="0011">To further aid the user in accurately identifying the boundaries of a particular object represented within a 3-D environment image, one or more 2-D images of different perspective views of the particular object may be simultaneously presented on the user interface (e.g., in a side-by-side configuration) in conjunction with the 3-D source image to thereby help avoid the difficulty and frustration of trying to place or manipulate boundaries when viewing the image from an arbitrary, 2-D perspective. For example, a 2-D side view of the particular object on which the user has focused attention on in the 3-D image (e.g., by selecting or otherwise indicating the particular object within the 3-D image), and/or a 2-D top view of the particular object on which focus is directed may be simultaneously presented on the user interface in conjunction with the presentation of 3-D environment image which the particular object is located. In an example scenario, when a user selects a particular cluster of data points on the 3-D image, one or more corresponding 2-D perspective views of the selected cluster may be automatically presented on the user interface as well. Each 2-D perspective view of the 3-D image may comprise a respective subset of the point cloud dataset comprising the selected cluster included in the 3-D image, in an embodiment. The displayed 2-D images may be linked to the displayed 3-D image so that objects and graphical representations of object boundaries that are presented on one of the images (e.g., on a particular 2-D image perspective view, or on the 3-D image) are automatically presented on the other image(s), and so that any user alterations or modifications to a graphical boundary representation that is displayed on one of the images are automatically reflected on the other images.</div>
<div class="description-paragraph" id="p-0013" num="0012">The system or tool may also provide various features that allow more efficient and accurate discernment, distinguishing, identification, and labeling of distinct objects that are located in vehicle environments depicted by 3-D images and/or by 2-D images. One such feature is the ability to automatically track an object across multiple frames of a video, where each frame is a different, static source image captured at a different moment in time. Generally speaking, each frame may include a respective image of the environment at a different instance in time, and collectively the multiple frames may provide a time-sequenced video of the environment. After a user has identified and labeled the boundaries of a particular object as depicted in a first frame depicting a vehicle's environment at a first time, e.g., via a first graphical representation, the system or tool may track the object in two a second frame depicting the vehicle's environment at a second time, and automatically maintain, carry forward, and/or present the first graphical representation into the second frame in conjunction with its label. The second frame may correspond to a later time or to an earlier time than the time corresponding to the first frame (that is, the object may be tracked in forward time or in reverse time). With either forward or reverse tracking, though, the user may refine the boundaries of the particular object as depicted in the second frame if necessary or desired, e.g., by modifying or adjusting the first graphical boundary representation of the particular object within the second frame, thereby forming a second graphical representation of the boundary of the object within the second frame, and thereby tracking the labeled, particular object between the first and the second frames. Indeed, the system or tool may automatically track an object across multiple frames to a frame in which only a single data point or pixel is tracked, identified, and labeled as being the particular object, such as when a car drives into the distance out of the field of view of the camera(s).</div>
<div class="description-paragraph" id="p-0014" num="0013">Another feature of the system or tool that allows a user to more efficiently and accurately identity and label objects located in vehicle environments that are depicted by 2-D and/or 3-D source environment is a paint feature. The paint feature includes a user control that provides a virtual paintbrush via which the user may paint over areas of an image to thereby indicate or identify a distinct object or type of object depicted therein (and/or to thereby indicate or identify its boundaries as depicted within the image), and associate the painted-over areas with a respective label. Typically, a user would use the paint feature to indicate an essentially planar surface area, such as a road, a driveway, or the side of a retaining wall, for example. Painting-over an area generally results in distinguishing the painted-over area within the source image via a different visual property, such as different fill color, pattern, or dynamic property. Different paint visual properties may uniquely correspond to different object types and/or labels, if desired. The paint feature particularly enables users to quickly and accurately identify surfaces and/or surface conditions depicted within an image, such as road surfaces, ice on a road surface, potholes, etc. Additionally, to further aid in quick and accurate identification, the width of the virtual paintbrush may be modified by the user as desired. For example, individual pixels or data points may be painted by using a thin or narrow brush size, or negative spaces of an image (e.g., areas of the image that do not include any objects that need to be identified and labeled for training a model) may be painted by using a wider or larger brush size. Additionally or alternatively, different implementations of painting, such as point-and-fill (e.g., a user pointing a cursor or other electronic pointer or indicator at a particular location and activating the point-and-fill user control while the cursor or electronic pointer is pointing to (or hovering over) the particular location, and the system or tool automatically filling in the entire surface area of which the pointed location is a part with the modified visual property), spilling paint, etc. may be provided by the paint feature to provide additional flexibility and convenience during object identification and labeling.</div>
<div class="description-paragraph" id="p-0015" num="0014">Yet another feature of the system or tool that allows a user to more efficiently and accurately identity and label objects located in vehicle environments that are depicted by 2-D environment images or by 3-D environment images is a lane-marking feature. The lane-marking feature is a user control that allows a user to easily and quickly identify and label the sides or edges of lanes and/or roads as depicted within a source image. In an example implementation, the user may indicate two points on the source image and activate the lane-marking feature. In response, the tool may automatically generate and present a graphical line between the two selected points to thereby distinguish or signify a portion of the side or edge of a lane or a road within the source image. The user may provide or indicate a label for the identified portion of the side or edge of the lane/road, or the system/tool may automatically generate a corresponding label. Of course, similar to the other types of object identification, the sides and edges of lanes/roads may be automatically tracked by the tool across frames. Accordingly, with the lane-marking feature, lane delineators, lane dividers, and edges of roads are easily identified and labeled with the lane-marking feature.</div>
<div class="description-paragraph" id="p-0016" num="0015">In some implementations, the system or tool may perform one or more image processing analyses of a source image to automatically discover or determine a subset of the total set of data points of which the source image is comprised that represents a distinct object, and/or to automatically determine the boundaries of the discovered distinct object as depicted within the source image. In some implementations, the system or tool may additionally or alternatively determine an initial label for an indicated object within a source image. The initial identification and/or the initial label may be presented, for example, on the user interface as a draft graphical representation of the object's boundaries and a draft label for the object, respectively. The user may then refine and/or modify the automatically generated, draft boundaries and/or labels using any one or more of the features disclosed herein.</div>
<div class="description-paragraph" id="p-0017" num="0016">Generally speaking, identified and labeled objects within an image of an environment in which vehicles operate are included in or incorporated into a set of training data utilized to train one or more machine-learning models that operate (or will operate) within a vehicle to autonomously control and/or operate the vehicle in various environments. The systems, methods, and techniques disclosed herein provide improvements in computer-related technology that previously were produced by human actions at a generic computer. Specifically, the systems, methods, and techniques disclosed herein provide specific, structured graphical user interfaces paired with prescribed functionality that is directly related to the specific, structured graphical user interface, as well as that provides additional specific ways to solve the problem of accurate identification and labeling of objects for use as training data for autonomous vehicle operation and control. For example, using the techniques disclosed herein, an object which appears across different perspective views and/or different frames of environment images (such as a neighboring vehicle, a traffic lane, a road, etc.) is able to be automatically identified and labeled across the different perspective views and/or frames. In contrast, existing identification and labeling techniques, while performed using a generic computer, nonetheless still require a person or human being to visually search for the object within each of the different perspective views and/or frames, manually identify the object's boundaries within each of the different perspective views and/or frames, and manually assign a label to the object in each view and/or frame.</div>
<div class="description-paragraph" id="p-0018" num="0017">For example, existing or known object identification and labeling tools generally present a two-dimensional (2-D) image on a user interface of a computer, where the 2-D image has been captured using an RGB camera (e.g., a camera that utilizes three independent sensors to detect red, green, and blue color signals) or other type of passive imaging device or system, e.g., by passively detecting light waves that are present within an environment via an open aperture. A person or user studies the 2-D image, visually discerns or identifies a particular object of interest depicted therein (e.g., another car, a pedestrian, the side of the road, etc.), and then manually “bounds” or “boxes” the particular object by using user controls provided by the known tools to draw line segments around the particular object within the 2-D image based on the user's visual perception. The user then applies a label to the bounded, particular object, e.g., by selecting from a drop-down list. The identified and labeled particular object within the 2-D image is stored and used as part of a set of training data for the autonomous vehicle machine-learning based models. In some cases, the 2-D source image is a first frame included in a 2-D video that has been captured by an RGB camera mounted on a vehicle. Thus, when a subsequent frame of the 2-D video is presented on the user interface, the user must then repeat the process for objects depicted in the subsequent frame (e.g., study the subsequent frame, visually identify particular objects, manually bound or box the identified objects, and manually apply respective labels). As such, a same vehicle that appears in multiple frames must be manually identified and labeled anew by the user in each frame.</div>
<div class="description-paragraph" id="p-0019" num="0018">These known techniques suffer from numerous drawbacks. For example, the requirement on a user to newly identify and label a particular object across each of multiple frames while using the known techniques increases the time and the cost that are incurred to identify and label the multiple (e.g., thousands or millions) of images that are necessary to sufficiently train a model to safely operate and/or control a vehicle in an autonomous manner. Further, and importantly, these known techniques introduce inaccuracies into the identification and labeling process, which then are absorbed into the training data that is used to train the machine-learning based models, which then are absorbed into the decisions made by the machine-learning based models to control and operate the autonomous vehicle. As such, due to these inaccuracies, the time and resources required to train the models may be increased to mitigate these inherent inaccuracies provided by existing labeling tools and techniques. More importantly, due to these inaccuracies, the accuracy and/or the safety of autonomous vehicle control and/or operation decisions made by the trained models may suffer, thereby increasing the risk to passengers, pedestrians, and other people and/or vehicles that are located within an autonomous vehicle's operational environment.</div>
<div class="description-paragraph" id="p-0020" num="0019">In particular, the known identification and labeling techniques rely on a user's personal judgment to visually identify objects within a 2-D image, and again rely on the user's personal judgment to consistently identify and label the same object across multiple frames. As such, inaccuracies may occur, such as when a user is identifying and labeling a plethora of objects within a first frame, and forgets to identify/label one of the many objects in a subsequent frame. Another example of the occurrence of inaccuracies is when a particular object decreases in size across multiple frames, such as when a passing vehicle disappears into the distance. In this example, in the later frames, the passing vehicle may be depicted in a very small size within the later frames; however, the user may not be able to discern the presence of the passing vehicle's image therein, such as when the passing vehicle is only depicted by a few pixels.</div>
<div class="description-paragraph" id="p-0021" num="0020">Yet another example of inaccuracies may be introduced by the known bounding techniques. As discussed above, a user bounds a particular object by drawing line segments around the object, or by placing a predefined box around the particular object and resizing the box to “fit” the image of the particular object according to the user's judgment. In these situations, as user discernment is used to bound the particular object, the boundaries of the image of the particular object within the 2-D image may be inaccurate, and may even inadvertently identify other image data as being included in the particular object, such as in images in which one object is partially obscured by another object, when the user mistakes a shadow as being included in the particular object, and the like. Further, different users may bound a same object differently, thus introducing still further inconsistencies within the training data used to train the models.</div>
<div class="description-paragraph" id="p-0022" num="0021">On the other hand, the systems, methods, and techniques disclosed herein greatly reduce the time needed to generate training data (which is significant, as thousands, if not millions, of labeled image data frames are needed to sufficiently train a vehicle's perception component to be able to identify, classify, and/or track objects within the vehicle's environment with enough accuracy and within a short enough time window to allow for safe control and operation of the vehicle during a variety of driving conditions), as well as greatly increase the accuracy of the training data itself. Both of these advantages significantly improve the safety of autonomous vehicles. For example, as the time needed to generate training data is decreased, more training data (e.g., more identified and labeled objects) can be produced in the same number of people-hours and then utilized to train models that are used to control and/or operate autonomous vehicles, thereby increasing the accuracy of trained models in their control and/or operation of autonomous vehicles due to the increased volume of training data. Additionally, as the accuracy of the identification and labeling of objects for training data is increased, the accuracy of the trained models in their control and/or operation of autonomous vehicles—and thus the degree or level of safe control and/or operation of autonomous vehicles—is further increased as well.</div>
<div class="description-paragraph" id="p-0023" num="0022">It is noted that for ease of reading herein, the term “autonomous vehicle” may refer to a vehicle that is controlled and/or operated in a fully-autonomous manner, and/or may refer to a vehicle that is controlled and/or operated in a semi-autonomous manner. As such, the term “autonomous,” as used herein, generally may be interpreted as “fully-autonomous and/or partially-autonomous.”</div>
<div class="description-paragraph" id="p-0024" num="0023">One example embodiment of the techniques of this disclosure is a computer-implemented method for identifying and labeling objects within images for training machine-learning based models that are used to autonomously operate or control vehicles, e.g., in a fully-autonomous or partially-autonomous manner. The method includes displaying, on a user interface, a three-dimensional (3-D) image of an environment in which vehicles operate, where the 3-D environment image depicts one or more physical objects located in the environment, and the 3-D environment image is presented on the user interface from a first perspective view. The method also includes receiving, via one or more user controls provided by the user interface, an indication of a first graphical representation of a boundary of a particular object as depicted within the 3-D environment image from the first perspective view; generating, based on the first graphical representation, data indicative of the boundary of the particular object within the 3-D environment image from the first perspective view; obtaining an indication of a particular label for the particular object; generating, based on the data indicative of the boundary of the particular object within the 3-D environment image from the first perspective view, an association between the particular label and a 3-D image of the particular object within the 3-D environment image, thereby distinguishing the 3-D image of the particular object within the 3-D environment image; and storing an indication of the association between the particular label and the 3-D image of the particular object within the 3-D environment image in one or more tangible, non-transitory memories as a part of a training data set utilized to train one or more machine-learning based models, where the one or more machine-learning based models are used to at least partially autonomously control and/or operate one or more vehicles. Further, the method includes receiving, via the one or more user controls, an instruction to present the 3-D environment image on the user interface from a second perspective view different than the first perspective view and, based on the received view perspective instruction, adjusting a presentation of the 3-D environment image on the user interface to be from the second perspective view. Still further, the method includes receiving, via the one or more user controls, an indication of a second graphical representation of the boundary of the particular object as depicted within the 3-D environment image from the second perspective view; generating, based on the second graphical representation, data indicative of the boundary of the particular object within the 3-D environment image from the second perspective view; and updating, based on the data indicative of the boundary of the particular object within the 3-D environment image from the second perspective view, the stored indication of the association between the particular label and the 3-D image of the particular object within the 3-D environment image, thereby refining the distinguishing of the 3-D image of the particular object within the 3-D environment image.</div>
<div class="description-paragraph" id="p-0025" num="0024">Another example embodiment of the techniques of this disclosure is a system for identifying and labeling objects within images for training machine-learning based models that are used to autonomously operate or control vehicles, e.g., in a fully-autonomous or partially-autonomous manner. The system comprises a communication module, one or more processors, and one or more non-transitory, tangible memories that are coupled to the one or more processors and that store computer executable instructions thereon. The computer executable instructions, when executed by the one or more processors, cause the system to display, on a user interface, a three-dimensional (3-D) image of an environment in which vehicles operate, where the 3-D environment image depicting one or more physical objects located in the environment, and the 3-D environment image is presented on the user interface from a first perspective view. The instructions are executable to additionally cause the system to receive, via the communication module, an indication of a first graphical representation of a boundary of a particular object as depicted within the 3-D environment image from the first perspective view, where the first graphical representation is generated via one or more user controls provided by the user interface; generate, based on the first graphical representation, data indicative of the boundary of the particular object within the 3-D environment image from the first perspective view; obtain an indication of a particular label for the particular object; generate, based on the data indicative of the boundary of the particular object within the 3-D environment image from the first perspective view, an association between the particular label and a 3-D image of the particular object within the 3-D environment image, thereby distinguishing the 3-D image of the particular object within the 3-D environment image; and store an indication of the association between the particular label and the 3-D image of the particular object within the 3-D environment image in the one or more tangible, non-transitory memories as a part of a training data set utilized to train one or more machine-learning based models, where the one or more machine-learning based models are used to autonomously control or operate vehicles, e.g., in a fully-autonomous or partially-autonomous manner. The instructions are executable to cause the system further to receive, via the communication module, a user instruction to present the 3-D environment image on the user interface from a second perspective view different than the first perspective view, and based on the received view perspective user instruction, adjust a presentation of the 3-D environment image on the user interface to be from the second perspective view. Additionally, the instructions are executable to cause the system to receive, via the communication module, an indication of a second graphical representation of the boundary of the particular object as depicted within the 3-D environment image from the second perspective view, where the second graphical representation is generated via the one or more user controls provided by the user interface; generate, based on the second graphical representation, data indicative of the boundary of the particular object within the 3-D environment image from the second perspective view; and update, based on the data indicative of the boundary of the particular object within the 3-D environment image from the second perspective view, the stored indication of the association between the particular label and the 3-D image of the particular object within the 3-D environment image, thereby refining the distinguishing of the 3-D image of the particular object within the 3-D environment image.</div>
<div class="description-paragraph" id="p-0026" num="0025">Yet another example embodiment of the techniques of this disclosure is a computer-implemented method for identifying and labeling objects within images for training machine-learning based models that are used to control or operate vehicles in a fully- or partially-autonomous manner. The method includes presenting, on a user interface of one or more computing devices, (i) a first frame comprising an image of an environment, at a first time, in which vehicles operate, the first frame depicting one or more physical objects located in the environment, and (ii) a first graphical representation indicating a boundary of a particular object located in the environment as depicted in the first frame at the first time, wherein an association of data indicative of the boundary of the particular object as depicted within the first frame at the first time and a particular label of the particular object (i) distinguishes an image of the particular object within the first frame, and (ii) is stored in one or more tangible, non-transitory memories as a part of a training data set utilized to train one or more machine-learning based models, where the one or more machine-learning based models are used to fully- or partially-autonomously control vehicles. The method also includes presenting, on the user interface, a second frame comprising an image of the environment at a second time different than the first time, the second frame depicting at least a portion of the particular object; automatically generating, based on the first graphical representation of the boundary of the particular object as depicted in the first frame, an interim graphical representation of the boundary of the particular object as depicted within the second frame, and presenting the interim graphical representation within the second frame. Additionally, the method includes receiving, via the user interface, an indication of a user modification to the interim graphical representation; altering, based on the received user modification, the interim graphical representation to thereby generate a second graphical representation of the boundary of the particular object as depicted in the second frame at the second time; generating data indicative of the second graphical representation of the boundary of the particular object as depicted within the second frame; and storing, in the one or more tangible, non-transitory memories, an association of the data indicative of the boundary of the particular object as depicted in the second frame at the second time and the particular label of the particular object as another part of the training data set.</div>
<div class="description-paragraph" id="p-0027" num="0026">Another example embodiment of the techniques of this disclosure is a system of for identifying and labeling objects within images for training machine-learning based models that are used to autonomously operate or control vehicles, e.g., in a fully-autonomous or semi-autonomous manner. The system comprises a communication module, one or more processors, and one or more non-transitory, tangible memories coupled to the one or more processors and storing computer-executable instructions thereon. The computer executable instructions, when executed by the one or more processors, cause the system to present, on a user interface of one or more computing devices, (i) a first frame comprising an image of an environment, at a first time, in which vehicles operate, the first frame depicting one or more physical objects located in the environment, and (ii) a first graphical representation indicating a boundary of a particular object located in the environment as depicted in the first frame at the first time, wherein an association of data indicative of the boundary of the particular object as depicted within the first frame at the first time and a particular label of the particular object (i) distinguishes an image of the particular object within the first frame, and (ii) is stored in one or more tangible, non-transitory memories as a part of a training data set utilized to train one or more machine-learning based models, where the one or more machine-learning based models used to fully- or partially-autonomously control or operate vehicles. The computer executable instructions are executable to cause the system further to present, on the user interface, a second frame comprising an image of the environment at a second time different than the first time, the second frame depicting at least a portion of the particular object; automatically generate, based on the first graphical representation of the boundary of the particular object as depicted in the first frame, an interim graphical representation of the boundary of the particular object as depicted within the second frame; and present the interim graphical representation within the second frame. Additionally, the computer executable instructions are executable to cause the system to receive, via the communication module, an indication of a user modification to the interim graphical representation; alter, based on the received user modification, the interim graphical representation to thereby generate a second graphical representation of the boundary of the particular object as depicted in the second frame at the second time; generate data indicative of the second graphical representation of the boundary of the particular object as depicted within the second frame; and store, in the one or more tangible, non-transitory memories, an association of the data indicative of the boundary of the particular object as depicted in the second frame at the second time and the particular label of the particular object as another part of the training data set.</div>
<div class="description-paragraph" id="p-0028" num="0027">Another example embodiment of the techniques of this disclosure is a computer-implemented method for identifying and labeling objects within images for training machine-learning based models that are used to autonomously operate or control vehicles, e.g., in a fully- or partially-autonomous manner. The method includes displaying, on a user interface of one or more computing devices, an image of an environment in which vehicles operate, the image depicting one or more physical objects located in the environment, and providing, the user interface, a paint user control for use by a user to indicate areas within images displayed on the user interface. The method further includes receiving, via a user activation of the paint user control, an indication of a location within the image; and based upon the indicated location, (i) automatically determining without any additional user input aside from the user activation, an area within the image, and (ii) automatically modifying, by the one or more computing devices based upon the received indication of the area, a visual property of the automatically determined area. The automatically determined area within the image is a subset of the total area of the image, includes both the indicated location and other locations within the image, and represents a surface area that is depicted within the image of the environment. Additionally, the method includes obtaining, by the one or more computing devices, an indication of a particular label for the indicated area of the image; and storing, by the one or more computing devices in one or more tangible, non-transitory memories, an indication of an association between data indicative of the indicated area of the image and the particular label, thereby distinguishing the indicated area from other areas of the image.</div>
<div class="description-paragraph" id="p-0029" num="0028">Still another example embodiment of the techniques of this disclosure is a system for identifying and labeling objects within images for training machine-learning based models that are used to autonomously operate vehicles, e.g., in a fully autonomous or partially autonomous manner. The system comprises a communication module, one or more processors, and one or more non-transitory, tangible memories coupled to the one or more processors and storing computer executable instructions thereon that, when executed by the one or more processors, cause the system to display, on a user interface, an image of an environment in which vehicles operate, the image depicting one or more physical objects located in the environment, and provide, on the user interface, a paint user control for use by a user to indicate areas within the image. Additionally, the computer executable instructions are executable to cause the system further to receive, via the communication module, an indication of a user activation of the paint user control thereby indicating a location within the image. Based upon the indicated location, the system may (i) automatically determine, without any additional user input aside from the user activation, an area within the image, and (ii) modify a visual property of the automatically determined area. The automatically determined area within the image is a subset of the total area of the image, includes the indicated location and additional locations within the image, and represents a surface area that is depicted in the image of the environment. The computer executable instructions are executable to cause the system to further obtain an indication of a particular label for the indicated area of the image; and store, in the one or more tangible, non-transitory memories, an indication of an association between data indicative of the indicated area of the image and the particular label, thereby distinguishing the indicated area from other areas of the image.</div>
<div class="description-paragraph" id="p-0030" num="0029">Another example embodiment of the techniques of this disclosure is a computer-implemented method for identifying and labeling objects depicted within images for training machine-learning based models that are used to autonomously control or operate vehicles, e.g., in a fully- or semi-autonomous manner. The method includes displaying, on a user interface of one or more computing devices, an image of an environment in which vehicles operate, the image depicting one or more physical objects located in the environment; and providing, on the user interface, a lane-marking user control for use by a user to indicate lane markings within the image. Additionally, the method includes receiving, via the lane-marking user control, a user selection of a first location within the image that is non-adjacent to a second location within the image, the first location and the second location indicating respective endpoints of a segment of an edge of a traffic lane depicted within the image of the environment; and based upon the second location and the received user selection of the first location, automatically generating and displaying on the image, by the one or more computing devices, a marking indicative of the segment of the edge of the traffic lane. Additionally, the method includes storing, by the one or more computing devices in one or more tangible, non-transitory memories, an indication of an association between a particular label for the traffic lane and data indicative of the segment of the edge of the traffic lane, thereby distinguishing the traffic lane from other areas and/or objects depicted within the image.</div>
<div class="description-paragraph" id="p-0031" num="0030">Another example embodiment of the techniques of this disclosure is a system for identifying and labeling objects depicted within images for training machine-learning based models that are used to autonomously control vehicles, e.g., in a semi- or fully-autonomous manner. The system comprises a communication module, one or more processors, and one or more non-transitory, tangible memories coupled to the one or more processors and storing computer executable instructions thereon that, when executed by the one or more processors, cause the system to display, on a user interface, an image of an environment in which vehicles operate, the image depicting one or more physical objects located in the environment; and provide, on the user interface, a lane-marking user control for use by a user to indicate lane markings within the image. Additionally, the computer executable instructions are executable to cause the system further to receive, via the communication module, an indication of a user selection, via the lane-marking user control, of a first location within the image that is non-adjacent to a second location within the image, the first location and the second location indicating respective endpoints of a segment of an edge of a traffic lane depicted within the image of the environment; based upon the second location and the received user selection of the first location, automatically generate and display on the image, by the one or more computing devices, a marking indicative of the segment of the edge of the traffic lane; and store, in the one or more tangible, non-transitory memories, an indication of an association between a particular label for the traffic lane and data indicative of the segment of the edge of the traffic lane, thereby distinguishing the traffic lane from other areas and/or objects depicted within the image.</div>
<div class="description-paragraph" id="p-0032" num="0031">Yet another example embodiment of the techniques of this disclosure is a computer-implemented method for identifying and labeling objects within images for training machine-learning based models that are used to fully- or partially-autonomously operate or control vehicles. The method comprises displaying, on a user interface, a three-dimensional (3-D) image of an environment in which vehicles operate, where the 3-D environment image includes respective 3-D images of one or more physical objects located in the environment, and the 3-D environment image is displayable from multiple perspective views on the user interface in response user input; and layering at least a portion of a two-dimensional (2-D) image of the environment with the 3-D environment image. The 2-D environment image includes a 2-D image of a particular physical object included in the one or more physical objects depicted in the 3-D environment image, thereby the layering of the 2-D image and the 3-D image generates a composite image of the particular physical object, where the composite image of the particular physical object includes a portion of the 2-D image of the particular physical object (e.g., from the 2-D environment image) and a portion of the 3-D image of the particular physical object (e.g., from the 3-D environment image); and displaying, on the user interface, the composite image of the particular object within the 3-D environment image of the environment. The method also includes receiving, via one or more user controls provided by the user interface, an indication of a boundary of the particular physical object depicted by the composite image within the 3-D environment image; generating data indicative of the boundary of the particular physical object within the 3-D environment image; receiving an indication of a particular label for the particular physical object; associating the particular label for the particular physical object in association with the data indicative of the boundary of the particular physical object within the 3-D environment image, thereby distinguishing a set of data points that are representative of the particular physical object within the 3-D environment image from other data points included in the 3-D environment image; and storing an indication of the association between the particular label and the data indicative of the boundary of the particular physical object within the 3-D environment image in one or more tangible memories as a part of a training data set utilized to train one or more machine-learning based models, where the one or more machine-learning based models are used to autonomously operate or control vehicles, e.g., in semi- or fully-autonomous manner.</div>
<div class="description-paragraph" id="p-0033" num="0032">Another example embodiment of the techniques of this disclosure is a system for identifying and labeling objects within images for training machine-learning based models that are used to operate or control vehicles, such as in a fully-autonomous or partially-autonomous manner. The system includes a communication module, one or more processors, and one or more non-transitory, tangible memories coupled to the one or more processors and storing computer executable instructions thereon that, when executed by the one or more processors, cause the system to display, on a user interface, a three-dimensional (3-D) image of an environment in which vehicles operate, where the 3-D environment image includes respective 3-D images of one or more physical objects located in the environment, and the 3-D environment image is displayable from multiple perspective views on the user interface in response user input; and layer at least a portion of a two-dimensional (2-D) image of at least a portion of the environment with the 3-D environment image, where the 2-D environment image includes a 2-D image of a particular physical object that is included in the one or more physical objects depicted in the 3-D environment image. The layering thereby generating a composite image of the particular physical object, where the composite image of the particular physical object includes a portion of the 2-D image of the particular physical object (e.g., from the 2-D environment image) and a portion of the 3-D image of the particular physical object (e.g., from 3-D environment image). The computer executable instructions are executable to cause the system further to display, on the user interface, the composite image of the particular object within the 3-D environment image of the environment; receive, via the communication module, an indication of a boundary of the particular physical object depicted by the composite image within the 3-D environment image, the indication of the boundary of the particular physical object provided by a user via the user interface; generate data indicative of the boundary of the particular physical object within the 3-D environment image based on the received indication of the boundary of the particular physical object depicted by the composite image within the 3-D environment image; and store, in the one or more non-transitory, tangible memories, data indicative of a particular label descriptive of the particular physical object in association with the data indicative of the boundary of the particular physical object within the 3-D environment image, thereby distinguishing a set of data points that are representative of the particular physical object within the 3-D environment image from other data points included in the 3-D environment image.</div>
<description-of-drawings>
<heading id="h-0005">BRIEF DESCRIPTION OF THE DRAWINGS</heading>
<div class="description-paragraph" id="p-0034" num="0033">The patent or application file contains at least one drawing executed in color. Copies of this patent or patent application publication with color drawing(s) will be provided by the Office upon request and payment of the necessary fee.</div>
<div class="description-paragraph" id="p-0035" num="0034"> <figref idrefs="DRAWINGS">FIG. 1A</figref> is a block diagram of an example computing system for controlling an autonomous vehicle;</div>
<div class="description-paragraph" id="p-0036" num="0035"> <figref idrefs="DRAWINGS">FIG. 1B</figref> is a block diagram of an example self-driving control architecture that may be used in the autonomous vehicle control system of <figref idrefs="DRAWINGS">FIG. 1A</figref>;</div>
<div class="description-paragraph" id="p-0037" num="0036"> <figref idrefs="DRAWINGS">FIG. 2</figref> is a block diagram of an example light detection and ranging (lidar) system that may be used to provide sensor data to the self-driving control architecture(s) of <figref idrefs="DRAWINGS">FIGS. 1A and 1B</figref>, and/or may be used to generate source images from which training data for autonomous vehicle control models may be mined;</div>
<div class="description-paragraph" id="p-0038" num="0037"> <figref idrefs="DRAWINGS">FIG. 3</figref> illustrates an example vehicle in which the lidar system of <figref idrefs="DRAWINGS">FIG. 2</figref> may operate;</div>
<div class="description-paragraph" id="p-0039" num="0038"> <figref idrefs="DRAWINGS">FIG. 4</figref> illustrates an example data flow diagram illustrating the generation of training data for training machine-learning based models that are used to autonomously operate and/or control vehicles;</div>
<div class="description-paragraph" id="p-0040" num="0039"> <figref idrefs="DRAWINGS">FIGS. 5A-5K</figref> depict example screenshots of graphical user interfaces that may be provided by the systems, methods, and techniques disclosed herein; and</div>
<div class="description-paragraph" id="p-0041" num="0040"> <figref idrefs="DRAWINGS">FIGS. 6A-6E</figref> each include a respective flow chart of a respective, exemplary method of identifying and labeling objects within images for training machine-learning based models that are used to autonomously operate and/or control vehicles.</div>
</description-of-drawings>
<heading id="h-0006">DETAILED DESCRIPTION</heading>
<div class="description-paragraph" id="p-0042" num="0041">As previously noted, for ease of reading herein, and not for limitation purposes, the term “autonomous vehicle,” as used herein, may refer to a vehicle that is controlled and/or operated in a fully-autonomous manner, that is, a vehicle that is entirely self-driving. Additionally or alternatively, the term “autonomous vehicle” as used herein, may refer to a vehicle that is controlled and/or operated in a semi-autonomous manner or that is partially self-driving. Accordingly, the term “autonomous vehicle,” as used herein, is applicable to vehicles that operate at Level 1, Level 2, Level 3, Level 4, or Level 5 of SAE (Society of Automotive Engineers) International Standard J3016. As such, the term “autonomous,” as used herein, generally may be interpreted as “fully-autonomous and/or partially-autonomous” and/or as “entirely self-driving and/or partially self-driving.”</div>
<heading id="h-0007">Example Self-Driving Control Architecture for Autonomous Vehicles</heading>
<div class="description-paragraph" id="p-0043" num="0042"> <figref idrefs="DRAWINGS">FIG. 1A</figref> includes a block diagram of an example computing system <b>10</b> for controlling and/operating an autonomous vehicle. The computing system <b>10</b> may be integrated within an autonomous vehicle in any suitable manner, and at any suitable location or locations within the vehicle. For example, the computing system <b>10</b> may be included, or partially included, within a vehicle controller that is on-board an autonomous vehicle, where the vehicle controller controls and/or operates at least some of the vehicle's driving subsystems that include mechanical components (e.g., accelerator, brakes, steering mechanism, lights, etc.) in a fully- or semi-autonomous manner. The computing system <b>10</b> includes one or more processors <b>12</b> and one or more tangible, non-transitory memories <b>14</b> storing thereon vehicle subsystem control and/or operation instructions <b>16</b>, which are referred to herein as self-driving control architecture (“SDCA”) instructions <b>16</b>. Generally speaking, the SDCA instructions <b>16</b> generate decisions for controlling various operations, behaviors, and maneuvers of the autonomous vehicle.</div>
<div class="description-paragraph" id="p-0044" num="0043">In embodiments where the processor(s) <b>12</b> include more than a single processor, each processor may be a different programmable microprocessor that executes software instructions stored in the memory <b>14</b>. Alternatively, each of the processor(s) <b>12</b> may be a different set of such microprocessors, or a set that includes one or more microprocessors and one or more other processor types (e.g., ASICs, FPGAs, etc.) for certain functions.</div>
<div class="description-paragraph" id="p-0045" num="0044">The memory <b>14</b> may include one or more physical memory devices with non-volatile memory. Any suitable memory type or types may be used, such as ROM, solid-state drives (SSDs), hard disk drives (HDDs), and so on. The processor(s) <b>12</b> are coupled to the memory <b>14</b> via a bus or other network <b>18</b>. The network <b>18</b> may be a single wired network, or may include any suitable number of wired and/or wireless networks. For example, the network <b>18</b> may be or include a controller area network (CAN) bus, a Local Interconnect Network (LIN) bus, and so on.</div>
<div class="description-paragraph" id="p-0046" num="0045">Also coupled to the network <b>18</b> are a vehicle control interface <b>20</b>, a passenger interface <b>22</b>, a sensor interface <b>24</b>, and a network interface <b>26</b>. Each of the interfaces <b>20</b>, <b>22</b>, <b>24</b> and <b>26</b> may include one or more processors (e.g., ASICs, FPGAs, microprocessors, etc.) and/or other hardware, firmware and/or software to enable communication with systems, subsystems, devices, etc., that are external to the computing system <b>10</b>.</div>
<div class="description-paragraph" id="p-0047" num="0046">The vehicle control interface <b>20</b> is generally configured to provide control data generated by the processor(s) <b>12</b> executing the SDCA instructions <b>16</b> to the appropriate operational subsystems of the autonomous vehicle, such that the appropriate subsystems can effectuate driving decisions made by the processor(s) <b>12</b>. For example, the vehicle control interface <b>20</b> may provide control signals to the appropriate driving-related subsystem(s) that include mechanical components, e.g., accelerator, brakes, steering mechanism, lights, etc. As another example, the vehicle control interface <b>20</b> may output or signals to appropriate subsystem(s) that plan the motion of the vehicle (e.g., a motion planner), and/or that control the execution of driving maneuvers (e.g., a maneuver executor). In some embodiments, the vehicle control interface <b>20</b> includes separate interface hardware, firmware and/or software for different operational subsystems.</div>
<div class="description-paragraph" id="p-0048" num="0047">The passenger interface <b>22</b> is generally configured to provide alerts, warnings, notifications, and/or other information to one or more passengers of the autonomous vehicle. In some embodiments where the vehicle is not fully autonomous (e.g., allowing human driving in certain modes and/or situations), the interface <b>22</b> may specifically provide such information to the driver (e.g., via dashboard indicators, etc.). As just one example, the passenger interface <b>22</b> may cause a display and/or speaker in the vehicle to generate an alert when the processor(s) <b>12</b> (executing the SDCA instructions <b>16</b>) determine that a collision with another object is likely. As another example, the passenger interface <b>22</b> may cause a display in the vehicle to show an estimated time of arrival (ETA) to passengers. In some embodiments, the passenger interface <b>22</b> also permits certain user inputs. If the vehicle supports passenger selection of specific driving styles, for example, the passenger interface <b>22</b> may cause a display to present a virtual control (e.g., button) that a passenger may activate (e.g., touch, scroll through, etc.) to select a particular driving style.</div>
<div class="description-paragraph" id="p-0049" num="0048">The sensor interface <b>24</b> is generally configured to convert raw sensor data obtained by one or more sensor devices (e.g., lidar, camera, microphones, thermal imaging units, IMUs, etc.) to a format that is consistent with a protocol of the network <b>18</b> and that is recognized by one or more of the processor(s) <b>12</b>. The sensor interface <b>24</b> may be coupled to an on-board lidar system and/or other type of active sensing system, for example, with the sensor interface <b>24</b> converting point cloud data generated by such system(s) into an appropriate format. In some embodiments, the sensor interface <b>24</b> includes separate interface hardware, firmware and/or software for each sensor device and/or each sensor type.</div>
<div class="description-paragraph" id="p-0050" num="0049">The network interface <b>26</b> is generally configured to convert data received from one or more devices or systems external to the autonomous vehicle to a format that is consistent with a protocol of the network <b>18</b> and is recognized by one or more of the processor(s) <b>12</b>. In some embodiments, the network interface <b>26</b> includes separate interface hardware, firmware and/or software for different external sources. For example, a remote mapping/navigation server may send mapping and navigation/route data (e.g., mapping and navigation signals) to the computing system <b>10</b> via a first type of wireless network interface included the network interface <b>26</b>, e.g., a cellular network interface, while one or more peer vehicles (e.g., other autonomous vehicles) may send data (e.g., current positions of the other vehicles) to the computing system <b>10</b> via a different type of wireless network interface included in the network interface <b>26</b>, e.g., a WiFi network interface. Other types of external data may also, or instead, be received via the network interface <b>26</b>. For example, the computing system <b>10</b> may use the network interface <b>26</b> to receive data representing rules or regulations (e.g., speed limits), object positions (e.g., road rails, overhanging signage, etc.), and/or other information from various infrastructure devices or systems.</div>
<div class="description-paragraph" id="p-0051" num="0050">In some embodiments, no sensor data (or only limited sensor data) of the autonomous vehicle is received via the sensor interface <b>24</b>. Instead, the processor(s) <b>12</b> execute the SDCA instructions <b>16</b> using, as input, only (or primarily) data that is received by the network interface <b>26</b> from other vehicles, infrastructure, and/or other external devices/systems. In such an embodiment, the external data may include raw sensor data that is indicative of the vehicle environment (but was generated off-board the vehicle), and/or may include higher-level information that was generated externally using raw sensor data (e.g., occupancy grids).</div>
<div class="description-paragraph" id="p-0052" num="0051">Although not illustrated in <figref idrefs="DRAWINGS">FIG. 1A</figref>, the network <b>18</b> may also couple to other types of interfaces and/or components. Additionally, in some embodiments, one or more of the interfaces shown in <figref idrefs="DRAWINGS">FIG. 1A</figref> may be omitted (e.g., the sensor interface <b>14</b>, as discussed above). Moreover, it is understood that the computing system <b>10</b> represents just one possible configuration for supporting the software architectures, functions, features, etc., described herein, and that others are also within the scope of this disclosure.</div>
<div class="description-paragraph" id="p-0053" num="0052"> <figref idrefs="DRAWINGS">FIG. 1B</figref> illustrates an example SDCA or self-driving control architecture <b>30</b> of an autonomous vehicle. The SDCA <b>30</b> may be implemented by the SDCA instructions <b>16</b> of <figref idrefs="DRAWINGS">FIG. 1A</figref>, in an embodiment. The SDCA <b>30</b> receives as input M sets of sensor data <b>32</b> generated by M different sensors, with M being any suitable integer equal to or greater than one. The sensor data <b>32</b> may be received via the sensor interface <b>24</b> of <figref idrefs="DRAWINGS">FIG. 1A</figref>, in an embodiment. As just one example, “sensor data <b>1</b>” may include frames of point cloud data generated by a first lidar device, “sensor data <b>2</b>” may include frames of point cloud data generated by a second lidar device, “sensor data <b>3</b>” (not shown in <figref idrefs="DRAWINGS">FIG. 1B</figref>) may include frames of digital images generated by an RGB (red, green, and blue) camera, and so on. As discussed above with respect to <figref idrefs="DRAWINGS">FIG. 1A</figref>, the sensors may include one or more lidar devices, cameras, radar devices, thermal imaging units, IMUs, and/or other sensor types. Generally speaking, the SDCA <b>30</b> is configured to process point cloud data and, in some embodiments, is also configured to process two-dimensional image data, such as RGB data.</div>
<div class="description-paragraph" id="p-0054" num="0053">The sensor data <b>32</b> is input to a perception component <b>36</b> of the SDCA <b>30</b>, and is processed by the perception component <b>36</b> to generate perception signals <b>38</b> descriptive of a current state of the environment in which the autonomous vehicle is located. It is understood that the term “current” may actually refer to a very short time prior to the generation of any given perception signals <b>38</b>, e.g., due to the short processing delay introduced by the perception component <b>36</b> and other factors. To generate the perception signals, the perception component may include a segmentation module <b>40</b>, a classification module <b>42</b>, and a tracking module <b>44</b>.</div>
<div class="description-paragraph" id="p-0055" num="0054">The segmentation module <b>40</b> is generally configured to identify distinct objects within the sensor data representing the sensed environment. Depending on the embodiment and/or scenario, the segmentation task may be performed separately for each of a number of different types of sensor data, or may be performed jointly on a fusion of multiple types of sensor data. In some embodiments where lidar devices are used, the segmentation module <b>40</b> analyzes frames that include point cloud datasets therein to identify subsets of points within each frame that correspond to probable physical objects located in the environment. In other embodiments, the segmentation module <b>40</b> jointly analyzes lidar point cloud data frames in conjunction with RGB camera image frames to identify objects that are located in the environment. Other suitable techniques, and/or data from other suitable sensor types, may also be used to identify objects. It is noted that, as used herein, references to different or distinct “objects” may encompass physical things that are entirely disconnected (e.g., with two vehicles being two different “objects,” and the road on which the vehicles are traveling as yet a different “object”), as well as physical things that are connected or partially connected (e.g., with a vehicle being a first “object” and the vehicle's hitched trailer being a second “object”). The segmentation module <b>40</b> may use predetermined rules or algorithms to identify objects. For example, the segmentation module <b>40</b> may utilize one or more neural networks that have been trained to identify distinct objects within the environment (e.g., using supervised learning with generated labels for different objects within test data point clouds, etc.), or may utilize one or more other types of machine-learning based models that have been trained, by using test or training data, to discern, distinguish, and/or identify probably distinct objects within a source image.</div>
<div class="description-paragraph" id="p-0056" num="0055">The classification module <b>42</b> is generally configured to determine classes (labels, categories, etc.) for different objects that have been identified or distinguished by the segmentation module <b>40</b>. Like the segmentation module <b>40</b>, the classification module <b>42</b> may perform classification separately for different sets of the sensor data <b>32</b>, or may classify objects based on data from multiple sensors, etc. Moreover, and also similar to the segmentation module <b>40</b>, the classification module <b>42</b> may utilize one or more neural networks or other machine-learning based models to classify objects, where the neural networks and/or machine-learning models have been trained, by using a set of test or training data, to perform object classification.</div>
<div class="description-paragraph" id="p-0057" num="0056">The tracking module <b>44</b> is generally configured to track distinct objects over time (e.g., across multiple lidar point cloud or RGB camera image frames). The tracked objects are generally objects that have been identified by the segmentation module <b>40</b>, but may or may not be objects that were classified by the classification module <b>42</b>, depending on the embodiment and/or scenario. The segmentation module <b>40</b> may assign identifiers to identified objects, and the tracking module <b>44</b> may associate existing identifiers with specific objects where appropriate (e.g., for lidar data, by associating the same identifier with different clusters of points, at different locations, in successive point cloud frames). Like the segmentation module <b>40</b> and the classification module <b>42</b>, the tracking module <b>44</b> may perform separate object tracking based on different sets of the sensor data <b>32</b>, or may track objects based on data from multiple sensors. Moreover, and also similar to the segmentation module <b>40</b> and the classification module <b>42</b>, the tracking module <b>44</b> may utilize one or more neural networks or other machine-learning models to track objects, where the neural networks and/or machine-learning models have been trained, by using a set of test or training data, to perform object tracking across frames and/or images.</div>
<div class="description-paragraph" id="p-0058" num="0057">The SDCA <b>30</b> also includes a prediction component <b>50</b>, which processes the perception signals <b>38</b> to generate prediction signals <b>52</b> descriptive of one or more predicted future states of the autonomous vehicle's environment. For a given object, for example, the prediction component <b>50</b> may analyze the type/class of the object (as determined by the classification module <b>42</b>) along with the recent tracked movement of the object (as determined by the tracking module <b>44</b>) to predict one or more future positions of the object. As a relatively simple example, the prediction component <b>50</b> may assume that any moving objects will continue to travel on their current direction and with their current speed, possibly taking into account first- or higher-order derivatives to better track objects that have continuously changing directions, objects that are accelerating, and so on. In some embodiments, the prediction component <b>50</b> also predicts movement of objects based on more complex behaviors. For example, the prediction component <b>50</b> may assume that an object that has been classified as another vehicle will follow rules of the road (e.g., stop when approaching a red light), and will react in a certain way to other dynamic objects (e.g., attempt to maintain some safe distance from other vehicles). The prediction component <b>50</b> may inherently account for such behaviors by utilizing a neural network or other machine learning model, for example. The prediction component <b>50</b> may be omitted from the SDCA <b>30</b>, in some embodiments.</div>
<div class="description-paragraph" id="p-0059" num="0058">In some embodiments, the perception signals <b>38</b> include data representing “occupancy grids” (e.g., one grid per T milliseconds), with each occupancy grid indicating object positions (and possibly object boundaries, orientations, etc.) within an overhead view of the autonomous vehicle's environment. Within the occupancy grid, each “cell” (e.g., pixel) may be associated with a particular class as determined by the classification module <b>44</b>, possibly with an “unknown” class for certain pixels that were not successfully classified. Similarly, the prediction signals <b>52</b> may include, for each such grid generated by the perception component <b>36</b>, one or more “future occupancy grids” that indicate predicted object positions, boundaries and/or orientations at one or more future times (e.g., 1, 2 and 5 seconds ahead).</div>
<div class="description-paragraph" id="p-0060" num="0059">A mapping component <b>55</b> obtains map data (e.g., a digital map including the area currently being traversed by the autonomous vehicle) and/or navigation data (e.g., data indicating a route for the autonomous vehicle to reach the destination, such as turn-by-turn instructions), and outputs the data (possibly in a converted format) as mapping and navigation signals <b>58</b>. In some embodiments, the mapping and navigation signals <b>58</b> include other map- or location-related information, such as speed limits, traffic indicators, and so on. The signals <b>58</b> may be obtained from a remote server (e.g., via a cellular or other communication network of the autonomous vehicle, or of a smartphone coupled to the autonomous vehicle, etc.), and/or may be locally stored in a persistent memory of the autonomous vehicle.</div>
<div class="description-paragraph" id="p-0061" num="0060">A motion planner <b>60</b> processes the perception signals <b>38</b>, the prediction signals <b>52</b>, and the mapping and navigation signals <b>58</b> to generate decisions <b>62</b> regarding the next movements of the autonomous vehicle. Depending on the type of the motion planner <b>60</b>, the decisions <b>62</b> may include operational parameters (e.g., braking, speed and steering parameters) and/or particular maneuvers (e.g., turn left, move to right lane, move onto shoulder of road, etc.). The decisions <b>62</b> may be provided to one or more operational subsystems of the autonomous vehicle (e.g., if the decisions <b>62</b> indicate specific operational parameters for subsystems that include mechanical components, such as steering mechanisms, accelerator, brakes, lights, etc.), or may be provided to one or more intermediate stages that convert the decisions <b>62</b> into operational parameters (e.g., if the decisions indicate specific maneuvers to be performed by a maneuver executor).</div>
<div class="description-paragraph" id="p-0062" num="0061">The motion planner <b>60</b> may utilize any suitable type(s) of rules, algorithms, heuristic models, machine learning models, or other suitable techniques to make driving decisions based on the perception signals <b>38</b>, prediction signals <b>52</b>, and mapping and navigation signals <b>58</b>. For example, the motion planner <b>60</b> may be a “learning based” planner (e.g., a planner that is trained using supervised learning or reinforcement learning), a “search based” planner (e.g., a continuous A* planner), a “sampling based” planner (e.g., a planner that performs random searches in a space that represents a universe of possible decisions), a “predictive control based” planner (e.g., a model predictive control (MPC) planner), and so on. In some embodiments, the motion planner <b>60</b> includes multiple, different motion planner types in order to provide a more diverse set of mechanisms for generating driving decisions, and thereby improve safety and/or other performance aspects of the autonomous vehicle.</div>
<div class="description-paragraph" id="p-0063" num="0062">Example Active Sensing System</div>
<div class="description-paragraph" id="p-0064" num="0063"> <figref idrefs="DRAWINGS">FIG. 2</figref> depicts an example of an active sensing system that may be used to provide at least a portion of the sensor data, such as sensor data <b>32</b> of <figref idrefs="DRAWINGS">FIG. 1B</figref>, and/or sensor data that is received via the sensor interface <b>24</b> of <figref idrefs="DRAWINGS">FIG. 1A</figref>. In particular, the example active sensing system depicted in <figref idrefs="DRAWINGS">FIG. 2</figref> is an example lidar system <b>70</b>. It is noted that while various lidar system components and characteristics are described herein, it is understood that any suitable lidar device(s) or system(s), and/or any other suitable types of sensors, may provide sensor data for processing using the software architectures described herein.</div>
<div class="description-paragraph" id="p-0065" num="0064">The example lidar system <b>70</b> may include a light source <b>72</b>, a mirror <b>75</b>, a scanner <b>78</b>, a receiver <b>88</b>, and a controller <b>92</b>. The light source <b>72</b> may be, for example, a laser (e.g., a laser diode) that emits light having a particular operating wavelength in the infrared, visible, or ultraviolet portions of the electromagnetic spectrum. In operation, the light source <b>72</b> emits an output beam of light <b>80</b> which may be continuous-wave, pulsed, or modulated in any suitable manner for a given application. The output beam of light <b>80</b> is directed downrange toward a remote target <b>82</b> located a distance D from the lidar system <b>70</b> and at least partially contained within a field of regard of the system <b>70</b>.</div>
<div class="description-paragraph" id="p-0066" num="0065">Once the output beam <b>80</b> reaches the downrange target <b>82</b>, the target <b>82</b> may scatter or, in some cases, reflect at least a portion of light from the output beam <b>80</b>, and some of the scattered or reflected light may return toward the lidar system <b>70</b>. In the example of <figref idrefs="DRAWINGS">FIG. 2</figref>, the scattered or reflected light is represented by input beam <b>85</b>, which passes through the scanner <b>78</b>, which may be referred to as a beam scanner, optical scanner, or laser scanner. The input beam <b>85</b> passes through the scanner <b>78</b> to the mirror <b>75</b>, which may be referred to as an overlap mirror, superposition mirror, or beam-combiner mirror. The mirror <b>75</b> in turn directs the input beam <b>85</b> to the receiver <b>88</b>.</div>
<div class="description-paragraph" id="p-0067" num="0066">The input beam <b>85</b> may include light from the output beam <b>80</b> that is scattered by the target <b>82</b>, light from the output beam <b>80</b> that is reflected by the target <b>82</b>, or a combination of scattered and reflected light from target <b>82</b>. According to some implementations, the lidar system <b>70</b> can include an “eye-safe” laser that present little or no possibility of causing damage to a person's eyes. The input beam <b>85</b> may contain only a relatively small fraction of the light from the output beam <b>80</b>.</div>
<div class="description-paragraph" id="p-0068" num="0067">The receiver <b>88</b> may receive or detect photons from the input beam <b>85</b> and generate one or more representative signals. For example, the receiver <b>88</b> may generate an output electrical signal <b>90</b> that is representative of the input beam <b>85</b>. The receiver may send the electrical signal <b>90</b> to the controller <b>92</b>. Depending on the implementation, the controller <b>92</b> may include one or more instruction-executing processors, an application-specific integrated circuit (ASIC), a field-programmable gate array (FPGA), and/or other suitable circuitry configured to analyze one or more characteristics of the electrical signal <b>90</b> in order to determine one or more characteristics of the target <b>82</b>, such as its distance downrange from the lidar system <b>70</b>. More particularly, the controller <b>92</b> may analyze the time of flight or phase modulation for the beam of light <b>80</b> transmitted by the light source <b>72</b>. If the lidar system <b>70</b> measures a time of flight of T (e.g., T representing a round-trip time of flight for an emitted pulse of light to travel from the lidar system <b>70</b> to the target <b>82</b> and back to the lidar system <b>70</b>), then the distance D from the target <b>82</b> to the lidar system <b>70</b> may be expressed as D=c*T/2, where c is the speed of light (approximately 3.0×10<sup>8 </sup>m/s).</div>
<div class="description-paragraph" id="p-0069" num="0068">The distance D from the lidar system <b>70</b> is less than or equal to a maximum range R<sub>MAX </sub>of the lidar system <b>70</b>. The maximum range R<sub>MAX </sub>(which also may be referred to as a maximum distance) of a lidar system <b>70</b> may correspond to the maximum distance over which the lidar system <b>70</b> is configured to sense or identify targets that appear in a field of regard of the lidar system <b>70</b>. The maximum range of lidar system <b>70</b> may be any suitable distance, such as 50 m, 200 m, 500 m, or 1 km, for example.</div>
<div class="description-paragraph" id="p-0070" num="0069">In some implementations, the light source <b>72</b>, the scanner <b>78</b>, and the receiver <b>88</b> may be packaged together within a single housing <b>95</b>, which may be a box, case, or enclosure that holds or contains all or part of the lidar system <b>70</b>. The housing <b>95</b> includes a window <b>98</b> through which the beams <b>80</b> and <b>85</b> pass. The controller <b>92</b> may reside within the same housing <b>95</b> as the components <b>72</b>, <b>78</b>, and <b>80</b>, or the controller <b>92</b> may reside outside of the housing <b>95</b>. In one embodiment, for example, the controller <b>92</b> may instead reside within, or partially within, the perception component <b>36</b> of the SDCA <b>30</b> shown in <figref idrefs="DRAWINGS">FIG. 1A</figref>. In some implementations, the housing <b>95</b> includes multiple lidar sensors, each including a respective scanner and a receiver. Depending on the particular implementation, each of the multiple sensors can include a separate light source or a common light source. The multiple sensors can be configured to cover non-overlapping adjacent fields of regard or partially overlapping fields of regard, for example, depending on the implementation.</div>
<div class="description-paragraph" id="p-0071" num="0070">With continued reference to <figref idrefs="DRAWINGS">FIG. 2</figref>, the output beam <b>80</b> and input beam <b>85</b> may be substantially coaxial. In other words, the output beam <b>80</b> and input beam <b>85</b> may at least partially overlap or share a common propagation axis, so that the input beam <b>85</b> and the output beam <b>80</b> travel along substantially the same optical path (albeit in opposite directions). As the lidar system <b>70</b> scans the output beam <b>80</b> across a field of regard, the input beam <b>85</b> may follow along with the output beam <b>80</b>, so that the coaxial relationship between the two beams is maintained.</div>
<div class="description-paragraph" id="p-0072" num="0071">Generally speaking, the scanner <b>78</b> steers the output beam <b>80</b> in one or more directions downrange. To accomplish this, the scanner <b>78</b> may include one or more scanning mirrors and one or more actuators driving the mirrors to rotate, tilt, pivot, or move the mirrors in an angular manner about one or more axes, for example. While <figref idrefs="DRAWINGS">FIG. 2</figref> depicts only a single mirror <b>75</b>, the lidar system <b>70</b> may include any suitable number of flat or curved mirrors (e.g., concave, convex, or parabolic mirrors) to steer or focus the output beam <b>80</b> or the input beam <b>85</b>. For example, the first mirror of the scanner may scan the output beam <b>80</b> along a first direction, and the second mirror may scan the output beam <b>80</b> along a second direction that is substantially orthogonal to the first direction.</div>
<div class="description-paragraph" id="p-0073" num="0072">A “field of regard” of the lidar system <b>70</b> may refer to an area, region, or angular range over which the lidar system <b>70</b> may be configured to scan or capture distance information. When the lidar system <b>70</b> scans the output beam <b>80</b> within a 30-degree scanning range, for example, the lidar system <b>70</b> may be referred to as having a 30-degree angular field of regard. The scanner <b>78</b> may be configured to scan the output beam <b>80</b> horizontally and vertically, and the field of regard of the lidar system <b>70</b> may have a particular angular width along the horizontal direction and another particular angular width along the vertical direction. For example, the lidar system <b>70</b> may have a horizontal field of regard of 10° to 120° and a vertical field of regard of 2° to 45°.</div>
<div class="description-paragraph" id="p-0074" num="0073">The one or more scanning mirrors of the scanner <b>78</b> may be communicatively coupled to the controller <b>92</b>, which may control the scanning mirror(s) so as to guide the output beam <b>80</b> in a desired direction downrange or along a desired scan pattern. In general, a scan (or scan line) pattern may refer to a pattern or path along which the output beam <b>80</b> is directed. The lidar system <b>70</b> can use the scan pattern to generate a point cloud with points or “pixels” that substantially cover the field of regard. The pixels may be approximately evenly distributed across the field of regard, or distributed according to a particular non-uniform distribution.</div>
<div class="description-paragraph" id="p-0075" num="0074">In operation, the light source <b>72</b> may emit pulses of light which the scanner <b>78</b> scans across a field of regard of the lidar system <b>70</b>. The target <b>82</b> may scatter one or more of the emitted pulses, and the receiver <b>88</b> may detect at least a portion of the pulses of light scattered by the target <b>82</b>. The receiver <b>88</b> may receive or detect at least a portion of the input beam <b>85</b> and produce an electrical signal that corresponds to the input beam <b>85</b>. The controller <b>92</b> may be electrically coupled or otherwise communicatively coupled to one or more of the light source <b>72</b>, the scanner <b>78</b>, and the receiver <b>88</b>. The controller <b>92</b> may provide instructions, a control signal, or a trigger signal to the light source <b>72</b> indicating when the light source <b>72</b> should produce optical pulses, and possibly characteristics (e.g., duration, period, peak power, wavelength, etc.) of the pulses. The controller <b>92</b> may also determine a time-of-flight value for an optical pulse based on timing information associated with when the pulse was emitted by light source <b>72</b> and when a portion of the pulse (e.g., the input beam <b>85</b>) was detected or received by the receiver <b>88</b>.</div>
<div class="description-paragraph" id="p-0076" num="0075">As indicated above, the lidar system <b>70</b> may be used to determine the distance to one or more downrange targets <b>82</b>. By scanning the lidar system <b>70</b> across a field of regard, the system can be used to map the distance to a number of points within the field of regard. Each of these depth-mapped points may be referred to as a pixel or a voxel. A collection of pixels captured in succession (which may be referred to as a depth map, a point cloud, or a point cloud frame) may be rendered as an image or may be analyzed to identify or detect objects or to determine a shape or distance of objects within the field of regard. For example, a depth map may cover a field of regard that extends 60° horizontally and 15° vertically, and the depth map may include a frame of 100-2000 pixels in the horizontal direction by 4-400 pixels in the vertical direction.</div>
<div class="description-paragraph" id="p-0077" num="0076">The lidar system <b>70</b> may be configured to repeatedly capture or generate point clouds of a field of regard at any suitable frame rate between approximately 0.1 frames per second (FPS) and approximately 1,000 FPS, for example. The point cloud frame rate may be substantially fixed or dynamically adjustable, depending on the implementation. In general, the lidar system <b>70</b> can use a slower frame rate (e.g., 1 Hz) to capture one or more high-resolution point clouds, and use a faster frame rate (e.g., 10 Hz) to rapidly capture multiple lower-resolution point clouds.</div>
<div class="description-paragraph" id="p-0078" num="0077">The field of regard of the lidar system <b>70</b> can overlap, encompass, or enclose at least a portion of the target <b>82</b>, which may include all or part of an object that is moving or stationary relative to lidar system <b>70</b>. For example, the target <b>82</b> may include all or a portion of a person, vehicle, motorcycle, truck, train, bicycle, wheelchair, pedestrian, animal, road sign, traffic light, lane marking, road-surface marking, parking space, pylon, guard rail, traffic barrier, pothole, railroad crossing, obstacle in or near a road, curb, stopped vehicle on or beside a road, utility pole, house, building, trash can, mailbox, tree, any other suitable object, or any suitable combination of all or part of two or more objects.</div>
<div class="description-paragraph" id="p-0079" num="0078">As discussed above, 3-D point cloud images generated by the lidar system <b>70</b> may be utilized by the SDCA <b>16</b> and/or the SDCA <b>30</b> to make decisions and generate/transmit corresponding signals to appropriate vehicle subsystems to thereby operate and/or control various real-time behaviors of the autonomous vehicle. In some embodiments, point clouds generated by the lidar system <b>70</b> may be processed to obtain or mine training data therefrom for training control models utilized by (or that are to be utilized by) the autonomous vehicle and/or by other autonomous vehicles for real-time control and operations. For example, point clouds that are obtained continuously over time by the lidar system <b>70</b> may be saved as a 3-D video whose frames are processed to mine training data therefrom.</div>
<div class="description-paragraph" id="p-0080" num="0079">Example Vehicle with On-Board Lidar System</div>
<div class="description-paragraph" id="p-0081" num="0080"> <figref idrefs="DRAWINGS">FIG. 3</figref> illustrates an example vehicle <b>100</b> with a lidar system <b>102</b>. The lidar system <b>102</b> includes a laser <b>105</b> with multiple sensor heads <b>108</b>A-D coupled to the laser <b>105</b> via multiple laser-sensor links <b>110</b>. Each of the sensor heads <b>108</b> may include some or all of the components of the lidar system <b>70</b> illustrated in <figref idrefs="DRAWINGS">FIG. 2</figref>, or may include components of other types of lidar systems.</div>
<div class="description-paragraph" id="p-0082" num="0081">Each of the laser-sensor links <b>110</b> may include one or more optical links and/or one or more electrical links. The sensor heads <b>108</b> in <figref idrefs="DRAWINGS">FIG. 3</figref> are positioned or oriented to provide a greater than 30-degree view of an environment around the vehicle. More generally, a lidar system with multiple sensor heads may provide a horizontal field of regard around a vehicle of approximately 30°, 45°, 60°, 90°, 120°, 180°, 270°, or 360°. Each of the sensor heads <b>108</b> may be attached to, or incorporated into, a bumper, fender, grill, side panel, spoiler, roof, headlight assembly, taillight assembly, rear-view mirror assembly, hood, trunk, window, or any other suitable part of the vehicle.</div>
<div class="description-paragraph" id="p-0083" num="0082">In the example of <figref idrefs="DRAWINGS">FIG. 3</figref>, four sensor heads <b>108</b> are positioned at or near the four corners of the vehicle (e.g., each of the sensor heads <b>108</b> may be incorporated into a light assembly, side panel, bumper, or fender), and the laser <b>105</b> may be located within the vehicle <b>100</b> (e.g., in or near the trunk). The four sensor heads <b>108</b> may each provide a 90° to 120° horizontal field of regard (FOR), and the four sensor heads <b>108</b> may be oriented so that together they provide a complete 360-degree view around the vehicle. As another example, the lidar system <b>102</b> may include six sensor heads <b>108</b> positioned on or around the vehicle <b>100</b>, where each of the sensor heads <b>108</b> provides a 60° to 90° horizontal FOR. As yet another example, the lidar system <b>102</b> may include eight sensor heads <b>108</b>, and each of the sensor heads <b>108</b> may provide a 45° to 60° horizontal FOR. As yet another example, the lidar system <b>102</b> may include six sensor heads <b>108</b>, where each of the sensor heads <b>108</b> provides a 70° horizontal FOR with an overlap between adjacent FORs of approximately 10°. As another example, the lidar system <b>102</b> may include two sensor heads <b>108</b> which together provide a forward-facing horizontal FOR of greater than or equal to 30°.</div>
<div class="description-paragraph" id="p-0084" num="0083">Data from each of the sensor heads <b>108</b> may be combined or stitched together to generate a point cloud that covers a greater than or equal to 30-degree horizontal view around a vehicle. For example, the laser <b>105</b> may include a controller or processor that receives data from each of the sensor heads <b>108</b> (e.g., via a corresponding electrical link <b>115</b>) and processes the received data to construct a point cloud covering a 360-degree horizontal view around a vehicle or to determine distances to one or more targets. The point cloud or information from the point cloud may be provided to a vehicle controller <b>118</b> via a corresponding electrical, optical, or radio link <b>115</b>. The vehicle controller <b>118</b> may include one or more CPUs, GPUs, and a non-transitory memory with persistent components (e.g., flash memory, an optical disk) and/or non-persistent components (e.g., RAM). In an embodiment, the vehicle controller <b>118</b> is implemented by the SDCA instructions <b>16</b> of <figref idrefs="DRAWINGS">FIG. 1A</figref>.</div>
<div class="description-paragraph" id="p-0085" num="0084">In some implementations, the point cloud is generated by combining data from each of the multiple sensor heads <b>108</b> at a controller included within the laser <b>105</b>, and is provided to the vehicle controller <b>118</b>. In other implementations, each of the sensor heads <b>108</b> includes a controller or processor that constructs a point cloud for a portion of the 360-degree horizontal view around the vehicle and provides the respective point cloud to the vehicle controller <b>118</b>. The vehicle controller <b>118</b> then combines or stitches together the point clouds from the respective sensor heads <b>108</b> to construct a combined point cloud covering a 360-degree horizontal view. Still further, the vehicle controller <b>118</b> in some implementations communicates with a remote server to process point cloud data.</div>
<div class="description-paragraph" id="p-0086" num="0085">In any event, the vehicle <b>100</b> may be an autonomous vehicle where the vehicle controller <b>118</b> provides control signals to various components <b>120</b> within the vehicle <b>100</b> to maneuver and otherwise control operation of the vehicle <b>100</b>. The components <b>120</b> are depicted in an expanded view in <figref idrefs="DRAWINGS">FIG. 3</figref> for ease of illustration only. The components <b>120</b> may include an accelerator <b>122</b>, brakes <b>125</b>, a vehicle engine <b>128</b>, a steering mechanism <b>130</b>, lights <b>132</b> such as brake lights, head lights, reverse lights, emergency lights, etc., a gear selector <b>135</b>, and/or other suitable components that effectuate and control movement of the vehicle <b>100</b>. The gear selector <b>135</b> may include the park, reverse, neutral, drive gears, etc. Each of the components <b>120</b> may include an interface via which the component receives commands from the vehicle controller <b>118</b> such as “increase speed,” “decrease speed,” “turn left 5 degrees,” “activate left turn signal,” etc. and, in some cases, provides feedback to the vehicle controller <b>118</b>.</div>
<div class="description-paragraph" id="p-0087" num="0086">In some implementations, the vehicle controller <b>118</b> receives point cloud data from the sensor heads <b>108</b> via the link <b>115</b> and analyzes the received point cloud data, using any one or more of the aggregate or individual SDCAs disclosed herein, to sense or identify targets <b>82</b> (see <figref idrefs="DRAWINGS">FIG. 2</figref>) and their respective locations, distances, speeds, shapes, sizes, type of target (e.g., vehicle, human, tree, animal), etc. The vehicle controller <b>118</b> then provides control signals via the link <b>115</b> to the components <b>120</b> to control operation of the vehicle based on the analyzed information. One, some or all of the components <b>120</b> may be the operational subsystems, or may be included within the operational subsystems, that receive the control signals generated by the SDCA <b>16</b> of <figref idrefs="DRAWINGS">FIG. 1A</figref>, or receive the decisions <b>62</b> of <figref idrefs="DRAWINGS">FIG. 1B</figref>, for example.</div>
<div class="description-paragraph" id="p-0088" num="0087">In addition to the lidar system <b>102</b>, the vehicle <b>100</b> may also be equipped with other sensors such a camera, a thermal imager, a conventional radar (none illustrated to avoid clutter), etc. The sensors can provide additional data to the vehicle controller <b>118</b> via wired or wireless communication links. Further, the vehicle <b>100</b> in an example implementation includes a microphone array operating as a part of an acoustic source localization system configured to determine sources of sounds.</div>
<div class="description-paragraph" id="p-0089" num="0088">In some implementations, the point cloud generated from the sensor data obtained from the sensors <b>108</b> is a source image from which training data is discovered and/or mined to use in training control models that are utilized by (or that are to be utilized by) autonomous vehicles for real-time control and operations. In these implementations, the vehicle <b>100</b> may or may not be autonomously operated. As such, the combining or stitching together of the data generated by the multiple sensor heads <b>108</b> to thereby generate the point cloud image may be performed by the vehicle controller <b>118</b> on-board the vehicle <b>100</b>, by another controller that is on-board the vehicle <b>100</b>, and/or by an off-board computing device or system to which a communication system on-board the vehicle <b>100</b> is communicatively connected, e.g., via one or more wireless links (not shown).</div>
<div class="description-paragraph" id="p-0090" num="0089">Example Data Flow Diagram of the Generation of Training Data for Autonomous Vehicle Control Models</div>
<div class="description-paragraph" id="p-0091" num="0090">Now turning to <figref idrefs="DRAWINGS">FIG. 4</figref>, <figref idrefs="DRAWINGS">FIG. 4</figref> illustrates an example data flow diagram <b>150</b> illustrating the mining, discovery, and/or generation of training data <b>152</b> that is used to train one or more machine-learning based models that control and/or operate autonomous vehicles according to the techniques disclosed herein. For example, as shown in <figref idrefs="DRAWINGS">FIG. 4</figref>, and with simultaneous reference to <figref idrefs="DRAWINGS">FIG. 1B</figref>, at least some of the training data <b>152</b> may be utilized to train the perception component or model <b>36</b> of the SDCA <b>30</b>, and/or portions thereof, such as the segmentation module <b>40</b>, classification module <b>42</b>, and/or the tracking module <b>44</b>.</div>
<div class="description-paragraph" id="p-0092" num="0091">As depicted in <figref idrefs="DRAWINGS">FIG. 4</figref>, source images <b>155</b> that are to be processed to generate training data <b>152</b> for the perception model <b>36</b> are stored in a data storage entity <b>158</b>, such as a database, databank, data cloud, or any suitable data storage implementation. The source images <b>155</b> may include 2-D images of environments in which vehicles operate (e.g., that have been generated by one or more passive imaging devices or systems) and/or may include 3-D images of environments in which vehicles operate (e.g., that have been generated by one or more active imaging devices or systems, such as the lidar system <b>70</b> of <figref idrefs="DRAWINGS">FIG. 3</figref>). For example, the source images <b>155</b> may include RGB camera images, thermal images, lidar data images, other types of point cloud data images, and/or other types of 2-D and/or 3-D images that have been captured by passive and/or active imaging devices or systems. In an embodiment, the source images <b>155</b> include only 3-D images. In another embodiment, the source images <b>155</b> include both 3-D and 2-D images of a particular environment, where the 2-D images and the 3-D images were captured or obtained at the same time or at a similar time. In some embodiments, the source images <b>155</b> include multiple frames of a 2-D video and/or of a 3-D video of a vehicle's environment.</div>
<div class="description-paragraph" id="p-0093" num="0092">The source images <b>155</b> are processed by using a tool or system <b>160</b> that utilizes at least a portion of the systems, methods, and/or techniques disclosed herein for identifying and labeling objects to thereby generate training data <b>152</b> used to train one or more machine-learning models that are, in turn, used by the perception model(s) <b>36</b> to autonomously control and/or/or operate vehicles. The tool <b>160</b> may be implemented as one or more computing devices, including one or more processors <b>162</b>, one or more tangible, non-transitory memories <b>165</b>, and one or more user interfaces <b>168</b> (e.g., one or more specific, particularly structured graphical user interfaces <b>168</b>) that are communicatively coupled via one or more communication buses or networks <b>170</b> (which may be, for example, any combination of one or more wired and/or wireless networks). At least some of the graphical user interface(s) <b>168</b> may be remotely located from the tool <b>160</b>, in an embodiment. The graphical user interface(s) <b>168</b> are specifically structured and paired with one or more prescribed functionalities <b>172</b> directly related to the graphical user interface(s) <b>168</b>, where the functionalities <b>172</b> include identifying and/or labeling particular objects depicted within the source images <b>155</b> in a specific manner or way via the specific graphical user interface <b>168</b>. Details of the specific graphical user interface(s) <b>168</b>, the prescribed functionalities <b>172</b>, and their interactions/relationships are provided in other sections of this disclosure.</div>
<div class="description-paragraph" id="p-0094" num="0093">At any rate, the specific graphical user interface(s) <b>168</b> and the prescribed functionality <b>172</b> may be implemented as one or more sets of computer executable instructions that are stored on the one or more memories <b>165</b> and that are executable by the one or more processors <b>162</b> to cause the tool <b>160</b> to perform the prescribed functionality <b>172</b> on the source frames <b>155</b> using the specific graphical user interface(s) <b>168</b>. The output <b>175</b> of the prescribed functionality <b>172</b> comprises identified objects that are depicted within the source images <b>155</b> and that are respectively labeled for the purposes of training the perception model <b>36</b>. The output <b>175</b> may be stored as processed image frames (e.g., each of which may include a respective source image <b>155</b> as well as the identifications and labels of particular objects depicted thereon). Additionally or alternatively, the output <b>175</b> may be stored as indications of the associations between respective identified object/label pairs, and/or may be stored using any other suitable format.</div>
<div class="description-paragraph" id="p-0095" num="0094">The output data <b>175</b> may be incorporated into or included in the training data set <b>152</b>. For example, the one or more processors <b>162</b> of the tool <b>160</b> (and/or one or more other processors which are not shown in <figref idrefs="DRAWINGS">FIG. 4</figref>) may push and/or pull at least some of the stored output <b>175</b> and store it with the training data <b>152</b>. Subsequently, the training data <b>152</b> including the output data <b>175</b> may be utilized to train the perception model <b>36</b>, e.g., to train one or more models included in the perception model <b>36</b>, using any desired training technique.</div>
<div class="description-paragraph" id="p-0096" num="0095">It is understood that the data flow diagram <b>150</b> and the components associated therewith as illustrated in <figref idrefs="DRAWINGS">FIG. 4</figref> is only one of many possible embodiments. For example, the tool <b>160</b> may be implemented by a network of the multiple computing devices, and/or by a cloud computing system. In some embodiments, at least one of the graphical user interfaces <b>168</b> may be implemented at a computing device that is remote from at least some of the one or more processors <b>162</b> and/or at least some of the one or more memories <b>165</b>, such as when the user interface <b>168</b> is implemented on a client device while the one or more processors <b>162</b> and the one or more memories <b>165</b> are implemented at one or more servers or back-end computing devices (for example, in a web service or other type of client/server application). In some embodiments, the one or more processors <b>162</b>, the one or more memories <b>165</b>, and/or the set of instructions <b>172</b> are implemented in a distributed manner. For example, a remote client device may include a first portion of the processors <b>162</b>, the memories <b>165</b>, and/or the instructions <b>172</b>, and one or more servers or back-end computing devices communicatively connected to the remote client device may include a second portion of the processors <b>162</b>, the memories <b>165</b>, and/or the instructions <b>172</b>. For ease of reading herein, the one or more processors <b>162</b>, one or more memories <b>165</b>, one or more graphical user interfaces <b>168</b>, and one or more functionalities <b>172</b> are referred to herein using the singular tense; however it is understood that this is for ease of reading purposes only, and is not limiting.</div>
<div class="description-paragraph" id="p-0097" num="0096">In some embodiments, the tool <b>160</b> accesses the data storage entity <b>158</b> via one or more wired and/or wireless networks (not shown in <figref idrefs="DRAWINGS">FIG. 4</figref>). In a similar manner, the output data <b>175</b> may be provided for storage in the training data set <b>152</b> via one or more wired and/or wireless networks (not shown). Further, although the output <b>175</b> is shown in <figref idrefs="DRAWINGS">FIG. 4</figref> as being stored in the data storage entity <b>155</b> along with the source frames <b>155</b>, this is only one exemplary implementation, as the output <b>175</b> may be additionally or alternatively be stored as desired in one or more other separate and distinct data storage entities (not shown).</div>
<div class="description-paragraph" id="p-0098" num="0097">Example Graphical User Interfaces</div>
<div class="description-paragraph" id="p-0099" num="0098"> <figref idrefs="DRAWINGS">FIGS. 5A-5K</figref> depict screen shots of example graphical user interfaces that are specifically structured and provided by the systems, methods, and techniques disclosed herein. For example, each of the example graphical user interfaces described herein may be provided by the system or tool <b>160</b> and/or may be paired with and/or operate in conjunction with at least some of the prescribed functionality <b>172</b> of <figref idrefs="DRAWINGS">FIG. 4</figref>. For ease of discussion, and not for limitation purposes, <figref idrefs="DRAWINGS">FIGS. 5A-5K</figref> are discussed with simultaneous reference to <figref idrefs="DRAWINGS">FIGS. 1-4</figref>, although other systems and methods may produce and/operate in conjunction with each of the example graphical interfaces discussed below.</div>
<div class="description-paragraph" id="p-0100" num="0099"> <figref idrefs="DRAWINGS">FIG. 5A</figref> depicts a screen shot of an example display <b>200</b> presented by the tool <b>160</b> via the graphical user interface <b>168</b>, e.g., by executing at least some of the instructions <b>172</b>. As depicted in <figref idrefs="DRAWINGS">FIG. 5A</figref>, the specifically structured display <b>200</b> serves as a home screen (or, splash screen and/or menu screen) of the tool <b>160</b>. In particular, in a first portion <b>202</b> <i>a </i>of the display <b>200</b>, various object identification features <b>205</b>-<b>222</b> enable a user to discern, distinguish, and/or identify particular objects depicted in source images and/or to adjust or verify automatically-identified objects depicted in source images. On a second portion <b>202</b> <i>b </i>of the display <b>200</b>, one or more object labeling user controls <b>203</b> <i>a</i>-<b>203</b> <i>n </i>are provided via which the user is able to obtain, select, create, and/or edit labels for depicted objects.</div>
<div class="description-paragraph" id="p-0101" num="0100">The object identification portion <b>202</b> <i>a </i>of the display <b>200</b> includes a number of selectable user controls thereon that respectively correspond to a number of features via which the user may use to discern, distinguish, and/or identify objects within 2-D images, for example, 2-D Bounding Boxes <b>205</b>, 2-D Lane Markings <b>208</b>, 2-D Paint <b>210</b>, 2-D Object Tracking <b>212</b>, and/or other 2-D object identification features (not shown). Additionally or alternatively, the tool <b>160</b> may a number of selectable user controls displayed on the object identification portion <b>202</b> <i>a </i>of the display <b>200</b> that respectively correspond to features via which the user may user to discern, distinguish, and/or identify objects within 3-D images, for example, 3-D Bounding Boxes <b>215</b>, 3-D Lane Markings <b>218</b>, 3-D Paint <b>220</b>, 3-D Object Tracking <b>222</b>, and/or other 3-D object identification features (not shown). Each of the identification feature icons/user controls <b>205</b>-<b>222</b> that is included on the display <b>200</b> is user-selectable, to thereby indicate the feature which the user desires to employ for identifying particular objects within images. It is noted that the use and/or activation of each of the features <b>205</b>-<b>222</b> is not mutually exclusive. For example, a user might may select 3-D Bounding Boxes user control <b>215</b> to identify objects included within a particular 3-D image, and while the particular 3-D image is being presented on the user interface <b>168</b> for bounding purposes, the user may invoke the 3-D Object Tracking feature <b>222</b> and/or the 3-D Lane Markings feature <b>218</b> therefrom. As such, the home screen <b>200</b> is only one of many embodiments via which the features <b>205</b>-<b>222</b> may be invoked.</div>
<div class="description-paragraph" id="p-0102" num="0101"> <figref idrefs="DRAWINGS">FIG. 5B</figref> illustrates a screen shot of an example display <b>230</b> depicting an example usage scenario of the 3-D Bounding Boxes feature <b>215</b> of the tool <b>160</b>. The example display <b>230</b> may be presented by the tool <b>160</b> via the graphical user interface <b>168</b>, e.g., by executing at least some of the instructions <b>172</b>, for example, the display <b>230</b> may be presented at the user interface <b>168</b> in response to a user selecting the 3-D Bounding Boxes <b>215</b> user control included on the display <b>200</b> of <figref idrefs="DRAWINGS">FIG. 5A</figref>.</div>
<div class="description-paragraph" id="p-0103" num="0102">As illustrated in <figref idrefs="DRAWINGS">FIG. 5B</figref>, the display <b>230</b> includes a first portion <b>232</b> <i>a </i>via which a user may discern, distinguish, and/or identify objects, and/or via which the user may verify or adjust automatic identifications of objects depicted within a 3-D image. The display <b>230</b> includes a second portion <b>232</b> <i>b </i>via which the user may label objects and/or verify or adjust automatic labeling of the objects. The object identification portion <b>232</b> <i>a </i>of the display <b>230</b> is itself divided into multiple parts or sections <b>235</b>-<b>242</b>. In section <b>235</b>, a 3-D source image of an environment in which vehicles operate is displayed so that it can be processed for object identification and labeling purposes. The 3-D image <b>235</b> depicted in <figref idrefs="DRAWINGS">FIG. 5B</figref> is a point cloud image or dataset that was generated using multiple lidar devices mounted on a vehicle (for example, as previously discussed with respect to <figref idrefs="DRAWINGS">FIG. 3</figref>), however any type of 3-D environment image, lidar or otherwise, may be presented in the section <b>235</b>. In section <b>238</b> of the display <b>230</b>, a corresponding 2-D image of the environment that was obtained at nearly the same time as the 3-D image of the environment is displayed in section <b>238</b>, so that the 2-D image and the 3-D image are presented on the display <b>230</b> in a side-by-side manner. For example, the 2-D image <b>238</b> and the 3-D image <b>235</b> may have respective time-stamps that are identical or within a short time interval of each other (e.g., within a quarter of a second, within a half of a second, within one second, within two seconds, etc.). As depicted in <figref idrefs="DRAWINGS">FIG. 5B</figref>, the 2-D image <b>238</b> is an RGB image captured via an RGB camera mounted on a vehicle along with the lidar devices that captured the 3-D image <b>235</b>, however any type of 2-D environment image, RGB or otherwise, may be presented in the section <b>238</b>. Furthermore, it is noted that the display of the corresponding 2-D image <b>238</b> on the display <b>230</b> is an optional feature and may be omitted from some implementations of the display <b>230</b>.</div>
<div class="description-paragraph" id="p-0104" num="0103">As shown in <figref idrefs="DRAWINGS">FIG. 5B</figref>, the 3-D image <b>235</b> is a 3-D point cloud data set or image included in a 3-D video which was obtained by several lidar devices onboard a vehicle, such as the lidar device <b>70</b> of <figref idrefs="DRAWINGS">FIG. 2</figref> and/or the lidar devices <b>108</b> <i>a</i>-<b>108</b> <i>d </i>of <figref idrefs="DRAWINGS">FIG. 3</figref>. Significantly, as the image <b>235</b> is a three-dimensional image, a user is able to be manipulate the presentation of the image <b>235</b> on the user interface <b>168</b> in three dimensions (e.g., by using one or more respective user controls, not shown in <figref idrefs="DRAWINGS">FIG. 5B</figref>). For example, the 3-D image <b>235</b> may be rotated, translated, and/or scaled along any combination of the x, y, and z axes of the field of view so that the 3-D image is able to be presented in section <b>235</b> from any desired viewing perspective.</div>
<div class="description-paragraph" id="p-0105" num="0104">As depicted in <figref idrefs="DRAWINGS">FIG. 5B</figref>, the 2-D image <b>238</b> is an RGB image included in a 2-D video which was obtained by an RGB camera mounted on the same vehicle as the lidar devices. In some embodiments, the user may translate and/or scale the 2-D image <b>238</b> in two-dimensions, e.g., along the x and y axes of the image, if desired.</div>
<div class="description-paragraph" id="p-0106" num="0105">The RGB camera and the lidar devices were operational on the vehicle at the same time, and therefore simultaneously captured respective videos of the environment of the vehicle as the vehicle traveled along a road. Accordingly, on the example display <b>230</b>, the 3-D video <b>235</b> is presented in an essentially time-synchronized or congruent manner with the presentation of the 2-D video <b>238</b>, e.g., in a side-by-side manner. That is, for any moment in time of the videos <b>235</b>, <b>238</b> during which both the RGB camera and the lidar devices mounted on the vehicle were capturing respective images of the environment, the 3-D section <b>238</b> displays the environment as captured by the lidar devices at that moment in time, and the 2-D section <b>238</b> displays the environment as captured by the RGB camera at that moment in time. A user is able to pause, forward, and rewind the video <b>235</b>, <b>238</b> synchronously between and/or across multiple frames, as desired. That is, due to the videos' synchronized or congruent presentations on the display <b>230</b>, pausing, forwarding, rewinding, and/or playing one of the videos <b>235</b>, <b>238</b> will cause the same action to take effect in the other video <b>235</b>, <b>238</b> at the same rate. As such, frames of the 3-D video that are displayed in the section <b>235</b> are aligned, with respect to time of capture, with respective frames of the 2-D video that are displayed in the 2-D video <b>238</b>, in an embodiment. Notably, the simultaneous, synchronous display of 3-D images/videos and 2-D images/videos on the display <b>230</b> is a specific, structured graphical user interface that provides a technological solution to help the user to better and more accurately discern or visually identify various objects within a vehicle environment. Point cloud and other types of 3-D images may be more abstract to the human eye then are RGB camera images, and thereby may be more difficult for user to visually (and accurately) discern and identify various distinct objects therein. The simultaneous, synchronous display of the 3-D images and the 2-D images on the screen <b>230</b> allows a user to verify that a cluster of points within the 3-D image is indeed the particular object that he or she wishes to bound, e.g., by viewing the presented, time-synchronized images <b>235</b>, <b>238</b> in conjunction with each other.</div>
<div class="description-paragraph" id="p-0107" num="0106">Another useful feature illustrated by the example usage scenario of <figref idrefs="DRAWINGS">FIG. 5B</figref> is “one-click” bounding boxes. As previously discussed, with known or existing object identification techniques, a user must visually identify as subset of the data points included in a source image as representing a particular object of significance (e.g., another car, a pedestrian, a cyclist, etc.), and the user must manually draw a set of individual lines (or drag and drop a box template) to form a box that surrounds a subset of data points to thereby distinguish the subset of data points represent the particular object of significance from other data points of the source image. The size and configuration of the manually-generated box is based on the user's judgment as to where the boundaries of the image of the particular object are located within the image. As such, the placement of a bounding box is based upon a user's visual acuity as well as the user subjective judgment, and thus is subject to wide variations in accuracy. Further, the manual drawing and placement of bounding boxes requires multiple user actions to apply each bounding box which, when producing a sufficiently large training data set, can add up to hundreds or thousands of person-hours.</div>
<div class="description-paragraph" id="p-0108" num="0107">In contrast, the “one-click” bounding box feature provided by the tool <b>168</b> allows a user to single-click anywhere on a cluster of points included in a 3-D source image (or single-click anywhere on a visual depiction of some object included in a 2-D source image), to thereby select an intended object for bounding. The tool <b>168</b> then automatically determines the boundaries of the object indicated by the single click within the source image, and automatically presents, on the source image, a corresponding bounding box that reflects the boundaries of the object within the source image. Thus, with “one-click” bounding boxes, a single user action results in the generation and placement of a bounding box. In <figref idrefs="DRAWINGS">FIG. 5B</figref>, the bounding box <b>245</b> has been automatically generated and located on the source image <b>235</b> in response to the user single-clicking on one of the data points of the cluster bounded by the box <b>245</b>.</div>
<div class="description-paragraph" id="p-0109" num="0108">It is noted that while the present disclosure utilizes the term “one-click” bounding boxes, using only a single-click is only an exemplary embodiment and as such the term “one-click” is utilized herein for ease of reading and not for limitation purposes. Indeed, a “one-click” bounding box need not be generated via a single mouse click, but may be generated by any suitable single or lone user activation of a user control. It is further noted that only a single click (or a single user activation of another type) is necessary to cause the tool <b>168</b> to automatically generate and present a bounding box that distinguishes, on the source image, the depiction of the object that includes the location that the user indicated.</div>
<div class="description-paragraph" id="p-0110" num="0109">In an embodiment, the automatic “one-click” bounding box utilizes an image processor, which may be implemented, for example, by the computer executable instructions <b>172</b> of the tool <b>160</b>. The tool <b>160</b> may utilize different image processors for 2-D images and 3-D images, in some configurations. Generally speaking, image processors that are used by “one-click” bounding boxes employ one or more types of statistical and/or computational rules and/or analyses, e.g., by using learning techniques that are based, at least in part, on historical image processing training data (which is different than the training data <b>152</b> used to train models for controlling autonomous vehicles), by using maximum distances between all points in a cluster, etc. Thus, an image processor included in the tool <b>160</b> may receive, as an input, the location within the source image indicated by the user's single click, determine a particular set of data points surrounding the indicated location that is most likely to represent a particular, distinct object, and automatically display a bounding box that is particularly sized and oriented to encompass the particular set of data points, thereby distinguishing the particular, distinct object within the source image. Subsequently, the user may simply accept the bounding box which was automatically determined and proposed by the tool <b>160</b> and indicate a respective label for the particular, distinct object identified by the tool-generated bounding box. Alternatively, for some objects, the user may treat the proposed bounding box as a draft or interim bounding box, and may manually apply particular modifications or adjustments thereto.</div>
<div class="description-paragraph" id="p-0111" num="0110">When a bounding box is to a user's liking, the user may, via the user control <b>260</b> included in the object labeling portion <b>232</b> <i>b </i>of the display <b>230</b>, associate the bounding box with a label to thereby identify the particular, distinct object distinguished within the source image by the 3-D bounding box. Typically, the label is one of a plurality of categories and/or classifications via which the perception model <b>36</b> utilizes to learn what types of groupings of data points represent different categories/classifications. An example set of labels includes “car”, “large vehicle”, “pedestrian”, “cyclist”, “motorcyclist”, “traffic sign”, “road”, “traffic light”, etc. Examples of labels that are more granular include “bus”, “semi-trailer truck”, “animal”, “ice patch”, “pothole”, etc. In an embodiment, the user may activate the user control <b>260</b>, thus causing a drop-down menu to appear, and the user may select one of the labels displayed in the drop-down menu to be associated with the bounding box <b>245</b>. In another embodiment, the user may activate the user control <b>260</b> to enter free-form text to generate a new label for a category or classification of the object that is identified or distinguished by the bounding box.</div>
<div class="description-paragraph" id="p-0112" num="0111">The automatic bounding box techniques that include image processing discussed herein address several of the drawbacks of known identification techniques. For example, unlike the human eye, an image processor is able to determine, by using one or more statistical and/or computational rules and/or analyses, that an abstract cluster of points within the point cloud image <b>235</b> does indeed represent a distinct object, and in some cases, based on how the image processor was trained, a type of object (e.g., car, person, traffic sign, etc.). Further, as the tool <b>160</b> has knowledge of which particular subset of data points of the source image represent the distinct object, the application of the bounding box may be accurate down to a level that is not consistently achievable by using the human eye and human judgment. For example, trained image processors may be able to distinguish between data points that represent a car and adjacent data points that represent a shadow. In another example, when a distinct object is represented by only a few data points of an image (such as when a car drives off into the distance), trained image processors of the tool <b>160</b> are able to statistically ascertain that the car is indeed represented by the few data points, which is also not achievable by the human eye and human judgment.</div>
<div class="description-paragraph" id="p-0113" num="0112">In some embodiments, a single user command or activation may result in multiple bounding boxes being generated and displayed on the screen <b>230</b>, and as such, is interchangeable referred to herein as group, batch, or categorical bounding. For example, a user may enter a single command to apply bounding boxes to all pedestrians displayed within the source image <b>235</b>. In these embodiments, the image processor(s) of the tool <b>160</b> may discover or determine all clusters of points included in the image <b>235</b> that respectively correspond to pedestrian, and may automatically generate, position, and display a respective bounding box for each pedestrian that is included in the source image. Thus, with the group bounding box feature, the person-hours required to produce a sufficiently sized set of training data may be still further reduced.</div>
<div class="description-paragraph" id="p-0114" num="0113">To discuss the work flow of 3-D bounding boxes <b>215</b> in more detail, and referring to the display <b>230</b> of <figref idrefs="DRAWINGS">FIG. 5B</figref>, the user has single-clicked on a location within a cluster of points within the 3-D point cloud image <b>235</b>, and the tool <b>160</b> has automatically generated and placed a bounding box <b>245</b> around the subset of points within the vicinity of the indicated location which the tool <b>160</b> has determined to be representing a particular, distinct object within the 3-D image <b>235</b>, in this case, a particular car. Note that as the image <b>235</b> is a 3-D image, the bounding box <b>245</b> is correspondingly three-dimensional. On the display <b>230</b>, the car which is bounded <b>245</b> within the 3-D image <b>235</b> (e.g., via the “one-click” feature) is also simultaneously depicted within the 2-D image <b>238</b>, as indicated by the reference <b>248</b>. As shown in <figref idrefs="DRAWINGS">FIG. 5B</figref>, the placement of the 3-D bounding box <b>245</b> on the 3-D image <b>235</b> causes a corresponding 2-D bounding box <b>250</b> to be automatically generated and placed on the 2-D image <b>238</b> by the tool <b>160</b>, in an embodiment. If the user subsequently re-sizes and/or re-positions either one of the automatically generated bounding boxes <b>245</b>, <b>250</b>, corresponding re-sizing and/or re-positioning may be automatically reflected on the other one of the bounding boxes <b>240</b>, <b>245</b>.</div>
<div class="description-paragraph" id="p-0115" num="0114">In embodiments in which the “one-click” bounding box features is not available or is not turned on, when a user manually generates a bounding box on either the 2-D image <b>238</b> or the 3-D image <b>235</b> (e.g., by visually identifying a particular object and manually drawing the sides of the bounding box around the identified object), the tool <b>160</b> may still automatically generate and present a corresponding bounding box in the other image <b>235</b>, <b>238</b>. As such, the tool <b>160</b> conveniently provides confirmation to the user (e.g., via the automatic generation and placement of the corresponding bounding box) that the bounded and abstractly represented object within the 3-D image <b>235</b> represents the same object as is represented by the RBG object <b>248</b> within the 2-D image <b>238</b>. Additionally, in these embodiments, when the user re-sizes and/or re-positions either one of the bounding boxes <b>245</b>, <b>250</b> (whether manually or automatically generated), corresponding re-sizing and/or re-positioning may be automatically reflected on the other one of the bounding boxes <b>240</b>, <b>245</b>.</div>
<div class="description-paragraph" id="p-0116" num="0115">To further aid the user in accurately bounding a distinct object within the 3-D image <b>235</b> (e.g., when the “one-click” feature is turned off or unavailable, and/or when the user desires to make modifications to automatically-generated bounding boxes), the object identification portion <b>232</b> <i>a </i>of the display <b>230</b> may include one or more other sections <b>240</b>, <b>242</b>, each of which displays a different, 2-D perspective view of the 3-D image shown in section <b>235</b>. The particular 2-D perspective view of the 3-D image or type thereof that is presented in each section <b>240</b>, <b>242</b> may be selectable by the user, in some embodiments. In <figref idrefs="DRAWINGS">FIG. 5B</figref>, a top-down, 2-D view of the bounded 3-D point cloud image of the car in the section <b>235</b> is presented in section <b>240</b>, and a side 2-D view of the bounded 3-D point cloud image of the car in the section <b>235</b> is presented in section <b>242</b>. Any bounding box that is generated and applied (either automatically and/or manually) to one of the areas <b>235</b> or <b>238</b> may be automatically generated and positioned in each of the sections <b>240</b>, <b>242</b>. The user may then manipulate the bounding box presented in the sections <b>240</b> and/or <b>242</b> to better refine and/or indicate the boundaries of the image of the particular object within the 3-D image <b>235</b>. Any modifications to a bounding box displayed within one of the sections <b>240</b>, <b>242</b> of the display <b>230</b> may be automatically reflected in the other one of the sections <b>240</b>, <b>242</b>, as well as in the sections <b>235</b> and <b>238</b> (and vice versa). Further, if the user shifts focus to another bounded object displayed within the source image <b>235</b> (not shown), a corresponding top-down, 2-D view and a corresponding side 2-D view of the other bounded object may be automatically presented in the sections <b>240</b>, <b>242</b>, respectively. As such, the user is able to simultaneously view the particular object of focus and its bounding box <b>245</b> from three different perspectives <b>235</b>, <b>240</b>, <b>242</b>, and make adjustments to the bounding box <b>245</b> in any of the three perspectives to better refine the 3-D boundary of the particular object of focus within the 3-D source image.</div>
<div class="description-paragraph" id="p-0117" num="0116">Via the object labeling portion <b>232</b> <i>b </i>of the display <b>230</b>, the user is able to apply an appropriate or desired label to the bounding box <b>245</b>. As illustrated in the second portion <b>232</b> <i>b</i>, an identifier <b>255</b> of the bounding box <b>245</b> (e.g., “Box ID <b>8705</b>”) has been automatically generated by the tool <b>160</b> and associated with the box <b>245</b>. The identifier <b>255</b> uniquely identifies, within the 3-D video, the particular, distinct object (in this scenario, the car) indicated by the bounding box <b>245</b> (e.g., as opposed to other cars and other objects which may be depicted within the 3-D video). The frame number of the 3-D video to which the source image <b>235</b> corresponds is indicated in the object labeling portion <b>232</b> <i>b </i>via a frames field <b>258</b> (e.g., “1”). A user selectable control <b>260</b> is provided to allow the user to generate, select, create, edit, and/or approve a label for the particular, distinct object identified by the bounding box <b>245</b> and by its identifier <b>255</b>. For example, upon activation of the user control <b>260</b>, a drop-down list may be presented from which the user may select a desired label (e.g., car, large vehicle, pedestrian, cyclist, motorcyclist, traffic sign, road, traffic light, etc.) for the bounded object <b>245</b>. Additionally or alternatively, upon activation of the user control <b>260</b> a free-form text entry box or field may be presented, via which the user may provide a custom label for the particular, distinct object <b>245</b>. In some embodiments, a label (e.g., a draft or interim label) may be automatically generated by the tool <b>168</b> in conjunction with automatically generating the bounding box <b>245</b>, for example, “car.” Via the second portion <b>232</b> <i>b</i>, the user may accept or approve the draft/interim label as proposed by the tool <b>168</b>, or the user may modify and/or edit the draft label as desired, for example, change “car” to “SUV.” When the user is satisfied with both the bounding box <b>245</b> and the label <b>260</b> associated therewith, the user may cause an indication of association between the bounding box <b>245</b> and the label <b>260</b> to be stored as training data <b>152</b> for autonomous vehicle perception models <b>36</b>.</div>
<div class="description-paragraph" id="p-0118" num="0117">It is noted that although the above discussion describes a workflow in which the user first applies a bounding box <b>245</b> and then adds a label <b>260</b> thereto, in other workflows, the label <b>260</b> may be generated before the bounding box <b>245</b>. For example, a user may first indicate, via the labeling portion <b>232</b> <i>b </i>of the display <b>230</b>, a particular label, e.g., “pedestrian.” The user may then single-click on all clusters of data points within the image <b>235</b> that are to be labeled “pedestrian.” Alternatively, the user may enter a command (not shown) for the tool <b>160</b> to automatically identify all clusters of data points within the image <b>235</b> that are likely to depict “pedestrians.” In response, the tool <b>160</b> may automatically generate a respective bounding box for each of the single-clicked or automatically determined clusters that depict “pedestrians,” and the tool <b>160</b> may automatically generate a respective instance of the label pedestrian <b>260</b> associated with each bounding box. Each instance of the label pedestrian <b>260</b> may include the frame or source image number/identifier <b>258</b> and a unique bounding box identifier <b>255</b>, for example.</div>
<div class="description-paragraph" id="p-0119" num="0118"> <figref idrefs="DRAWINGS">FIG. 5C</figref> illustrates a screen shot of an example display <b>270</b> depicting an example usage scenario of the 3-D Object Tracking feature <b>222</b> of the tool <b>160</b>, in which the tool <b>160</b> automatically tracks a distinct object across multiple frames of a 3-D video. The display <b>270</b> may be presented on the graphical user interface <b>168</b> of the tool <b>160</b> (e.g., by executing at least some of the instructions <b>172</b> stored on the memory <b>165</b>), for example. In the example usage scenario illustrated by <figref idrefs="DRAWINGS">FIG. 5C</figref>, the display <b>270</b> is a later frame of a 3-D video in which the display <b>230</b> of <figref idrefs="DRAWINGS">FIG. 5B</figref> is also included, e.g., the display <b>230</b> of <figref idrefs="DRAWINGS">FIG. 5B</figref> is Frame <b>1</b> of the 3-D video, and the display <b>270</b> of <figref idrefs="DRAWINGS">FIG. 5C</figref> is Frame <b>5</b> of the same 3-D video. The display <b>270</b> may presented on the user interface <b>168</b> in response to a user selecting the 3-D Object Tracking <b>222</b> user control of the home screen <b>200</b>, in some scenarios. However, in the example usage scenario depicted in <figref idrefs="DRAWINGS">FIG. 5C</figref>, the user has advanced the 3-D video from Frame <b>1</b> (which is depicted in section <b>235</b> of the display <b>230</b> of <figref idrefs="DRAWINGS">FIG. 5B</figref>) forward in time to Frame <b>5</b>, thereby automatically invoking the 3-D Object Tracking feature <b>222</b>. As such, within the object identification portion <b>272</b> <i>a </i>of the display <b>270</b>, the first part or section <b>275</b> displays Frame <b>5</b> of the 3-D video, and the second part or section <b>278</b> displays a corresponding 2-D image captured by the RGB camera at the time that Frame <b>5</b> of the 3-D source video was captured by the lidar devices. (It is noted that although the example scenario discussed with respect to <figref idrefs="DRAWINGS">FIG. 5C</figref> describes the 3-D video as being forwarded, the techniques, features, and concepts discussed herein equally apply to scenarios in which the 3-D video is rewound.)</div>
<div class="description-paragraph" id="p-0120" num="0119">In the display <b>270</b>, tool <b>160</b> has automatically tracked the particular car that was identified and distinguished in Frame <b>1</b> (e.g., by the bounding box <b>245</b> and uniquely identifier “Box ID <b>8705</b>”) to its updated location at the time at which Frame <b>5</b> was captured. For example, in the 3-D image <b>275</b> that comprises Frame <b>5</b>, a draft bounding box <b>285</b> corresponding to the particular car “Box ID <b>8705</b>” from other data points of the 3-D image <b>275</b> is automatically generated and presented by the tool <b>160</b> based on the previous bounding box <b>245</b>. When the user focuses attention on the draft bounding box <b>285</b> in the section <b>275</b> of the object identification portion <b>272</b> <i>a </i>of the display <b>270</b>, its corresponding indicia is displayed within the object labeling portion <b>272</b> <i>b </i>of the display <b>270</b>. For example, the unique identifier “Box ID <b>8705</b>” (reference <b>255</b>) and its associated label “car” (reference <b>260</b>) indicates the car of interest in the portion <b>272</b> <i>b</i>, and the frames field <b>258</b> indicates that said car has been tracked across Frames <b>1</b> through <b>5</b> of the 3-D video.</div>
<div class="description-paragraph" id="p-0121" num="0120">In the example scenario of <figref idrefs="DRAWINGS">FIG. 5C</figref>, the draft bounding box <b>285</b> is automatically generated by the tool <b>160</b> upon displaying Frame <b>5</b> of the 3-D video on the display <b>270</b>. In particular, the draft bounding box <b>285</b> is automatically generated by the tool <b>160</b> based on the sizes, orientations, and/or locations of previous bounding boxes distinguishing the car identified as “Box ID <b>8705</b>” within previous frames, such as the bounding box <b>245</b>. Additionally or alternatively, the tool <b>160</b> may perform image processing on Frame <b>5</b> to verify or determine the accuracy of the particular cluster of data points representing the car identified by “Box ID <b>8705</b>,” and the output of the image processing may inform the size, orientation, and/or location of the draft bounding box <b>285</b>. Any image processing of Frame <b>5</b> may be based on particular configurations and locations of clusters of data points that represented Box ID <b>8705</b> in previously identified and labeled frames, or at least some of the image processing may be independently performed without consideration to other frames.</div>
<div class="description-paragraph" id="p-0122" num="0121">Similar to that discussed above of the display <b>230</b> corresponding to Frame <b>1</b> of the 3-D video, in the display <b>270</b> corresponding to Frame <b>5</b> of the 3-D video, a corresponding 2-D image of the car “Box ID <b>8705</b>” (reference <b>288</b>) may be automatically distinguished within the 2-D image <b>278</b> by respective 2-D bounding box <b>290</b>, and various 2-D perspective views of the cluster of data points that is representative of the car of interest “Box ID <b>8705</b>” may be presented in sections <b>280</b> and <b>282</b> of the display <b>270</b>. Also similar to that discussed above with respect to the display <b>230</b>, in the display <b>270</b>, any user adjustments to any of the bounding boxes displayed in any of the portions <b>275</b>, <b>278</b>, <b>280</b>, <b>282</b> of the display <b>270</b> may be automatically reflected in the bounding boxes displayed in the other portions <b>275</b>, <b>278</b>, <b>280</b>, <b>282</b>.</div>
<div class="description-paragraph" id="p-0123" num="0122">Also similar to that discussed above for the display <b>230</b>, within the display <b>270</b>, any of the automatically generated bounding boxes on any of the sections <b>275</b>, <b>278</b>, <b>280</b>, <b>282</b> may be considered to be a draft or interim bounding box that the user may either accept, or modify and accept. When the user is satisfied with the bounding box <b>285</b>, the user may cause an indication of the association between the bounding box <b>245</b> corresponding to the representation of the car “Box ID <b>8705</b>” in Frame <b>5</b> and its respective label <b>260</b> to be stored as additional training data <b>152</b> for autonomous vehicle perception model <b>36</b>.</div>
<div class="description-paragraph" id="p-0124" num="0123"> <figref idrefs="DRAWINGS">FIGS. 5D and 5E</figref> illustrate an example usage scenario of the 3-D bounding boxes feature <b>215</b> provided by the tool <b>160</b> in which a user refines a draft 3-D bounding box across multiple perspective views of a 3-D source image. In particular, <figref idrefs="DRAWINGS">FIG. 5D</figref> depicts a screenshot of a display <b>300</b> that is presented on the graphical user interface <b>168</b> of the tool <b>160</b>, e.g., by executing at least some of the instructions <b>172</b> stored on the memory <b>165</b> of the tool <b>160</b>. The display <b>300</b> presents, on the user interface, Frame <b>2</b> of the 3-D video in which Frame <b>1</b> of <figref idrefs="DRAWINGS">FIG. 5B</figref> and Frame <b>5</b> of <figref idrefs="DRAWINGS">FIG. 5C</figref> are also included. In <figref idrefs="DRAWINGS">FIG. 5D</figref>, the user has finished identifying and labeling the car “Box ID <b>8705</b>” (reference <b>301</b>) across Frames <b>1</b> through <b>5</b> of the 3-D video, as is indicated in the object tracking portion <b>302</b> <i>b </i>of the display <b>300</b> by references <b>255</b>, <b>258</b> and <b>260</b>, and the user has navigated back to Frame <b>2</b> (reference <b>320</b>) of the 3-D video to identify and label another object that is depicted within the 3-D video.</div>
<div class="description-paragraph" id="p-0125" num="0124">In the display <b>300</b>, similar to the displays <b>230</b> and <b>270</b>, an object identification portion <b>302</b> <i>a </i>of the display <b>300</b> includes a first section <b>305</b> in which Frame <b>2</b> of the 3-D video is presented, a second section <b>308</b> in which a corresponding 2-D image that was captured by the RGB camera at the time that Frame <b>5</b> was captured by the lidar devices is presented, and third and fourth sections <b>310</b>, <b>312</b> each of which respectively displays a different 2-D perspective view of a 3-D cluster of data points to which focus is directed within the 3-D image <b>305</b>, e.g., by selection of the user or by other suitable means of focus. In the example usage scenario depicted by <figref idrefs="DRAWINGS">FIG. 5D</figref>, in the first section of the display, focus is directed to a 3-D cluster of data points that represent a truck, and the cluster has been indicated (e.g., automatically by the tool <b>160</b> and/or manually by the user, such as in a manner discussed above) by a draft 3-D bounding box <b>315</b>. The draft 3-D bounding box <b>315</b> has been assigned the identifier “Box ID <b>38</b>”, as illustrated by reference <b>318</b> in the object labeling section <b>302</b> <i>b </i>of the display <b>300</b>. In conjunction with the identifier “Box ID <b>38</b>”, the corresponding frame number of the 3-D video is indicated (reference <b>320</b>), however, the user has not yet associated a label with the object identified as “Box ID <b>38</b>” (reference <b>322</b>).</div>
<div class="description-paragraph" id="p-0126" num="0125">Accordingly, the corresponding 2-D image displayed in the second section <b>308</b> of the display <b>300</b> includes a 2-D representation of the same truck (reference <b>323</b> <i>a</i>) and its corresponding bounding box (reference <b>323</b> <i>b</i>). The third section <b>310</b> displays a top-down, 2-D perspective view of the 3-D cluster of data points indicated by the draft bounding box <b>315</b> (e.g., the truck), and the fourth section <b>312</b> of the display <b>300</b> presents a 2-D side view of the same.</div>
<div class="description-paragraph" id="p-0127" num="0126">In <figref idrefs="DRAWINGS">FIG. 5E</figref>, the user has changed the perspective view from which the 3-D environment image <b>305</b> is presented on the display <b>300</b>. The indicia <b>318</b>, <b>320</b>, <b>322</b> for “Box ID <b>38</b>” in the object labeling portion <b>302</b> <i>b </i>of the display <b>300</b> remains the same as in <figref idrefs="DRAWINGS">FIG. 5D</figref>, as the user is still the process of identifying and labeling the distinct object “Box ID <b>38</b>” and has not yet assigned a label <b>322</b> thereto. Similarly, the corresponding 2-D RGB image of the environment shown in section <b>308</b> of the object identification portion <b>302</b> <i>a </i>also remains the same as depicted in <figref idrefs="DRAWINGS">FIG. 5D</figref>, as the 2-D image <b>308</b> is not able to be manipulated by the user in a third dimension.</div>
<div class="description-paragraph" id="p-0128" num="0127">In <figref idrefs="DRAWINGS">FIG. 5E</figref>, the draft 3-D bounding box <b>315</b> that was presented on the first perspective view is included in the section <b>305</b> now displaying the environment from the second perspective view, and both the bounding box <b>315</b> and the cluster of data points that the draft bounding box <b>315</b> distinguishes from the remainder of the data points of the 3-D source image are displayed in the section <b>305</b> from the second perspective view. From the second perspective view, the user is able to further modify and/or adjust the draft bounding box <b>315</b> to refine the location, position, sizing, orientation, etc. of the box <b>315</b> to more accurately indicate the particular data points that the box <b>315</b> encompasses, as compared to only using the first particular perspective view to do so. For example, the second perspective view may visually display a part of a particular boundary of the object that was not visible from the first perspective view, and the user may refine the bounding box characteristics based on the now visible part of the particular boundary. Similar to that discussed above for the displays <b>230</b> and <b>270</b>, corresponding presentations of the draft 3-D bounding box <b>315</b> may be simultaneously presented in the portion <b>308</b>, <b>310</b>, and <b>312</b>. Note that in the portions <b>310</b> and <b>312</b>, the different 2-D perspective views of the cluster of data points representing “Box ID <b>38</b>” are depicted in <figref idrefs="DRAWINGS">FIG. 5B</figref> with reference to the presentation of said cluster of data points from the second perspective view of the 3-D environment image <b>305</b>.</div>
<div class="description-paragraph" id="p-0129" num="0128">The user is able to change the perspective view of the 3-D environment image <b>305</b> to any number of three-dimensional perspective views, as desired, and may further refine the draft bounding box <b>315</b> on each different perspective view, if desired. When the user is satisfied with the draft bounding box <b>315</b> corresponding to the representation of the truck “Box ID <b>38</b>” in Frame <b>2</b>, the user may accept or approve the draft bounding box <b>315</b>, associate a label <b>322</b> therewith, and store an indication of the association between the cluster of data points identified as “Box ID <b>38</b>” by the now-approved bounding box <b>315</b> and the label <b>322</b> for use as still additional training data <b>152</b> for autonomous vehicle perception models <b>36</b>.</div>
<div class="description-paragraph" id="p-0130" num="0129"> <figref idrefs="DRAWINGS">FIGS. 5F and 5G</figref> illustrate an example usage scenario in which the user utilizes the 3-D Paint feature <b>222</b> of the tool <b>160</b> to distinguish and identify various objects depicted within a 3-D source image. <figref idrefs="DRAWINGS">FIG. 5F</figref> depicts a screenshot of a 3-D source image <b>330</b> that is being presented on the graphical user interface <b>168</b> of the tool <b>160</b>, e.g., by executing at least some of the instructions <b>172</b> stored on the memory <b>165</b>. The 3-D source image <b>330</b> may be displayed, for example, on any portion of a graphical user interface that is designated for presenting 3-D images thereon, such as previously referenced portions <b>235</b>, <b>275</b>, and/or <b>305</b> included respectively on displays <b>230</b>, <b>270</b>, and <b>300</b>. In <figref idrefs="DRAWINGS">FIG. 5F</figref>, the user has invoked the 3-D Paint feature <b>220</b>, e.g., by activating the 3-D Paint <b>220</b> user control of the home screen <b>200</b>. The 3-D Paint feature <b>220</b> allows a user to easily indicate (or otherwise cause to be distinguished), within a 3-D source image, surface areas that are essentially planar, such as roads, center medians, lawns, driveways, parking lots, the ground, sides of retaining walls or barrier walls, etc.</div>
<div class="description-paragraph" id="p-0131" num="0130">In one of several features included in the 3-D Paint tool <b>220</b>, and similar to the “one-click” bounding box feature discussed above with respect to <figref idrefs="DRAWINGS">FIG. 5B</figref>, the 3-D Paint feature <b>220</b> allows a user to single-click anywhere on a cluster or grouping of points included in a 3-D image to thereby automatically select and visually distinguish/identify an intended surface area depicted therein. As such, said feature is referred to here is “one-click” or “point-and-fill” painting. For example, as shown in <figref idrefs="DRAWINGS">FIG. 5F</figref>, the user is hovering a cursor <b>335</b> over a particular location within the lower left portion of a 3-D source image <b>330</b>. Upon entering a single click, the tool <b>168</b> then automatically determines the surface area comprising a set of data points surrounding the clicked location and other data points that are contiguous thereto that are mostly likely to represent a same surface (which, in this image <b>330</b>, is a road), and automatically changes or modifies a visual property, on the source image, of the data points that are included in the contiguous surface area. The resulting visual property modification of the user invoking “point-and-fill” painting at the location <b>335</b> (which in this example scenario is a color change) is shown by reference <b>338</b> in <figref idrefs="DRAWINGS">FIG. 5G</figref>. That is, in response to only the single click at the particular location, the tool <b>160</b> automatically paints over or fills the contiguous surface area in which the particular location is included <b>338</b> with the changed visual property. In the example shown in <figref idrefs="DRAWINGS">FIG. 5G</figref>, the modified or changed visual property of the contiguous service area is a change in color, however any change in visual property may be utilized to distinguish surface areas within 3-D source images, e.g., color change, pattern change, dynamic treatment such as blinking, removal of a particular visual property, etc. In some implementations of the tool <b>160</b>, different visual properties of the 3-D Paint feature <b>220</b> may signify different types of surfaces. For example, the color red may denote a road, a flashing white may denote an ice patch, gray may denote a vertical surface such as a side of a retaining wall, etc.</div>
<div class="description-paragraph" id="p-0132" num="0131">Note that in <figref idrefs="DRAWINGS">FIG. 5G</figref>, while the changed visual property is applied to the group of points <b>338</b> representing the road, the changed visual property is not applied to the clusters of points that are representative of cars or other objects that are on or next to the road (denoted by references <b>340</b>, <b>342</b>), as the tool <b>160</b> has automatically determined that said clusters <b>340</b>, <b>342</b> do not represent the road. To that end, the data points identified by the tool <b>160</b> as being contiguous and representing a same surface may be determined, at least in part, based upon respective, relative levels of virtual elevations of surrounding data points. Thus, for some environments, the data points that are mostly likely to represent a same surface may be essentially planar (such as when said data points represent a vertical wall or a flat stretch of road). In other environments, the data points that are mostly likely to represent a same surface may have relative maximum differences of relative elevations therebetween (such as when said data points represent a road that goes up and down steep hills).</div>
<div class="description-paragraph" id="p-0133" num="0132">In an embodiment, the tool <b>160</b> invokes one or more image processors (e.g., in a manner similar to that discussed with respect to <figref idrefs="DRAWINGS">FIG. 5B</figref>) to ascertain which subsets of points included in the source image <b>300</b> are statistically likely to represent a road or other type of planar surface and therefore to which the changed visual property is to be applied, e.g., by using neural networks, trained machine-learning models, etc. In some implementations, the tool <b>160</b> may provide a one-to-many, group, or batch painting option, where a single point-and-fill user command causes the tool <b>160</b> to paint over (e.g., change a visual property of) all data clusters within the 3-D source image <b>330</b> that are most likely to represent roads or other essentially two-dimensional surfaces.</div>
<div class="description-paragraph" id="p-0134" num="0133">Thus, with the 3-D Paint tool <b>220</b>, a single user action (and only the single, user action) results in the automatic coverage of a surface area indicated by the single user action, e.g., via the application (or removal) of a visual property to data points representing the surface area. Similar to the “one-click” bounding boxes, it is noted that while the present disclosure refers to the user selecting a particular surface area using a single click, this is only an exemplary embodiment and worded as such for ease of reading and not limitation purposes. Indeed, a selection and painting of a particular surface area need not be generated via a single mouse click, but may be generated by any suitable single user activation of a corresponding user control.</div>
<div class="description-paragraph" id="p-0135" num="0134">In addition to the one-click or point-and-fill painting feature, the 3-D Paint tool <b>220</b> provides other features that may be useful to the user. For example, a user may select an option to utilize the cursor as a virtual paint brush, so that as a user moves the cursor/paint brush across a particular area of the source image, the visual property of the data points over which the paintbrush is moved is modified accordingly to thereby indicate or identify the surface (e.g., similar to virtually applying the visual property to the area with a brush). In another example, the user may select an option to switch the 3-D Paint feature <b>220</b> into an erase mode to thereby remove a visual property from a desired area of the source image. For instance, in a “one-click” or “point-and-fill” erase mode, a single click or selection of a location within the desired area may automatically cause the entirety of the contiguous area to be cleared of the visual property. In “custom” or “paint brush” erase mode, the user may move the cursor/paint brush across the desired area, thereby virtually erasing the visual property from the data points over which the cursor/paint brush is moving. The erase mode is especially useful for modifying or editing a previous single-click 3-D Paint command. For example, in <figref idrefs="DRAWINGS">FIG. 5G</figref>, a portion of the cluster of data points <b>340</b> representing a car that is traveling on the road <b>338</b> were “splashed” by paint as a result of the “one-click” user action of <figref idrefs="DRAWINGS">FIG. 5F</figref>. The user may switch the 3-D Paint feature <b>220</b> to the erase mode (as visually denoted by the change of the cursor <b>335</b> from being a pointer as in <figref idrefs="DRAWINGS">FIG. 5F</figref> to being a plus sign as in <figref idrefs="DRAWINGS">FIG. 5G</figref>), and use the virtual paint brush to remove or clean up the undesired visual property from selected data points included in the cluster <b>340</b> representing the car.</div>
<div class="description-paragraph" id="p-0136" num="0135">When the user is satisfied with the indications of the surface areas within the 3-D source image <b>330</b> (e.g., is satisfied with the sub-areas of the image <b>330</b> that have been “painted over,” the user may accept or approve said indications, associate one or more labels therewith, and store the indication the association between the surface areas identified the respective labels for use as training data <b>152</b> for autonomous vehicle perception models <b>36</b>, e.g., via an object labeling portion of the display on which the image <b>330</b> is presented (not shown in <figref idrefs="DRAWINGS">FIGS. 5F and 5G</figref>). Example labels for surfaces may include “road”, “shoulder”, “median”, “ground,” “driveway”, “wall”, and like. As such, a contiguous surface area that is identified and indicated within a 3-D source image via the graphical representation of a modified visual property may be considered as a distinct object within the source image and may be labeled as such.</div>
<div class="description-paragraph" id="p-0137" num="0136">Another useful feature provided by the system or tool <b>160</b> to help a user in identifying and labeling distinct objects within source images is Hide/Unhide. A user may select any objects represented on a source image (e.g., on sections of the displays <b>230</b>, <b>270</b>, <b>300</b>, <b>330</b>, or another display screen) that have been previously distinguished and identified (e.g., by a respective bounding box, paint, or some other graphical means of indication), and the user may instruct the tool <b>160</b> to hide the selected objects. In response, the tool <b>160</b> may render the selected objects as being non-visible or greyed-out within the source image. In an implementation, the tool <b>160</b> hides both the data points that have been indicated as representing the distinct object and the object's respective bounding box. Of course, with another instruction, the user may instruct the tool <b>160</b> to unhide distinct objects that were previously hidden. The Hide/Unhide tool is particularly useful for a user to reduce clutter while he or she is working on identifying and labeling the source image. For instance, a user may instruct the tool <b>160</b> to hide all objects that have been identified and labeled as “car”, thereby rendering such objects non-visible on the working display, and then the user may apply the Paint feature <b>220</b> as desired without inadvertently painting over any identified/labeled cars or portions thereof. In a sense, by using the hide feature, the user may ensure that the Paint feature does not splash onto other types of objects.</div>
<div class="description-paragraph" id="p-0138" num="0137"> <figref idrefs="DRAWINGS">FIGS. 5H and 5I</figref> illustrate an example usage scenario of the 3-D Lane Marking feature <b>218</b> provided by the system or tool <b>160</b>. The 3-D Lane Marking <b>218</b> feature allows a user to quickly and easily mark, delineate, or indicate the edge of a traffic lane within a source image, e.g., the side of a road, separate lanes of opposite-moving traffic, separate lanes of traffic moving in the same direction, etc. <figref idrefs="DRAWINGS">FIG. 5H</figref> depicts a screenshot of a display <b>360</b> that is presented on the graphical user interface <b>168</b> of the tool <b>160</b> (e.g., by executing at least some of the instructions <b>172</b> stored on the memory <b>165</b> of the tool <b>160</b>). Similar to the displays <b>230</b>, <b>270</b>, and <b>300</b>, the display <b>360</b> includes an object identification portion <b>362</b> <i>a </i>and an object labeling portion <b>362</b> <i>b</i>. Within the object identification portion <b>362</b> <i>a</i>, a first section <b>365</b> presents thereon a 3-D source image of an environment that was captured by lidar devices mounted on a vehicle, and a second section <b>368</b> presents thereon a corresponding 2-D source image that was captured by an RGB camera mounted on the same vehicle at a similar time at which the 3-D source image <b>365</b> was captured. The 3-D source image <b>365</b> depicts therein respective representations of a truck <b>369</b> and a cyclist <b>370</b> that are traveling in the same direction in respective, separate traffic lanes that are separated by a lane marker <b>372</b>. With respect to the direction of travel of the truck <b>369</b> and the cyclist <b>370</b>, the far right edge of the two lanes is depicted by the linear set of data points <b>375</b>, and the far left edge of the two lanes abutting the center median is depicted by the linear set of data points <b>378</b>.</div>
<div class="description-paragraph" id="p-0139" num="0138"> <figref idrefs="DRAWINGS">FIG. 5I</figref> depicts the display <b>360</b> in which the user has used the 3-D Lane Marking feature <b>218</b> to explicitly distinguish, identify, or indicate the lane marker <b>372</b> within the source image <b>365</b>. Specifically, a first location <b>380</b> <i>a </i>is initially designated within the source image <b>365</b> as being included in the edge of a lane or included in a lane marker <b>372</b> that is physically located within the environment, as is indicated by the round dot or point. The first location <b>380</b> <i>a </i>may be manually designated by the user via the 3-D Lane Marking tool <b>218</b> (e.g., via a mouse click or the activation of some other suitable user control) or, in some implementations, the first location <b>380</b> <i>a </i>may be automatically designated by the tool <b>160</b> (e.g., by utilizing one or more image processors in a manner such as described above for other features of the tool <b>160</b>). The user may subsequently designate one or more other locations <b>380</b> <i>b</i>, <b>380</b> <i>c </i>depicted within the source image <b>365</b> that correspond to respective other locations along the edge of the lane, e.g., that are included in the lane marker <b>372</b>. In most cases, the subsequent other locations <b>380</b> <i>b</i>, <b>380</b> <i>c </i>are non-adjacent to the location <b>380</b> <i>a</i>; however, the distance between the indicated locations <b>380</b> <i>a</i>, <b>380</b> <i>b</i>, <b>380</b> <i>c </i>may vary as a user desires and/or as needed. For example, if a road curves to an extreme degree, the indicated locations of the edge of the road may be nearer to one another, whereas for a relatively straight stretch of road, the indicated locations of the edge of the road may be spaced further apart. At any rate, upon the user indicating one or more locations <b>380</b> <i>a</i>-<b>380</b> <i>c </i>of the source image <b>365</b>, the user may activate a respective user control of the 3-D Lane Marking feature <b>218</b> that causes the tool <b>160</b> to interconnect the indicated locations <b>380</b> <i>a</i>-<b>380</b> <i>c </i>using respective graphical line segments <b>382</b> <i>a</i>, <b>382</b> <i>b</i>, thereby distinguishing, designating, or identifying the edge of the lane or the lane marker <b>372</b> from other distinct objects represented on the source image <b>365</b>. When the user is satisfied with the graphical representation of the lane marker <b>372</b>, the user may generate and associate a label for the lane marker <b>372</b>, e.g., via the object labeling portion <b>362</b> <i>b </i>of the display <b>360</b>. The labels for lane markers may be as simple as “Lane &lt;&lt;number&gt;&gt;”, as shown by reference <b>385</b> in <figref idrefs="DRAWINGS">FIG. 5I</figref>. In another embodiment (not shown), the lane marking labels may have a syntax similar to that of the binding boxes, e.g., “Lane ID &lt;&lt;alphanumeric ID&gt;&gt;: [Add Label]”, where the label may be designated via a drop-down menu, such as “median,” “shoulder”, “bike lane”, “opposite-traffic”, “same-direction traffic”, etc., or the label may be entered by the user using free-form text. Note that as shown in <figref idrefs="DRAWINGS">FIG. 5I</figref>, an indication of the number of indicated locations/points along the marked lane edge <b>372</b> may also be provided (reference <b>388</b>). The user may cause an indication of the association between the marked Lane <b>372</b> and its label <b>385</b> to be added to the training data <b>152</b> that is to be used for trading autonomous vehicle perception models <b>36</b>.</div>
<div class="description-paragraph" id="p-0140" num="0139">It is noted that while the example usage scenario depicted in <figref idrefs="DRAWINGS">FIGS. 5H and 5I</figref> uses the 3-D Lane Marking tool <b>218</b> to identify a lane <b>372</b> that is physically marked within the physical environment by paint, reflective pavement markers, non-reflected pavement markers, raised pavement markers, lane delineators, lane dividers, etc. (e.g., as denoted by reference <b>382</b> in section <b>368</b> of the display <b>360</b>), the 3-D Lane Marking tool <b>218</b> may also be used to mark lanes or edges of roads that are not physically delineated within the physical environment, for example, the edges of dirt roads or gravel roads.</div>
<div class="description-paragraph" id="p-0141" num="0140"> <figref idrefs="DRAWINGS">FIGS. 5J and 5K</figref> illustrate an example of another particular graphical user interface technique provided by the tool <b>160</b> that a user may use for identifying and labeling distinct objects within 3-D images for use in training machine-learning based models that are used to autonomously control vehicles, e.g., that of layering the 2-D and 3-D source images. Turning to <figref idrefs="DRAWINGS">FIG. 5J</figref>, the example display <b>400</b> includes a first portion <b>405</b> in which a 3-D video of an environment in which vehicles operate is presented or played, and includes the second portion <b>408</b> in which a 2-D video of the same environment is presented or played. In the embodiment depicted in <figref idrefs="DRAWINGS">FIG. 5J</figref>, a set of lidar devices mounted on a vehicle captured the 3-D video <b>405</b> and an RGB camera mounted on the vehicle simultaneously captured the 2-D video <b>408</b>. Thus, the videos <b>405</b>, <b>408</b> are presented or played on the display <b>400</b> in a side-by-side and synchronous manner, as denoted by the timestamps and progress bar at the bottom of the display <b>400</b> (reference <b>410</b>). As such, the 3-D video <b>405</b> may advance or rewind in a synchronous manner with the 2-D video <b>408</b>. For example, as shown in <figref idrefs="DRAWINGS">FIG. 5J</figref>, the RGB image of the environment that was being captured in the 2-D video at the time 0:42/1:44 (reference <b>410</b>) is displayed in the portion <b>408</b>, and the point cloud image of the environment that was being captured in the 3-D video at 0:42/1:44 is displayed in the portion <b>405</b>. In <figref idrefs="DRAWINGS">FIG. 5K</figref>, the user has advanced the videos and has paused them at the subsequent time 0:57/1:44 (reference <b>415</b>). Accordingly, the RGB image of the environment that was captured in the 2-D video at the time 0:57/1:44 is displayed in the portion <b>408</b>, and the point cloud image of the environment that was captured in the 3-D video at 0:57/1:44 is displayed in the portion <b>405</b>.</div>
<div class="description-paragraph" id="p-0142" num="0141">Significantly, within the display <b>400</b>, the tool <b>160</b> layers at least a portion of the 2-D video with the 3-D video, and displays the layered videos within the first portion <b>405</b>. In the embodiment illustrated in <figref idrefs="DRAWINGS">FIG. 5J</figref>, in the first portion <b>405</b> of the display <b>400</b>, several respective representations of respective distinct objects from the 2-D video have been layered with (e.g., overlaid on top of) the corresponding objects as represented within the 3-D video, thereby representing the distinct objects in the first portion by respective composite object images, each of which is comprised of both respective representative data points from the 2-D image and respective representative data points from the 3-D image. For example, in <figref idrefs="DRAWINGS">FIG. 5J</figref>, the two oncoming vehicles <b>412</b> <i>a</i>, <b>412</b> <i>b</i>, the two vehicles <b>412</b> <i>c</i>, <b>412</b> <i>d </i>that are traveling in the same direction as the vehicle on which the 2-D and 3-D cameras are mounted, various lane markers <b>412</b> <i>e</i>-<b>412</b> <i>g</i>, and a guardrail <b>412</b> <i>h </i>are depicted within the portion <b>405</b> of the display <b>400</b> by respective composite images as distinct objects. Importantly, the layered, 3-D image/video <b>405</b> and composite representation of distinct objects thereon allows the user to more easily visually interpret the more abstract 3-D source image <b>405</b> to thereby perform desired object identification and labeling of data points included therein. As composite object images include 2-D data points that are specifically associated with or tied to 3-D data points, as a 3-D source image is manipulated or explored on the display <b>400</b> in three dimensions, any 2-D data points included in a composite object image may be subject to the three dimensional manipulation/exploration in conjunction with the integrate or associated 3-D data points also included in the composite object image, thus may be displayed within the composite object image from different three-dimensional perspective view.</div>
<div class="description-paragraph" id="p-0143" num="0142">To illustrate, in <figref idrefs="DRAWINGS">FIG. 5K</figref>, the user has changed the perspective from which the 3-D video is being viewed in the portion <b>405</b> from the perspective view that was presented in <figref idrefs="DRAWINGS">FIG. 5J</figref> to a different perspective view. That is, the user has changed the virtual, 3-D camera angle from which the vehicle environment is presented in the portion <b>405</b> of the display <b>400</b> to being slightly elevated above the scene as compared to the perspective view shown in <figref idrefs="DRAWINGS">FIG. 5J</figref>. Note that, in <figref idrefs="DRAWINGS">FIG. 5K</figref>, the perspective view of the 2-D video in portion <b>408</b> of the display <b>400</b> remains the same as in <figref idrefs="DRAWINGS">FIG. 5J</figref>, as the 2-D video is not inherently able to be manipulated in a third dimension to vary the 3-D perspective view from which it is presented. However, the composite object images <b>412</b> <i>c</i>-<b>412</b> <i>h </i>depicted in <figref idrefs="DRAWINGS">FIG. 5J</figref> do appear in accord with the changed perspective view now displayed in <figref idrefs="DRAWINGS">FIG. 5K</figref>. (The composite object images representing the cars <b>412</b> <i>a </i>and <b>412</b> <i>b </i>in <figref idrefs="DRAWINGS">FIG. 5J</figref> are no longer depicted in <figref idrefs="DRAWINGS">FIG. 5K</figref>, as they have passed out of the field of view of the cameras.) This convenience is possible as, as discussed above, the composite object images <b>412</b> <i>c</i>-<b>412</b> <i>h </i>are formed by associating, combining, integrating, or linking respective 2-D data points of the 2-D source image with/to respective 3-D data points of the 3-D source image. As such, when a perspective view of the 3-D source image is modified, thereby presenting the 3-D data points that represent the distinct object from a different perspective view, the 2-D data points that are integrated therewith follow the 3-D data points in accordance with their layered, composite configuration. This layering feature allows the user to more easily ascertain, discern, and/or identify a same distinct object across multiple frames of the 3-D video <b>405</b>.</div>
<div class="description-paragraph" id="p-0144" num="0143">Further, not only does the tool <b>160</b> allows the user to change the perspective view of the 3-D, layered video being presented in the portion <b>405</b>, the tool <b>160</b> allows a user to dynamically vary the perspective view of the 3-D, layered video presented in the portion <b>405</b> while the video is being executed (e.g., played or dynamically advanced) on the display <b>400</b>. That is, while the 3-D video is being played, the user may, a respective user controls, virtually and dynamically navigate within the 3-D environment to view various compositely-represented objects therein from different perspective angles. For example, if the user is in the process of distinguishing and labeling the car <b>412</b> <i>c</i>, the user may dynamically change the virtual perspective view of the 3-D cameras (and therefore, the perspective views of the car <b>412</b> <i>c </i>and any other composite and non-composite object images depicted within the 3-D video) to find the most suitable perspective(s) via which the user may identify and distinguish the car <b>412</b> <i>c </i>from other data points of the 3-D video. For example, the user may dynamically cause the virtual 3-D camera to “fly” above a car <b>412</b> <i>c </i>(as shown in <figref idrefs="DRAWINGS">FIG. 5K</figref>), zoom in, zoom out, move to look at the left side of the car <b>412</b> <i>c</i>, move to look at the right side of the car <b>412</b> <i>c </i>and/or dynamically perform any other virtual movement within the displayed 3-D environment, all while the 3-D video continues to play. The user may pause the video(s) at any desired time and from any perspective view, and may define and/or refine the boundaries of the car <b>412</b> <i>c </i>within the displayed frame of the 3-D video <b>405</b>.</div>
<div class="description-paragraph" id="p-0145" num="0144">The layering of at least portions of a 2-D image with respective, time synchronized 3-D image may be performed by the tool <b>160</b> on any 3-D source image, irrespective of whether the 3-D source images is simultaneously displayed with a corresponding 2-D image or not. For example, in some implementations, the portion <b>408</b> may be omitted from the display <b>400</b>. The user thus is able to utilize said layering feature when he or she finds it useful to do so.</div>
<div class="description-paragraph" id="p-0146" num="0145">Any one or more of the techniques and features described above with respect to <figref idrefs="DRAWINGS">FIGS. 5A-5K</figref> that are provided by the system or tool <b>164</b> for identifying/distinguishing distinct objects within source images and associating respecting labels therewith for generating training data for training machine-learning based models that are used to autonomously control vehicles may be utilized individually or in combination when processing a source image. For example, a user may utilize the 3-D Bounding Boxes tool <b>215</b> and the 3-D Lane Markings tool <b>218</b> to initially distinguish and identify various cars and lanes depicted within a first frame of a 3-D video, and then may utilize 3-D Object Tracking <b>222</b> through subsequent frames to automatically carry forward the bounding boxes, lane markings, and associated labels that were approved in The first frame. Additionally, it is noted that although the techniques and features described above have been described with respect to 3-D source images, the majority of the above-discussed features may be individually and/or applied in combination to 2-D source images. For example, 2-D Bounding Boxes <b>205</b> may be applied to 2-D images in a manner similar to that described above for 3-D Bounding Boxes <b>215</b>, 2-D Lane Markings <b>208</b> may be applied to 2-D images in a manner similar to that described above for 3-D Lane Markings <b>218</b>, 2-D Object Tracking <b>212</b> may be applied to 2-D images in a manner similar to that described above for 3-D Object Tracking <b>222</b>, and 2-D Paint <b>210</b> may be applied to 2-D source images in a manner similar to that described above for 3-D Paint <b>220</b>. However, certain features described above may be applicable only to 3-D source images due to the inherent nature of 2-D images. For example, a user will typically not be able to manipulate a 2-D environment image to be viewed from different 3-D perspective views.</div>
<div class="description-paragraph" id="p-0147" num="0146">Example Methods and Functionalities</div>
<div class="description-paragraph" id="p-0148" num="0147"> <figref idrefs="DRAWINGS">FIGS. 6A-6E</figref> depict flow diagrams of example methods and/or functionalities that may be provided by the systems, methods, and techniques disclosed herein, e.g., as provided by the tool <b>160</b> and/or its prescribed functionalities <b>172</b> of <figref idrefs="DRAWINGS">FIG. 4</figref>. For ease of discussion, and not for limitation purposes, each of <figref idrefs="DRAWINGS">FIGS. 6A-6E</figref> is discussed below with simultaneous reference to <figref idrefs="DRAWINGS">FIGS. 1-4 and 5A-5K</figref>. In embodiments, at least a portion of each of the methods of <figref idrefs="DRAWINGS">FIGS. 6A-6E</figref> may be performed in conjunction with and/or by at least a part of any one or more of the systems described elsewhere herein. Additionally or alternatively, in embodiments, at least a portion of each of the methods of <figref idrefs="DRAWINGS">FIGS. 6A-6E</figref> may be performed in conjunction with at least a portion of any one or more of the other methods of <figref idrefs="DRAWINGS">FIGS. 6A-6E</figref>, and/or as described elsewhere herein.</div>
<div class="description-paragraph" id="p-0149" num="0148">Generally speaking, each of the methods discussed with respect to <figref idrefs="DRAWINGS">FIGS. 6A-6E</figref> is directed to identifying and labeling distinct objects within source images for training machine-learning based models that are used to autonomously control and/or operate vehicles. A source image may be one of a plurality of source images that are processed to generate or mine training data therefrom for training the models that (or that will) operate and/or control autonomous vehicles, for example. The processing of each of the source images includes distinguishing and/or identifying respective subsets of data points included in a total set of data points of which the source image comprises as being representative of respective, distinct objects located within the environment depicted by the source image, and associating respective labels therewith, thereby indicating various categories, classifications, or other indications of types of distinct objects represented by the respective subsets of data points.</div>
<div class="description-paragraph" id="p-0150" num="0149"> <figref idrefs="DRAWINGS">FIG. 6A</figref> depicts a flow chart of an example computer-implemented method <b>500</b> for identifying and/or distinguishing and labeling distinct objects that are depicted within images for training machine-learning based models that are used to autonomously control and/or operate vehicles. The method <b>500</b> includes displaying, on a user interface, a three-dimensional (3-D) image of an environment in which vehicles operate (block <b>502</b>), where the 3-D environment image depicts one or more physical objects located in the environment, and where the 3-D environment image is presented on the user interface from a first perspective view. Generally speaking, the 3-D environment image is made up of a set of data points. For instance, the 3-D environment image may comprise a point cloud dataset generated by one or more lidar camera devices mounted on a vehicle traversing through the environment. In other embodiments, the data points that make up the 3-D environment image include one or more other types of data (e.g., sonar data, infrared data, etc.) that has been generated by one or more other types of active sensing systems or devices. In some embodiments, at least a part of the 3-D environment image is constructed from one or more 2-D images of the environment. For example, at least a part of the 3-D environment image may be constructed by image processing and/or otherwise combining information provided by multiple RGB images of the environment that were obtained by different RGB cameras disposed at different locations within the environment. In some usage scenarios, the 3-D environment image may include one or more composite images of various objects located within the environment, where the one or more composite images have been generated from layering data points that are natively included in the 3-D environment image with corresponding native data points of a 2-D image of the environment, such as discussed above with respect to <figref idrefs="DRAWINGS">FIGS. 5J and 5K</figref>.</div>
<div class="description-paragraph" id="p-0151" num="0150">The method <b>500</b> also includes receiving, via one or more user controls provided by the user interface, an indication of a first graphical representation of a boundary of a particular object as depicted within the 3-D environment image from the first perspective view (block <b>505</b>). That is, a user may utilize one or more user controls to draw or otherwise create the first graphical representation of the boundary of the particular object within the 3-D environment image from the first perspective view. The first graphical representation may be of any desired format, such as a 3-D bounding box or set of lines, a 3-D lane marking, a 3-D painted surface area, or any other suitable graphical representation. The first graphical representation may be generated by any of the features <b>215</b>, <b>218</b>, <b>220</b>, <b>222</b> of the tool <b>160</b> described above, for example.</div>
<div class="description-paragraph" id="p-0152" num="0151">The method <b>500</b> includes generating, based on the first graphical representation, data indicative of the boundary of the particular object within the 3-D environment image from the first perspective view (block <b>508</b>). For example, the first graphical representation of the boundary of the particular object may define or otherwise indicate a particular subset of data points of the 3-D image from the first perspective view that, in turn, represents a particular, distinct object located in the environment. For example, data points that are encompassed by the first graphical representation may represent a particular distinct object, data points that are painted over by the first graphical representation may represent a particular, distinct object, etc. The generated data indicative of the boundary of particular object may indicate, in an embodiment, the particular subset of data points within the 3-D source image that represent the particular distinct object, and/or the particular data points within the 3-D source image that do not represent the particular distinct object.</div>
<div class="description-paragraph" id="p-0153" num="0152">Additionally, the method <b>500</b> includes obtaining an indication of a particular label for the particular, distinct object (block <b>510</b>). In an embodiment, the indication of the particular label for the particular object may be obtained via the user interface, for example, via a user selection of a label from a drop-down menu or similar, or via a user entering a free-form text label via the user interface. In an embodiment, the indication of the particular label for the particular object may be automatically generated, e.g., by the tool <b>160</b>. For example, the tool <b>160</b> may automatically generate the particular label based on the data indicative of the boundary the particular object within the 3-D environment image from the first perspective view. In an example implementation of automatic labeling, at least some of the data indicative of the boundary of the particular object may be input into a label prediction model that has been previously trained based on distinguished/identified and labeled objects depicted within a plurality of historical images of one or more environments in which vehicles operate, and the label prediction model may automatically generate a label based on the input data. In some embodiments, the tool automatically generates a draft label (e.g., via the label prediction model or via some other means), and the user may accept or approve the automatically-generated draft label, or the user may modify the automatically-generated draft label as desired, and then accept or approve the modified label for association with the indicated particular, distinct object depicted within the 3-D environment image from the first perspective view.</div>
<div class="description-paragraph" id="p-0154" num="0153">The method <b>500</b> may include generating, based on the data indicative of the boundary of the particular object within the 3-D environment image from the first perspective view, an association between the particular label and a 3-D image of the particular object within the 3-D environment image (block <b>512</b>), thereby distinguishing the 3-D image of the particular object within the 3-D environment image. The method <b>500</b> may further include storing an indication of the association between the particular label and the 3-D image of the particular object within the 3-D environment image in one or more tangible, non-transitory memories as a part of a training data set utilized to train one or more machine-learning based models (block <b>515</b>), where the one or more machine-learning based models are used to autonomously operate and/or control vehicles. In some embodiments, an indication of the particular label may be stored in conjunction with each data point included in the subset of data points of the 3-D image of the particular object within the 3-D image.</div>
<div class="description-paragraph" id="p-0155" num="0154">Additionally, the method <b>500</b> includes receiving, via the one or more user controls, an instruction to present the 3-D environment image on the user interface from a second perspective view different than the first perspective view (block <b>518</b>). Based on the received instruction, the method <b>500</b> may adjust or modify a presentation of the 3-D environment image on the user interface to be from the second perspective view (block <b>520</b>). Adjusting the presentation of the 3-D environment image to be from the second perspective view <b>520</b> may include rotating, in three dimensions, the 3-D environment image displayed on the user interface from the first perspective view, and/or rotating the virtual camera angle from which the 3-D environment image is presented. Additionally or alternatively, adjusting the presentation of the 3-D environment image <b>520</b> may include scaling and/or translating the presentation of the 3-D environment image in three dimensions and/or the virtual camera angle from which the 3-D environment image is presented. In an example implementation, the 3-D environment image may be included in a virtual reality representation of the environment. As such, in this example implementation, receiving the instruction to present the 3-D environment image from a second perspective view <b>518</b> may include receiving an indication of one or more spatial user interactions with the virtual reality environment, and adjusting the presentation of the 3-D environment image based on the received instruction <b>520</b> may include adjusting the presentation of the 3-D environment image in accordance with the one or more spatial user interactions.</div>
<div class="description-paragraph" id="p-0156" num="0155">The method <b>500</b> may include receiving, via the one or more user controls, an indication of a second graphical representation of the boundary of the particular object as depicted within the 3-D environment image from the second perspective view (block <b>522</b>), and generating, based on the second graphical representation, data indicative of the boundary of the particular object within the 3-D environment image from the second perspective view (block <b>525</b>). In an embodiment, receiving the second graphical representation of the boundary the particular object as depicted within the 3-D environment image from the second perspective view (block <b>522</b>) and generating data indicative of the particular object based on the second graphical representation (block <b>525</b>) may be performed in a manner similar to that discussed above for the blocks <b>505</b> and <b>508</b>.</div>
<div class="description-paragraph" id="p-0157" num="0156">In some embodiments, though, receiving the indication of the second graphical representation of the boundary of the particular object (block <b>522</b>) comprises receiving a modification to a draft or interim graphical representation of the boundary of the particular object. In such embodiments, the method <b>500</b> may include (e.g., at some time prior to receiving the indication of the second graphical representation <b>522</b>, and not shown in <figref idrefs="DRAWINGS">FIG. 6A</figref>), automatically generating and displaying a draft or interim graphical representation of the boundary of the particular object within the 3-D environment image from the second perspective view, e.g., by the tool <b>160</b>, and/or upon adjusting the presentation of the 3-D environment image to be from the second perspective view (block <b>520</b>). In an example implementation, the draft or interim graphical representation of the boundary of the particular object may be automatically generated by using a boundary prediction model that has been trained based on distinguished/identified, distinct objects depicted within a plurality of historical images of one or more environments in which vehicles operate. For example, data indicative of the first graphical representation of the boundary and/or data indicative of the change in or adjustment to the displayed perspective view of the source image may be input into the boundary prediction model, and the boundary prediction model may output the draft or interim graphical representation based on its received input.</div>
<div class="description-paragraph" id="p-0158" num="0157">Accordingly, in such embodiments in which a draft or interim graphical representation is automatically generated, the modification received at the block <b>522</b> may be applied to the draft or interim graphical representation, and accordingly, the modified draft/interim graphical representation may be the second graphical representation indicative of the boundary of the particular object within the 3-D environment image from the second perspective view. At any rate, irrespective of whether the second graphical representation is manually generated, is automatically generated, or is generated by using some combination of manual and automatic generation techniques, data indicative of the boundary of the particular object within the 3-D environment image from the second perspective view may be generated based on the second graphical representation (block <b>525</b>).</div>
<div class="description-paragraph" id="p-0159" num="0158">At the block <b>528</b>, the method <b>500</b> may include updating, based on the data indicative of the boundary of the particular object within the 3-D environment image from the second perspective view, the stored indication of the association between the particular label and the 3-D image of the particular object within the 3-D environment image (block <b>528</b>), thereby refining the distinguishing of the 3-D image of the particular object within the 3-D environment image.</div>
<div class="description-paragraph" id="p-0160" num="0159">In some embodiments (not shown in <figref idrefs="DRAWINGS">FIG. 6A</figref>), the method <b>500</b> includes displaying a 2-D image of the environment in conjunction with displaying 3-D image of the environment (block <b>502</b>) on the user interface. The 2-D image may have been captured by one or more passive imaging devices, systems, and/or cameras, such as an RGB camera or sensors that measure reflected light that has been emitted by the sun or by other external or third-party light sources, and the 2-D image and the 3-D image may have been captured at a similar or same time. For example, each of the 2-D image and the 3-D image may have a respective timestamp indicative of its time of capture, and the two timestamps may differ by only a short time interval (e.g., within a time interval of a half a second, within a time interval of one second, etc.). In arrangements in which the 2-D image is included in a 2-D video and the 3-D image is included in the 3-D video, the 2-D and 3-D videos may be played or otherwise presented on the user interface and a time-synchronized manner, e.g., in a manner such as previously described above.</div>
<div class="description-paragraph" id="p-0161" num="0160">In some embodiments, when the user has focused on a particular subset of data points included in the 3-D image of the environment that represent a distinct object (e.g., by manually selecting or otherwise indicating a cluster of group of data points included in the 3-D environment image), corresponding 2-D perspective views of the particular subset of data points that represent the particular object of focus may be displayed in respective portions of the user interface in conjunction with the display of the 3-D image on the user interface. For example, a top or overhead 2-D perspective view of the particular subset of data points may presented in a first portion of the user interface, and a side, 2-D perspective view of the particular subset of data points may be presented in a second portion of the user interface.</div>
<div class="description-paragraph" id="p-0162" num="0161">At any rate, the inclusion of various 2-D images on the user interface (e.g., a 2-D image of the environment that has been captured using passive sensing technology, and/or one or more 2-D images that provide different 2-D perspective views of the subset of data points included in the object of focus within the 3-D image) in conjunction with the 3-D environment image allows a user to more easily visually discern, distinguish, and/or identify particular, distinct objects that are located in the environment depicted by the 3-D environment image, and thereby more quickly and accurately define the boundaries of said objects as depicted within the 3-D environment image.</div>
<div class="description-paragraph" id="p-0163" num="0162">In some embodiments (not shown in <figref idrefs="DRAWINGS">FIG. 6A</figref>), the method <b>500</b> may include tracking the particular, distinct object across numerous frames, e.g., in scenarios in which the 3-D image is one of a plurality frames of a 3-D video. For example, after the particular, distinct object has been identified/discerned within a first frame (e.g., via the first graphical representation and/or the second graphical representation), a draft graphical representation of the boundary of the particular, distinct object within a second frame may be automatically generated upon the second frame being displayed on the user interface. In an implementation, the draft or interim graphical representation of the boundary of the particular object within the second frame may be automatically generated by using a boundary prediction model that has been trained based on distinguished/identified, distinct objects depicted within a plurality of historical images of one or more environments in which vehicles operate. For example, data indicative of the first graphical representation of the boundary within the first frame, data indicative of the second graphical representation of the boundary within the first frame, and/or data indicative of a subset of the data points included in the second frame may be input into the boundary prediction model, and the boundary prediction model may output the draft or interim graphical representation based on its received input. The user may accept or approve the draft or interim graphical representation that has been automatically generated for the object in the second frame. Alternatively, the user may modify or alter the draft or interim graphical representation within the second frame, and subsequently accept or approve the modified, draft or interim graphical representation as the defined graphical representation of the boundary of the particular object within the second frame. The label of the particular object (e.g., as previously associated in the first frame) may be automatically associated with the defined graphical representation of the boundary of the particular object within the second frame, an indication of said association may be stored as yet additional training data for the autonomous vehicle operation/control models.</div>
<div class="description-paragraph" id="p-0164" num="0163">The method <b>500</b> may include additional, less, or alternate actions, including those discussed elsewhere herein, e.g., in conjunction with other embodiments. In an example implementation, an embodiment of the method <b>500</b> may generate one or more of the displays and/or may utilize one or more of the features discussed with respect to <figref idrefs="DRAWINGS">FIGS. 5B-5E</figref>.</div>
<div class="description-paragraph" id="p-0165" num="0164"> <figref idrefs="DRAWINGS">FIG. 6B</figref> depicts a flow chart of an example computer-implemented method <b>540</b> for identifying and/or distinguishing and labeling distinct objects depicted within images for training machine-learning based models that are used to autonomously control and/or operate vehicles. The method <b>540</b> includes presenting, on a user interface of one or more computing devices, (i) a first frame comprising an image of an environment, at a first time, in which vehicles operate, the first frame depicting one or more physical objects located in the environment, and (ii) a first graphical representation indicating a boundary of a particular object located in the environment as depicted in the first frame at the first time (block <b>542</b>). Specifically, an association of data indicative of the boundary of the particular object as depicted within the first frame at the first time and a particular label of the particular object (i) distinguishes an image of the particular object within the first frame and (ii) is stored in one or more tangible, non-transitory memories as a part of a training data set utilized to train one or more machine-learning based models, where the one or more machine-learning based models used to autonomously operate and/or control vehicles. The first graphical representation may include any one or more graphical representations that are provided by the system or tool <b>160</b> as discussed above, e.g., bounding boxes, painting, line marking, layering, and/or other types of graphical representations, and may have been generated with or without human input.</div>
<div class="description-paragraph" id="p-0166" num="0165">The method <b>540</b> also includes presenting, on the user interface, a second frame comprising an image of the environment at a second time different than the first time, where the second frame depicts at least a portion of the particular object (block <b>545</b>); and automatically generating, based on the first graphical representation of the boundary of the particular object as depicted in the first frame, a draft or interim graphical representation of the boundary of the particular object as depicted within the second frame (block <b>548</b>). In an embodiment, automatically generating, based on the first graphical representation, the interim graphical representation of the boundary of the particular object as depicted within the second frame includes copying the first graphical representation from the first frame to the second frame, e.g., copying a size, orientation, and/or location of the first graphical representation. In another embodiment, automatically generating, based on the first graphical representation, the interim graphical representation of the boundary of the particular object as depicted within the second frame includes automatically predicting or determining that a particular subset of data points included in the second frame corresponds to the image of the particular object as depicted within the second frame, and presenting the interim graphical representation within the second frame at a location corresponding to the predicted or determined subset of data points. The automatic prediction may be based on, for example, the data indicative of the boundary of the particular object as depicted within the first frame, image processing at least a portion of the second frame, and/or one or more suitable data analytics techniques. For instance, the automatic prediction may utilize a boundary prediction model that has been trained based on identified objects depicted within a plurality of historical images of one or more environments in which vehicles operate, and in particular, historical images that include ordered and/or time-sequenced frames therein. The data indicative of the boundary of the particular object as depicted within the first frame and/or data indicative of at least a portion of data points included in the second frame may be input into the boundary prediction model, and based on its input, the boundary prediction model may generate the interim graphical representation, for example. At any rate, irrespective of the techniques utilized automatically generate interim graphical representation (block <b>548</b>), the interim graphical representation may be presented, on the user interface, within the second frame upon its automatic generation (block <b>550</b>).</div>
<div class="description-paragraph" id="p-0167" num="0166">The method <b>540</b> may include receiving, via the user interface, an indication of a user modification to the interim graphical representation (block <b>552</b>). For example, the indication of the user modification may include an indication of a modification to a visual property, orientation, size, configuration, and/or other property of the interim graphical representation. The method <b>540</b> may further include altering, based on the received indication of the user modification, the interim graphical representation to thereby generate a second graphical representation of the boundary of the particular object as depicted in the second frame at the second time (block <b>555</b>); and generating data indicative of the second graphical representation of the boundary of the particular object as depicted within the second frame (block <b>558</b>).</div>
<div class="description-paragraph" id="p-0168" num="0167">Additionally, the method <b>540</b> may include storing, in the one or more tangible, non-transitory memories, an association of the data indicative of the boundary of the particular object as depicted in the second frame at the second time and the particular label of the particular object as a part of the training data set utilized to train machine-learning models that autonomously operate and/or control vehicles, e.g., in conjunction with the association of data indicative of the boundary of the particular object as depicted within the first frame at the first time and the particular label of the particular object. In an embodiment, an indication of the particular label may be stored in conjunction with each pixel or data point that has been determined to be included in the representation of the particular object in the second frame.</div>
<div class="description-paragraph" id="p-0169" num="0168">In an embodiment, the first frame and the second frame operated on by the method <b>540</b> are included in a 2-D video that was generated by one or more passive sensing devices, systems, or cameras. For example, the first frame and the second frame may be included in a 2-D video that was generated by one or more RGB video cameras by measuring reflected light within the environment that was generated and/or emitted by the sun or by other external or third-party light sources, and generating the 2-D video therefrom.</div>
<div class="description-paragraph" id="p-0170" num="0169">In another embodiment, the first frame and the second frame operated on by the method <b>540</b> are included in the 3-D video that was generated by one or more active sensing devices, systems, or cameras. For example, the first frame and the second frame may be included in a 3-D video that was generated by one or more active cameras by generating and transmitting electromagnetic waves into their environment, measuring the backscatter that was reflected back to the 3-D cameras, and generating the 3-D video therefrom. In an example implementation, the one or more active cameras that generated the 3-D video include one or more lidar cameras, and the 3-D video comprises point cloud data. In this embodiment, as the first frame and the second frame are respective 3-D images, their respective presentations on the user interface able to be manipulated, changed, or varied in three dimensions so as to be viewed from different 3-D perspective views. Thus, in some scenarios, the perspective view of the environment as depicted in the second frame may be changed, by the user, to be different than the perspective view of the environment as depicted in the first frame.</div>
<div class="description-paragraph" id="p-0171" num="0170">The method <b>540</b> may include additional, less, or alternate actions, including those discussed elsewhere herein, e.g., in conjunction with other embodiments. In an example implementation, an embodiment of the method <b>540</b> may generate one or more of the displays and/or may utilize one or more of the features discussed with respect to <figref idrefs="DRAWINGS">FIGS. 5B-5E</figref>.</div>
<div class="description-paragraph" id="p-0172" num="0171"> <figref idrefs="DRAWINGS">FIG. 6C</figref> depicts a flow chart of an example computer-implemented method <b>570</b> for identifying and/or distinguishing and labeling distinct objects that are depicted within images for training machine-learning based models that are used to autonomously control and/or operate vehicles. The method <b>570</b> includes displaying, on a user interface of one or more computing devices, an image of an environment in which vehicles operate (block <b>572</b>), where the image depicts one or more physical objects located in the environment, and providing, via the user interface, a paint user control for use by a user to indicate, distinguish, and/or identify areas within images displayed on the user interface as particular distinct objects (block <b>575</b>).</div>
<div class="description-paragraph" id="p-0173" num="0172">The method <b>570</b> also includes receiving, via a user activation of the paint user control, an indication of an area within the image that is to be “painted” (block <b>578</b>), e.g., an indication of an area within the image to which a visual property is to be applied, removed, or modified. The indicated area of the image typically is a subset of a total area of the image, and typically depicts a surface area located in the environment, such as a road, a driveway, the ground, the side of a guard rail or retaining wall, or another type of surface, which may be essentially planar (such as a flat road), or may be limited in its changes in elevation throughout the surface area (such as a hilly road). Further, the method <b>570</b> includes modifying, by the one or more computing devices and based upon the received indication of the area, a visual property of the indicated area (block <b>580</b>), to thereby distinguish the area as a distinct object within the source image.</div>
<div class="description-paragraph" id="p-0174" num="0173">In an embodiment, the user activation of the paint user control received at the block <b>578</b> is sole or single user action (e.g., “one-click” or “point-and-fill”), such as a single mouse click while hovering the mouse over a particular location, that automatically causes the visual property of the entirety of the area to be modified (block <b>580</b>). As such, with one-click or point-and-fill painting, the user no longer needs to first block out, outline, or trace the boundaries of a surface area that is to be painted over using a first set of user actions, and then use additional user actions to paint or effect the modified visual property over the defined surface area. Instead, a user merely points and clicks once on a single location on the source image (block <b>578</b>), and the tool <b>160</b> automatically determines a contiguous surface area that includes the clicked location and applies, removes, or modifies a visual property of the contiguous surface area (block <b>580</b>).</div>
<div class="description-paragraph" id="p-0175" num="0174">In this embodiment, automatically determining a contiguous surface area in which the clicked location is included may be performed by the method <b>570</b> by utilizing an object identification model that has been trained based on identified (and possibly labeled) objects depicted within a plurality of historical images of one or more environments in which vehicles operate. Accordingly, the indication of the location selected by the user via the paint user control and/or at least some of the data points included in the source image may be provided as input to the object identification model, and based on its input, the object identification model may generate the contiguous surface area corresponding to the clicked location.</div>
<div class="description-paragraph" id="p-0176" num="0175">In some embodiments, the paint user control may be additionally or alternatively utilized as a paint brush, a paint spill, an eraser, or any desired type of usage to thereby indicate areas within the source image that are to be labeled, e.g., such as previously described with respect to <figref idrefs="DRAWINGS">FIGS. 5F and 5G</figref>. Accordingly, in these embodiments, the block <b>580</b> may include using multiple user actions or interactions with the paint user control to modify the visual property of an indicated area.</div>
<div class="description-paragraph" id="p-0177" num="0176">The method <b>570</b> additionally includes obtaining, by the one or more computing devices, an indication of a particular label for the indicated area of the image (block <b>582</b>). In an embodiment, the indication of the particular label for the particular object may be obtained via the user interface, for example, via a user selection of a label from a drop-down menu or similar, or via a user entering a free-form text label via the user interface. In an embodiment, the indication of the particular label for the particular object may be automatically generated. For example, the tool <b>160</b> may automatically generate the particular label based on the user-indicated area within the 3-D environment image. In an example implementation of automatic labeling, at least some of data indicative of the user-indicated area may be input into a label prediction model that has been previously trained based on distinguished/identified and labeled objects depicted within a plurality of historical images of one or more environments in which vehicles operate, and the label prediction model may automatically generate a label based on the input data. In some embodiments, the tool automatically generates a draft label (e.g., via the label prediction model or via some other means). The user may accept or approve the automatically-generated draft label, or the user may modify the automatically-generated draft label as desired, and then accept or approve the modified label for association with the indicated particular, distinct object depicted within the 3-D environment image from the first perspective view.</div>
<div class="description-paragraph" id="p-0178" num="0177">In some embodiments, the label is included in a plurality of labels, each of which is associated with a different type of surface area, and each of which is denoted by a different visual property. For example, the label “road” may be denoted by the color red, whereas the label “ice patch” may be denoted by flashing.</div>
<div class="description-paragraph" id="p-0179" num="0178">It is noted that the block <b>582</b> may be performed by the method <b>570</b> either prior to or upon the execution of blocks <b>578</b>, <b>580</b>. In an example scenario, a user may first select and modify a visual property of a particular surface area depicted within the source image via the blocks <b>578</b>, <b>580</b>, and subsequently may obtain a corresponding label via the block <b>582</b>. In another example scenario, a user may first obtain a label via the block <b>582</b>, and subsequently may select and modify the visual property of one or more surface areas that are depicted within the source image that are to be associated with the obtained label via the blocks <b>578</b>, <b>580</b>.</div>
<div class="description-paragraph" id="p-0180" num="0179">The method <b>570</b> additionally includes storing, by the one or more computing devices in one or more tangible, non-transitory memories, an indication of an association between data indicative of the indicated area of the image and the particular label (block <b>585</b>), thereby distinguishing the indicated area from other areas of the image. In an embodiment, an indication of the particular label may be stored in conjunction with each pixel or data point that has been determined to be included in the representation of the surface area of focus within the source image.</div>
<div class="description-paragraph" id="p-0181" num="0180">The method <b>570</b> may include additional, less, or alternate actions, including those discussed elsewhere herein, e.g., in conjunction with other embodiments. For example, the method <b>550</b> may be executed conjunction with the Hide/Unhide feature provided by the tool <b>160</b>. In an example implementation, an embodiment of the method <b>570</b> may generate one or more of the displays and/or may utilize one or more features discussed with respect to <figref idrefs="DRAWINGS">FIGS. 5F-5G</figref>.</div>
<div class="description-paragraph" id="p-0182" num="0181"> <figref idrefs="DRAWINGS">FIG. 6D</figref> depicts a flow chart of an example computer-implemented method <b>600</b> for identifying/distinguishing and labeling distinct objects that are depicted within images for training machine-learning based models that are used to autonomously control and/or operate vehicles. The method <b>600</b> includes displaying, on a user interface of one or more computing devices, an image of an environment in which vehicles operate (block <b>602</b>), where the image depicts one or more physical objects located in the environment; and providing, on the user interface, a lane-marking user control for use by a user to indicate lane markings within the image (block <b>605</b>).</div>
<div class="description-paragraph" id="p-0183" num="0182">The method <b>600</b> also includes receiving, via the lane-marking user control, a user selection of a first location within the image that is non-adjacent to a second location within the image (block <b>608</b>). The first location and the second location indicate respective endpoints of a segment of an edge of a traffic lane depicted within the image of the environment, in an example. The edge of the physical traffic lane may be physically marked within the physical environment, e.g., by painted lines, raised or flat pavement markers, reflectors, etc., or the edge of the physical traffic lane may not be physically marked within the physical environment, such as when the physical traffic lane is a gravel road or a dirt road. While the first location is selected by the user (block <b>608</b>), the second location may be manually or automatically indicated. For example, in an embodiment, the method <b>600</b> may receive a user selection of the second location. In another embodiment, the second location may be automatically determined, e.g., by using one or more image processing techniques.</div>
<div class="description-paragraph" id="p-0184" num="0183">The method <b>600</b> includes, based upon the second location and the received user selection of the first location, automatically generating and displaying on the image, by the one or more computing devices, a marking indicative of the segment of the edge of the traffic lane (block <b>610</b>). For example, the marking indicative of the segment of the edge of the traffic lane may be represented by a line or other suitable indicator. Additionally, the method includes storing, by the one or more computing devices in one or more tangible, non-transitory memories, an indication of an association between a particular label for the traffic lane and data indicative of the segment of the edge of the traffic lane (block <b>612</b>), thereby distinguishing the traffic lane from other areas and/or objects depicted within the image. In an embodiment, an indication of the particular label may be stored in conjunction with each pixel or data point that has been determined to be included in the representation of the edge of the traffic lane of focus within the source image.</div>
<div class="description-paragraph" id="p-0185" num="0184">In an embodiment (not depicted in <figref idrefs="DRAWINGS">FIG. 6D</figref>), the method <b>600</b> may further include automatically determining, by the one or more computing devices and based upon the data indicative of the edge of the traffic lane, a second segment of the edge of the traffic lane that abuts the first segment. For example, the one or more computing devices may perform an image analysis of a portion of the source image that surrounds the first location or that surrounds a second location, and determine a third location to which the edge of the traffic lane extends from either the first location or the second location. The method <b>600</b> may automatically generate and display a second marking indicative of the second segment of the edge of the traffic lane, where the second segment has an endpoint at the third location, and may store, and the one or more memories, an indication of an association between the particular label and data indicative of the second segment of the edge of the traffic lane, thereby further establishing the traffic lane from other areas and/or objects depicted within the source image. In some situations, the second marking may be a draft or interim marking which the user may modify to generate a modified second marking to thereby indicate the second segment of the edge of the traffic lane abutting the first segment.</div>
<div class="description-paragraph" id="p-0186" num="0185">In an embodiment (not shown), the method <b>600</b> may be performed in conjunction with adjusting the perspective view from which the source image is presented on the user interface, e.g., when the source image is a 3-D image, thereby allowing the user to have more accuracy in selecting the first location, for example.</div>
<div class="description-paragraph" id="p-0187" num="0186">In an embodiment, the method <b>600</b> may be executed across multiple frames of a video. For example, the block <b>602</b>-<b>612</b> may be performed for first frame of the video to thereby mark and label a first segment of the edge of the traffic lane. Subsequently (not shown in <figref idrefs="DRAWINGS">FIG. 5D</figref>), a second frame may be presented on the user interface where the second frame depicts at least a part of the traffic lane that was marked and labeled in the first frame. In this embodiment, the method <b>600</b> may automatically determine, based upon the data indicative of the first segment of the edge of the traffic lane, at least one endpoint of an abutting segment of the edge of the traffic lane that is displayed in the second frame, and may automatically mark the abutting segment in the second frame. For example, the abutting segment may be determined based upon image analysis that is performed on the second frame, and/or based on data indicative of the first segment of the edge of the traffic lane represented in the first frame. The user may approve the automatically generated and marked abutting segment, or the user may make desire modifications thereto prior to approval. The method <b>600</b> may include storing, in the one or more memories, an indication of an association between the particular label for the traffic lane and data indicative of the abutting segment of the edge of the traffic lane as depicted within the second frame.</div>
<div class="description-paragraph" id="p-0188" num="0187">The method <b>600</b> may include additional, less, or alternate actions, including those discussed elsewhere herein, e.g., in conjunction with other embodiments. In an example implementation, an embodiment of the method <b>600</b> may generate one or more of the displays and/or may utilize one or more features discussed with respect to <figref idrefs="DRAWINGS">FIGS. 5H-5I</figref>.</div>
<div class="description-paragraph" id="p-0189" num="0188"> <figref idrefs="DRAWINGS">FIG. 6E</figref> depicts a flow chart of an example computer-implemented method <b>620</b> for identifying and/or distinguishing and labeling distinct objects depicted within images for training machine-learning based models that are used to autonomously control and/or operate vehicles. The method <b>620</b> includes displaying, on a user interface, a three-dimensional (3-D) image of an environment in which vehicles operate (block <b>622</b>), where the 3-D environment image depicts one or more physical objects located in the environment, and where the 3-D environment image is able to be displayed from multiple perspective views on the user interface in response user input. In an embodiment, the 3-D image comprises a point cloud dataset generated by one or more active sensing devices located within the environment.</div>
<div class="description-paragraph" id="p-0190" num="0189">The method <b>620</b> also includes layering a two-dimensional (2-D) image of at least a portion of the environment with the 3-D environment image, where the 2-D environment image depicts a particular physical object, at least a part of which is also depicted in the 3-D environment image (block <b>625</b>). For example, a portion of the 2-D image that represents a particular object may be overlaid onto a portion of the 3-D image that also represents the particular object, and respective overlay data points of the 2-D and 3-D images may be associated, linked, combined, or integrated, thereby generating a composite image of the particular physical object, where the composite image of the particular physical object includes both a portion of the 2-D environment image and a portion of the 3-D environment image. In an embodiment, layering at least a portion of the 2-D image on the 3-D image (block <b>625</b>) may be performed in response to a user action, such as the user dragging and dropping the 2-D image (or, a selectable object included therein) onto the 3-D image, or the user indicating that the tool <b>106</b> should enter into a layering mode. The method <b>620</b> includes displaying, on the user interface, the composite image of the particular object within the 3-D image of the environment (block <b>628</b>).</div>
<div class="description-paragraph" id="p-0191" num="0190">Further, the method <b>620</b> includes receiving, via one or more user controls provided by the user interface, an indication of a boundary of the particular physical object depicted by the composite image within the 3-D environment image (block <b>630</b>). For example, a user may indicate the boundary via a graphical representation such as a bounding box, a paint treatment, a lane marking, or any other suitable graphical representation. In an embodiment, a draft or interim boundary of the particular physical object depicted by the composite image may be automatically generated, e.g., by using one or more of the analytic techniques described above, and the user may modify the draft or interim boundary via the user controls provided by the user interface (block <b>630</b>). At any rate, whether the boundary of the composite object is manually generated, automatically generated, or generated via combination of manual and automatic methods, the method <b>620</b> may include generating data indicative of the boundary of the particular physical object within the 3-D environment image (block <b>632</b>), for example, based upon the graphical representation indicated or approved by the user.</div>
<div class="description-paragraph" id="p-0192" num="0191">Still further, the method <b>620</b> includes receiving an indication of a particular label for the particular physical object (block <b>635</b>); associating the particular label in with the data indicative of the boundary of the particular physical object within the 3-D environment image (block <b>638</b>), thereby distinguishing the particular physical object within the 3-D environment image; and storing an indication of the association between the particular label and the data indicative of the boundary of the particular physical object within the 3-D environment image in one or more tangible memories as a part of a training data set utilized to train one or more machine-learning based models (block <b>640</b>), the one or more machine-learning based models used to autonomously operate vehicles. In an embodiment, an indication of the particular label may be stored in conjunction with each pixel or data point included in the composite object, irrespective of a pixel/data point being native to the 2-D image or being native to the 3-D image.</div>
<div class="description-paragraph" id="p-0193" num="0192">Composite object images may be viewed from different 3-D perspectives as composite object images inherently include data points native to the 3-D image. As such, a composite object may be presented on the user for interface in concert with various perspective views from which the 3-D image is presented on the user interface, and information provided by the native 2-D data points that are integrated with the 3-D data points may be presented in concert with various perspective views of the 3-D image. Indeed, in scenarios in which the 3-D image is included in a 3-D video, a user may dynamically vary the perspective view from which the 3-D video is displayed while the video is playing, and as such the perspective view from which any composite object images depicted therein will also dynamically vary in accordance with the 3-D video. A user may pause the video at any frame and in any perspective view to define and/or refine a boundary of a composite object. Accordingly, the boundary of a composite object may be defined and/or refined across multiple perspective views and/or multiple frames, and respective data indicative thereof may be stored in conjunction with the label for the composite object as training data.</div>
<div class="description-paragraph" id="p-0194" num="0193">The method <b>620</b> may include additional, less, or alternate actions, including those discussed elsewhere herein, e.g., in conjunction with other embodiments. In an example implementation, an embodiment of the method <b>620</b> may generate one or more of the displays and/or may utilize one or more features discussed with respect to <figref idrefs="DRAWINGS">FIGS. 5J-5K</figref>.</div>
<div class="description-paragraph" id="p-0195" num="0194">General Considerations</div>
<div class="description-paragraph" id="p-0196" num="0195">Embodiments of the techniques described in the present disclosure may include any number of the following aspects or features, either alone or in combination:</div>
<div class="description-paragraph" id="p-0197" num="0196">1. A computer-implemented method for identifying and labeling objects within images for training machine-learning based models that are used to autonomously control vehicles, the method comprising: displaying, on a user interface, a three-dimensional (3-D) image of an environment in which vehicles operate, the 3-D environment image depicting one or more physical objects located in the environment, and the 3-D environment image presented on the user interface from a first perspective view; receiving, via one or more user controls provided by the user interface, an indication of a first graphical representation of a boundary of a particular object as depicted within the 3-D environment image from the first perspective view; and generating, based on the first graphical representation, data indicative of the boundary of the particular object within the 3-D environment image from the first perspective view.</div>
<div class="description-paragraph" id="p-0198" num="0197">The method further comprises: obtaining an indication of a particular label for the particular object; generating, based on the data indicative of the boundary of the particular object within the 3-D environment image from the first perspective view, an association between the particular label and a 3-D image of the particular object within the 3-D environment image, thereby distinguishing the 3-D image of the particular object within the 3-D environment image; and storing an indication of the association between the particular label and the 3-D image of the particular object within the 3-D environment image in one or more tangible, non-transitory memories as a part of a training data set utilized to train one or more machine-learning based models, the one or more machine-learning based models used to autonomously control vehicles.</div>
<div class="description-paragraph" id="p-0199" num="0198">The method still further comprises: receiving, via the one or more user controls, an instruction to present the 3-D environment image on the user interface from a second perspective view different than the first perspective view, and based on the received view perspective instruction, adjusting a presentation of the 3-D environment image on the user interface to be from the second perspective view; receiving, via the one or more user controls, an indication of a second graphical representation of the boundary of the particular object as depicted within the 3-D environment image from the second perspective view; generating, based on the second graphical representation, data indicative of the boundary of the particular object within the 3-D environment image from the second perspective view; and updating, based on the data indicative of the boundary of the particular object within the 3-D environment image from the second perspective view, the stored indication of the association between the particular label and the 3-D image of the particular object within the 3-D environment image, thereby refining the distinguishing of the 3-D image of the particular object within the 3-D environment image.</div>
<div class="description-paragraph" id="p-0200" num="0199">2. The computer-implemented method of aspect 1, wherein: the 3-D environment image consists of a set of data points; and generating, based on the first graphical representation, the data indicative of the boundary of the particular object within the 3-D environment image from the first perspective view comprises determining, based on the first graphical representation, a subset of the set of data points of the 3-D environment image as being included in the 3-D image of the particular object.</div>
<div class="description-paragraph" id="p-0201" num="0200">3. The computer-implemented method of aspect 2, wherein storing the indication of the association between the particular label and the 3-D image of the particular object within the 3-D environment image comprises storing data indicative of the particular label in association with each data point included in the subset of data points.</div>
<div class="description-paragraph" id="p-0202" num="0201">4. The computer-implemented method of aspect 2, wherein the set of data points includes point cloud data.</div>
<div class="description-paragraph" id="p-0203" num="0202">5. The computer-implemented method of aspect 4, wherein the point cloud data is generated by one or more active sensing systems or devices.</div>
<div class="description-paragraph" id="p-0204" num="0203">6. The computer-implemented method of aspect 5, wherein the one or more active sensing systems or devices include one or more lidar devices.</div>
<div class="description-paragraph" id="p-0205" num="0204">7. The computer-implemented method of aspect 1, wherein displaying the 3-D environment image on the user interface comprises displaying a 3-D image of the environment that has been at least partially generated by image processing a two-dimensional (2-D) image of the environment.</div>
<div class="description-paragraph" id="p-0206" num="0205">8. The computer-implemented method of aspect 1, wherein obtaining the indication of the particular label for the particular object comprises receiving the indication of the particular label for the particular object via the user interface.</div>
<div class="description-paragraph" id="p-0207" num="0206">9. The computer-implemented method of aspect 1, wherein obtaining the indication of the particular label comprises automatically generating the particular label for the particular object based upon the data indicative of the boundary of the particular object within the 3-D environment image.</div>
<div class="description-paragraph" id="p-0208" num="0207">10. The computer-implemented method of aspect 9, wherein automatically generating the particular label based upon the data indicative of the boundary of the particular object within the 3-D environment image comprises providing at least some of the data indicative of the boundary of the particular object within the 3-D environment image as input into a label prediction model that has been trained based on identified and labeled objects depicted within a plurality of historical images of one or environments in which vehicles operate.</div>
<div class="description-paragraph" id="p-0209" num="0208">11. The computer-implemented method of aspect 1, wherein: the method further comprises automatically generating and displaying an interim graphical representation of the boundary of the particular object within the 3-D environment image from the second perspective view based on the first graphical representation of the boundary of the particular object on the 3-D environment image from the first perspective view; and receiving, via the one or more user controls, the indication of the second graphical representation of the boundary of the particular object as depicted within the 3-D environment image from the second perspective view comprises receiving, via the one or more user controls, a modification to the interim graphical representation and applying the modification to the interim graphical representation, thereby generating the second graphical representation.</div>
<div class="description-paragraph" id="p-0210" num="0209">12. The computer-implemented method of aspect 11, wherein automatically generating the interim graphical representation comprises automatically determining an initial modification to the first graphical representation by utilizing a boundary prediction model that has been trained based on identified objects depicted within a plurality of historical images of one or more environments in which vehicles operate, and applying the initial modification to the first graphical representation, thereby generating the interim graphical representation.</div>
<div class="description-paragraph" id="p-0211" num="0210">13. The computer-implemented method of aspect 1, wherein adjusting the presentation of the 3-D environment image to be from the second perspective view comprises rotating, in three dimensions, the 3-D environment image displayed on the user interface.</div>
<div class="description-paragraph" id="p-0212" num="0211">14. The computer-implemented method of aspect 13, wherein adjusting the presentation of the 3-D environment image to be from the second perspective view further comprises at least one of scaling or translating, in three dimensions, the 3-D environment image displayed on the user interface.</div>
<div class="description-paragraph" id="p-0213" num="0212">15. The computer-implemented method of aspect 1, wherein: the 3-D environment image is included in a virtual reality representation of the environment presented via the user interface; and receiving the instruction to present the 3-D environment image on the user interface from the second perspective view and adjusting the presentation of the 3-D environment image on the user interface based on the received view perspective instruction comprises receiving, via the user interface, one or more user interactions with the virtual reality representation of the environment and automatically responding to the one or more user interactions.</div>
<div class="description-paragraph" id="p-0214" num="0213">16. The computer-implemented method of aspect 1, wherein receiving, via the one or more user controls, the indication of the first graphical representation of the boundary of the particular object comprises determining, based on one or more activations of the one or more user controls, two or more connected line segments collectively indicating the boundary of the particular object.</div>
<div class="description-paragraph" id="p-0215" num="0214">17. The computer-implemented method of aspect 16, wherein: the one or more activations of the one or more user controls is a single activation of the one or more user controls; and the method further comprises automatically determining the two or more connected line segments based on the single activation of the one or more user controls, including utilizing a boundary prediction model that has been trained based on identified objects depicted within a plurality of historical images of one or more environments in which vehicles operate.</div>
<div class="description-paragraph" id="p-0216" num="0215">18. The computer-implemented method of aspect 1, wherein receiving, via the one or more user controls, the indication of the boundary of the particular object comprises receiving, via the one or more user controls, an indication of one or more surface areas of the particular object.</div>
<div class="description-paragraph" id="p-0217" num="0216">19. The computer-implemented method of aspect 1, wherein receiving, via the one or more user controls, the indication of the boundary of the particular object comprises receiving, via the one or more user controls, respective indications of two points defining endpoints of at least a portion of the boundary.</div>
<div class="description-paragraph" id="p-0218" num="0217">20. The computer-implemented method of aspect 1, further comprising: receiving, via the user interface, an instruction to hide respective 3-D images of one or more selected objects depicted within the 3-D environment image; and based on the received hiding instruction, greying out or rendering non-visible the respective 3-D images of the one or more selected objects depicted within the 3-D environment image while maintaining respective levels of visibility of respective 3-D images of other objects depicted within the 3-D environment image.</div>
<div class="description-paragraph" id="p-0219" num="0218">21. The computer-implemented method of aspect 1, further comprising displaying a two-dimensional (2-D) image of the environment on the user interface in conjunction with displaying the 3-D environment image on the user interface.</div>
<div class="description-paragraph" id="p-0220" num="0219">22. The computer-implemented method of aspect 21, wherein displaying the 2-D image of the environment on the user interface comprises displaying a plurality of 2-D images of the environment on the user interface, each 2-D image of the environment depicting a different fixed perspective view of a respective at least a portion of the 3-D environment image.</div>
<div class="description-paragraph" id="p-0221" num="0220">23. The computer-implemented method of aspect 21, wherein displaying the 2-D image of the environment comprises displaying a 2-D, visual image of the environment generated by one or more passive imaging sensors, devices, or cameras.</div>
<div class="description-paragraph" id="p-0222" num="0221">24. The computer-implemented method of aspect 21, further comprising simultaneously displaying, on the user interface, both the first graphical representation of the boundary of the particular object on the 2-D environment image and the first graphical representation of the boundary of the particular object on the 3-D environment image.</div>
<div class="description-paragraph" id="p-0223" num="0222">25. The computer-implemented method of aspect 24, further comprising at least one of: causing a user manipulation of the first graphical representation of the boundary of the particular object displayed on the 2-D environment image to be automatically reflected on the 3-D environment image; or causing a user manipulation of the first graphical representation of the boundary of the particular object displayed on the 3-D environment image to be automatically reflected on the 2-D environment image.</div>
<div class="description-paragraph" id="p-0224" num="0223">26. The computer-implemented method of aspect 21, wherein displaying the 3-D environment image comprises displaying a point cloud data set.</div>
<div class="description-paragraph" id="p-0225" num="0224">27. The computer-implemented method of aspect 1, the 3-D environment image is generated by one or more active imaging sensors or devices.</div>
<div class="description-paragraph" id="p-0226" num="0225">28. The computer-implemented method of aspect 21, wherein: the 2-D image is layered on top of or under the 3-D image, thereby forming a composite image of at least one of the one or more physical objects located in the environment; and the indication of the first graphical representation of the boundary of the particular object is based on the composite image.</div>
<div class="description-paragraph" id="p-0227" num="0226">29. The computer-implemented method of aspect 1, wherein receiving, via the one or more user controls, the indication of the first graphical representation of the boundary of the particular object comprises: receiving, via the one or more user controls, a user selection of the particular object; and based upon the user selection of the particular object, automatically generating and displaying, on the user interface, an interim graphical representation of the boundary of the particular object corresponding to the first graphical representation.</div>
<div class="description-paragraph" id="p-0228" num="0227">30. The computer-implemented method of aspect 29, further comprising: receiving, via the user interface, a user modification to the interim graphical representation of the boundary of the particular object; and updating, based on the user modification, the display of the interim graphical representation of the boundary of the particular object on the user interface to thereby generate the first graphical representation.</div>
<div class="description-paragraph" id="p-0229" num="0228">31. The computer-implemented method of aspect 1, wherein: the 3-D environment image is a first frame depicting the environment at a first time; and the method further comprises displaying, on the user interface, a second frame including a 3-D image depicting the environment at a second time different than the first time, the second frame including a respective 3-D image of the particular object, and the second frame including an interim graphical representation of the boundary of the particular object as depicted within the second frame, the interim graphical representation generated based on the first graphical representation of the boundary of the particular object or the second graphical representation of the boundary of the particular object as depicted within the first frame; and receiving a modification to the interim graphical representation of the boundary of the particular object displayed in the second frame.</div>
<div class="description-paragraph" id="p-0230" num="0229">The method further includes, based upon the received modification: altering, on the user interface, the interim graphical representation in accordance with the received modification thereby generating and displaying a third graphical representation of the boundary of the particular object within the second frame and associated with the second time; generating, based on the third graphical representation of the boundary of the particular object within the second frame, data indicative of the boundary of the particular object within the second frame; and storing, in the one or more tangible, non-transitory memories as another part of the training data set, an indication of an association between the particular label and the data indicative of the boundary of the particular object within the second frame.</div>
<div class="description-paragraph" id="p-0231" num="0230">32. The computer-implemented method of aspect 31, wherein receiving the modification to the interim graphical representation of the boundary of particular object displayed in the second frame comprises receiving, via the one or more user controls provided by the user interface, a user modification to at least a portion of the interim graphical representation of the boundary of the particular object displayed in the second frame.</div>
<div class="description-paragraph" id="p-0232" num="0231">33. The computer-implemented method of aspect 31, further comprising automatically determining at least a portion of the modification to the interim graphical representation of the boundary of the particular object displayed in the second frame by utilizing a boundary prediction model that has been trained based on identified objects depicted within a plurality of historical images of one or more environments in which vehicles operate; and wherein receiving the modification to the interim graphical representation of the boundary of the particular object displayed in the second frame comprises receiving the automatically determined at least the portion of the modification to the interim graphical representation of the boundary of the particular object displayed in the second frame.</div>
<div class="description-paragraph" id="p-0233" num="0232">34. A system for identifying and labeling objects within images for training machine-learning based models that are used to autonomously operate vehicles, the system comprising: a communication module; one or more processors; and one or more non-transitory, tangible memories coupled to the one or more processors and storing computer executable instructions thereon that, when executed by the one or more processors, cause the system to: display, on a user interface, a three-dimensional (3-D) image of an environment in which vehicles operate, the 3-D environment image depicting one or more physical objects located in the environment, and the 3-D environment image presented on the user interface from a first perspective view; receive, via the communication module, an indication of a first graphical representation of a boundary of a particular object as depicted within the 3-D environment image from the first perspective view, the first graphical representation generated via one or more user controls provided by the user interface; and generate, based on the first graphical representation, data indicative of the boundary of the particular object within the 3-D environment image from the first perspective view.</div>
<div class="description-paragraph" id="p-0234" num="0233">The computer executable instructions are further executable by the one or more processors to cause the system to: obtain an indication of a particular label for the particular object; generate, based on the data indicative of the boundary of the particular object within the 3-D environment image from the first perspective view, an association between the particular label and a 3-D image of the particular object within the 3-D environment image, thereby distinguishing the 3-D image of the particular object within the 3-D environment image; and store an indication of the association between the particular label and the 3-D image of the particular object within the 3-D environment image in the one or more tangible, non-transitory memories as a part of a training data set utilized to train one or more machine-learning based models, the one or more machine-learning based models used to autonomously control vehicles.</div>
<div class="description-paragraph" id="p-0235" num="0234">The computer executable instructions are still further executable by the one or more processors to cause the system to: receive, via the communication module, a user instruction to present the 3-D environment image on the user interface from a second perspective view different than the first perspective view; based on the received view perspective user instruction, adjust a presentation of the 3-D environment image on the user interface to be from the second perspective view; receive, via the communication module, an indication of a second graphical representation of the boundary of the particular object as depicted within the 3-D environment image from the second perspective view, the second graphical representation generated via the one or more user controls provided by the user interface; generate, based on the second graphical representation, data indicative of the boundary of the particular object within the 3-D environment image from the second perspective view; and update, based on the data indicative of the boundary of the particular object within the 3-D environment image from the second perspective view, the stored indication of the association between the particular label and the 3-D image of the particular object within the 3-D environment image, thereby refining the distinguishing of the 3-D image of the particular object within the 3-D environment image.</div>
<div class="description-paragraph" id="p-0236" num="0235">35. The system of aspect 34, wherein: the 3-D environment image consists of a set of data points; the 3-D image of the particular object consists of a subset of the set of data points; and the data indicative of the particular label is stored particularly in association with data indicative of the subset of data points.</div>
<div class="description-paragraph" id="p-0237" num="0236">36. The system of aspect 35, wherein the data indicative of the particular label is stored in association with each data point included in the subset of data points.</div>
<div class="description-paragraph" id="p-0238" num="0237">37. The system of aspect 35, wherein the set of data points includes point cloud data.</div>
<div class="description-paragraph" id="p-0239" num="0238">38. The system of aspect 37, wherein the point cloud data is generated by one or more active sensing systems or devices.</div>
<div class="description-paragraph" id="p-0240" num="0239">39. The system of aspect 38, wherein the one or more active sensing systems or devices include one or more lidar devices.</div>
<div class="description-paragraph" id="p-0241" num="0240">40. The system of aspect 34, wherein the 3-D image of the environment has been at least partially generated by image processing a two-dimensional (2-D) image of the environment.</div>
<div class="description-paragraph" id="p-0242" num="0241">41. The system of aspect 34, wherein the indication of the particular label is obtained via the user interface.</div>
<div class="description-paragraph" id="p-0243" num="0242">42. The system of aspect 34, wherein the particular label is automatically generated based upon the data indicative of the boundary of the particular object within the 3-D environment image.</div>
<div class="description-paragraph" id="p-0244" num="0243">43. The system of aspect 42, wherein the particular label is automatically generated by providing at least some of the data indicative of the boundary of the particular object within the 3-D environment image as input into a label prediction model that has been trained based on identified and labeled objects depicted within a plurality of historical images of one or environments in which vehicles operate.</div>
<div class="description-paragraph" id="p-0245" num="0244">44. The system of aspect 34, wherein: the computer executable instructions are further executable by the one or more processors to cause the system to automatically generate and display an interim graphical representation of the boundary of the particular object within the 3-D environment image from the second perspective view; and the indication of the second graphical representation of the boundary of the particular object as depicted within the 3-D environment image from the second perspective view comprises a modification to the interim graphical representation displayed within the 3-D environment image from the second perspective view, the modification received via the one or more user controls provided by the user interface.</div>
<div class="description-paragraph" id="p-0246" num="0245">45. The system of aspect 44, wherein the automatic generation of the interim graphical representation comprises a copy of the first graphical representation.</div>
<div class="description-paragraph" id="p-0247" num="0246">46. The system of aspect 44, wherein the automatic generation of the interim graphical representation utilizes a boundary prediction model that has been trained based on identified objects depicted within a plurality of historical images of one or more environments in which vehicles operate.</div>
<div class="description-paragraph" id="p-0248" num="0247">47. The system of aspect 34, wherein the adjustment of the presentation of the 3-D environment image to be from the second perspective view comprises a rotation, in three dimensions, of the 3-D environment image presented on the user interface.</div>
<div class="description-paragraph" id="p-0249" num="0248">48. The system of aspect 47, wherein the adjustment of the presentation of the 3-D environment image to be from the second perspective view further comprises at least one of a scaling or a translation, in three dimensions, of the 3-D environment image presented on the user interface.</div>
<div class="description-paragraph" id="p-0250" num="0249">49. The system of aspect 34, wherein: the 3-D environment image is included in a virtual reality representation of the environment presented via the user interface; and the adjustment of the presentation of the 3-D environment image on the user interface is in response to one or more user interactions with the virtual reality representation of the environment.</div>
<div class="description-paragraph" id="p-0251" num="0250">50. The system of aspect 34, wherein the indication of the boundary of the particular object within the 3-D environment image comprises an indication of two or more connected line segments collectively indicating the boundary of the particular object received via the one or more user controls.</div>
<div class="description-paragraph" id="p-0252" num="0251">51. The system of aspect 50, wherein the two or more connected line segments are indicated by a single activation of a user control.</div>
<div class="description-paragraph" id="p-0253" num="0252">52. The system of aspect 34, wherein the indication of the boundary of the particular object within the 3-D environment image comprises an indication of one or more surface areas of the particular object received via the one or more user controls.</div>
<div class="description-paragraph" id="p-0254" num="0253">53. The system of aspect 34, wherein the indication of the boundary of the particular object within the 3-D environment image comprises respective indications of two points defining endpoints of at least a portion of the boundary received via the one or more user controls.</div>
<div class="description-paragraph" id="p-0255" num="0254">54. The system of aspect 34, wherein the computer executable instructions are further executable by the one or more processors to cause the system to: receive, via the communication module, a user instruction to hide respective 3-D images of one or more selected objects depicted within the 3-D environment image; and based on the received hiding instruction, grey-out or render non-visible the respective 3-D images of the one or more selected objects depicted within the 3-D environment image while maintaining respective levels of visibility of respective 3-D images of other objects depicted within the 3-D environment image.</div>
<div class="description-paragraph" id="p-0256" num="0255">55. The system of aspect 34, wherein the 3-D image of the environment is displayed in a first portion of the user interface, and the computer executable instructions are further executable to display a 2-D image of the environment on a second portion of the user interface.</div>
<div class="description-paragraph" id="p-0257" num="0256">56. The system of aspect 55, wherein the 2-D image of the environment is included in a plurality of 2-D images of the environment, each of which is displayed on a respective portion of the user interface, and each of which depicts a different fixed perspective view of a respective at least a portion of the 3-D environment image.</div>
<div class="description-paragraph" id="p-0258" num="0257">57. The system of aspect 55, wherein the 2-D image is generated by one or more passive imaging sensors, devices, or cameras.</div>
<div class="description-paragraph" id="p-0259" num="0258">58. The system of aspect 55, wherein the first graphical representation is simultaneously displayed on both the 2-D environment image and the 3-D environment image.</div>
<div class="description-paragraph" id="p-0260" num="0259">59. The system of aspect 58, wherein at least one of: a user manipulation of the first graphical representation displayed on the 2-D environment image is automatically reflected on the 3-D environment image; or a user manipulation of the first graphical representation displayed on the 3-D environment image is automatically reflected on the 2-D environment image.</div>
<div class="description-paragraph" id="p-0261" num="0260">60. The system of aspect 55, wherein: the 2-D image is included in a 2-D video presented on the second portion of the user interface; the 3-D image is included in a 3-D video presented on the first portion of the user interface; and the presentation of the 2-D video on the second portion of the user interface tracks the presentation of the 3-D video on the first portion of the user interface.</div>
<div class="description-paragraph" id="p-0262" num="0261">61. The system of aspect 55, wherein the 3-D environment image comprises a point cloud data set generated by one or more active imaging sensors or devices.</div>
<div class="description-paragraph" id="p-0263" num="0262">62. The system of aspect 55, wherein: the 2-D image is layered on top of or under the 3-D image, thereby forming a composite image of at least one of the one or more physical objects located in the environment; and the indication of the first graphical representation of the boundary of the particular object is based on the composite image.</div>
<div class="description-paragraph" id="p-0264" num="0263">63. The system of aspect 34, wherein: the indication of the boundary of the particular object comprises a user selection of the particular object; and the computer executable instructions are further executable by the one or more processors to cause the system to automatically generate and display an interim graphical representation of the boundary of the particular object based on the user selection of the particular object, the interim graphical representation corresponding to the first graphical representation.</div>
<div class="description-paragraph" id="p-0265" num="0264">64. The system of aspect 63, wherein the computer executable instructions are further executable by the one or more processors to cause the system to: receive, via the communication module, a user modification to the interim graphical representation of the boundary of the particular object; and update, based on the user modification, the display of the interim graphical representation of the boundary of the particular object to thereby generate the first graphical representation.</div>
<div class="description-paragraph" id="p-0266" num="0265">65. The system of aspect 34, wherein the 3-D environment image is a first frame depicting the environment at a first time, the stored data indicative of the boundary of the particular object within the first frame is data indicative of the boundary of the particular object at the first time, and the computer executable instructions are further executable by the one or more processors to cause the system to: display, on the user interface, a second frame depicting the environment at a second time different than the first time, the second frame including an interim graphical representation of the boundary of the particular object as depicted within the second frame, the interim graphical representation generated based on the first graphical representation of the boundary of the particular object or the second graphical representation of the boundary of the particular object as depicted within the first frame; and obtain a modification to the interim graphical representation of the boundary of the particular object as depicted in the second frame.</div>
<div class="description-paragraph" id="p-0267" num="0266">The computer executable instructions are still further executable by the one or processors to cause the system to, based upon the received modification: alter, on the user interface, the interim graphical representation in accordance with the received modification thereby generating and displaying a third graphical representation of the boundary of the particular object within the second frame; generate, based on the third graphical representation of the boundary of the particular object within the second frame, data indicative of the boundary of the particular object within the second frame; and store, in the one or more tangible, non-transitory memories as another part of the training data set, an indication of an association between the particular label and 3-D image of the particular object within the second frame.</div>
<div class="description-paragraph" id="p-0268" num="0267">66. The system of aspect 65, wherein the obtained modification is a user modification obtained via the user interface.</div>
<div class="description-paragraph" id="p-0269" num="0268">67. The system of aspect 65, wherein the obtained modification is automatically generated by utilizing a boundary prediction model that has been trained based on identified objects depicted within a plurality of historical images of one or more environments in which vehicles operate.</div>
<div class="description-paragraph" id="p-0270" num="0269">68. Any one of the previous aspects in combination with any other one of the previous aspects.</div>
<div class="description-paragraph" id="p-0271" num="0270">In some cases, a computing device may be used to implement various modules, circuits, systems, methods, or algorithm steps disclosed herein. As an example, all or part of a module, circuit, system, method, or algorithm disclosed herein may be implemented or performed by a general-purpose single- or multi-chip processor, a digital signal processor (DSP), an ASIC, a FPGA, any other suitable programmable-logic device, discrete gate or transistor logic, discrete hardware components, or any suitable combination thereof. A general-purpose processor may be a microprocessor, or, any conventional processor, controller, microcontroller, or state machine. A processor may also be implemented as a combination of computing devices, e.g., a combination of a DSP and a microprocessor, a plurality of microprocessors, one or more microprocessors in conjunction with a DSP core, or any other such configuration.</div>
<div class="description-paragraph" id="p-0272" num="0271">In particular embodiments, one or more implementations of the subject matter described herein may be implemented as one or more computer programs (e.g., one or more modules of computer-program instructions encoded or stored on a computer-readable non-transitory storage medium). As an example, the steps of a method or algorithm disclosed herein may be implemented in a processor-executable software module which may reside on a computer-readable non-transitory storage medium. In particular embodiments, a computer-readable non-transitory storage medium may include any suitable storage medium that may be used to store or transfer computer software and that may be accessed by a computer system. Herein, a computer-readable non-transitory storage medium or media may include one or more semiconductor-based or other integrated circuits (ICs) (such, as for example, field-programmable gate arrays (FPGAs) or application-specific ICs (ASICs)), hard disk drives (HDDs), hybrid hard drives (HHDs), optical discs (e.g., compact discs (CDs), CD-ROM, digital versatile discs (DVDs), blue-ray discs, or laser discs), optical disc drives (ODDs), magneto-optical discs, magneto-optical drives, floppy diskettes, floppy disk drives (FDDs), magnetic tapes, flash memories, solid-state drives (SSDs), RAM, RAM-drives, ROM, SECURE DIGITAL cards or drives, any other suitable computer-readable non-transitory storage media, or any suitable combination of two or more of these, where appropriate. A computer-readable non-transitory storage medium may be volatile, non-volatile, or a combination of volatile and non-volatile, where appropriate.</div>
<div class="description-paragraph" id="p-0273" num="0272">In some cases, certain features described herein in the context of separate implementations may also be combined and implemented in a single implementation. Conversely, various features that are described in the context of a single implementation may also be implemented in multiple implementations separately or in any suitable sub-combination. Moreover, although features may be described above as acting in certain combinations and even initially claimed as such, one or more features from a claimed combination may in some cases be excised from the combination, and the claimed combination may be directed to a sub-combination or variation of a sub-combination.</div>
<div class="description-paragraph" id="p-0274" num="0273">While operations may be depicted in the drawings as occurring in a particular order, this should not be understood as requiring that such operations be performed in the particular order shown or in sequential order, or that all operations be performed. Further, the drawings may schematically depict one more example processes or methods in the form of a flow diagram or a sequence diagram. However, other operations that are not depicted may be incorporated in the example processes or methods that are schematically illustrated. For example, one or more additional operations may be performed before, after, simultaneously with, or between any of the illustrated operations. Moreover, one or more operations depicted in a diagram may be repeated, where appropriate. Additionally, operations depicted in a diagram may be performed in any suitable order. Furthermore, although particular components, devices, or systems are described herein as carrying out particular operations, any suitable combination of any suitable components, devices, or systems may be used to carry out any suitable operation or combination of operations. In certain circumstances, multitasking or parallel processing operations may be performed. Moreover, the separation of various system components in the implementations described herein should not be understood as requiring such separation in all implementations, and it should be understood that the described program components and systems may be integrated together in a single software product or packaged into multiple software products.</div>
<div class="description-paragraph" id="p-0275" num="0274">Various implementations have been described in connection with the accompanying drawings. However, it should be understood that the figures may not necessarily be drawn to scale. As an example, distances or angles depicted in the figures are illustrative and may not necessarily bear an exact relationship to actual dimensions or layout of the devices illustrated.</div>
<div class="description-paragraph" id="p-0276" num="0275">The scope of this disclosure encompasses all changes, substitutions, variations, alterations, and modifications to the example embodiments described or illustrated herein that a person having ordinary skill in the art would comprehend. The scope of this disclosure is not limited to the example embodiments described or illustrated herein. Moreover, although this disclosure describes or illustrates respective embodiments herein as including particular components, elements, functions, operations, or steps, any of these embodiments may include any combination or permutation of any of the components, elements, functions, operations, or steps described or illustrated anywhere herein that a person having ordinary skill in the art would comprehend.</div>
<div class="description-paragraph" id="p-0277" num="0276">The term “or” as used herein is to be interpreted as an inclusive or meaning any one or any combination, unless expressly indicated otherwise or indicated otherwise by context. Therefore, herein, the expression “A or B” means “A, B, or both A and B.” As another example, herein, “A, B or C” means at least one of the following: A; B; C; A and B; A and C; B and C; A, B and C. An exception to this definition will occur if a combination of elements, devices, steps, or operations is in some way inherently mutually exclusive.</div>
<div class="description-paragraph" id="p-0278" num="0277">As used herein, words of approximation such as, without limitation, “approximately, “substantially,” or “about” refer to a condition that when so modified is understood to not necessarily be absolute or perfect but would be considered close enough to those of ordinary skill in the art to warrant designating the condition as being present. The extent to which the description may vary will depend on how great a change can be instituted and still have one of ordinary skill in the art recognize the modified feature as having the required characteristics or capabilities of the unmodified feature. In general, but subject to the preceding discussion, a numerical value herein that is modified by a word of approximation such as “approximately” may vary from the stated value by ±0.5%, ±1%, ±2%, ±3%, ±4%, ±5%, ±10%, ±12%, or ±15%.</div>
<div class="description-paragraph" id="p-0279" num="0278">As used herein, the terms “first,” “second,” “third,” etc. may be used as labels for nouns that they precede, and these terms may not necessarily imply a particular ordering (e.g., a particular spatial, temporal, or logical ordering). As an example, a system may be described as determining a “first result” and a “second result,” and the terms “first” and “second” may not necessarily imply that the first result is determined before the second result.</div>
<div class="description-paragraph" id="p-0280" num="0279">As used herein, the terms “based on” and “based at least in part on” may be used to describe or present one or more factors that affect a determination, and these terms may not exclude additional factors that may affect a determination. A determination may be based solely on those factors which are presented or may be based at least in part on those factors. The phrase “determine A based on B” indicates that B is a factor that affects the determination of A. In some instances, other factors may also contribute to the determination of A. In other instances, A may be determined based solely on B.</div>
</div>
</div>
</section><section itemprop="claims" itemscope="">
<h2>Claims (<span itemprop="count">29</span>)</h2>
<div html="" itemprop="content"><div class="claims" lang="EN" load-source="patent-office" mxw-id="PCLM175456165">
<claim-statement>What is claimed:</claim-statement>
<div class="claim"> <div class="claim" id="CLM-00001" num="00001">
<div class="claim-text">1. A computer-implemented method for identifying and labeling objects within images for training machine-learning based models that are used to autonomously control vehicles, the method comprising:
<div class="claim-text">displaying, on a user interface, a three-dimensional (3-D) image of an environment in which vehicles operate, the 3-D environment image depicting one or more physical objects located in the environment, and the 3-D environment image presented on the user interface from a first perspective view;</div>
<div class="claim-text">receiving, via one or more user controls provided by the user interface and displayed on the user interface in conjunction with the 3-D environment image from the first perspective view, an indication of a graphical representation of a boundary of a particular object as depicted within the 3-D environment image from the first perspective view;</div>
<div class="claim-text">generating, based on the graphical representation, data indicative of the boundary of the particular object within the 3-D environment image from the first perspective view;</div>
<div class="claim-text">obtaining an indication of a particular label for the particular object, the particular label uniquely identifying the particular object;</div>
<div class="claim-text">generating, based on the data indicative of the boundary of the particular object within the 3-D environment image from the first perspective view, an association between the particular label uniquely identifying the particular object and a 3-D image of the particular object within the 3-D environment image, thereby distinguishing the 3-D image of the particular object within the 3-D environment image;</div>
<div class="claim-text">storing an indication of the association between the particular label uniquely identifying the particular object and the 3-D image of the particular object within the 3-D environment image in one or more tangible, non-transitory memories as a part of a training data set utilized to train one or more machine-learning based models, the one or more machine-learning based models used to autonomously control vehicles;</div>
<div class="claim-text">receiving, via the one or more user controls, an instruction to present the 3-D environment image on the user interface from a second perspective view different than the first perspective view, and based on the received view perspective instruction, adjusting a presentation of the 3-D environment image on the user interface to be from the second perspective view so that the 3-D environment image from the second perspective view and the graphical representation of the boundary of the particular object are displayed on the user interface;</div>
<div class="claim-text">receiving, via the one or more user controls, an indication of a refinement to the graphical representation of the boundary of the particular object as depicted within the 3-D environment image from the second perspective view;</div>
<div class="claim-text">generating, based on the refined graphical representation, data indicative of the boundary of the particular object within the 3-D environment image from the second perspective view; and</div>
<div class="claim-text">updating, based on the data indicative of the boundary of the particular object within the 3-D environment image from the second perspective view, the stored indication of the association between the particular label uniquely identifying the particular object and the 3-D image of the particular object within the 3-D environment image, thereby refining the distinguishing of the 3-D image of the particular object within the 3-D environment image.</div>
</div>
</div>
</div> <div class="claim-dependent"> <div class="claim" id="CLM-00002" num="00002">
<div class="claim-text">2. The computer-implemented method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein:
<div class="claim-text">the 3-D environment image consists of a set of data points; and</div>
<div class="claim-text">generating, based on the graphical representation, the data indicative of the boundary of the particular object within the 3-D environment image from the first perspective view comprises determining, based on the graphical representation, a subset of the set of data points of the 3-D environment image as being included in the 3-D image of the particular object.</div>
</div>
</div>
</div> <div class="claim-dependent"> <div class="claim" id="CLM-00003" num="00003">
<div class="claim-text">3. The computer-implemented method of <claim-ref idref="CLM-00002">claim 2</claim-ref>, wherein storing the indication of the association between the particular label uniquely identifying the particular object and the 3-D image of the particular object within the 3-D environment image comprises storing data indicative of the particular label uniquely identifying the particular object in association with each data point included in the subset of data points.</div>
</div>
</div> <div class="claim-dependent"> <div class="claim" id="CLM-00004" num="00004">
<div class="claim-text">4. The computer-implemented method of <claim-ref idref="CLM-00002">claim 2</claim-ref>, wherein the set of data points includes point cloud data generated by one or more active sensing systems or devices.</div>
</div>
</div> <div class="claim-dependent"> <div class="claim" id="CLM-00005" num="00005">
<div class="claim-text">5. The computer-implemented method of <claim-ref idref="CLM-00004">claim 4</claim-ref>, wherein the one or more active sensing systems or devices include one or more lidar devices.</div>
</div>
</div> <div class="claim-dependent"> <div class="claim" id="CLM-00006" num="00006">
<div class="claim-text">6. The computer-implemented method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein displaying the 3-D environment image on the user interface comprises displaying a 3-D image of the environment that has been at least partially generated by image processing a two-dimensional (2-D) image of the environment.</div>
</div>
</div> <div class="claim-dependent"> <div class="claim" id="CLM-00007" num="00007">
<div class="claim-text">7. The computer-implemented method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein obtaining the indication of the particular label uniquely identifying the particular object comprises receiving the indication of the particular label uniquely identifying the particular object via the user interface.</div>
</div>
</div> <div class="claim-dependent"> <div class="claim" id="CLM-00008" num="00008">
<div class="claim-text">8. The computer-implemented method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein obtaining the indication of the particular label uniquely identifying the particular object comprises automatically generating the particular label uniquely identifying the particular object the particular object based upon the data indicative of the boundary of the particular object within the 3-D environment image.</div>
</div>
</div> <div class="claim-dependent"> <div class="claim" id="CLM-00009" num="00009">
<div class="claim-text">9. The computer-implemented method of <claim-ref idref="CLM-00008">claim 8</claim-ref>, wherein automatically generating the particular label uniquely identifying the particular object based upon the data indicative of the boundary of the particular object within the 3-D environment image comprises providing at least some of the data indicative of the boundary of the particular object within the 3-D environment image as input into a label prediction model that has been trained based on identified and labeled objects depicted within a plurality of historical images of one or environments in which vehicles operate.</div>
</div>
</div> <div class="claim-dependent"> <div class="claim" id="CLM-00010" num="00010">
<div class="claim-text">10. The computer-implemented method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein:
<div class="claim-text">the method further comprises automatically generating and displaying an interim graphical representation of the boundary of the particular object within the 3-D environment image from the second perspective view based on the graphical representation of the boundary of the particular object on the 3-D environment image from the first perspective view; and</div>
<div class="claim-text">receiving, via the one or more user controls, the indication of the refinement to the graphical representation of the boundary of the particular object as depicted within the 3-D environment image from the second perspective view comprises receiving, via the one or more user controls, a modification to the interim graphical representation and applying the modification to the interim graphical representation, thereby generating the refinement to the graphical representation.</div>
</div>
</div>
</div> <div class="claim-dependent"> <div class="claim" id="CLM-00011" num="00011">
<div class="claim-text">11. The computer-implemented method of <claim-ref idref="CLM-00010">claim 10</claim-ref>, wherein automatically generating the interim graphical representation comprises automatically determining an initial modification to the graphical representation by utilizing a boundary prediction model that has been trained based on identified objects depicted within a plurality of historical images of one or more environments in which vehicles operate, and applying the initial modification to the graphical representation, thereby generating the interim graphical representation.</div>
</div>
</div> <div class="claim-dependent"> <div class="claim" id="CLM-00012" num="00012">
<div class="claim-text">12. The computer-implemented method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein adjusting the presentation of the 3-D environment image to be from the second perspective view comprises at least one of rotating, scaling, or translating, in three dimensions, the 3-D environment image displayed on the user interface.</div>
</div>
</div> <div class="claim-dependent"> <div class="claim" id="CLM-00013" num="00013">
<div class="claim-text">13. The computer-implemented method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein:
<div class="claim-text">the 3-D environment image is included in a virtual reality representation of the environment presented via the user interface; and</div>
<div class="claim-text">receiving the instruction to present the 3-D environment image on the user interface from the second perspective view and adjusting the presentation of the 3-D environment image on the user interface based on the received view perspective instruction comprises receiving, via the user interface, one or more user interactions with the virtual reality representation of the environment and automatically responding to the one or more user interactions.</div>
</div>
</div>
</div> <div class="claim-dependent"> <div class="claim" id="CLM-00014" num="00014">
<div class="claim-text">14. The computer-implemented method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein receiving, via the one or more user controls, the indication of the graphical representation of the boundary of the particular object comprises receiving, based on one or more activations of the one or more user controls, an indication at least one of: a shape, a surface area of the particular object, or two points defining endpoints of at least a portion of the boundary of the particular object.</div>
</div>
</div> <div class="claim-dependent"> <div class="claim" id="CLM-00015" num="00015">
<div class="claim-text">15. The computer-implemented method of <claim-ref idref="CLM-00014">claim 14</claim-ref>, wherein the method further comprises automatically determining the graphical representation based on the one or more activations of the one or more user controls, including utilizing a boundary prediction model that has been trained based on identified objects depicted within a plurality of historical images of one or more environments in which vehicles operate.</div>
</div>
</div> <div class="claim-dependent"> <div class="claim" id="CLM-00016" num="00016">
<div class="claim-text">16. The computer-implemented method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein receiving, via the one or more user controls, the indication of the graphical representation of the boundary of the particular object comprises:
<div class="claim-text">receiving, via the one or more user controls, a user selection of the particular object;</div>
<div class="claim-text">based upon the user selection of the particular object, automatically generating and displaying, on the user interface, an interim graphical representation of the boundary of the particular object corresponding to the graphical representation; and</div>
<div class="claim-text">one of:
<div class="claim-text">(i) receiving, via the user interface, an indication of an acceptance of the interim graphical representation to thereby generate the graphical representation, or</div>
<div class="claim-text">(ii) receiving, via the user interface, a user modification to the interim graphical representation of the boundary of the particular object; and updating, based on the user modification, the display of the interim graphical representation of the boundary of the particular object on the user interface to thereby generate the graphical representation.</div>
</div>
</div>
</div>
</div> <div class="claim-dependent"> <div class="claim" id="CLM-00017" num="00017">
<div class="claim-text">17. The computer-implemented method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein:
<div class="claim-text">the 3-D environment image is a first frame depicting the environment at a first time;</div>
<div class="claim-text">the method further comprises displaying, on the user interface, a second frame including a 3-D image depicting the environment at a second time different than the first time, the second frame including a respective 3-D image of the particular object, and the second frame including an interim graphical representation of the boundary of the particular object as depicted within the second frame, the interim graphical representation generated based on the graphical representation of the boundary of the particular object or the refinement to the graphical representation of the boundary of the particular object as depicted within the first frame; and</div>
<div class="claim-text">receiving a modification to the interim graphical representation of the boundary of the particular object displayed in the second frame, and based upon the received modification:
<div class="claim-text">altering, on the user interface, the interim graphical representation in accordance with the received modification thereby generating and displaying an altered graphical representation of the boundary of the particular object within the second frame and associated with the second time;</div>
<div class="claim-text">generating, based on the altered graphical representation of the boundary of the particular object within the second frame, data indicative of the boundary of the particular object within the second frame; and</div>
<div class="claim-text">storing, in the one or more tangible, non-transitory memories as another part of the training data set, an indication of an association between the particular label uniquely identifying the particular object and the data indicative of the boundary of the particular object within the second frame.</div>
</div>
</div>
</div>
</div> <div class="claim-dependent"> <div class="claim" id="CLM-00018" num="00018">
<div class="claim-text">18. The computer-implemented method of <claim-ref idref="CLM-00017">claim 17</claim-ref>, wherein receiving the modification to the interim graphical representation of the boundary of particular object displayed in the second frame comprises receiving, via the one or more user controls provided by the user interface, a user modification to at least a portion of the interim graphical representation of the boundary of the particular object displayed in the second frame.</div>
</div>
</div> <div class="claim-dependent"> <div class="claim" id="CLM-00019" num="00019">
<div class="claim-text">19. The computer-implemented method of <claim-ref idref="CLM-00017">claim 17</claim-ref>,
<div class="claim-text">further comprising automatically determining at least a portion of the modification to the interim graphical representation of the boundary of the particular object displayed in the second frame by utilizing a boundary prediction model that has been trained based on identified objects depicted within a plurality of historical images of one or more environments in which vehicles operate; and</div>
<div class="claim-text">wherein receiving the modification to the interim graphical representation of the boundary of the particular object displayed in the second frame comprises receiving the automatically determined at least the portion of the modification to the interim graphical representation of the boundary of the particular object displayed in the second frame.</div>
</div>
</div>
</div> <div class="claim"> <div class="claim" id="CLM-00020" num="00020">
<div class="claim-text">20. A computer-implemented method for identifying and labeling objects within images for training machine-learning based models that are used to autonomously control vehicles, the method comprising:
<div class="claim-text">displaying, on a user interface, a three-dimensional (3-D) image of an environment in which vehicles operate, the 3-D environment image depicting one or more physical objects located in the environment, and the 3-D environment image presented on the user interface from a first perspective view;</div>
<div class="claim-text">receiving, via the user interface, an instruction to hide respective 3-D images of one or more selected objects depicted within the 3-D environment image;</div>
<div class="claim-text">based on the received hiding instruction, greying out or rendering non-visible the respective 3-D images of the one or more selected objects depicted within the 3-D environment image while maintaining respective levels of visibility of respective 3-D images of other objects depicted within the 3-D environment image;</div>
<div class="claim-text">receiving, via one or more user controls provided by the user interface, an indication of a first graphical representation of a boundary of a particular object as depicted within the 3-D environment image from the first perspective view, the particular object excluded from the one or more selected objects that are greyed out or rendered non-visible;</div>
<div class="claim-text">generating, based on the first graphical representation, data indicative of the boundary of the particular object within the 3-D environment image from the first perspective view;</div>
<div class="claim-text">obtaining an indication of a particular label for the particular object;</div>
<div class="claim-text">generating, based on the data indicative of the boundary of the particular object within the 3-D environment image from the first perspective view, an association between the particular label and a 3-D image of the particular object within the 3-D environment image, thereby distinguishing the 3-D image of the particular object within the 3-D environment image;</div>
<div class="claim-text">storing an indication of the association between the particular label and the 3-D image of the particular object within the 3-D environment image in one or more tangible, non-transitory memories as a part of a training data set utilized to train one or more machine-learning based models, the one or more machine-learning based models used to autonomously control vehicles;</div>
<div class="claim-text">receiving, via the one or more user controls, an instruction to present the 3-D environment image on the user interface from a second perspective view different than the first perspective view, and based on the received view perspective instruction, adjusting a presentation of the 3-D environment image on the user interface to be from the second perspective view;</div>
<div class="claim-text">receiving, via the one or more user controls, an indication of a second graphical representation of the boundary of the particular object as depicted within the 3-D environment image from the second perspective view;</div>
<div class="claim-text">generating, based on the second graphical representation, data indicative of the boundary of the particular object within the 3-D environment image from the second perspective view; and</div>
<div class="claim-text">updating, based on the data indicative of the boundary of the particular object within the 3-D environment image from the second perspective view, the stored indication of the association between the particular label and the 3-D image of the particular object within the 3-D environment image, thereby refining the distinguishing of the 3-D image of the particular object within the 3-D environment image.</div>
</div>
</div>
</div> <div class="claim-dependent"> <div class="claim" id="CLM-00021" num="00021">
<div class="claim-text">21. A computer-implemented method of <claim-ref idref="CLM-00017">claim 17</claim-ref>, for identifying and labeling objects within images for training machine-learning based models that are used to autonomously control vehicles, the method comprising:
<div class="claim-text">displaying, on a user interface, a three-dimensional (3-D) image of an environment in which vehicles operate, the 3-D environment image depicting one or more physical objects located in the environment, the 3-D environment image presented on the user interface from a first perspective view, and the 3-D environment image generated by one or more active imaging sensors or devices;</div>
<div class="claim-text">displaying a plurality of 2-D images of the environment in conjunction with displaying the 3-D environment image on the user interface, each 2-D image of the environment depicting a different fixed perspective view of a respective at least a portion of the 3-D environment image, and the plurality of 2-D images of the environment generated by one or more passive imaging sensors, devices, or cameras;</div>
<div class="claim-text">receiving, via one or more user controls provided by the user interface, an indication of a first graphical representation of a boundary of a particular object as depicted within the 3-D environment image from the first perspective view;</div>
<div class="claim-text">generating, based on the first graphical representation, data indicative of the boundary of the particular object within the 3-D environment image from the first perspective view;</div>
<div class="claim-text">obtaining an indication of a particular label for the particular object;</div>
<div class="claim-text">generating, based on the data indicative of the boundary of the particular object within the 3-D environment image from the first perspective view, an association between the particular label and a 3-D image of the particular object within the 3-D environment image, thereby distinguishing the 3-D image of the particular object within the 3-D environment image;</div>
<div class="claim-text">storing an indication of the association between the particular label and the 3-D image of the particular object within the 3-D environment image in one or more tangible, non-transitory memories as a part of a training data set utilized to train one or more machine-learning based models, the one or more machine-learning based models used to autonomously control vehicles;</div>
<div class="claim-text">receiving, via the one or more user controls, an instruction to present the 3-D environment image on the user interface from a second perspective view different than the first perspective view, and based on the received view perspective instruction, adjusting a presentation of the 3-D environment image on the user interface to be from the second perspective view;</div>
<div class="claim-text">receiving, via the one or more user controls, an indication of a second graphical representation of the boundary of the particular object as depicted within the 3-D environment image from the second perspective view;</div>
<div class="claim-text">generating, based on the second graphical representation, data indicative of the boundary of the particular object within the 3-D environment image from the second perspective view; and</div>
<div class="claim-text">updating, based on the data indicative of the boundary of the particular object within the 3-D environment image from the second perspective view, the stored indication of the association between the particular label and the 3-D image of the particular object within the 3-D environment image, thereby refining the distinguishing of the 3-D image of the particular object within the 3-D environment image.</div>
</div>
</div>
</div> <div class="claim-dependent"> <div class="claim" id="CLM-00022" num="00022">
<div class="claim-text">22. The computer-implemented method of <claim-ref idref="CLM-00021">claim 21</claim-ref>, further comprising simultaneously displaying, on the user interface, both the first graphical representation of the boundary of the particular object on the 2-D environment image and the first graphical representation of the boundary of the particular object on the 3-D environment image.</div>
</div>
</div> <div class="claim-dependent"> <div class="claim" id="CLM-00023" num="00023">
<div class="claim-text">23. The computer-implemented method of <claim-ref idref="CLM-00022">claim 22</claim-ref>, further comprising at least one of:
<div class="claim-text">causing a user manipulation of the first graphical representation of the boundary of the particular object displayed on the 2-D environment image to be automatically reflected on the 3-D environment image; or</div>
<div class="claim-text">causing a user manipulation of the first graphical representation of the boundary of the particular object displayed on the 3-D environment image to be automatically reflected on the 2-D environment image.</div>
</div>
</div>
</div> <div class="claim-dependent"> <div class="claim" id="CLM-00024" num="00024">
<div class="claim-text">24. The computer-implemented method of <claim-ref idref="CLM-00021">claim 21</claim-ref>, wherein:
<div class="claim-text">the 2-D image is layered on top of or under the 3-D image, thereby forming a composite image of at least one of the one or more physical objects located in the environment; and</div>
<div class="claim-text">the indication of the first graphical representation of the boundary of the particular object is based on the composite image.</div>
</div>
</div>
</div> <div class="claim"> <div class="claim" id="CLM-00025" num="00025">
<div class="claim-text">25. A system for identifying and labeling objects within images for training machine-learning based models that are used to autonomously operate vehicles, the system comprising:
<div class="claim-text">a user interface;</div>
<div class="claim-text">one or more processors; and</div>
<div class="claim-text">one or more non-transitory, tangible memories coupled to the one or more processors and storing computer executable instructions thereon that, when executed by the one or more processors, cause the system to:
<div class="claim-text">display, on the user interface, a three-dimensional (3-D) image of an environment in which vehicles operate, the 3-D environment image depicting one or more physical objects located in the environment, and the 3-D environment image presented on the user interface from a first perspective view;</div>
<div class="claim-text">receive, via the user interface, an indication of a graphical representation of a boundary of a particular object as depicted within the 3-D environment image from the first perspective view, the graphical representation generated via one or more user controls provided by the user interface and displayed on the user interface in conjunction with the 3-D environment image from the first perspective view;</div>
<div class="claim-text">generate, based on the graphical representation, data indicative of the boundary of the particular object within the 3-D environment image from the first perspective view;</div>
<div class="claim-text">obtain an indication of a particular label for the particular object, the particular label uniquely identifying the particular object;</div>
<div class="claim-text">generate, based on the data indicative of the boundary of the particular object within the 3-D environment image from the first perspective view, an association between the particular label uniquely identifying the particular object and a 3-D image of the particular object within the 3-D environment image, thereby distinguishing the 3-D image of the particular object within the 3-D environment image;</div>
<div class="claim-text">store an indication of the association between the particular label uniquely identifying the particular object and the 3-D image of the particular object within the 3-D environment image in the one or more tangible, non-transitory memories as a part of a training data set utilized to train one or more machine-learning based models, the one or more machine-learning based models used to autonomously control vehicles;</div>
<div class="claim-text">receive, via the user interface, a user instruction to present the 3-D environment image on the user interface from a second perspective view different than the first perspective view;</div>
<div class="claim-text">based on the received view perspective user instruction, adjust a presentation of the 3-D environment image on the user interface to be from the second perspective view so that the 3-D environment image from the second perspective view and the graphical representation of the boundary of the particular object are displayed on the user interface;</div>
<div class="claim-text">receive, via the user interface, an indication of a refinement to the graphical representation of the boundary of the particular object as depicted within the 3-D environment image from the second perspective view, the refinement generated via the one or more user controls provided by the user interface;</div>
<div class="claim-text">generate, based on the refined graphical representation, data indicative of the boundary of the particular object within the 3-D environment image from the second perspective view; and</div>
<div class="claim-text">update, based on the data indicative of the boundary of the particular object within the 3-D environment image from the second perspective view, the stored indication of the association between the particular label uniquely identifying the particular object and the 3-D image of the particular object within the 3-D environment image, thereby refining the distinguishing of the 3-D image of the particular object within the 3-D environment image.</div>
</div>
</div>
</div>
</div> <div class="claim-dependent"> <div class="claim" id="CLM-00026" num="00026">
<div class="claim-text">26. The system of <claim-ref idref="CLM-00025">claim 25</claim-ref>, wherein:
<div class="claim-text">the computer executable instructions are further executable by the one or more processors to cause the system to automatically generate and display an interim graphical representation of the boundary of the particular object within the 3-D environment image from the second perspective view; and</div>
<div class="claim-text">the indication of the refinement to the graphical representation of the boundary of the particular object as depicted within the 3-D environment image from the second perspective view comprises a modification to the interim graphical representation displayed within the 3-D environment image from the second perspective view, the modification received via the one or more user controls provided by the user interface.</div>
</div>
</div>
</div> <div class="claim-dependent"> <div class="claim" id="CLM-00027" num="00027">
<div class="claim-text">27. The system of <claim-ref idref="CLM-00025">claim 25</claim-ref>, wherein:
<div class="claim-text">the 3-D environment image is included in a virtual reality representation of the environment presented via the user interface; and</div>
<div class="claim-text">the adjustment of the presentation of the 3-D environment image on the user interface is in response to one or more user interactions with the virtual reality representation of the environment.</div>
</div>
</div>
</div> <div class="claim-dependent"> <div class="claim" id="CLM-00028" num="00028">
<div class="claim-text">28. The system of <claim-ref idref="CLM-00025">claim 25</claim-ref>, wherein:
<div class="claim-text">the indication of the boundary of the particular object comprises a user selection of the particular object; and</div>
<div class="claim-text">the computer executable instructions are further executable by the one or more processors to:
<div class="claim-text">cause the system to automatically generate and display an interim graphical representation of the boundary of the particular object based on the user selection of the particular object;</div>
<div class="claim-text">receive, via the user interface, an indication of (i) a user acceptance of the interim graphical representation of the boundary of the particular object, or (ii) a user modification to the interim graphical representation of the boundary of the particular object; and</div>
<div class="claim-text">generate the graphical representation based on the received indication of the user acceptance or the user modification.</div>
</div>
</div>
</div>
</div> <div class="claim-dependent"> <div class="claim" id="CLM-00029" num="00029">
<div class="claim-text">29. The system of <claim-ref idref="CLM-00028">claim 28</claim-ref>, wherein the interim graphical representation of the boundary of the particular object is automatically generated by utilizing a boundary prediction model that has been trained based on identified objects depicted within a plurality of historical images of one or more environments in which vehicles operate.</div>
</div>
</div> </div>
</div>
</section>
                </article>
            </search-app>
        </body>
    </html>
    