
    <html>
        <body>
            <search-app>
                <article class="result" itemscope="" itemtype="http://schema.org/ScholarlyArticle">
    <h1 itemprop="pageTitle">US10338853B2 - Media aware distributed data layout 
        - Google Patents</h1><section itemprop="abstract" itemscope="">
<h2>Abstract</h2>
<div html="" itemprop="content"><abstract lang="EN" load-source="docdb" mxw-id="PA301717621" source="national office">
<div class="abstract">A storage system includes a plurality of vdisks, with each vdisk containing a plurality of storage segments, and each segment providing a specific class of service (CoS) for storage. Each vdisk stores files with data and meta data distributed among its storage segments. A storage system includes a memory having multiple classes of service. The system includes an interface for storing a file as blocks of data associated with a class of service in the memory. The interface chooses the class of service for a block on a block by block basis for storage. A file system for storing a file. A file system for storing includes a plurality of vdisks. A method for storing a file.</div>
</abstract>
</div>
</section><section itemprop="description" itemscope="">
<h2>Description</h2>
<div html="" itemprop="content"><div class="description" lang="EN" load-source="patent-office" mxw-id="PDES194356780">
<heading id="h-0001">CROSS-REFERENCE TO RELATED APPLICATIONS</heading>
<div class="description-paragraph" id="p-0002" num="0001">This is a continuation of U.S. patent application Ser. No. 15/222,449 filed Jul. 28, 2016, now U.S. Pat. No. 9,710,195, which is a continuation of U.S. patent application Ser. No. 14/175,801 filed Feb. 7, 2014, now U.S. Pat. No. 9,405,487 issued on Aug. 2, 2016, which is a continuation of U.S. patent application Ser. No. 13/493,701 filed Jun. 11, 2012, now U.S. Pat. No. 8,655,931 issued Feb. 18, 2014, which is a continuation of U.S. patent application Ser. No. 12/218,085 filed Jul. 11, 2008, now U.S. Pat. No. 8,214,404 issued Jul. 3, 2012, all of which are incorporated by reference herein.</div>
<heading id="h-0002">FIELD OF THE INVENTION</heading>
<div class="description-paragraph" id="p-0003" num="0002">The present invention is related to the storage of a file system on a plurality of segments, each of which has a different class of service. (As used herein, references to the present invention” or “invention” relate to exemplary embodiments and not necessarily to every embodiment encompassed by the appended claims.) Additionally, the present invention constructs said file system from multiple sub-file systems (vdisks), with operations involving multiple vdisks providing atomicity using a persistent operations table to record partial operation state.</div>
<heading id="h-0003">BACKGROUND OF THE INVENTION</heading>
<div class="description-paragraph" id="p-0004" num="0003">This section is intended to introduce the reader to various aspects of the art that may be related to various aspects of the present invention. The following discussion is intended to provide information to facilitate a better understanding of the present invention. Accordingly, it should be understood that statements in the following discussion are to be read in this light, and not as admissions of prior art.</div>
<div class="description-paragraph" id="p-0005" num="0004">Most file systems today lack certain features useful for supporting mixed types of storage, as well as huge amounts of storage. In addition, most file systems today have meta data bottlenecks that limit their performance scaling in multi-core and distributed systems. The invention presented here is a novel file system implementation addressing these issues.</div>
<heading id="h-0004">BRIEF SUMMARY OF THE INVENTION</heading>
<div class="description-paragraph" id="p-0006" num="0005">This invention divides a file system into a number of pools of inodes, otherwise called vdisks, and within each inode pool, data is stored in multiple segments, each potentially with a different class of service. Automated policies can choose the appropriate segment for different files, or even different portions of a file, chosen either statically or dynamically.</div>
<div class="description-paragraph" id="p-0007" num="0006">Each inode pool can be served by a separate processor in a multiprocessor system, limiting the amount of inter-processor communication within a file system to communication between inode pools. And each inode pool can be checked for consistently independently, greatly reducing the amount of computation and IO operations required to recover from even a severe system failure or software bug resulting in file system inconsistency, since only the inode pool with the detected inconsistency needs to be checked and repaired.</div>
<div class="description-paragraph" id="p-0008" num="0007">The present invention pertains to a storage system. The system comprises a memory having a first segment for storing data having a first class of service, and at least a second segment for storing data having a second class of service which is different than the first class of service of the first segment. The system comprises an interface which receives a file and stores a first portion of the file in the first segment and a second portion of the file on the second segment, and retrieves a file by reading the first portion and the second portion from the first and second segment, respectively.</div>
<div class="description-paragraph" id="p-0009" num="0008">The present invention pertains to a storage system. The system comprises a plurality of vdisks, with each vdisk containing a plurality of storage segments, and each segment providing a specific class of service (CoS). Each vdisk stores files with data and meta data distributed among its storage segments.</div>
<div class="description-paragraph" id="p-0010" num="0009">The present invention pertains to a storage system. The system comprises a memory having multiple classes of service. The system comprises an interface for storing a file as blocks of data associated with a class of service in the memory. The interface chooses the class of service for a block on a block by block basis.</div>
<div class="description-paragraph" id="p-0011" num="0010">The present invention pertains to a file system for storing a file. The system comprises a plurality of vdisks, with each vdisk having a plurality of inodes. Each inode of each vdisk stores data on one or more segments, with each segment having a different class of service. The system comprises a controller which stores data of a file in an inode of a vdisk, in one or more segments of that vdisk</div>
<div class="description-paragraph" id="p-0012" num="0011">The present invention pertains to a file system for storing a file; the system comprises a plurality of vdisks, and each vdisk having a plurality of inodes. The system comprises a controller including a plurality of processors, with each processor serving one or more of the vdisks.</div>
<div class="description-paragraph" id="p-0013" num="0012">The present invention pertains to a file system for storing files. The system comprises a plurality of vdisks, with each vdisk having a plurality of inodes, a plurality of inodes of at least one vdisk storing data on a plurality of segments, each segment having a different class of service. The system comprises a controller which stores data of the file in one or more segments of one vdisk.</div>
<div class="description-paragraph" id="p-0014" num="0013">The present invention pertains to a method for storing a file. A method comprises the steps of receiving the file at an interface. There is the step of storing data of the file with a controller in one or more segments of one vdisk of a plurality of vdisks, each vdisk having a plurality of inodes. The plurality of inodes of at least one vdisk, store data in a plurality of segments, with each segment having a different class of service.</div>
<div class="description-paragraph" id="p-0015" num="0014">The present invention pertains to a method for storing a file. The method comprises the steps of receiving the file at an interface. There is the step of storing a first portion of the file in a first segment of a memory and a second portion of the file in a second segment of the memory. There is the step of retrieving the file by reading the first portion and the second portion from the first and second segment, respectively.</div>
<description-of-drawings>
<heading id="h-0005">BRIEF DESCRIPTION OF THE SEVERAL VIEWS OF THE DRAWING</heading>
<div class="description-paragraph" id="p-0016" num="0015">In the accompanying drawings, the preferred embodiment of the invention and preferred methods of practicing the invention are illustrated in which:</div>
<div class="description-paragraph" id="p-0017" num="0016"> <figref idrefs="DRAWINGS">FIG. 1</figref> shows two servers, each with multiple drives, with each drive holding multiple chunks. Some chunks hold file system data, while others store RAID parity information.</div>
<div class="description-paragraph" id="p-0018" num="0017"> <figref idrefs="DRAWINGS">FIG. 2</figref> shows the construction of vdisk block address spaces from one or more segments.</div>
<div class="description-paragraph" id="p-0019" num="0018"> <figref idrefs="DRAWINGS">FIG. 3</figref> shows an alternative construction of segments from individual disk drives instead of from chunks of disk drives.</div>
<div class="description-paragraph" id="p-0020" num="0019"> <figref idrefs="DRAWINGS">FIG. 4</figref> shows the distribution of files and directories in a file system between multiple vdisks.</div>
<div class="description-paragraph" id="p-0021" num="0020"> <figref idrefs="DRAWINGS">FIG. 5</figref> shows the modular decomposition of the invention, along with the binding of processors to vdisk module instances, and the binding of vdisk instances to the set of drives each vdisk controls.</div>
<div class="description-paragraph" id="p-0022" num="0021"> <figref idrefs="DRAWINGS">FIG. 6</figref> shows the message traffic involved in a file or directory delete operation, in the simple case where locks are all obtained without conflict.</div>
<div class="description-paragraph" id="p-0023" num="0022"> <figref idrefs="DRAWINGS">FIG. 7</figref> shows the message traffic involved in a file or directory delete operation, in the case where optimistic locking fails and locks have to be obtained again in a different order.</div>
<div class="description-paragraph" id="p-0024" num="0023"> <figref idrefs="DRAWINGS">FIG. 8</figref> shows the message traffic involved in a hard link operation (source locked first case).</div>
<div class="description-paragraph" id="p-0025" num="0024"> <figref idrefs="DRAWINGS">FIG. 9</figref> shows the message traffic involved in a hard link operation (target locked first case).</div>
<div class="description-paragraph" id="p-0026" num="0025"> <figref idrefs="DRAWINGS">FIG. 10</figref> shows the message traffic involved in a file create/directory create or symbolic link create operation.</div>
<div class="description-paragraph" id="p-0027" num="0026"> <figref idrefs="DRAWINGS">FIG. 11</figref> shows the message traffic involved in a rename operation.</div>
<div class="description-paragraph" id="p-0028" num="0027"> <figref idrefs="DRAWINGS">FIG. 12</figref> shows the fields stored in an Mode for use by the simple policy mechanism.</div>
<div class="description-paragraph" id="p-0029" num="0028"> <figref idrefs="DRAWINGS">FIG. 13</figref> shows the modules present in a Unix operating system including the vdisk module instances of this invention.</div>
<div class="description-paragraph" id="p-0030" num="0029"> <figref idrefs="DRAWINGS">FIG. 14</figref> is a block diagram of the present invention.</div>
</description-of-drawings>
<heading id="h-0006">DETAILED DESCRIPTION OF THE INVENTION</heading>
<div class="description-paragraph" id="p-0031" num="0030">Referring now to the drawings wherein like reference numerals refer to similar or identical parts throughout the several views, and more specifically to <figref idrefs="DRAWINGS">FIGS. 5 and 14</figref> thereof, there is shown a storage system <b>10</b>. The system <b>10</b> comprises a memory <b>12</b> having a first segment <b>14</b> for storing data having a first class of service, and at least a second segment <b>16</b> for storing data having a second class of service which is different than the first class of service of the first segment <b>14</b>. The system <b>10</b> comprises an interface <b>18</b> which receives a file and stores a first portion of the file in the first segment <b>14</b> and a second portion of the file on the second segment <b>16</b>, and retrieves a file by reading the first portion and the second portion from the first and second segment <b>14</b>, <b>16</b>, respectively.</div>
<div class="description-paragraph" id="p-0032" num="0031">The interface <b>18</b> can store the file within one or more stripes of data in the first segment <b>14</b> and within one or more stripes of data in the second segment <b>16</b>. The system <b>10</b> can include a plurality of disks <b>20</b> and wherein the interface <b>18</b> stores each segment as a plurality of data chunks distributed among several disks <b>20</b>. The segment can include a parity strip and the interface <b>18</b> stores the parity strip in the memory <b>12</b> according to RAID techniques so if any one strip is unavailable, the data blocks can still be retrieved from the other strips and the parity strip. The inode describing a file includes meta data which keeps track of the first portion and the second. The meta data is preferably a Unix-style indirect block tree.</div>
<div class="description-paragraph" id="p-0033" num="0032">The present invention pertains to a storage system <b>10</b>. The system <b>10</b> comprises a plurality of vdisks <b>24</b>. Each vdisk <b>24</b> contains a plurality of storage segments, with each segment <b>24</b> providing a specific class of service (CoS). Each vdisk <b>24</b> stores files with data and meta data distributed among its storage segments.</div>
<div class="description-paragraph" id="p-0034" num="0033">A first portion of the file data can be stored in a first segment <b>14</b>, and a second portion of the file can be stored in either the first segment <b>14</b> or a second segment <b>16</b>. The system <b>10</b> can include a plurality of vdisks <b>24</b>, and where a vdisk <b>24</b> to hold a newly created file or directory is chosen from the plurality of vdisks <b>24</b> based on a predetermined mapping into the plurality of vdisks <b>24</b> in the storage system <b>10</b>. The predetermined mapping can be a round-robin assignment algorithm. The predetermined mapping can choose the vdisk <b>24</b> with a largest available space, or a largest percentage of available space.</div>
<div class="description-paragraph" id="p-0035" num="0034">The system <b>10</b> can include a plurality of processors <b>26</b> and wherein the predetermined mapping chooses the vdisk <b>24</b> served by a least loaded processor <b>26</b>. Each vdisk <b>24</b> can be a random collection of directories and files, and at least one file is stored in at least two segments. Each vdisk <b>24</b> can be a random collection of directories and files, and at least two vdisks <b>24</b> hold at least one file. The system <b>10</b> can include an interface <b>18</b> to initiate a file system <b>10</b> consistency check on an individual vdisk <b>24</b>, triggered by an indication of an inconsistency in a specific vdisk <b>24</b>.</div>
<div class="description-paragraph" id="p-0036" num="0035">The present invention pertains to a storage system <b>10</b>. The system <b>10</b> comprises a memory <b>12</b> having multiple classes of service. The system <b>10</b> comprises an interface <b>18</b> for storing a file as blocks of data associated with a class of service in the memory <b>12</b>, with the interface <b>18</b> choosing the class of service for a block on a block by block basis.</div>
<div class="description-paragraph" id="p-0037" num="0036">The present invention pertains to a file system <b>10</b> for storing a file. The system <b>10</b> comprises a plurality of vdisks <b>24</b>, with each vdisk <b>24</b> having a plurality of inodes. Each inode of each vdisk <b>24</b> stores data on one or more segments, with each segment having a different class of service. The system <b>10</b> comprises a controller <b>28</b> which stores data of a file in an inode of a vdisk <b>24</b>, in one or more segments of that vdisk <b>24</b>.</div>
<div class="description-paragraph" id="p-0038" num="0037">The system <b>10</b> can include a plurality of processors <b>26</b>, with each processor <b>26</b> serving at least one of the vdisks <b>24</b>. Each segment can have a plurality of chunks. Each segment can have chunks added to it dynamically over time. Each vdisk <b>24</b> can include an inode table <b>30</b> describing each file in the vdisk <b>24</b>. One of the vdisks <b>24</b> is preferably a root vdisk <b>24</b>. One of the inodes in the root vdisk <b>24</b> is preferably a root inode of the file system <b>10</b>. The system <b>10</b> can include at least one directory storing mappings of file names to inode pointers. Each inode can have a back pointer to the directory entry pointing to the inode.</div>
<div class="description-paragraph" id="p-0039" num="0038">The file system <b>10</b> described in this invention is preferably a tree, with a single top-most directory, containing a mix of files and directories. Each other directory also contains a set of files and other directories. So, the root inode is the topmost inode in the file system <b>10</b> tree, and the only directory that doesn't have a parent directory.</div>
<div class="description-paragraph" id="p-0040" num="0039">Directories, including the root directory, are just normal files, marked with a special file type so that they can't just be read and written by users, but can instead only have directory operations like “create file in directory” performed on them. But, like normal files, they exist in a single vdisk and have their data blocks stored in one or more segments in that vdisk.</div>
<div class="description-paragraph" id="p-0041" num="0040">The present invention pertains to a file system <b>10</b> for storing a file. The system <b>10</b> comprises a plurality of vdisks <b>24</b>, with each vdisk <b>24</b> having a plurality of inodes. The system <b>10</b> comprises a controller <b>28</b> including a plurality of processors <b>26</b>, with each processor <b>26</b> serving one or more of the vnodes.</div>
<div class="description-paragraph" id="p-0042" num="0041">The present invention pertains to a file system <b>10</b> for storing a file. The system <b>10</b> comprises a plurality of vdisks <b>24</b>, with each vdisk <b>24</b> having a plurality of inodes, a plurality of inodes of at least one vdisk <b>24</b> storing data on a plurality of segments, each segment having a different class of service. The system <b>10</b> comprises a controller <b>28</b> which stores data of the file in multiple segments of one vdisk <b>24</b>.</div>
<div class="description-paragraph" id="p-0043" num="0042">The present invention pertains to a method for storing a file. A method comprises the steps of receiving the file at an interface <b>18</b>. There is the step of storing data of the file with a controller <b>28</b> in multiple segments of one vdisk <b>24</b> of a plurality of vdisks <b>24</b>. Each vdisk <b>24</b> stores a plurality of inodes. At least one inode of at least one vdisk <b>24</b> stores data in a plurality of segments, each segment having a different class of service.</div>
<div class="description-paragraph" id="p-0044" num="0043">The present invention pertains to a method for storing a file. The method comprises the steps of receiving the file at an interface <b>18</b>. There is the step of storing a first portion of the file in a first segment <b>14</b> of a memory <b>12</b> and storing a second portion of the file in a second segment <b>16</b> of the memory <b>12</b>. There is the step of retrieving the file by reading the first portion and the second portion from the first and second segment <b>14</b>, <b>16</b>, respectively.</div>
<div class="description-paragraph" id="p-0045" num="0044">In the operation of the invention, physical disks <b>20</b> and/or RAID arrays are divided into fixed sized chunks of storage with identical or very similar performance and reliability characteristics. These chunks may store data or parity (or checksum) information. These chunks are combined into variable sized segments, each segment providing a linear disk block address space, as well as a meta data description of the class of storage provided, including RAID class, average seek time, and read and write data transfer rates. Chunks may be combined into segments by simple concatenation, as shown by the diamond or horizontally marked segments in <figref idrefs="DRAWINGS">FIG. 1</figref>, in which chunks are concatenated sequentially in the order of their tag. Chunks may also be combined with RAID parity protection, as shown by the diagonally marked or shaded segments in the same figure, where, in these examples, every three chunks of data is stored with an additional parity chunk that stores the RAID 5 parity information for those three chunks. In the case of segments with parity chunks, the parity data is not included in the segment's linear address space, so that the linear address is comprised of the concatenation of the data chunks only. For example, the linear address space of the shaded segment in <figref idrefs="DRAWINGS">FIG. 1</figref> is comprised of the ordered set of chunks {0, 1, 2, 3, 4, 5}.</div>
<div class="description-paragraph" id="p-0046" num="0045">A chunk is the smallest part of the disk that we *assign* to one segment or another. A block, on the other hand, is the smallest addressable part of a disk for the purposes of doing *I/O*. That is, we do disk space allocation in units of chunks, but we do individual reads and writes at a more granular level.</div>
<div class="description-paragraph" id="p-0047" num="0046">Segments are a simple block storage concept, and are combined into sparse linear address spaces called vdisks <b>24</b>. Different segments within a vdisk <b>24</b> may have varying storage types. A vdisk <b>24</b> also includes an inode table <b>30</b>, with each entry identified by a 64 bit inode ID, specifying the inode within the vdisk <b>24</b>. Within a vdisk <b>24</b>, any inode's data or meta data blocks may be stored on any of the segments within the vdisk <b>24</b>. For example, a policy might specify that the first megabyte of every file is allocated from a segment with very low latency, with the remaining blocks allocated from segments stored on storage with higher latencies.</div>
<div class="description-paragraph" id="p-0048" num="0047">A file system <b>10</b> comprises multiple vdisks <b>24</b>. One vdisk <b>24</b> is designated the root vdisk <b>24</b>, and a designated inode within that vdisk <b>24</b> represents the root of the entire file system <b>10</b>. The set of the inodes within the file system <b>10</b> is then the union of all of the inodes in all of the file system's <b>10</b> vdisks <b>24</b>.</div>
<div class="description-paragraph" id="p-0049" num="0048">Directories store mappings from file names (represented as UTF-8 strings, without loss of generality) to (vdisk <b>24</b>, inode) pairs. Each inode has a back pointer (or a set of back pointers) to the directory entry or entries pointing to the Mode, to help in validating the directory entry. These back pointers also allow directory reconstruction by scanning for Modes stored in the directory, and can also be used in generating path names from inodes.</div>
<div class="description-paragraph" id="p-0050" num="0049">In <figref idrefs="DRAWINGS">FIG. 1</figref>, each small box is a chunk of storage, allocated from a drive represented by a horizontal box. <figref idrefs="DRAWINGS">FIG. 1</figref> contains two logical servers, possibly residing within the same computer system, the first of which supports three drives, and the second of which supports four drives. Each logical server provides storage for the chunks making up one or two segments. Each segment is striped among that server's drives: server <b>1</b> stores chunks from the dotted segment, and server <b>2</b> stores chunks from the shaded and white segments. Each segment provides storage with a different class of service: the dotted segment on sever <b>1</b> provides RAID 0 storage; the shaded segment on sever <b>2</b> provides RAID 5 storage, and the white segment on server <b>2</b> provides additional RAID 0 storage.</div>
<div class="description-paragraph" id="p-0051" num="0050">Note that RAID is implemented in this figure across chunks stored on different drives, rather than across multiple drives in their entirety. This means that for RAID segments, some of the segment's chunks store parity information instead of file system data. For example, drive <b>7</b>'s leftmost shaded chunk stores parity information for chunks <b>0</b>, <b>1</b> and <b>2</b> for the shaded segment, and drive <b>6</b>'s rightmost shaded chunk stores parity information for the shaded segment's chunks <b>5</b>, <b>4</b> and <b>3</b>.</div>
<div class="description-paragraph" id="p-0052" num="0051"> <figref idrefs="DRAWINGS">FIG. 2</figref> shows the shaded, white and dotted chunks being combined into shaded, white and dotted segments. The dotted and white segments make up the block address space for vdisk <b>1</b>, so that files whose inodes are stored on vdisk <b>1</b> can have blocks placed on either of the these segments, as the class of service policies for that file dictate. This invention's ability to aggregate multiple types of segments in a single vdisk <b>24</b> allows the dynamic allocation of storage with a desired CoS to portions of a file. Vdisk <b>2</b> consists of only the shaded segment. In common practice, when creating separate vdisks <b>24</b> for scalability and fault isolation reasons, each vdisk <b>24</b> would typically be comprised of sets of similarly typed segments.</div>
<div class="description-paragraph" id="p-0053" num="0052">Each vdisk <b>24</b> provides a separate Mode space, and in this example, the two vdisks <b>24</b> are combined into a single file system name space. The root is a specially designated Mode in a designated vdisk <b>24</b>, and files are allocated out of each vdisk <b>24</b> on either a random, or a policy-driven, basis as new files and directories are created. It is not expected to move files between vdisks <b>24</b> frequently, or even transparently; changing the class of service of all or part of a file is accomplished not by changing a file's vdisk <b>24</b>, but by migrating a file's individual blocks to those segments within the file's original vdisk <b>24</b> providing the desired class of service. Overall load balancing can be accomplished by moving the responsibility for processing entire vdisks <b>24</b> between processors <b>26</b> or systems <b>10</b>.</div>
<div class="description-paragraph" id="p-0054" num="0053">Note that vdisk <b>1</b> is actually comprised of storage (segments) controlled by both server <b>1</b> and server <b>2</b>. A file allocated on vdisk <b>1</b> could have its blocks allocated half from the diagonally marked segment, and half from the shaded segment. When vdisk <b>1</b>'s owner needs to read or write data on another logical server, it does so by sending a request to that server to perform the read or write operation on its behalf. This flexibility allows one to build loosely coupled servers serving a single file system <b>10</b>.</div>
<div class="description-paragraph" id="p-0055" num="0054">In other words, vdisks <b>24</b> have owning processes that control the meta data operations for all segments within that vdisk <b>24</b>. Each segment has its own controlling process performing its basic read and write operations. In some cases, the process controlling a vdisk <b>24</b> is the same as the process controlling all of that vdisk's <b>24</b> segments, but this need not be the case in general.</div>
<div class="description-paragraph" id="p-0056" num="0055">Note that alternative mechanisms for creating segments from disk storage are also possible. For example, <figref idrefs="DRAWINGS">FIG. 3</figref> shows segments comprised of entire RAID groups, with each RAID group made from chunks comprised of entire disk drives. As in the previous example, the segments are mapped into the vdisk <b>24</b> address space sparsely, to provide room for additional growth in an existing segment.</div>
<div class="description-paragraph" id="p-0057" num="0056"> <figref idrefs="DRAWINGS">FIG. 4</figref> shows the mapping between a file system directory structure and a collection of vdisks <b>24</b>. In this example, vdisk <b>1</b> (the horizontal stripes) holds the root directory, and stores references to another set of directories, some of which are located on the same vdisk <b>24</b> and some of which are stored on vdisk <b>2</b>. Each of those directories in turn store references to some files (in this example), again, some of which are on the same vdisk <b>24</b> as the parent directory, and some of which are on a different vdisk <b>24</b>. In this invention, each object within a directory may be located on any vdisk <b>24</b> in the system <b>10</b>, whether or not it is the same vdisk <b>24</b> as holds the directory.</div>
<div class="description-paragraph" id="p-0058" num="0057">Meta data operations that affect a single file are typically done by a designated vdisk <b>24</b> owner, typically updating local or remote (preferentially local) segments containing the appropriate meta data.</div>
<div class="description-paragraph" id="p-0059" num="0058">This architecture allows several different forms of striping. The files within a directory will typically be striped among multiple vdisks <b>24</b>, allowing concurrent data and meta data operations on different files within even a single directory. In addition, a file's blocks can be allocated from multiple segments, each controlled by a different server, and allowing multiple processes to perform IO operations for different portions of even a single file.</div>
<div class="description-paragraph" id="p-0060" num="0059">Aside from meta data operations that affect a single file, some vdisk <b>24</b> operations affect multiple files, located in multiple vdisks <b>24</b>. All of these operations, either affecting one or affecting more than one vdisk <b>24</b>, are described below.</div>
<div class="description-paragraph" id="p-0061" num="0060">The implementation of a device to implement the vdisk <b>24</b> interface <b>18</b> is now described. The vdisk <b>24</b> interface <b>18</b> plugs into a standard Unix operating system kernel just below the vnode layer, with a thin glue layer mapping incoming vnode calls from the kernel into the vdisk <b>24</b> operations described here. This glue layer is described in detail below, but every significant vnode operation has a corresponding vdisk <b>24</b> layer operation.</div>
<div class="description-paragraph" id="p-0062" num="0061">Referring to <figref idrefs="DRAWINGS">FIG. 5</figref>, a file create followed by a write of new data to the newly created file is described. The figure illustrates a system <b>10</b> with two processors <b>26</b>. Processor <b>1</b> has software ownership of drives <b>1</b> and <b>2</b> (the horizontal boxes below the processor <b>26</b>), containing two segments, A and B. Segments A and B provide storage to vdisk <b>1</b>, which stores a subset of the inodes in the single file system <b>10</b> exported in this example. Similarly, processor <b>2</b> has ownership of drives <b>3</b> and <b>4</b>, which collectively store segment C. Segment C is the sole segment providing storage for vdisk <b>2</b>, which stores the remainder of the inodes in this exported file system <b>10</b>. All operations on vdisk <b>1</b>, segments A and B, and drives <b>1</b> and <b>2</b>, are performed by processor <b>1</b>, while all operations on vdisk <b>2</b>, segment C and drives <b>3</b> and <b>4</b> are performed by processor <b>2</b>.</div>
<div class="description-paragraph" id="p-0063" num="0062">A file create request from the protocol servers (NFSv3 and CIFS servers in <figref idrefs="DRAWINGS">FIG. 13</figref>), through a vnode shim layer, and finally into the top of the vdisk layer as a file create request contains a file handle identifying the directory in which to create the file, and also contains the name of the file to be created. The create operation returns the file handle of the newly created file. <figref idrefs="DRAWINGS">FIG. 13</figref> shows how file system <b>10</b> requests enter the system <b>10</b>.</div>
<div class="description-paragraph" id="p-0064" num="0063">The file create request begins by consulting a vdisk <b>24</b> location service, accessible from any processor <b>26</b>, to determine the current owner of the target vdisk <b>24</b> storing the directory in which the new file is to be created; the target vdisk <b>24</b> can be located in a subfield of the incoming directory file handle. The create request is then forwarded to the processor <b>26</b> owning this vdisk <b>24</b>. If it is assumed the target directory resides on vdisk <b>1</b>, then the file create request begins execution on processor <b>1</b>.</div>
<div class="description-paragraph" id="p-0065" num="0064">The vdisk <b>24</b> module for vdisk <b>1</b> will choose the vdisk <b>24</b> to hold the file to be created, based upon policies such as load balancing or balancing the space used by the various vdisks <b>24</b>. In this example, it is assumed the policy module chooses vdisk <b>2</b> to hold the new file. Since the file create operation needs to atomically create a file by changing both the directory's vdisk <b>24</b>, and the new file's vdisk <b>24</b>, the create operation begins by creating a transaction entry in the directory vdisk's <b>24</b> (vdisk <b>1</b>'s) persistent operations table (POT) <b>22</b>, storing the parameters of the operation tagged with a unique transaction ID. In the event of a system failure, this information can be used to restart the operation. The first vdisk <b>24</b> then sends a perform-object-create operation to the second vdisk <b>24</b>, with this same transaction ID. The second vdisk <b>24</b> now creates a file in its inode table <b>30</b>, updating storage in segment C. As part of this operation, vdisk <b>2</b> creates its own POT <b>22</b> entry in its own POT <b>22</b>, tagged by the same transaction ID, and giving the inode number of the created object. This object's file handle (derived from its inode number) is passed back in the perform-object-create's response to vdisk <b>1</b>. The create operation in vdisk <b>1</b>, running again on processor <b>1</b>, completes the new directory entry so that it stores both the new file name and the newly created file's inode number. It then marks the POT <b>22</b> entry for this operation as complete. Cleanup of the POT <b>22</b> entries is described below.</div>
<div class="description-paragraph" id="p-0066" num="0065">As part of creating the directory entry, vdisk <b>1</b> needs to update the contents of the new file's parent directory. It does this by updating disk blocks on either segment A or segment B, depending upon the directory's associated storage policy. Similarly, as part of allocating a new object (inode), vdisk <b>2</b> allocates an inode by writing to segment C.</div>
<div class="description-paragraph" id="p-0067" num="0066">This illustrates several key features of this invention. First, the invention's ability to create files on an arbitrary, policy specified, vdisk <b>24</b>, greatly simplifies load balancing in the system <b>10</b>, since no administrator defined volume boundaries between vdisks <b>24</b> exist. This description also illustrates how a storage policy associated with a directory can guide the vdisk's <b>24</b> write operation to choose a storage segment with the desired class of service, allowing a policy to specify a different class of service at as fine a level of granularity as an individual disk block.</div>
<div class="description-paragraph" id="p-0068" num="0067">Next is described the writing of a block of data to the newly created file. The write request specifies the file to update by file handle. In the example, the write request will be received by an arbitrary processor <b>26</b>, and since the file was created on vdisk <b>2</b>, the request will be forwarded to vdisk <b>2</b>'s processor <b>26</b>, which is processor <b>2</b>. That processor <b>26</b> will call vdisk <b>2</b>'s write operation, which will allocate a block from segment C (the policy module will not have much to do in this case, since there is only one segment to choose from), and then write the updated data to drive <b>3</b> or drive <b>4</b>, as required.</div>
<div class="description-paragraph" id="p-0069" num="0068">Next, the details of the various vdisk <b>24</b> operations provided are examined.</div>
<div class="description-paragraph" id="p-0070" num="0069">The vdisk <b>24</b> operations are divided into three classes. The first set consists of simple operations that affect a single file at a time. The second set consists of operations that read the contents of directories, either to lookup an individual object, or to read the contents of a directory, possibly returning file attributes simultaneously. Finally, a third set of operations consists of operations that modify directories, by creating, deleting or renaming file objects within the directory or directories.</div>
<div class="description-paragraph" id="p-0071" num="0070">Increased concurrency compared with the state of the art can be obtained for operations in the first set above easily by executing the operations for each vdisk <b>24</b> on a separate processor <b>26</b>. Since these operations require no shared state between different vdisks <b>24</b>, the operations can execute completely concurrently on separate vdisks <b>24</b>. Similarly, the second set of operations either operates on a single vdisk <b>24</b> as well (the readdir operation), or naturally decomposes into two separate operations which run first on one vdisk <b>24</b> and then on another (lookup, readdirplus), which can also run with high concurrency. Even the third set of operations, those that modify one or more directory entries, only affect a small number of vdisks <b>24</b> (typically two, but occasionally three or four), meaning that in a system <b>10</b> with dozens of vdisks <b>24</b>, many such operations can run concurrently without creating a bottleneck on a single vdisk <b>24</b>. Again, this allows considerably improved levels of concurrency, as compared with more centralized file system architectures.</div>
<div class="description-paragraph" id="p-0072" num="0071">One of the key innovations of this invention is the implementation of directory modifying operations as multi-stage operations, where each stage affects a single vdisk <b>24</b>, and where the overall operation and its progress is recorded persistently so that the operation is performed atomically, and can complete successfully even if multiple processors <b>26</b> involved in the operation repeatedly restart during the operation; this is described further below.</div>
<div class="description-paragraph" id="p-0073" num="0072">Details of the implementation of these operations are provided below.</div>
<div class="description-paragraph" id="p-0074" num="0073">The vdisk <b>24</b> interface <b>18</b> exports a set of simple operations affecting a single file or directory, which are very straightforward to implement in this invention. Each operation is performed on a single file, and reads or updates either the status of the file or its data blocks. Any vdisk <b>24</b> implementation that supports inode structures comparable to the FreeBSD operating system's UFS file system can use the inodes stored in a file system <b>10</b> as a vdisk <b>24</b> implementation for these operations.</div>
<div class="description-paragraph" id="p-0075" num="0074">The vdisk <b>24</b> interface <b>18</b> includes the following single file operations:
</div> <ul> <li id="ul0001-0001" num="0000"> <ul> <li id="ul0002-0001" num="0075">getattr—get the attributes of a file</li> <li id="ul0002-0002" num="0076">setattr—change the attributes or file length of a file</li> <li id="ul0002-0003" num="0077">read—read data from a file</li> <li id="ul0002-0004" num="0078">write—update the contents of a file, and update the inode change time (POSIX file system ctime) and data modification time (POSIX file system mtime) fields simultaneously.</li> <li id="ul0002-0005" num="0079">readdir—read the contents of a directory in a standard format.</li> </ul> </li> </ul>
<div class="description-paragraph" id="p-0076" num="0080">The write operation ties into the choice of segment for a file. Specifically, the implementation of a write operation will create dirty buffers in the memory <b>12</b> cache that are tagged by the inode identifying the file, and the offset within the file at which the data is located. A background “cleaner” thread within the vdisk <b>24</b> module will examine the inode for its link to a predetermined class of service policy. This policy might be, for example, that the first N megabytes of a file's data should be written to the segment in the vdisk <b>24</b> with the lowest read and write latency, and the remaining data should be written to the segment with the largest available free space; this would be an example of a static policy, since the parameters of the policy are defined once, and then followed for multiple files. The cleaner would then allocate blocks of data for the dirty buffers from the vdisk's <b>24</b> segments according to the inode's policy and then write the dirty data to those newly allocated blocks.</div>
<div class="description-paragraph" id="p-0077" num="0081">A more dynamic policy might write the most heavily accessed small files to a segment comprised of flash, or other low latency storage. In this case, some simple per-inode statistics would need to be gathered by the vdisk <b>24</b> manager so that the policies for heavily accessed files could be applied to the correct set of files.</div>
<div class="description-paragraph" id="p-0078" num="0082">Two vdisk <b>24</b> operations, lookup and readdirplus, interpret the contents of a directory, while also returning the attributes associated with one or more files in that directory.</div>
<div class="description-paragraph" id="p-0079" num="0083">The lookup operation searches the directory for an entry with a specific name, and returns the file's inode number (which, when qualified with the vdisk's <b>24</b> ID, gives the file's file handle), along with the file's attributes. To obtain the file attributes, the implementation of the vdisk lookup operation will send a vdisk <b>24</b> getattr call to the vdisk <b>24</b> server, and return the attributes of the file along with the rest of the vnode lookup results. Making this call to the vdisk server <b>24</b> allows the lookup call to handle the case where the target file is located in another vdisk <b>24</b>.</div>
<div class="description-paragraph" id="p-0080" num="0084">Similarly, readdirplus returns a set of file names, along with the attributes of each of the files. Each separate file's attributes may, as in the lookup case, come from a different vdisk <b>24</b>, and, as with lookup, the attributes for these files come from vdisk <b>24</b> getattr cals made to the other vdisk(s) <b>24</b>.</div>
<div class="description-paragraph" id="p-0081" num="0085">Neither lookup nor readdirplus make any guarantees about reading the directory and obtaining the file attributes atomically, so the implementation can straightforwardly be done in two steps, first reading the directory's contents, and then obtaining the target file's attributes.</div>
<div class="description-paragraph" id="p-0082" num="0086">Directory modification operations are considerably more complex, in that they involve modifying multiple vdisks <b>24</b> atomically. To implement these multi-stage operations atomically, they are implemented as transactions, and make use of a persistent operations table (the POT <b>22</b>) in each vdisk <b>24</b>. The POT <b>22</b> entry stores the status of each complex operation, as well as any lock state involved in the operation. Each operation is implemented as a persistent state machine, recording the current state in the persistent operations table (POT <b>22</b>). For a given operation, each vdisk <b>24</b> stores its own POT <b>22</b> entry for its portion of the transaction.</div>
<div class="description-paragraph" id="p-0083" num="0087">Each operation in the POT <b>22</b> is assigned a UUID when it first arrives at the first vdisk <b>24</b> receiving the vdisk <b>24</b> operation; this is called the primary POT <b>22</b> entry. The execution of the operation may require sending requests to other vdisks <b>24</b> storing other objects modified by this vdisk <b>24</b> operation. These are called secondary requests, and their execution at a vdisk <b>24</b> may create secondary POT <b>22</b> entries in their local vdisk's persistent operations table. These secondary POT <b>22</b> entries are tagged with the same request UUID, and are used to ensure that sub-components of atomic operations are executed exactly once, even in the case of multiple server crashes and restarts.</div>
<div class="description-paragraph" id="p-0084" num="0088">Some secondary operations set locks on files, directories, or portions of directories. Locks in this system <b>10</b> are comprised of either a lock on an entire file handle, or a lock on a specific file name within a directory file handle. Locks on file handles conflict only if they specify the same file handle. Locks with names and file handles conflict only if both components match exactly. If one lock is a whole file handle lock and the other is a name lock, they conflict only if the file handle components match. Independent of the above, two read locks never conflict, even if the file handles would otherwise conflict.</div>
<div class="description-paragraph" id="p-0085" num="0089">Each class of directory modification operation in turn is now examined.</div>
<div class="description-paragraph" id="p-0086" num="0090">A vdisk delete or rmdir operation begins at the parent directory's vdisk <b>24</b> server. The request creates a primary POT <b>22</b> entry with a new UUID, and the target file handle (including the target vdisk <b>24</b>) is determined by doing a lookup operation on the directory. Before doing the lookup operation, the request establishes a lock on the &lt;parent-dir, name&gt; mapping, and once the local lookup operation completes, the operation sends a perform-unlink-target request to the target vdisk <b>24</b> tagged with the POT <b>22</b> entry's UUID. The perform-unlink-target operation sets a write lock on the target file handle; a parameter to this operation tells the server whether the vdisk <b>24</b> server should wait for the lock or should instead fail the operation on a lock conflict, and for file delete, the caller sets the “wait” flag only if the target file handle follows the source directory lock in the global locking hierarchy. If, as is likely, there is no conflicting lock, the target object is locked, a secondary POT <b>22</b> entry is created with the request's UUID, the object is destroyed, the lock on the target object is released, and the perform-unlink-target operation is marked as complete. While the object has been destroyed, and the operation is marked as complete, the request, tagged by UUID, stays in the persistent operation table until the operation is also completed at the primary server, to ensure that retransmissions of the perform-unlink-target operation are detected, and not re-executed. Once the perform-unlink-target call has completed, the primary vdisk <b>24</b> removes the file name from the source directory, and drops all of its local locks. The primary removes the operation's POT <b>22</b> entry is removed from its POT <b>22</b>, and a response is sent to the originator of the request. In addition, the request's UUID is batched up and sent eventually to the target object's vdisk <b>24</b> as part of a batch persistent-operation-complete request. Upon receipt of this message, the secondary removes the operation from its persistent-operation-table as well, since at this point, it knows that the operation will never be retransmitted (as it has been removed from the primary server). Note that if a crash occurs before the persistent-operation-complete request has been processed, the secondary can iterate over all of its pending operations, checking with the primary vdisk <b>24</b> to see if the operation is still in progress; if the secondary finds any operations that are no longer present in the primary's POT <b>22</b>, the secondary can remove the operation from its table, as well.</div>
<div class="description-paragraph" id="p-0087" num="0091"> <figref idrefs="DRAWINGS">FIG. 6</figref> shows the message flow for this example.</div>
<div class="description-paragraph" id="p-0088" num="0092"> <figref idrefs="DRAWINGS">FIG. 6</figref> shows the behavior of the system <b>10</b> when the target succeeds at obtaining its lock, or if the target waits for its lock. It is also possible that the target failed to obtain its lock on the target file handle. In this case, the source vdisk <b>24</b> releases its locks, and tries the operation in two phases, first locking the target, and then doing the actual work. In this case, the primary vdisk <b>24</b> server sends a slightly different request, prepare-unlink-target to the target. The prepare-unlink-target specifies that upon a conflict, the request should wait for the lock. After the primary gets the response, the source locks the source handle, and verifies that the name still matches the locked object. If it doesn't, the entire operation is restarted; otherwise the source server marks the operation as committed, and removes the entry from the directory, while concurrently telling the prepared target vdisk <b>24</b> to remove the target object, by sending it a commit-unlink-target operation. Once the removal is complete, the source vdisk <b>24</b> completes the operation and removes the operation from the persistent operation table. <figref idrefs="DRAWINGS">FIG. 7</figref> shows the behavior of the system <b>10</b> if the lock can't be obtained initially.</div>
<div class="description-paragraph" id="p-0089" num="0093">The hard link operation works fairly similarly to remove, but is simpler because all of the file handles are known at the start of the operation. The parameters to link include a source directory and file name, and a target file handle that may be part of another vdisk <b>24</b>. The source vdisk <b>24</b> determines the locking order between the target file and the source directory. The operation begins, as usual, by creating a request with a new UUID on the source vdisk <b>24</b>.</div>
<div class="description-paragraph" id="p-0090" num="0094">If the source needs to be locked first, the source vdisk <b>24</b> locks the directory+name, and then sends a perform-link-target operation that does the locking for the link target, and updates the attributes as well (primarily the link count and ctime fields). Once the source receives the response, its local entry transitions to “complete” state and batches up a cleanup operation to the target vdisk <b>24</b>, upon receipt of which the target can remove the operation from its persistent operation table and release all of its locks. <figref idrefs="DRAWINGS">FIG. 8</figref> illustrates this message flow.</div>
<div class="description-paragraph" id="p-0091" num="0095">If the target needs to be locked first, the source then sends it a prepare-link-target request, which locks the target file. The source then locks the source directory+name, ensures the entry doesn't exist, and creates it. Finally, it sends the target a commit-link-target request. The target executes the request, and keeps it in its POT <b>22</b> until the source contacts it, as part of a piggy-backed operation, indicating that the operation is complete and the request has been deleted. At this point, the target can remove the operation from its persistent operation table. <figref idrefs="DRAWINGS">FIG. 9</figref> illustrates the message flow in this case.</div>
<div class="description-paragraph" id="p-0092" num="0096">Create and mkdir function similarly to each other. Because both operations create new entities, rather than dealing with existing entities that might be active, they are simpler than remove/rmdir to implement.</div>
<div class="description-paragraph" id="p-0093" num="0097">The operation begins at the directory's vdisk <b>24</b> (the primary vdisk <b>24</b>), where the operation locks the file handle+name, and adds a new create request to the persistent operations table.</div>
<div class="description-paragraph" id="p-0094" num="0098">The target vdisk <b>24</b> is chosen either via a policy associated with the parent directory in which the object is being created, or via a global policy associated with the entire file system <b>10</b>. No matter what the policy's source, the policy can select a vdisk <b>24</b> on which to create the new object based on the load on the vdisk <b>24</b>, the operations/second capacity of the vdisk <b>24</b>, the space available on the segments comprising the vdisk <b>24</b>, or any other function of the state and configuration of the system's vdisks <b>24</b>.</div>
<div class="description-paragraph" id="p-0095" num="0099">The target object's vdisk <b>24</b> (the secondary vdisk <b>24</b>) then receives a perform-object-create operation, telling it the type of object to create (empty directory, symbolic link, or file). The object is created and the response is returned to the primary vdisk <b>24</b>, along with the identity of the newly created object. The primary vdisk <b>24</b> then creates the directory entry with the new object name and file handle, and marks the operation as complete. The vdisk <b>24</b> then batches up the completion notification to the secondary vdisk <b>24</b>, which removes the operation from its persistent operations table.</div>
<div class="description-paragraph" id="p-0096" num="0100"> <figref idrefs="DRAWINGS">FIG. 10</figref> illustrates the message flow.</div>
<div class="description-paragraph" id="p-0097" num="0101">Since these operations actually create new objects, one of the most important functions they perform is choosing a vdisk <b>24</b> to hold the new file, directory, or symbolic link. There are many potential algorithms to follow here, and the specific algorithm can be chosen either as a policy associated with the parent directory, or by a more global policy, perhaps pertaining to the entire global file system <b>10</b>.</div>
<div class="description-paragraph" id="p-0098" num="0102">One vdisk <b>24</b> choice function might be to create a new object on the vdisk <b>24</b> with the most free space, or the greatest percentage of free space; this would automatically keep space utilization balanced. Another vdisk <b>24</b> choice function might be to create a new object on the vdisk <b>24</b> with the lowest operation rate, i.e. on the vdisk <b>24</b> with the most spare capacity in operations/second. Obviously hybrid scoring approaches that combine a “free space” store and a “available operations/second” score could also be used. For example, the choice function could compute a linear combination of the two scores, and the invention could then create new objects on the vdisk <b>24</b> with the lowest hybrid score. Other hybrid functions might be include quadratic functions over the relevant scores, as well.</div>
<div class="description-paragraph" id="p-0099" num="0103">Of course, a very simple vdisk <b>24</b> choice function might be a simple round-robin algorithm, where vdisks <b>24</b> are chosen in a repeated, circular pattern, but it is unlikely that such an approach would be optimal under any useful metric.</div>
<div class="description-paragraph" id="p-0100" num="0104">The rename operation can involve up to four separate objects: a source directory, a target directory, a source file or directory, and a target file or directory. All four, in the worst case, may be located on separate vdisks <b>24</b>. Semantically, the rename operation removes the directory entry in the source directory, and creates a new directory entry to the same object in the target directory. If an entry already exists at the target directory, it is first destroyed.</div>
<div class="description-paragraph" id="p-0101" num="0105">The operation begins at the source directory's vdisk <b>24</b>. If the source and target directories are identical, a simplified operation can be performed. The source (and thus target) directory is simply locked and the contents of the directory are updated directly. There are two cases, depending upon whether the target of the rename exists or not. If the target does not exist, then nothing changes except the directory storing the file/directory names. In this case, there is no message flow to illustrate, because all work is done by the source/target directory's vdisk <b>24</b> server.</div>
<div class="description-paragraph" id="p-0102" num="0106">If the target file or directory does exist, there is a somewhat more complex operation, similar to a file delete. Specifically, a rename where the source and target directories are the same, and where the target of the rename exists (and will be unlinked), works similarly to a file delete operation, except that the locking is a bit more complex, since there are two names to lock in the parent directory, as well as the target object's file handle. A locking hierarchy is chosen that orders locks by file handle first, and then by file name within the file handle (for locks that include a file name component). With this locking order, either both file names are locked in the directory before locking the target file, or both file names are locked in the directory after locking the target file. A rename in this case begins by creating a POT <b>22</b> entry for the rename operation, and locking both the source and target file names within that directory. It then sends a perform-unlink-target operation to the target file's vdisk <b>24</b> (the secondary vdisk <b>24</b>), setting the flag saying that the target should wait for the lock only if the target's file handle is ordered after the directory's file handle in the locking order. If the target succeeds at setting the lock, it creates a POT <b>22</b> entry for the rename operation, and unlinks the target. It then responds to the primary vdisk <b>24</b>, which completes the rename operation by removing the source directory entry and changing the target directory entry to point to the source file's file handle. The message flow is the same as for the simple file delete case illustrated in <figref idrefs="DRAWINGS">FIG. 6</figref>. If the attempt at locking the target fails, the rename operation then drops all of its locks, and sends a prepare-unlink-target to the secondary vdisk <b>24</b>, and, upon receiving a response, then locks the parent directory's source and target file names. At this point, it verifies that the target file's identity is unchanged; if it has changed, the entire operation restarts. Otherwise, the primary vdisk <b>24</b> sends a commit-unlink-target operation to the secondary vdisk <b>24</b>, while updating the directory as in the first case. Finally, the primary vdisk <b>24</b> sends a response back to the caller, and batches a persistent-op-complete operation to clean up the state on the secondary vdisk <b>24</b>. <figref idrefs="DRAWINGS">FIG. 7</figref> shows the message flow for this case, with the difference that the directory update is as is described above in this paragraph.</div>
<div class="description-paragraph" id="p-0103" num="0107">In the most complex rename case, when the source and target directories differ, the operation is significantly more complex and expensive. The source directory's vdisk <b>24</b> server begins by creating a request that starts by sending a lookup operation to the target directory's vdisk <b>24</b>, looking up the target handle, while simultaneously locally looking up the source file handle. Once these operations complete, the identity of all objects involved in the rename operation is known, but no locks are held.</div>
<div class="description-paragraph" id="p-0104" num="0108">The source vdisk <b>24</b> server then sends a prepare-rename-source message to the source object's vdisk <b>24</b>, locking that object; it sends a prepare-rename-target message to the target object's vdisk <b>24</b>; and it sends a prepare-rename-tdir message to the target directory's vdisk <b>24</b>. Each of these operations locks the entity in question, with the source directory's vdisk <b>24</b> locking the source directory, and sending these messages sequentially in the order required by the global locking hierarchy. Once all of the entities are locked, the source vdisk verifies that the results of the initial lookups remain unchanged; if not, the entire operation restarts. Note that the prepare_rename_tdir operation performs the target lookup verification step itself, to avoid requiring an additional message exchange. Once all entities are prepared, locked and verified, the source then sends commit-source-object, commit-target-object and commit-target-directory operations to each of the source-object vdisk <b>24</b>, the target-object vdisk <b>24</b>, and the target-directory vdisks <b>24</b>, respectively. Once those operations have completed, the request enters the completed state at the source directory vdisk <b>24</b>, and batches up completion notifications to all of the other vdisks <b>24</b>, so they can free their state knowing that the source directory vdisk <b>24</b> will never again send operations associated with the completed request.</div>
<div class="description-paragraph" id="p-0105" num="0109"> <figref idrefs="DRAWINGS">FIG. 11</figref> shows the message flow in the most general version of the rename vdisk <b>24</b> operation.</div>
<div class="description-paragraph" id="p-0106" num="0110">The persistent operations table is utilized in the functioning of the above operations. Here is an example POT <b>22</b>:</div>
<div class="description-paragraph" id="p-0107" num="0111">
<tables id="TABLE-US-00001" num="00001">
<patent-tables colsep="0" frame="none" pgwide="1" rowsep="0">
<table align="left" class="description-table" cols="7" colsep="0" rowsep="0" width="100%">
<thead>
<tr class="description-tr">
<td align="center" class="description-td" colspan="7" nameend="7" namest="1" rowsep="1"> </td>
</tr>
<tr class="description-tr">
<td class="description-td"> </td>
<td class="description-td"> </td>
<td class="description-td">Primary</td>
<td class="description-td">Secondary</td>
<td class="description-td">Rename2</td>
<td class="description-td">Rename2</td>
<td class="description-td"> </td>
</tr>
<tr class="description-tr">
<td class="description-td">UUID</td>
<td class="description-td">OpCode</td>
<td class="description-td">Dir FH</td>
<td class="description-td">FH/Name</td>
<td class="description-td">Dir FH</td>
<td class="description-td">FH/Name</td>
<td class="description-td">Status</td>
</tr>
<tr class="description-tr">
<td align="center" class="description-td" colspan="7" nameend="7" namest="1" rowsep="1"> </td>
</tr>
</thead>
<tbody><tr class="description-tr">
<td class="description-td">1</td>
<td class="description-td">Create</td>
<td class="description-td">VDisk = 1</td>
<td class="description-td">VDisk = 2</td>
<td class="description-td">NA</td>
<td class="description-td">NA</td>
<td class="description-td">Complete</td>
</tr>
<tr class="description-tr">
<td class="description-td"> </td>
<td class="description-td"> </td>
<td class="description-td">Inode = 100</td>
<td class="description-td">Inode = 121</td>
</tr>
<tr class="description-tr">
<td class="description-td"> </td>
<td class="description-td"> </td>
<td class="description-td"> </td>
<td class="description-td">Name = “foo”</td>
</tr>
<tr class="description-tr">
<td class="description-td">2</td>
<td class="description-td">Delete</td>
<td class="description-td">VDisk = 1</td>
<td class="description-td">VDisk = 3</td>
<td class="description-td">NA</td>
<td class="description-td">NA</td>
<td class="description-td">Remote</td>
</tr>
<tr class="description-tr">
<td class="description-td"> </td>
<td class="description-td"> </td>
<td class="description-td">Inode = 102</td>
<td class="description-td">Inode = 122</td>
<td class="description-td"> </td>
<td class="description-td"> </td>
<td class="description-td">Done</td>
</tr>
<tr class="description-tr">
<td class="description-td"> </td>
<td class="description-td"> </td>
<td class="description-td"> </td>
<td class="description-td">Name = “bar”</td>
</tr>
<tr class="description-tr">
<td class="description-td">5</td>
<td class="description-td">Rename</td>
<td class="description-td">VDisk = 1</td>
<td class="description-td">VDisk = 30</td>
<td class="description-td">VDisk = 3</td>
<td class="description-td">VDisk = 11</td>
<td class="description-td">Remote</td>
</tr>
<tr class="description-tr">
<td class="description-td"> </td>
<td class="description-td"> </td>
<td class="description-td">Inode = 110</td>
<td class="description-td">Inode = 130</td>
<td class="description-td">Inode = 211</td>
<td class="description-td">Inode = 212</td>
<td class="description-td">Sent</td>
</tr>
<tr class="description-tr">
<td class="description-td"> </td>
<td class="description-td"> </td>
<td class="description-td"> </td>
<td class="description-td">Name = “m”</td>
<td class="description-td"> </td>
<td class="description-td">Name = “p”</td>
</tr>
<tr class="description-tr">
<td class="description-td">4</td>
<td class="description-td">Perform</td>
<td class="description-td">VDisk = 30</td>
<td class="description-td">VDisk = 1</td>
<td class="description-td">NA</td>
<td class="description-td">NA</td>
<td class="description-td">Response</td>
</tr>
<tr class="description-tr">
<td class="description-td"> </td>
<td class="description-td">Unlink</td>
<td class="description-td">Inode = NA</td>
<td class="description-td">Inode = 132</td>
<td class="description-td"> </td>
<td class="description-td"> </td>
<td class="description-td">Sent</td>
</tr>
<tr class="description-tr">
<td class="description-td"> </td>
<td class="description-td">Target</td>
<td class="description-td"> </td>
<td class="description-td">Name = NA</td>
</tr>
<tr class="description-tr">
<td align="center" class="description-td" colspan="7" nameend="7" namest="1" rowsep="1"> </td>
</tr>
</tbody></table>
</patent-tables>
</tables>
</div>
<div class="description-paragraph" id="p-0108" num="0112">In the table above, a set of POT <b>22</b> entries for vdisk <b>1</b> is shown. For the first four entries, vdisk <b>1</b> is the primary vdisk <b>24</b>, driving the distributed file system <b>10</b> operation in question. In the last entry, vdisk <b>1</b> is the secondary vdisk <b>24</b> for a delete operation being driven from vdisk <b>30</b>.</div>
<div class="description-paragraph" id="p-0109" num="0113">In more detail, the first POT <b>22</b> entry describes a file create operation for a file name “foo” being created in the directory whose file handle is 1.100 (in the format &lt;vdisk.inode&gt;). The file is being created in vdisk <b>2</b>, selected by policy rule associated with vdisk <b>1</b>. The operation has run to completion, and when the response from the secondary vdisk was received, it included the actual allocated inode from vdisk <b>2</b>, specifically inode <b>121</b>, which was entered into this POT <b>22</b> entry as well. The operation is in “complete” state, meaning the operation completed at the secondary vdisk, and the primary vdisk has completed its work as well. The entry remains in the POT <b>22</b> only until the entry for this same UUID in vdisk <b>2</b>'s POT <b>22</b> entry can be removed.</div>
<div class="description-paragraph" id="p-0110" num="0114">The second entry describes a file delete operation, where the directory holding the file being deleted has file handle 1.102. The file being deleted has file handle 3.122, which was obtained by looking up the file name “bar” in the primary directory; from its file handle we see that it is stored in vdisk <b>3</b>. The POT <b>22</b> entry is in “remote done” state, indicating that the secondary vdisk has freed the file, but the primary vdisk <b>24</b> has yet completed the removal of the directory entry from the primary directory itself.</div>
<div class="description-paragraph" id="p-0111" num="0115">The third entry (UUID <b>5</b>) describes a rename operation, where the source and target directories have file handles 1.110 and 3.211, respectively. The file being renamed has file handle 30.130, which was determined by the primary vdisk <b>24</b> doing a lookup operation on “m”, the file's name. The new name for the file is “p”, and that file already exists on vdisk <b>11</b>, with file handle 11.212. The operation is in “remote sent” state, meaning that primary vdisk is waiting for responses from the secondary vdisks <b>24</b> before it can continue. Note that there are three remote vdisks <b>24</b>, <b>3</b> (holding the second, target directory), <b>30</b> (holding the file being renamed) and <b>11</b> (holding the file being deleted by virtue of being the target of the rename operation).</div>
<div class="description-paragraph" id="p-0112" num="0116">Finally, the fourth entry describes a delete operation where the directory holding the file being deleted is stored on primary vdisk <b>30</b>, which is driving the delete operation for a file that happens to be stored on vdisk <b>1</b> (the secondary vdisk for this operation); the primary vdisk is requesting that this vdisk destroy that file, based on its file handle. The vdisk of the parent directory is 30, and the secondary vdisk does not need to know the inode component of that file handle. The file being deleted has file handle 1.132, and once that file has been destroyed, a response is sent back to the primary, telling it that the operation requested by transaction ID <b>4</b> is complete. Note that the entry is in “response sent” mode, meaning that the operation is complete at the secondary, and a response has been sent to the primary vdisk module.</div>
<div class="description-paragraph" id="p-0113" num="0117">The primary use of the persistent operations table (POT <b>22</b>) is in failure recovery, where a processor <b>26</b> handling one of the vdisks <b>24</b> involved in a multi-vdisk directory update fails. Essentially, a POT <b>22</b> entry acts as an intentions log entry, describing the operation to be performed, and whether that operation completed successfully or not.</div>
<div class="description-paragraph" id="p-0114" num="0118">In all of the directory modifying operations described above in this section, there is a primary vdisk, which is the vdisk <b>24</b> at which the operation begins execution. For any given operation, the POT <b>22</b> entries created by the primary vdisk <b>24</b> are called primary POT <b>22</b> entries. Similarly, the vdisks <b>24</b> contacted by the primary vdisk <b>24</b> to perform part of a directory operation are called secondary vdisks <b>24</b>, and their POT <b>22</b> entries are called secondary POT <b>22</b> entries. Note that these labels apply to the role a vdisk <b>24</b> and its POT <b>22</b> entries play in a particular operation; thus, a vdisk <b>24</b> may be the primary vdisk <b>24</b> for a particular operation even while it is a secondary vdisk <b>24</b> for another concurrently executing operation. For a given operation, the primary POT <b>22</b> entry and its secondary POT <b>22</b> entries are implicitly linked together by sharing the same unique transaction ID. In the example above, the first three entries are primary POT <b>22</b> entries, and the last entry is a secondary POT <b>22</b> entry.</div>
<div class="description-paragraph" id="p-0115" num="0119">Each POT <b>22</b> entry may be in one of two major states. It may be in the “executing” state, meaning that the request is currently executing, or it may be in the “complete” state, meaning that the request is finished, and has stored its results in the POT <b>22</b> entry.</div>
<div class="description-paragraph" id="p-0116" num="0120">If the processor <b>26</b> that created the primary POT <b>22</b> fails, then upon restart, the processor <b>26</b> restarts the request, skipping the stages it has already executed, but resending any subsidiary requests to the secondary vdisks <b>24</b>. All of these retransmitted requests are resent with the original transaction ID, so that they can be matched with any already existing POT <b>22</b> entries, should the secondary vdisk <b>24</b> already have received the request before the primary's failure. If a secondary POT <b>22</b> entry indicates that the request is still in “executing” state, the secondary simply continues its execution. If there is no secondary POT <b>22</b> entry, indicating that the request has not yet executed, the new request begins execution. Finally, if the POT <b>22</b> entry is in “complete” state, the results of the operation, stored in the POT <b>22</b> entry, are sent back to the primary vdisk <b>24</b>, without re-executing the secondary request.</div>
<div class="description-paragraph" id="p-0117" num="0121">Similarly, if a processor <b>26</b> processing a secondary POT <b>22</b> entry fails, then after recovering, if the secondary POT <b>22</b> entry is not in “complete” state, the operation is re-executed, skipping any portions that were already executed, and sending the response back to the primary vdisk <b>24</b> when the request completes. If the request is in “complete” state, the response is re-sent to the primary vdisk <b>24</b> without re-executing the request. If the primary vdisk <b>24</b> does not recognize the request's transaction ID, this means that the primary vdisk <b>24</b> had already received an earlier response from the secondary vdisk <b>24</b>, completed the primary request, and cleaned up its POT <b>22</b> entry. In this case, the secondary vdisk can delete its POT <b>22</b> entry.</div>
<div class="description-paragraph" id="p-0118" num="0122">A basic disk block allocation policy is implemented by storing some descriptive tags with every segment in the system <b>10</b>. These tags are implemented as a bitmap of administrator-defined attributes. For example, an administrator might define a “low latency” attribute, a “write efficient RAID” attribute and an “archival” attribute for storage, and apply the “low latency” attribute to segments comprised of flash storage, the “write efficient RAID” attribute to segments stored in RAID 1 storage, and the “archival” attribute to segments stored in RAID 5 storage. Other, orthogonal properties could also be defined, such as “rotating media” for drive-based segments, or “slow” media for data rotating below 5400 RPM.</div>
<div class="description-paragraph" id="p-0119" num="0123">In this basic policy implementation, each Mode has policy descriptors for three separate classes of storage used by the Mode. One policy descriptor describes the meta data associated with this file: in particular, the indirect blocks used for locating the file's data blocks. A second policy descriptor applies to the first N blocks of the file, and the final policy descriptor describes the storage holding the remainder of the file. Each policy descriptor may be represented as a pair of bitmaps, a mask bitmap and a value bitmap. A segment matches one of these pairs if the segment's tag bitmap, ANDed with the policy descriptor's mask bitmap, matches the descriptor's value bitmap. The fields stored in each Mode to represent these policies are shows in <figref idrefs="DRAWINGS">FIG. 12</figref>. In this structure, the field “Initial part block count” gives the value of N above, while each of the remaining boxes hold the mask and value bitmaps describing the segments to be used for indirect block allocation, initial part data allocation, and the segment to be used for the rest of the file's data allocation. Thus, for example, when a data block for an offset less than N needs to be allocated by a cleaner, one of the vdisk's segments is chosen from the set of segments whose descriptive tag matches the Mode's second policy descriptor.</div>
<div class="description-paragraph" id="p-0120" num="0124">When a new file or directory is created, the policy descriptor shown in <figref idrefs="DRAWINGS">FIG. 12</figref> is inherited from the object's parent directory, and used to select storage for the data written to the newly created object.</div>
<div class="description-paragraph" id="p-0121" num="0125">There are a number of operations used for managing policies and the files using policies.
</div> <ul> <li id="ul0003-0001" num="0000"> <ul> <li id="ul0004-0001" num="0126">GetPolicy—returns the policy object associated with a file or directory.</li> <li id="ul0004-0002" num="0127">SetPolicy—sets the policy object associated with a file or directory.</li> <li id="ul0004-0003" num="0128">Recursive apply—sets the policy object for a directory, and all of its descendent directories and files.</li> </ul> </li> </ul>
<div class="description-paragraph" id="p-0122" num="0129">Note that this is an exemplary policy description, and that many others are possible, including policy descriptors that are only applied if a file's attributes (size, owner, etc) have certain values, or the file's name matches a regular expression. One might, for example, define a policy that says all files whose name match the regular expression “.*\.o” and whose size is greater than 1 MB should be stored on “archival” class storage.</div>
<div class="description-paragraph" id="p-0123" num="0130">When the policy description in an Mode is updated, the current data layout for the corresponding file may no longer match the updated policy. In this case, the inode is placed in a persistent queue for a background process to update the Mode data's allocation to match the new policy. Similarly, when new storage is placed into service for a vdisk <b>24</b>, the existing policy descriptors may, upon re-examination, select some of the new storage to hold meta-data or normal data. In this case, all of the Modes need to be placed in a queue for a background task to verify that the inodes' block allocations still match the allocation policies.</div>
<div class="description-paragraph" id="p-0124" num="0131">For example, the administrator might add some new, very low latency storage to a vdisk's segment list, and then specify a new policy for one or more inodes to place the first 64 KB of data in that new segment. The system <b>10</b> would do this by performing an “AddSegment” operation to add the storage to the vdisk <b>24</b>, adjusting the table describing which segments are part of the storage space for a vdisk <b>24</b> to include the new segment, possibly followed by a SetPolicy operation to specify where to use this new storage for a portion of the file system name space. The system <b>10</b> would then internally perform a “readjust” operation on the inode or inodes in question, as the policies are updated, which would check that each block in each file whose policy is updated is allocated from a segment with the appropriate class of service. If a file fails this test, then the readjust operation would, for every block allocated from the wrong segment, allocate a block in the desired segment, copy the data from the old segment to the new segment, and adjust the indirect block pointing to the original block to point to the new block's location.</div>
<div class="description-paragraph" id="p-0125" num="0132">The preceding described the operation of the system <b>10</b> in terms of operations on individual Modes. These Modes are implemented in a manner similar to that in which Unix systems like FreeBSD implement inodes in their UFS file system, with the exception that in this invention, a policy-based block allocator is invoked to choose the segments from which to obtain blocks to add to a file. This allocator is invoked any time that any of the operations discussed in this section need to allocate more disk blocks to an existing or new file, directory, or symbolic link.</div>
<div class="description-paragraph" id="p-0126" num="0133">It is assumed that when a file or directory is created, it inherits a reference to a policy object from its parent directory. This policy specifies in some manner which blocks within the file should be allocated with which CoS from among the segments making up the file's vdisk <b>24</b>. For example, one such policy might be the disk block allocator described above.</div>
<div class="description-paragraph" id="p-0127" num="0134">When disk blocks are later allocated to such a file, the file's policy is consulted, and this policy specifies the desired class of service for the newly allocated blocks. The block allocator then chooses a segment with the desired CoS from among those within the file's vdisk <b>24</b>, and allocates space from that segment's block number subspace. Note that since all of the segments in a vdisk <b>24</b> reside at different locations within the same block number space, once blocks have been chosen to hold a file's newly added data blocks, the rest of the file allocation process proceeds in the same way as allocating space to files in the FreeBSD system.</div>
<div class="description-paragraph" id="p-0128" num="0135">The blocks within a segment are located via a very simple per-segment table that identifies the block number space for each segment within each vdisk <b>24</b>. For example, the segment table shown below might specify that the dotted segment resides at vdisk <b>24</b> block offset 10000 for 5000 blocks, and that the white segment resides at vdisk <b>24</b> block offset 20000 for 3000 blocks. Once a segment has been chosen for block allocation, the file system <b>10</b> determines the corresponding block number range for the segment, and then consults the UFS-like block allocation bitmap to determine which blocks are actually available in that segment. These block numbers are then stored in the UFS-like inodes and indirect blocks, just like the file system <b>10</b> stores any block number generated through any other block allocation algorithm used by the file system <b>10</b>. A global (per vdisk <b>24</b>) policy describes the class of service desired for the allocation of global file system meta data, such as inodes; typically, these would be allocated from relatively low latency storage.</div>
<div class="description-paragraph" id="p-0129" num="0136">The internals of a system <b>10</b> designed to implement a file system <b>10</b> comprised of multiple vdisks <b>24</b> is based upon implementing a set of vnode interface <b>18</b> calls on top of the vdisk <b>24</b> layer. Most vdisk <b>24</b> operations have analogous vnode operations, so that the interface <b>18</b> function for those operations is very simple.</div>
<div class="description-paragraph" id="p-0130" num="0137"> <figref idrefs="DRAWINGS">FIG. 13</figref> shows the relationship between vnodes, vdisks <b>24</b>, and segments, in the context of a Unix operating system such as OpenSolaris running a file system <b>10</b> based on this invention. In <figref idrefs="DRAWINGS">FIG. 13</figref>, there is a VFS instance for each exported file system tree, and each such instance is composed of one or more vdisks <b>24</b>.</div>
<div class="description-paragraph" id="p-0131" num="0138">In <figref idrefs="DRAWINGS">FIG. 13</figref>, it is shown how the vdisk <b>24</b> manager can fit into a standard Unix kernel. The figure shows three file systems <b>10</b> (and thus three name spaces). The first, VFS A, is comprised of files from vdisk <b>1</b> and vdisk <b>2</b>. The second, VFS B, is comprised of files allocated from vdisk <b>3</b>. The third, “Local UFS VFS” is a file system implemented from a normal Unix file system on its own local disks <b>20</b>.</div>
<div class="description-paragraph" id="p-0132" num="0139">The VFS shim layer implements a very thin layer mapping vnode operations to vdisk operations, and is described below in detail.</div>
<div class="description-paragraph" id="p-0133" num="0140">Once a vdisk <b>24</b> operation is invoked, some vdisk <b>24</b> operations, especially the directory modification operations described above, require performing internal vdisk <b>24</b> operations at other vdisks <b>24</b> implementing the same VFS. For example, a file create performed on a directory within VFS A that happens to be located on vdisk <b>1</b> might create its file on vdisk <b>2</b>, and thus might invoke the perform-create-target function on vdisk <b>2</b>. The block labeled “VDisk &lt;n&gt; Secondary” represents the server for those internal vdisk operations, and the dashed arrows indicate that these internal vdisk operations are typically invoked by the primary vdisk implementation for some other vdisk within the same file system tree (and thus the same VFS).</div>
<div class="description-paragraph" id="p-0134" num="0141">Each vdisk <b>24</b> implements its own pool of inodes, and so needs to store data persistently. Each vdisk <b>24</b> has a set of segments, accessed via the interface <b>18</b> described below, that it uses to store its persistent data. Different segments provide differing classes of service. For example, vdisk <b>1</b> has reasonably fast disk storage in a RAID 1 segment, and some extremely fast but expensive storage in a flash memory <b>12</b> segment. The vdisk <b>24</b> may have, for example, an automatic allocation policy specifying that the first 256 KB of each file should be allocated from blocks in the flash segment, and any additional storage should be allocated from the slower disk segment. If this policy is followed, for example, sequential access to a randomly chosen set of files would be very fast, since the first 256 KB of data would be available nearly instantaneously, during which time the disk arms are positioned to transfer the remainder of the file.</div>
<div class="description-paragraph" id="p-0135" num="0142">Similarly, vdisks <b>2</b> and <b>3</b> are compromised of a mix of relatively inexpensive RAID 5 storage, along with more expensive, but better performing RAID 1 storage.</div>
<div class="description-paragraph" id="p-0136" num="0143">The vdisk interface <b>18</b> consists of two sets of functions, a primary interface <b>18</b> called from the vnode layer on incoming file system calls, and a secondary interface <b>18</b> invoked by directory modifying primary vdisk operations that span multiple vdisks <b>24</b>.</div>
<div class="description-paragraph" id="p-0137" num="0144">The calls in the vdisk <b>24</b> primary interface <b>18</b> are typically called from the vnode layer. Many of the calls are applied to, or take, VDiskInode parameters describing the files themselves. The following describes the operations in the vdisk primary interface used by the vnode layer; some common operating system specific parameters, such as authentication credentials, have been omitted for clarity:</div>
<div class="description-paragraph" id="p-0138" num="0145">VDiskInode::release( )—decrease the reference count on an inode.</div>
<div class="description-paragraph" id="p-0139" num="0146">VDiskIndode::hold( )—increment the reference count on an inode.</div>
<div class="description-paragraph" id="p-0140" num="0147">VDiskInode::getattr(VDiskInodeAttr *attrsp)—get attributes associated with an inode.</div>
<div class="description-paragraph" id="p-0141" num="0148">VDiskInode::setattr(VDiskInodeSetAttr *newAttrsp, VDiskInodeAttr *updatedAttrsp)—update attributes as described by newAttrsp, returning the updated attributed in *updatedAttrsp.</div>
<div class="description-paragraph" id="p-0142" num="0149">VDiskInode::blocckRead(uint64_t offset, uint32_t count, uint32_t flags, uint32_t *bufCount, buf **bufpp, VDiskInodeAttr *attrsp)—read data from Mode starting at offset, for count bytes. The value *bufCount on input specifies the size of the bufpp array, and on output is set to the actual number of referenced buffers returned. The Mode's attributes at the time of the read are returned in *attrsp. Note that a flag of 1 specifies that buffers should be obtained held for writing instead of reading, indicating that the operation is part of a write operation that will modify the buffers.</div>
<div class="description-paragraph" id="p-0143" num="0150">VDiskInode::truncate(uint64_t offset, uint32_t count, VDiskInodeSetAttr *newAttrsp)—zero bytes from byte ‘offset’ for ‘count’ bytes. Any whole blocks that can be freed are freed. The newAttrsp parameter optionally may specify updated mtime, ctime or atime values for the file.</div>
<div class="description-paragraph" id="p-0144" num="0151">VDiskInode::readdir(uint64_t *cookiep, uint32_t *countp, char *resultsp, uint32_t *flagsp)—returns directory listing results, starting at an opaque value. In the initial call to readdir, *cookiep should be 0. *countp gives the number of bytes available in the *resultsp buffer. Each entry consists of a 16 bit file name length, followed by a UTF-8 encoded file name, followed by a 64 bit “Mode number”, followed by a 64 bit cookie value for the directory entry following this entry. An integral number of directory entries is always returned by readdir. The 1 bit is set in *flagsp if EOF is encountered, and no further entries will be returned. Note that *countp is updated to indicate the actual number of bytes returned, and *cookiep is updated to give the cookie value that, when passed into a new instance of this call, will return the next entry in the directory listing after those returned by this call.</div>
<div class="description-paragraph" id="p-0145" num="0152">VDiskInode::lookup(char *namep, VDiskInode **newInodepp)—lookup the name ‘namep’ in the specified directory, returning a held reference to the target Mode in *newInodepp. If the entry can't be located, a non-zero error code is returned. A return code of ENOENT is reserved for an indication that no temporary error occurred, and the file name definitely does not exist in the directory.</div>
<div class="description-paragraph" id="p-0146" num="0153">VDiskInode::readdirplus(uint64_t *cookiep, uint32_t *countp, uint32_t *entriesp, char *resultsp, VDiskInodeAttr *attrsp, uint32_t *flagsp)—This function acts like a combination of a readdir operation followed by getattr operations for each file entry returned. The parameters are the same as in readdir, with the following changes. The field *entriesp on entry gives the space available to hold returned attributes in the attrsp array, which points to an array of attribute structures. On exit, this field is set to the number of entries actually returned. The information returned in attrsp is ordered the same as the entries in the resultsp array.</div>
<div class="description-paragraph" id="p-0147" num="0154">VDiskInode::create(char *namep, VDisk *newVDiskp, VDiskInodeSetAttr *newAttrsp, VDiskInode **inodepp, VDiskInodeAttr *attrsp)—create a new file in the specified directory, using the new attributes specified by newAttrsp. The resulting attributes are returned in *attrsp, and a new inode is returned in *inodepp. Note that the file is created in a new vdisk specified by the newVDiskp parameter; if this parameter is null, the target vdisk is determined by consulting the policy choice module shown in <figref idrefs="DRAWINGS">FIG. 5</figref>.</div>
<div class="description-paragraph" id="p-0148" num="0155">VDiskInode::remove(char *namep, VDiskInode **inodepp, VDiskInodeAttr *attrsp)—remove a file or symbolic link from the specified directory. The updated object attributes are returned in *attrsp; this is meaningful for file's whose link count was decremented, but not to zero; in this case, *inodepp will also be set to a held reference to the inode in question.</div>
<div class="description-paragraph" id="p-0149" num="0156">VDiskInode::mkdir(char *namep, VDisk *newVDiskp, VDiskInodeSetAttr *newAttrsp, VDiskInode **inodepp, VDiskInodeAttr *attrsp)—create a directory with the specified name, with the new attributes specified by *newAttrsp. A held reference to the newly created inode is returned in *inodepp, and the newly created file's attributes are returned in *attrsp. Note that the file is created in a new vdisk specified by the newVDiskp parameter; if this parameter is null, the target vdisk is determined by consulting the policy choice module shown in <figref idrefs="DRAWINGS">FIG. 5</figref> </div>
<div class="description-paragraph" id="p-0150" num="0157">VDiskInode::rmdir(char *namep)—remove the directory named ‘namep’ from the directory to which this operation is applied.</div>
<div class="description-paragraph" id="p-0151" num="0158">VDiskInode::symlink(char *namep, VDisk *newVDiskp, char *contentsp, VDiskInodeSetAttr *newAttrsp, VDiskInodeAttr *attrsp)—create a symbolic link named ‘namep’ with initial contents ‘contentsp’. The initial attributes are set from *newAttrsp, and the resulting full attributes are returned in *attrsp. Note that the file is created in a new vdisk specified by the newVDiskp parameter; if this parameter is null, the target vdisk is determined by consulting the policy choice module shown in <figref idrefs="DRAWINGS">FIG. 5</figref>.</div>
<div class="description-paragraph" id="p-0152" num="0159">VDiskInode::link(char *namep, VDiskInode *inodep, VDiskInodeAttr *attrsp)—create a hard link with name ‘namep’ in the specified directory, to the object specified by the Mode *inodep. Updated attributes for the target object are returned in *attrsp, as of immediately after the operation's completion.</div>
<div class="description-paragraph" id="p-0153" num="0160">VDiskInode::rename(char *snamep, VDiskInode *targetp, char *tnamep, VDiskInode **sinodep, VDiskInode **tinodep, VDiskInodeAttr *sattrp, VDiskInodeAttr *tattrp)—rename the file name ‘snamep’ in the source (applied) inode, changing its name to ‘tnamep’ in the target directory ‘targetp’. A read reference to the updated source Mode is returned in **sinodep, and a reference to the target Mode is returned in *tinodepp if the target object continues to exist after the unlink operation. Attributes immediately after the operation are returned in *sattrp and *tattrp for the source and target objects, respectively.</div>
<div class="description-paragraph" id="p-0154" num="0161">The operations above refer to the following non-opaque structures: bufs (disk buffers), VDiskInodeAttr and VDiskInodeSetAttr structures. The following tables show the contents of each of these structures:</div>
<div class="description-paragraph" id="p-0155" num="0162">The buf structure represents a disk buffer. While there are many possible implementations of such a structure, the key fields that are typically present include the virtual and physical addresses of the data in the buffer, as well as a reference count, that, when zero, indicates that no processes are actively accessing the buffer.</div>
<div class="description-paragraph" id="p-0156" num="0163">
<tables id="TABLE-US-00002" num="00002">
<patent-tables colsep="0" frame="none" rowsep="0">
<table align="left" class="description-table" cols="3" colsep="0" rowsep="0" width="100%">
<thead>
<tr class="description-tr">
<td class="description-td"> </td>
<td align="center" class="description-td" colspan="3" nameend="2" namest="offset" rowsep="1"> </td>
</tr>
<tr class="description-tr">
<td class="description-td"> </td>
<td class="description-td">buf structure</td>
</tr>
<tr class="description-tr">
<td class="description-td"> </td>
<td align="center" class="description-td" colspan="3" nameend="2" namest="offset" rowsep="1"> </td>
</tr>
</thead>
<tbody><tr class="description-tr">
<td class="description-td"> </td>
</tr>
</tbody></table>
<table align="left" class="description-table" cols="4" colsep="0" rowsep="0" width="100%">
<tbody><tr class="description-tr">
<td class="description-td"> </td>
<td class="description-td">Data</td>
<td class="description-td">char *</td>
<td class="description-td">Points to mapped in data buffer.</td>
</tr>
<tr class="description-tr">
<td class="description-td"> </td>
<td class="description-td">PhysAddr</td>
<td class="description-td">char *</td>
<td class="description-td">Points to phyocation of buffer.</td>
</tr>
<tr class="description-tr">
<td class="description-td"> </td>
<td class="description-td">RefCount</td>
<td class="description-td">uint32_t</td>
<td class="description-td">Reference count for buffer.</td>
</tr>
<tr class="description-tr">
<td class="description-td"> </td>
<td align="center" class="description-td" colspan="4" nameend="3" namest="offset" rowsep="1"> </td>
</tr>
</tbody></table>
</patent-tables>
</tables>
</div>
<div class="description-paragraph" id="p-0157" num="0164">Next, the VDiskInodeAttr structure is described; this gives information similar to the Unix “struct stat” structure:</div>
<div class="description-paragraph" id="p-0158" num="0165">
<tables id="TABLE-US-00003" num="00003">
<patent-tables colsep="0" frame="none" rowsep="0">
<table align="left" class="description-table" cols="2" colsep="0" rowsep="0" width="100%">
<thead>
<tr class="description-tr">
<td align="center" class="description-td" colspan="2" nameend="2" namest="1" rowsep="1"> </td>
</tr>
<tr class="description-tr">
<td class="description-td">VDiskInodeAttr</td>
<td class="description-td"> </td>
</tr>
<tr class="description-tr">
<td class="description-td">structure</td>
</tr>
<tr class="description-tr">
<td align="center" class="description-td" colspan="2" nameend="2" namest="1" rowsep="1"> </td>
</tr>
</thead>
<tbody><tr class="description-tr">
<td class="description-td"> </td>
</tr>
</tbody></table>
<table align="left" class="description-table" cols="3" colsep="0" rowsep="0" width="100%">
<tbody><tr class="description-tr">
<td class="description-td">Device</td>
<td class="description-td">uint64_t</td>
<td class="description-td">A unique value indicating the vdisk</td>
</tr>
<tr class="description-tr">
<td class="description-td"> </td>
<td class="description-td"> </td>
<td class="description-td">storing the file.</td>
</tr>
<tr class="description-tr">
<td class="description-td">Inode</td>
<td class="description-td">uint64_t</td>
<td class="description-td">A unique value indicating the inode with</td>
</tr>
<tr class="description-tr">
<td class="description-td"> </td>
<td class="description-td"> </td>
<td class="description-td">the vdisk/device corresponding to this file.</td>
</tr>
<tr class="description-tr">
<td class="description-td">UnixModeBits</td>
<td class="description-td">uint16_t</td>
<td class="description-td">16 bits of file mode as defined by POSIX</td>
</tr>
<tr class="description-tr">
<td class="description-td"> </td>
<td class="description-td"> </td>
<td class="description-td">file system specification (1003.1). This</td>
</tr>
<tr class="description-tr">
<td class="description-td"> </td>
<td class="description-td"> </td>
<td class="description-td">includes both the file type as well as the</td>
</tr>
<tr class="description-tr">
<td class="description-td"> </td>
<td class="description-td"> </td>
<td class="description-td">basic file protection bits.</td>
</tr>
<tr class="description-tr">
<td class="description-td">Owner</td>
<td class="description-td">uint32_t</td>
<td class="description-td">File owner's ID.</td>
</tr>
<tr class="description-tr">
<td class="description-td">Group</td>
<td class="description-td">uint32_t</td>
<td class="description-td">File's group owner.</td>
</tr>
<tr class="description-tr">
<td class="description-td">Length</td>
<td class="description-td">uint64_t</td>
<td class="description-td">File length in bytes.</td>
</tr>
<tr class="description-tr">
<td class="description-td">Access time</td>
<td class="description-td">2 ×</td>
<td class="description-td">Time file data last accessed, in seconds</td>
</tr>
<tr class="description-tr">
<td class="description-td"> </td>
<td class="description-td">uint32_t</td>
<td class="description-td">and microseconds since midnight,</td>
</tr>
<tr class="description-tr">
<td class="description-td"> </td>
<td class="description-td"> </td>
<td class="description-td">1/1/1970 GMT.</td>
</tr>
<tr class="description-tr">
<td class="description-td">Modification time</td>
<td class="description-td">2 ×</td>
<td class="description-td">Time file data last modified, in same</td>
</tr>
<tr class="description-tr">
<td class="description-td"> </td>
<td class="description-td">uint32_t</td>
<td class="description-td">format.</td>
</tr>
<tr class="description-tr">
<td class="description-td">Change time</td>
<td class="description-td">2 ×</td>
<td class="description-td">Time file attributes last changed, in same</td>
</tr>
<tr class="description-tr">
<td class="description-td"> </td>
<td class="description-td">uint32_t</td>
<td class="description-td">format.</td>
</tr>
<tr class="description-tr">
<td class="description-td">Space</td>
<td class="description-td">uint64_t</td>
<td class="description-td">Space allocated for file, in bytes.</td>
</tr>
<tr class="description-tr">
<td align="center" class="description-td" colspan="3" nameend="3" namest="1" rowsep="1"> </td>
</tr>
</tbody></table>
</patent-tables>
</tables>
</div>
<div class="description-paragraph" id="p-0159" num="0166">The VDiskInodeSetAttr structure includes all of the fields of a VDiskInodeAttr structure, plus a single bit for each field, which is set to 1 if the value is valid in the structure, and should be set into the file's attributes, and 0 if the value should be ignored. This structure is passed to calls that set or change file attributes, and only the fields for which the corresponding bit is set are updated by the call.</div>
<div class="description-paragraph" id="p-0160" num="0167">Note that in all cases except for persistentOpComplete, the POT <b>22</b> entry remains in the persistent operations table until a persistentOpComplete is received for the entry. At any time, the secondary may also verify that the operation tagged with a POT <b>22</b> entry's request ID is still present at the primary; if it isn't, this means that the operation completed, the persistentOpComplete request got lost, and the secondary can and should remove the operation's entry from the POT <b>22</b> as well.</div>
<div class="description-paragraph" id="p-0161" num="0168">VDisk::performUnlinkTarget(UUID requestId, VDiskInode *targetp)—Create POT <b>22</b> entry with ID requested, then lock the file handle for Mode targetp, decrement the target object's link count, and drop the file handle lock, leaving the POT <b>22</b> entry to catch duplicate operations.</div>
<div class="description-paragraph" id="p-0162" num="0169">VDisk::prepareUnlinkTarget(UUID requestId, VDiskInode *targetp)—Create POT <b>22</b> entry with ID requested, and lock the file handle for Mode targetp, leaving locks set and POT <b>22</b> entry to catch duplicate operations.</div>
<div class="description-paragraph" id="p-0163" num="0170">VDisk::commitUnlinkTarget(UUID requestId)—Unlink the locked object referenced from the exiting POT <b>22</b> entry, drop the lock and return. This call expects to be applied to a requestId specifying an operation that has already performed a prepareUnlinkTarget operation, which specified the Mode to be unlinked.</div>
<div class="description-paragraph" id="p-0164" num="0171">VDisk::performLinkTarget(UUID requestId, VDiskInode *targetp)—Create a POT <b>22</b> entry with the specified requestId, lock the specified object, increment object's link count, drop locks and return. The entry remains in the POT <b>22</b> until the receipt of a persistentOpComplete request with the same transaction UUID.</div>
<div class="description-paragraph" id="p-0165" num="0172">VDisk::prepareLinkTarget(UUID requestId, VDiskInode *targetp)—Create a POT <b>22</b> entry with ID from requestId, lock the specified object *targetp and return.</div>
<div class="description-paragraph" id="p-0166" num="0173">VDisk::commitLinkTarget(UUID requestId)—Increment the link count on the object locked by the request and stored in the POT <b>22</b> entry, update its attributes, drop the lock on the target and return. This call expected to be performed on a POT <b>22</b> entry for which a prepareLinkTarget operation has already been successfully performed.</div>
<div class="description-paragraph" id="p-0167" num="0174">VDisk::performCreateTarget(UUID requestId, VDiskInode **targetpp, VDiskInodeSetAttrs setAttrs)—Create a POT <b>22</b> entry for the request ID, allocate the target file Mode (storing it in the POT <b>22</b> entry) and return the new object's file handle to the caller. On a retransmission of this request, resend the already allocated object's file handle. The setAttrs parameter specifies the type of object to be created, as well as its initial attributes.</div>
<div class="description-paragraph" id="p-0168" num="0175">VDiskInode::lookup—This is the same function as present in the primary interface <b>18</b>; it can also be invoked via the secondary interface <b>18</b>.</div>
<div class="description-paragraph" id="p-0169" num="0176">VDisk::prepareRenameOperand(UUID requestId, VDiskInode *targetp)—Create POT <b>22</b> entry, lock file handle of target Mode, and return. This operation is performed for the target directory Mode, the source object Mode and the target object Mode for rename operations affecting two directories. For single directory renames, the operation is performed on the source object Mode, and, if it exists, the target object Mode.</div>
<div class="description-paragraph" id="p-0170" num="0177">VDisk::commitRenameTargetDir(UUID requestId, char *namep, VDiskInode *inodep)—Update the target directory's entry for the target ‘namep’ to point to the file handle for the mode specified by inodep. Then drop all locks on inodep's file handle, and return. This call requires that a prepareRenameOperand call have been previously made with this requestId to this server.</div>
<div class="description-paragraph" id="p-0171" num="0178">VDisk::commitRenameTargetFile(UUID requestId)—Update the target inode associated with the request's POT <b>22</b> entry by decrementing its link count, then drop its locks and return. This call requires that a prepareRenameOperand call have been previously made with this requestId to this server.</div>
<div class="description-paragraph" id="p-0172" num="0179">VDisk::commitRenameSourceFile(UUID requestId, VDiskInode *targetDirp)—If we're renaming a directory, update the “. . . ” pointer in the directory associated with the locked file handle (obtained via the POT <b>22</b> entry), drop the locks and return. This call requires that a prepareRenameOperand call have been previously made with this requestId to this server.</div>
<div class="description-paragraph" id="p-0173" num="0180">VDisk::persistentOpComplete(UUID requestId)—This operation removes the POT <b>22</b> entry tagged with requestId from the persistent operations table. To guard against this message being lost due to a poorly timed system crash, the secondary also periodically verifies that old POT <b>22</b> entries are still valid by contacting the primary vdisk with the VDisk::checkOperationStatus call to determine whether the POT <b>22</b> entry's request ID is still active.</div>
<div class="description-paragraph" id="p-0174" num="0181">VDisk::checkOperationStatus(UUID requestId, int *statusp)—This operation checks on the status of the transaction tagged with requestId. The value of *statusp is set, on return, to the status of the operation, that is, one of remoteSent, remoteDone, complete, or unknown. The first three are normal states recorded in a POT <b>22</b> entry, while the last indicates that there is no POT <b>22</b> entry with the specified transaction UUID.</div>
<div class="description-paragraph" id="p-0175" num="0182">Every operation in the vnode interface <b>18</b> has a corresponding operation in the vdisk <b>24</b> interface <b>18</b>, having the same name. Most operations in the vdisk <b>24</b> interface <b>18</b> require a subset of the input parameters of the corresponding vnode operation with the same name, or return a superset of the output parameters required by the vnode operation, and so can trivially be implemented by calling the corresponding vdisk <b>24</b> operation with the underling VDiskInode object (which can even be embedded in the vnode structure used by the vnode interface <b>18</b>).</div>
<div class="description-paragraph" id="p-0176" num="0183">The segment interface <b>18</b> provides operations for reading and writing file data, and reading and caching directory information. The following operations are provided:</div>
<div class="description-paragraph" id="p-0177" num="0184">Segment::read(uint64_t offset, uint32_t count, buf *datap). This call reads the data located at the specified offset, returning it in the provided buffer. Only count bytes are transferred.</div>
<div class="description-paragraph" id="p-0178" num="0185">Segment:write(uint64_t offset, uint32 t count, buf *datap). This call works like read, only the data is written from the start of the provided buffer.</div>
<div class="description-paragraph" id="p-0179" num="0186">Segment::readdir(uint64_t offset, uin32_t count, buf *datap). This call returns a block of directory entries in a standard form from a physical offset within a directory. The data returned is an integral number of records, each giving a file name (including the file name's length in bytes), and the 64 bit Mode number within the vdisk <b>24</b> of the file.</div>
<div class="description-paragraph" id="p-0180" num="0187">Note that these calls can be executed as local calls to access drives connected to the same computer system as the caller, or as remote procedure calls to access drives connected to other computer systems.</div>
<div class="description-paragraph" id="p-0181" num="0188">This invention provides two significant advantages over the state of the art today in file systems. First, the invention provides a much more flexible mechanism for changing the class of service of files, and even portions of files, than traditional file systems, both with directed attached storage and network attached storage systems. By class of service, we mean without restriction any property of the storage such as transfer rate, request latency, reliability or expense. Second, the invention provides a significantly simpler administrative model for file systems that can be serviced by multiple processes, for example, on a multiprocessor or on multiple systems in a clustered server. Because the invention divides a file system into an arbitrarily large number of independently servable and individually repairable components automatically, rather than by requiring the administrator to define a set of volumes, the administrative model is much simpler, and the resulting system is much easier to service.</div>
<div class="description-paragraph" id="p-0182" num="0189">For example, with this invention, an administrator could specify a policy where the first megabyte of every file would be located on segments having very low latency, perhaps comprised of very fast disks <b>20</b> or flash memory <b>12</b>. The remaining blocks would be allocated from normal storage vdisks <b>24</b>. With an operations mix that chooses files at random and then reads each chosen file sequentially, this policy would reduce overall latency to the data in these files, since the first portion of the data would be accessible at very low latency, and during the transfer of this first portion of the data, the remaining data could be accessed from drives with a higher latency.</div>
<div class="description-paragraph" id="p-0183" num="0190">In many of today's storage systems, there are several options for changing the class of service of stored data. Systems like IBM's AFS, the Open Software Foundation's DCE/DFS, NetApp's Ontap GX, and Sun's ZFS provide a mechanism for moving volumes, representing subtrees of the file system name space, from one storage area to another. When the two storage areas provide different classes of storage, the administrator effectively changes the class of storage for the relevant subtree when moving the volume from one area to another. This invention improves upon this art in several ways. First, volume boundaries are administratively difficult to adjust after the creation of the volumes, while this invention does not have a comparable volume concept whose boundaries might need adjustment to match the desired class of service boundaries. Instead, this invention provides multiple classes of storage within the block address space used by a single pool of inodes (a vdisk <b>24</b>), so that any file can be moved to storage with a new class of storage at any time, without changing where it resides in the file system name space. Second, class of service policies that adjust the class of service for data stored in a file system, in this invention, can make adjustments on a block-by-block basis. The above systems would all need to relocate an entire volume to make any class of service adjustments, and would furthermore be unable to make any adjustments at any granularity below that of an entire directory and its contents, while this invention can adjust data's class of service on a file by file, or even a block by block basis.</div>
<div class="description-paragraph" id="p-0184" num="0191">In terms of administrative model simplicity, again, comparing this invention with volume-based data architectures, this invention has the advantage that no volume boundaries have to be chosen at all—instead, data is randomly distributed among vdisks <b>24</b>. The class of service of the storage is associated not with the specific vdisk chosen, but with the type of segment storing the data within a particular vdisk <b>24</b>.</div>
<div class="description-paragraph" id="p-0185" num="0192">In the realm of serviceability, instead of having to run disk consistency checks over the entire file system, or over a single volume, both administratively visible concepts, in the invention, disk consistency checks are run over individual vdisks <b>24</b>. Vdisks <b>24</b> are not individually managed by administrators, so that having many vdisks <b>24</b> making up an individual file system does not add administrative complexity to the system management task.</div>
<div class="description-paragraph" id="p-0186" num="0193">In terms of meta data scalability, this invention improves on the state of the art for a global name space in a number of ways. As compared with a name space with a meta data synchronization server, such as Red Hat's (originally Sistina's) GFS, this system performs indirect block updates completely within an individual vdisk <b>24</b>, without any communication with other vdisks <b>24</b>. This system also performs directory updates on at most two vdisks <b>24</b> (except for the infrequently executed rename operation, which typically involves one vdisk <b>24</b>, but can in some complex cases involve up to four). When multiple vdisks <b>24</b> collaborate on a single directory operation, they do so by exchanging a small number of messages among themselves (as described above), where each individual operation actually executes on a single vdisk <b>24</b>. Since operations on each vdisk <b>24</b> can be performed by separate processors <b>26</b> without any references to data structures controlled by other vdisks <b>24</b>, this architecture allows significant global file system scaling without requiring a meta data server acting as a synchronization point, via the splitting of an active file system into a moderately large number of vdisks <b>24</b>. Thus, because files are distributed among vdisks <b>24</b> automatically, a single name space can be distributed among multiple processors <b>26</b> without any manual administrative intervention.</div>
<div class="description-paragraph" id="p-0187" num="0194">As compared with systems like IBM's AFS, the OSF's DCE/DFS and NetApp's Ontap/GX, which divide the global file system into a number of independent subtrees (volumes), this invention's scalability benefits come from its ability to divide the files within a given directory into a number of vdisks <b>24</b>, all of which can be processed independently (as opposed to the above systems, which require that all files within a single directory reside in a single volume, and thus be served by a single processor <b>26</b> system.) In addition, because vdisk <b>24</b> creation and deletion can be automated much more easily than volume creation and deletion (since the latter requires an administrator's choosing the volume boundaries in the global name space), this invention allows for the creation of many more vdisks <b>24</b> than the above systems, allowing automatic load balancing algorithms more flexibility to smoothly distribute vdisks <b>24</b> across processors.</div>
<div class="description-paragraph" id="p-0188" num="0195">In terms of class of service management (CoS), this invention is believed to improve on the state of the art in a number of ways. The state of the art in CoS management has been to relocate volumes in designs such as AFS, DCE/DFS and Ontap/GX from underlying storage with one class of service to underlying storage having a different class of service. The weaknesses of this approach are that the granularity of the data whose CoS is changed is that of an entire volume, that all of the data within the volume are copied in order to change the CoS of any of the data, and that the volume boundaries are chosen initially to match the boundaries at which the administrator, sometime in the future, will require for CoS updates. Changing volume boundaries after volume creation is both complicated and difficult to do while the data is being accessed by clients concurrently, since file handles held by client systems include a volume ID as part of that file handle. This means that operations changing volume boundaries will change client resident file handles, limiting the transparency of those operations. This invention, on the other hand, determines the CoS for an individual block of a file by choosing the appropriate segment from which to allocate the block, and thus operates at a lower level of abstraction than file handles. No administrative boundaries need to be determined or changed before changing a CoS policy. Furthermore, if new data with a different class of service becomes available, it can be divided into a number of segments, and each segment can be joined to existing vdisks <b>24</b> automatically, providing convenient access to the new type of storage for existing files. A new policy could then specify which blocks of which files should use the new storage, and that new storage could be automatically used for newly written files, while in the background, files conforming to the new policy could have their data migrated into the new. In this case, in other words, the ability to dynamically add new segments to a vdisk's block address space, combined with invention's ability to allocate and reallocate file data from any of a vdisk's segments, allows a very inexpensive CoS management mechanism that can specify different classes of service at a very fine level of granularity (that of individual blocks in a file), and that can also change the class of service of an existing file continuously in the background, also on a block by block basis.</div>
<div class="description-paragraph" id="p-0189" num="0196">A glossary of various terms used here follows.
</div> <ul> <li id="ul0005-0001" num="0000"> <ul> <li id="ul0006-0001" num="0197">Chunk—A fixed sized, contiguous portion of a single disk. Chunks may store data or checksum/parity information. Multiple chunks sharing the same class of service, or basic attributes, are concatenated into segments, a variable sized piece of storage.</li> <li id="ul0006-0002" num="0198">File attributes—Meta data information describing the properties of a file, including the file's length in bytes, the user ID of the owner of the file, the file's last accessed time, last modified time and last “attributes modified” time.</li> <li id="ul0006-0003" num="0199">Persistent Operations Table (POT <b>22</b>)—A per-vdisk table tracking the progress of atomic file system operations that affect a single VFS, but one or more vdisks <b>24</b>. For example, a file create or delete may affect a directory stored within one vdisk <b>24</b>, and a file stored within another vdisk <b>24</b> in the same virtual file system. The persistent operations table on each vdisk <b>24</b> keeps track of the progress of each such file system operation. All entries describing the progress of a single atomic operation are tagged with the same operation UUID.</li> <li id="ul0006-0004" num="0200">RAID array—An array of physical disks grouped together with some form of RAID parity scheme, and storing a number of fixed sized chunks.</li> <li id="ul0006-0005" num="0201">Segment—A variable length collection of a number of chunks, all sharing the same type of storage, for example, RAID 1 storage comprised of 15K RPM disk drives. A segment can be addressed internally by a virtual 64 bit block pointer; these addresses only map to the data chunks of a segment, not the parity chunks. The virtual addresses are all contiguous within a single segment, but the underlying physical addresses of the individual chunks of which the segment is made may be scattered throughout the disks <b>20</b> attached to a computing system.</li> <li id="ul0006-0006" num="0202">Segment Interface—A simple interface providing operations to read and write data stored in a segment.</li> <li id="ul0006-0007" num="0203">UUID—Universal Unique IDentifier, a 128 bit, easy to construct identifier that is unique over all systems and all time. Typically, these are constructed using the IEEE 48 bit hardware address of some card in the computing system, combined with a very fine granularity clock value, and a process ID and/or boot counter.</li> <li id="ul0006-0008" num="0204">VDisk—An arbitrary collection of inodes, not connected as a single file system tree, storing its data and meta data in a dedicated collection of segments. The different segments within a vdisk <b>24</b> may have different properties, e.g. RAID levels, transfer rates, etc, and individual files may be allocated entirely from one segment within a vdisk <b>24</b>, or from multiple segments, depending upon externally provided policies. For example, one possible policy might be to place all meta data in a RAID 1 vdisk, and put all user data in a RAID 5 vdisk. Another possible policy might be to store the first megabyte of every file in a segment having very low latency (perhaps comprising flash memory <b>12</b>), with the remaining blocks allocated from normal RAID 5 storage, so that reading random medium-sized files in their entirety could be done with very low overall latency. One or more vdisks are combined to create a VFS, or file system.</li> <li id="ul0006-0009" num="0205">VDisk Interface—An interface used in this invention to perform file system modifying operations on files stored in vdisks <b>24</b> making up a single VFS. The key new functionality in the vdisk interface allows directory operations to change objects in more than one vdisk, by beginning execution at one of the vdisks <b>24</b>, which then forwards subsidiary requests to the other vdisk(s) involved in the operation on a secondary interface <b>18</b>.</li> <li id="ul0006-0010" num="0206">VDisk Primary Interface—The primary interface used by components such as the local NFS server to access files stored in the vdisks <b>24</b> making up a VFS. For each VFS operation, there is a corresponding vdisk operation, with slightly different parameters, as described in the section above on the VDisk interface.</li> <li id="ul0006-0011" num="0207">VDisk Secondary Interface—The interface invoked by those vdisk primary interface operations that update objects on more than one vdisk, to effect changes to those objects on the other vdisks <b>24</b>. For example, the secondary vdisk interface includes an operation to allocate an inode, which is invoked by the primary vdisk file create operation. Most operations in the secondary interface create POT <b>22</b> entries to ensure that their changes occur atomically with respect to the invoking primary interface operation.</li> <li id="ul0006-0012" num="0208">VFS or Virtual File System. A collection of files and directories stored in one or more vdisks <b>24</b>, and making up together a connected file system tree, with a root directory and a collection of subdirectories, each containing other files and subdirectories. A VFS contains a number of vdisks <b>24</b>, and each vdisk is a member of exactly one VFS.</li> <li id="ul0006-0013" num="0209">VFS Interface—A reasonably standard interface to virtual file systems, first introduced in the 1980s by Sun Microsystems in the SunOS 3.X operating system, and today present in some form in many Unix and Linux-based kernels, including Sun's OpenSolaris operating system. Typical operations including reading and writing blocks within files, reading and changing file attributes, and creating and deleting files within a directory.</li> </ul> </li> </ul>
<div class="description-paragraph" id="p-0190" num="0210">Although the invention has been described in detail in the foregoing embodiments for the purpose of illustration, it is to be understood that such detail is solely for that purpose and that variations can be made therein by those skilled in the art without departing from the spirit and scope of the invention except as it may be described by the following claims.</div>
</div>
</div>
</section><section itemprop="claims" itemscope="">
<h2>Claims (<span itemprop="count">11</span>)</h2>
<div html="" itemprop="content"><div class="claims" lang="EN" load-source="patent-office" mxw-id="PCLM188261196">
<claim-statement>The invention claimed is:</claim-statement>
<div class="claim"> <div class="claim" id="CLM-00001" num="00001">
<div class="claim-text">1. A digital file storage system having an architecture that improves scalability of the digital file storage system as a result of dividing files within a given directory into a number of vdisks which can each be processed independently, and that improves class of service management by determining class of service for individual blocks of a file stored in different segments of persistent memory, comprising:
<div class="claim-text">an interface that receives digital files;</div>
<div class="claim-text">a controller comprising a plurality of processors;</div>
<div class="claim-text">a memory comprising a plurality of vdisks each served by one of the plurality of processors, wherein scalability of the digital file storage system is improved as a result of dividing files within a given directory into a number of vdisks which can each be processed independently and wherein each vdisk comprises:
<div class="claim-text">a plurality of persistent storage segments, each persistent storage segment providing a specific class of service for storage different from the class of service for storage of the other persistent storage segments; and</div>
<div class="claim-text">a policy module in communication with the controller, wherein the policy module determines vdisk and segment choice for storage of digital files by determining the class of service for an individual block of a digital file by choosing a segment having an appropriate class of service to which to allocate the block;</div>
</div>
<div class="claim-text">wherein one or more digital files with data and meta data are distributed among the persistent storage segments of one or more vdisk;</div>
<div class="claim-text">wherein a vdisk is chosen from the plurality of vdisks to hold a newly created digital file or directory based on a predetermined mapping of the plurality of vdisks;</div>
<div class="claim-text">wherein storing the newly created digital file is done in a manner that improves class of service management by storing a first portion of the newly created digital file in a first segment of the persistent memory and by storing a second portion of the newly created digital file in a second segment of the persistent memory, wherein the second segment has a different class of service for storage than the first segment's class of service; and</div>
<div class="claim-text">wherein retrieving the stored newly created digital file is done by reading the first portion of the stored digital file from the first segment of the persistent memory, and by reading the second portion of the stored digital file from the second segment of the persistent memory.</div>
</div>
</div>
</div> <div class="claim-dependent"> <div class="claim" id="CLM-00002" num="00002">
<div class="claim-text">2. A digital file storage system as described in <claim-ref idref="CLM-00001">claim 1</claim-ref>, where the predetermined mapping chooses the vdisk with a largest available space.</div>
</div>
</div> <div class="claim-dependent"> <div class="claim" id="CLM-00003" num="00003">
<div class="claim-text">3. A digital file storage system as described in <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the predetermined mapping comprises choosing the vdisk served by a least loaded processor.</div>
</div>
</div> <div class="claim-dependent"> <div class="claim" id="CLM-00004" num="00004">
<div class="claim-text">4. A digital file storage system as described in <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein each vdisk contains an arbitrary collection of directories and digital files, and at least two vdisks hold at least one digital file.</div>
</div>
</div> <div class="claim-dependent"> <div class="claim" id="CLM-00005" num="00005">
<div class="claim-text">5. A digital file storage system as described in <claim-ref idref="CLM-00001">claim 1</claim-ref>, including an interface to initiate a file system consistency check on an individual vdisk, and wherein the consistency check is triggered by an indication of an inconsistency in a specific vdisk.</div>
</div>
</div> <div class="claim-dependent"> <div class="claim" id="CLM-00006" num="00006">
<div class="claim-text">6. A digital file storage system as described in <claim-ref idref="CLM-00001">claim 1</claim-ref>, where the predetermined mapping comprises choosing the vdisk with or a largest percentage of available space.</div>
</div>
</div> <div class="claim-dependent"> <div class="claim" id="CLM-00007" num="00007">
<div class="claim-text">7. A digital file storage system as described in <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein each vdisk is an arbitrary collection of directories and digital files, and at least one digital file is stored in at least two persistent storage segments.</div>
</div>
</div> <div class="claim-dependent"> <div class="claim" id="CLM-00008" num="00008">
<div class="claim-text">8. A digital file storage system as described in <claim-ref idref="CLM-00001">claim 1</claim-ref> wherein each persistent storage segment comprises a plurality of chunks.</div>
</div>
</div> <div class="claim"> <div class="claim" id="CLM-00009" num="00009">
<div class="claim-text">9. A computer-implemented method for improving scalability of a digital file storage system as a result of dividing files within a given directory into a number of vdisks which can each be processed independently, and that improves class of service management by determining class of service for individual blocks of a file stored in different segments of persistent memory provided by the digital file system when storing digital files, wherein the computer-implemented method is performed by one or more processors executing computer executable instructions, and the computer-implemented method comprising:
<div class="claim-text">improving scalability of the digital file storage system as a result of dividing files within a given directory into a number of vdisks which can each be processed independently;</div>
<div class="claim-text">receiving a newly created digital file at a communication interface;</div>
<div class="claim-text">selecting a vdisk to store the newly created digital file, wherein the selected vdisk is chosen from a plurality of vdisks based on a predetermined mapping of the plurality of vdisks, and wherein each vdisk comprises a plurality of storage segments of a persistent memory, and wherein each vdisk comprises meta data disk block pointers that point only to blocks within a same vdisk;</div>
<div class="claim-text">improving class of service management by storing a first portion of the newly created digital file in a first segment of the persistent memory and by storing a second portion of the newly created digital file in a second segment of the persistent memory, wherein the second segment has a different class of service for storage than the first segment's class of service; and</div>
<div class="claim-text">retrieving the stored digital file by reading the first portion of the stored digital file from the first segment of the persistent memory, and by reading the second portion of the stored digital file from the second segment of the persistent memory.</div>
</div>
</div>
</div> <div class="claim"> <div class="claim" id="CLM-00010" num="00010">
<div class="claim-text">10. A digital file storage system having an architecture that improves scalability of the digital file storage system as a result of dividing files within a given directory into a number of vdisks which can each be processed independently, and that improves class of service management by determining class of service for individual blocks of a file stored in different segments of persistent memory, comprising:
<div class="claim-text">an interface that receives digital files;</div>
<div class="claim-text">a controller comprising a plurality of processors;</div>
<div class="claim-text">a memory comprising a plurality of vdisks each served by one of the plurality of processors, wherein scalability of the digital file storage system is improved as a result of dividing files within a given directory into a number of vdisks which can each be processed independently, and wherein each vdisk comprises:
<div class="claim-text">a plurality of persistent storage segments, each persistent storage segment providing a specific class of service for storage different from the class of service for storage of the other persistent storage segments; and</div>
<div class="claim-text">a persistent operations table that implements directory modifying operations atomically;</div>
</div>
<div class="claim-text">wherein one or more digital files with data and meta data are distributed among the persistent storage segments of one or more vdisks; and</div>
<div class="claim-text">wherein each vdisk comprises an arbitrary collection of directories and digital files;</div>
<div class="claim-text">wherein each vdisk comprises meta data disk block pointers that point only to blocks within a same vdisk;</div>
<div class="claim-text">wherein storing digital files is done in a manner that improves class of service management by storing a first portion of at least one newly created digital file in a first segment of the persistent memory and by storing a second portion of the newly created digital file in a second segment of the persistent memory, wherein the second segment has a different class of service for storage than the first segment's class of service; and</div>
<div class="claim-text">wherein retrieving the stored newly created digital file is done by reading the first portion of the stored digital file from the first segment of the persistent memory, and by reading the second portion of the stored digital file from the second segment of the persistent memory.</div>
</div>
</div>
</div> <div class="claim"> <div class="claim" id="CLM-00011" num="00011">
<div class="claim-text">11. A digital file storage system having an architecture that improves scalability of the digital file storage system as a result of dividing files within a given directory into a number of vdisks which can each be processed independently, and that improves class of service management by determining class of service for individual blocks of a file stored in different segments of persistent memory, comprising:
<div class="claim-text">an interface that receives digital files;</div>
<div class="claim-text">a controller comprising a plurality of processors;</div>
<div class="claim-text">a memory comprising a plurality of vdisks each served by one of the plurality of processors, wherein scalability of the digital file storage system is improved as a result of dividing files within a given directory into a number of vdisks which can each be processed independently, and wherein each vdisk comprises a plurality of persistent storage segments, each persistent segment providing a specific class of service for storage different from the class of service for storage of the other persistent storage segments;</div>
<div class="claim-text">wherein one or more digital files with data and meta data are distributed among the persistent storage segments of one or more vdisks;</div>
<div class="claim-text">wherein each persistent storage segment comprises disk blocks, the disk blocks being allocated to digital files and being marked as in use by a corresponding bit being set in a bitmap allocation table, indexed by physical block address;</div>
<div class="claim-text">wherein each vdisk comprises meta data disk block pointers that point only to blocks within a same vdisk;</div>
<div class="claim-text">a persistent operations table which tracks file system operations that affect a single vdisk;</div>
<div class="claim-text">wherein storing digital files is done in a manner that improves class of service management by storing a first portion of at least one newly created digital file in a first segment of the persistent memory and by storing a second portion of the newly created digital file in a second segment of the persistent memory, wherein the second segment has a different class of service for storage than the first segment's class of service; and</div>
<div class="claim-text">wherein retrieving the stored newly created digital file is done by reading the first portion of the stored digital file from the first segment of the persistent memory, and by reading the second portion of the stored digital file from the second segment of the persistent memory.</div>
</div>
</div>
</div> </div>
</div>
</section>
                </article>
            </search-app>
        </body>
    </html>
    