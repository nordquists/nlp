
    <html>
        <body>
            <search-app>
                <article class="result" itemscope="" itemtype="http://schema.org/ScholarlyArticle">
    <h1 itemprop="pageTitle">US10366002B2 - Apparatus, system, and method for destaging cached data 
        - Google Patents</h1><section itemprop="abstract" itemscope="">
<h2>Abstract</h2>
<div html="" itemprop="content"><abstract lang="EN" load-source="docdb" mxw-id="PA305440277" source="national office">
<div class="abstract">Apparatuses, systems, methods, and computer program products are disclosed for destaging cached data. A method includes caching write in a nonvolatile solid-state cache by appending the data to a log of the nonvolatile solid-state cache. The log includes a sequential, log-based structure preserved in the nonvolatile solid-state cache. A method includes destaging at least a portion of the data from the nonvolatile solid-state cache to the backing store in a cache log order. The cache log order comprises an order in which the data was appended to the log of the nonvolatile solid-state cache.</div>
</abstract>
</div>
</section><section itemprop="description" itemscope="">
<h2>Description</h2>
<div html="" itemprop="content"><div class="description" lang="EN" load-source="patent-office" mxw-id="PDES197484290">
<heading id="h-0001">CROSS-REFERENCE TO RELATED APPLICATIONS</heading>
<div class="description-paragraph" id="p-0002" num="0001">This application claims priority to and is a continuation of U.S. patent application Ser. No. 13/088,211 entitled “APPARATUS, SYSTEM, AND METHOD FOR DESTAGING CACHED DATA” filed on Apr. 15, 2011, and which claims priority to: U.S. Provisional Patent Application No. 61/435,192 entitled “APPARATUS, SYSTEM, AND METHOD FOR DESTAGING CACHED DATA” filed on Jan. 21, 2011 for David Atkisson et. al; U.S. Provisional Patent Application No. 61/373,271 entitled “APPARATUS, SYSTEM, AND METHOD FOR CACHING DATA” filed on Aug. 12, 2010 for David Flynn; U.S. patent application Ser. No. 11/952,123 entitled “APPARATUS, SYSTEM, AND METHOD FOR SOLID-STATE STORAGE AS CACHE FOR HIGH-CAPACITY, NONVOLATILE STORAGE” filed on Dec. 6, 2007 for David Flynn et. al and issued as U.S. Pat. No. 8,019,938 on Sep. 13, 2011, and which claims priority to U.S. Provisional Patent Application No. 60/974,470 filed on Sep. 22, 2007 and to U.S. Provisional Patent Application No. 60/873,111 filed on Dec. 6, 2006; and U.S. patent application Ser. No. 12/877,971 entitled “APPARATUS, SYSTEM, AND METHOD FOR CACHING DATA ON A SOLID-STATE STORAGE DEVICE” filed on Sep. 8, 2010 for David Flynn et. al and issued as U.S. Pat. No. 8,719,501 on May 6, 2014, and which claims priority to U.S. Provisional Patent Application No. 61/373,271 filed on Aug. 12, 2010, U.S. Provisional Patent Application No. 61/240,966 filed on Sep. 9, 2009, and to U.S. Provisional Patent Application No. 61/240,573 filed on Sep. 8, 2009, each of which are incorporated herein by reference.</div>
<heading id="h-0002">FIELD</heading>
<div class="description-paragraph" id="p-0003" num="0002">The subject matter disclosed herein relates to caching data and more particularly relates to destaging cached data.</div>
<heading id="h-0003">BACKGROUND</heading>
<heading id="h-0004">Description of the Related Art</heading>
<div class="description-paragraph" id="p-0004" num="0003">For write-back caches, write data stored in the cache is written back to an associated backing store to persist the data in the backing store. The rate and the timing with which cached data is written to a backing store can affect the operation and the efficiency of a cache, because data that is not yet stored in the backing store cannot be evicted from the cache without losing the data. Attributes of a backing store can also affect the rate and the timing with which cached data can be written to the backing store.</div>
<heading id="h-0005">BRIEF SUMMARY</heading>
<div class="description-paragraph" id="p-0005" num="0004">From the foregoing discussion, it should be apparent that a need exists for an apparatus, system, and method that destage cached data from a cache to a backing store. Beneficially, such an apparatus, system, and method would account for attributes of the cache and of the backing store.</div>
<div class="description-paragraph" id="p-0006" num="0005">The present invention has been developed in response to the present state of the art, and in particular, in response to the problems and needs in the art that have not yet been fully solved by currently available data caches. Accordingly, the present invention has been developed to provide an apparatus, system, and method for destaging cached data that overcome many or all of the above-discussed shortcomings in the art.</div>
<div class="description-paragraph" id="p-0007" num="0006">Methods are presented for destaging cached data. In one embodiment a method includes detecting caching write data in a nonvolatile solid-state cache by appending the data to a log of the nonvolatile solid-state cache. The log, in one embodiment, includes a sequential, log-based structure preserved in the nonvolatile solid-state cache. In a further embodiment, a method includes destaging at least a portion of the data from the nonvolatile solid-state cache to the backing store in a cache log order. The cache log order, in one embodiment, includes an order in which the data was appending to the log of the nonvolatile solid-state cache.</div>
<div class="description-paragraph" id="p-0008" num="0007">In another embodiment, the method includes destaging at least a portion of the data from the nonvolatile solid-state cache to the backing store in a sequential backing store address order. The method, in one embodiment, includes determining a destaging pressure for the nonvolatile solid-state cache. In a further embodiment, the destaging pressure includes a level of demand for destaging cached data of the nonvolatile solid-state cache. In one embodiment, the method includes adjusting a destaging rate to satisfy the destaging pressure by adjusting a ratio of data to destage in the cache log order to data to destage in the sequential backing store address order. In another embodiment, the method includes preventing destaging of data within a predefined distance of an append point of the log of the nonvolatile solid-state cache.</div>
<div class="description-paragraph" id="p-0009" num="0008">The sequential backing store address order, in one embodiment, includes one or more ranges of address contiguous data in a range that is sequentially ordered by backing store address. Destaging data in sequential backing store address order, in a further embodiment, includes traversing a mapping structure that maps backing store addresses to locations on physical storage media of the nonvolatile solid-state cache. Destaging data in sequential backing store address order, in a further embodiment, includes selecting a destage data range such that members of the data range have contiguous and sequential backing store addresses.</div>
<div class="description-paragraph" id="p-0010" num="0009">In one embodiment, each backing store address directly maps to a logical address of the nonvolatile solid-state cache. In a further embodiment, logical addresses of the cache are independent of physical storage media addresses of the nonvolatile solid-state cache. A logical address space of the nonvolatile solid-state cache, in another embodiment, exceeds a storage capacity of the physical storage media of the nonvolatile solid-state cache.</div>
<div class="description-paragraph" id="p-0011" num="0010">The nonvolatile solid-state cache, in one embodiment, has a lower destage rate destaging in the cache log order than destaging in the sequential backing store address order. The backing store, in a further embodiment, has a higher write rate destaging in the sequential backing store address order than destaging in the cache log order.</div>
<div class="description-paragraph" id="p-0012" num="0011">Determining the destaging pressure, in one embodiment, includes determining a difference between an actual amount of dirty write data of the nonvolatile solid-state cache and a target amount of dirty write data of the nonvolatile solid-state cache. In a further embodiment, dirty write data includes write data cached in the nonvolatile solid-state cache that has not been destaged to the backing store.</div>
<div class="description-paragraph" id="p-0013" num="0012">Destaging in cache log order, in one embodiment, includes destaging data region by region, the regions ordered in cache log order. In a further embodiment, the method includes re-ordering data of a region to a backing store address order so that the regions destage in the cache log order and data within a region destages to the backing store in the backing store address order for addresses within a region.</div>
<div class="description-paragraph" id="p-0014" num="0013">In one embodiment, the log of the nonvolatile solid-state cache includes multiple append points for writing cached data. The multiple append points, in a further embodiment, include at least one append point for writing dirty data and at least one append point for writing clean data. Dirty data, in one embodiment, includes write data that has not been destaged to the backing store. Clean data, in one embodiment, includes data that is stored in the backing store.</div>
<div class="description-paragraph" id="p-0015" num="0014">Apparatuses to destage cached data are provided. A cache write module, in one embodiment, is configured to append data to a log of a nonvolatile solid-state device. A log, in certain embodiments, comprises a sequential, log-based structure. In a further embodiment, a destage module is configured to destage at least a portion of the data from the nonvolatile solid-state device to a backing store in a sequential, backing store address order.</div>
<div class="description-paragraph" id="p-0016" num="0015">The cache controller, in one embodiment, is configured to detect one or more write requests to store data in the backing store. The cache controller, in another embodiment, is configured to send the write requests to the storage controller of a nonvolatile solid-state storage device. The storage controller, in one embodiment, receives the write requests and caches the data associated with the write requests in the nonvolatile solid-state storage device by appending the data to a log of the nonvolatile solid-state storage device. In one embodiment, the log includes a sequential, log-based structure preserved in the nonvolatile solid-state storage device. The cache controller, in another embodiment, is configured to receive at least a portion of the data from the storage controller in a cache log order. In a further embodiment, the cache controller is configured to destage the data to the backing store in the cache log order. The cache log order, in one embodiment, includes an order in which the data was appended to the log of the nonvolatile solid-state storage device.</div>
<div class="description-paragraph" id="p-0017" num="0016">In one embodiment, the cache controller is configured to receive at least a portion of the data from the storage controller in a sequential backing store address order. The cache controller, in another embodiment, is configured to destage the data to the backing store in the sequential backing store address order. In a further embodiment, the cache controller is configured to determine a destaging pressure for the nonvolatile solid-state cache. The destaging pressure, in one embodiment, includes a level of demand for destaging cached data of the nonvolatile solid-state cache. The cache controller, in another embodiment, is configured to request the data from the storage controller in the sequential backing store address order in response to the destaging pressure satisfying a predefined destaging pressure criteria.</div>
<div class="description-paragraph" id="p-0018" num="0017">In one embodiment, the cache controller interfaces with the backing store controller of the backing store to destage the data to the backing store. The cache controller, the storage controller, and the backing store controller, in a certain embodiment, each include a device driver executing on a host computer system.</div>
<div class="description-paragraph" id="p-0019" num="0018">A system of the present invention is also presented to destage cached data. The system may be embodied by a processor, a nonvolatile solid-state storage device, a backing store, one or more communication buses, a cache controller, and a storage controller. In particular, the system, in one embodiment, includes a host computer system.</div>
<div class="description-paragraph" id="p-0020" num="0019">In one embodiment, the nonvolatile solid-state storage device preserves a log. The log, in one embodiment, includes a sequential, log-based structure. The nonvolatile solid-state storage device and the backing store, in a further embodiment, are m communication with the processor over the one or more communication buses.</div>
<div class="description-paragraph" id="p-0021" num="0020">In one embodiment, the cache controller is in communication with the backing store. The cache controller, in another embodiment, is configured to detect one or more write requests to store data in the backing store. In a further embodiment, the cache controller is configured to send the write requests to the storage controller for the nonvolatile solid-state storage device. The storage controller, in one embodiment, receives the write requests and caches the data associated with the write requests in the nonvolatile solid-state storage device. The storage controller, in a further embodiment, caches the data by appending the data to the log of the nonvolatile solid-state storage device.</div>
<div class="description-paragraph" id="p-0022" num="0021">The cache controller, in one embodiment, is configured to receive at least a portion of the data from the storage controller in a cache log order. In another embodiment, the cache controller is configured to destage the data to the backing store in the cache log order. The cache log order, in one embodiment, includes an order in which the data was appended to the log of the nonvolatile solid-state storage device. In another embodiment, the cache controller is configured to receive at least a portion of the data from the storage controller in a sequential backing store address order. The cache controller, in a further embodiment, is configured to destage the data to the backing store in the sequential backing store address order.</div>
<div class="description-paragraph" id="p-0023" num="0022">Computer program products are presented. In one embodiment, a computer program product includes a computer readable storage media storing computer usable program code executable to perform operations for destaging cached data. An operation, in certain embodiments, includes destaging a portion of data from a cache to a backing store in a cache log order in which the data was appended to a sequential log of the cache. In a further embodiment, an operation includes destaging a portion of data from a cache to a backing store in a sequential backing store address order. In another embodiment, an operation includes adjusting a destaging rate to satisfy a destaging pressure.</div>
<div class="description-paragraph" id="p-0024" num="0023">References throughout this specification to features, advantages, or similar language do not imply that all of the features and advantages may be realized in any single embodiment. Rather, language referring to the features and advantages is understood to mean that a specific feature, advantage, or characteristic is included in at least one embodiment. Thus, discussion of the features and advantages, and similar language, throughout this specification may, but do not necessarily, refer to the same embodiment.</div>
<div class="description-paragraph" id="p-0025" num="0024">Furthermore, the described features, advantages, and characteristics of the embodiments may be combined in any suitable manner. One skilled in the relevant art will recognize that the embodiments may be practiced without one or more of the specific features or advantages of a particular embodiment. In other instances, additional features and advantages may be recognized in certain embodiments that may not be present in all embodiments.</div>
<div class="description-paragraph" id="p-0026" num="0025">These features and advantages of the embodiments will become more fully apparent from the following description and appended claims, or may be learned by the practice of embodiments as set forth hereinafter.</div>
<description-of-drawings>
<heading id="h-0006">BRIEF DESCRIPTION OF THE DRAWINGS</heading>
<div class="description-paragraph" id="p-0027" num="0026">In order that the advantages of the invention will be readily understood, a more particular description of the invention briefly described above will be rendered by reference to specific embodiments that are illustrated in the appended drawings. Understanding that these drawings depict only typical embodiments of the invention and are not therefore to be considered to be limiting of its scope, the invention will be described and explained with additional specificity and detail through the use of the accompanying drawings, in which:</div>
<div class="description-paragraph" id="p-0028" num="0027"> <figref idrefs="DRAWINGS">FIG. 1A</figref> is a schematic block diagram illustrating one embodiment of a system for destaging cached data in accordance with the present invention;</div>
<div class="description-paragraph" id="p-0029" num="0028"> <figref idrefs="DRAWINGS">FIG. 1B</figref> is a schematic block diagram illustrating another embodiment of a system for destaging cached data in accordance with the present invention;</div>
<div class="description-paragraph" id="p-0030" num="0029"> <figref idrefs="DRAWINGS">FIG. 2</figref> is a schematic block diagram illustrating one embodiment of a solid-state storage device controller in a cache device in accordance with the present invention;</div>
<div class="description-paragraph" id="p-0031" num="0030"> <figref idrefs="DRAWINGS">FIG. 3</figref> is a schematic block diagram illustrating one embodiment of a solid-state storage controller with a write data pipeline and a read data pipeline in a solid-state storage device in accordance with the present invention;</div>
<div class="description-paragraph" id="p-0032" num="0031"> <figref idrefs="DRAWINGS">FIG. 4</figref> is a schematic block diagram illustrating one embodiment of a bank interleave controller in the solid-state storage controller in accordance with the present invention;</div>
<div class="description-paragraph" id="p-0033" num="0032"> <figref idrefs="DRAWINGS">FIG. 5</figref> is a schematic block diagram illustrating one embodiment of a host device in accordance with the present invention;</div>
<div class="description-paragraph" id="p-0034" num="0033"> <figref idrefs="DRAWINGS">FIG. 6</figref> is a schematic block diagram illustrating one embodiment of a direct cache module in accordance with the present invention;</div>
<div class="description-paragraph" id="p-0035" num="0034"> <figref idrefs="DRAWINGS">FIG. 7</figref> is a schematic block diagram illustrating another embodiment of a direct cache module in accordance with the present invention;</div>
<div class="description-paragraph" id="p-0036" num="0035"> <figref idrefs="DRAWINGS">FIG. 8</figref> is a schematic block diagram illustrating one embodiment of a forward map and a reverse map in accordance with the present invention;</div>
<div class="description-paragraph" id="p-0037" num="0036"> <figref idrefs="DRAWINGS">FIG. 9</figref> is a schematic block diagram illustrating one embodiment of a mapping structure, a logical address space of a cache, a sequential, log-based, append-only writing structure, and an address space of a storage device in accordance with the present invention;</div>
<div class="description-paragraph" id="p-0038" num="0037"> <figref idrefs="DRAWINGS">FIG. 10</figref> is a schematic block diagram illustrating one embodiment of address order destaging and log order destaging in accordance with the present invention;</div>
<div class="description-paragraph" id="p-0039" num="0038"> <figref idrefs="DRAWINGS">FIG. 11</figref> is a schematic flow chart diagram illustrating one embodiment of a method for destaging cached data in accordance with the present invention;</div>
<div class="description-paragraph" id="p-0040" num="0039"> <figref idrefs="DRAWINGS">FIG. 12</figref> is a schematic flow chart diagram illustrating another embodiment of a method for destaging cached data in accordance with the present invention;</div>
<div class="description-paragraph" id="p-0041" num="0040"> <figref idrefs="DRAWINGS">FIG. 13</figref> is a schematic flow chart diagram illustrating one embodiment of a method for log order destaging in accordance with the present invention; and</div>
<div class="description-paragraph" id="p-0042" num="0041"> <figref idrefs="DRAWINGS">FIG. 14</figref> is a schematic flow chart diagram illustrating one embodiment of a method for address order destaging in accordance with the present invention.</div>
</description-of-drawings>
<heading id="h-0007">DETAILED DESCRIPTION OF THE INVENTION</heading>
<div class="description-paragraph" id="p-0043" num="0042">As will be appreciated by one skilled in the art, aspects of the present invention may be embodied as a system, method or computer program product. Accordingly, aspects of the present invention may take the form of an entirely hardware embodiment, an entirely software embodiment (including firmware, resident software, micro-code, etc.) or an embodiment combining software and hardware aspects that may all generally be referred to herein as a “circuit,” “module” or “system.” Furthermore, aspects of the present invention may take the form of a computer program product embodied in one or more computer readable medium(s) having computer readable program code embodied thereon.</div>
<div class="description-paragraph" id="p-0044" num="0043">Many of the functional units described in this specification have been labeled as modules, in order to more particularly emphasize their implementation independence. For example, a module may be implemented as a hardware circuit comprising custom VLSI circuits or gate arrays, off-the-shelf semiconductors such as logic chips, transistors, or other discrete components. A module may also be implemented in programmable hardware devices such as field programmable gate arrays, programmable array logic, programmable logic devices or the like.</div>
<div class="description-paragraph" id="p-0045" num="0044">Modules may also be implemented in software for execution by various types of processors. An identified module of computer readable program code may, for instance, comprise one or more physical or logical blocks of computer instructions which may, for instance, be organized as an object, procedure, or function. Nevertheless, the executables of an identified module need not be physically located together, but may comprise disparate instructions stored in different locations which, when joined logically together, comprise the module and achieve the stated purpose for the module.</div>
<div class="description-paragraph" id="p-0046" num="0045">Indeed, a module of computer readable program code may be a single instruction, or many instructions, and may even be distributed over several different code segments, among different programs, and across several memory devices. Similarly, operational data may be identified and illustrated herein within modules, and may be embodied in any suitable form and organized within any suitable type of data structure. The operational data may be collected as a single data set, or may be distributed over different locations including over different storage devices, and may exist, at least partially, merely as electronic signals on a system or network. Where a module or portions of a module are implemented in software, the computer readable program code may be stored and/or propagated on or in one or more computer readable medium(s).</div>
<div class="description-paragraph" id="p-0047" num="0046">The computer readable medium may be a tangible computer readable storage medium storing the computer readable program code. The computer readable storage medium may be, for example, but not limited to, an electronic, magnetic, optical, electromagnetic, infrared, holographic, micromechanical, or semiconductor system, apparatus, or device, or any suitable combination of the foregoing.</div>
<div class="description-paragraph" id="p-0048" num="0047">More specific examples of the computer readable medium may include but are not limited to a portable computer diskette, a hard disk, a random access memory (RAM), a read-only memory (ROM), an erasable programmable read-only memory (EPROM or Flash memory), a portable compact disc read-only memory (CD-ROM), a digital versatile disc (DVD), an optical storage device, a magnetic storage device, a holographic storage medium, a micromechanical storage device, or any suitable combination of the foregoing. In the context of this document, a computer readable storage medium may be any tangible medium that can contain, and/or store computer readable program code for use by and/or in connection with an instruction execution system, apparatus, or device.</div>
<div class="description-paragraph" id="p-0049" num="0048">The computer readable medium may also be a computer readable signal medium. A computer readable signal medium may include a propagated data signal with computer readable program code embodied therein, for example, in baseband or as part of a carrier wave. Such a propagated signal may take any of a variety of forms, including, but not limited to, electrical, electro-magnetic, magnetic, optical, or any suitable combination thereof A computer readable signal medium may be any computer readable medium that is not a computer readable storage medium and that can communicate, propagate, or transport computer readable program code for use by or in connection with an instruction execution system, apparatus, or device. Computer readable program code embodied on a computer readable signal medium may be transmitted using any appropriate medium, including but not limited to wireless, wireline, optical fiber cable, Radio Frequency (RF), or the like, or any suitable combination of the foregoing. In one embodiment, the computer readable medium may comprise a combination of one or more computer readable storage mediums and one or more computer readable signal mediums. For example, computer readable program code may be both propagated as an electro-magnetic signal through a fiber optic cable for execution by a processor and stored on RAM storage device for execution by the processor.</div>
<div class="description-paragraph" id="p-0050" num="0049">Computer readable program code for carrying out operations for aspects of the present invention may be written in any combination of one or more programming languages, including an object oriented programming language such as Java, Smalltalk, C++ or the like and conventional procedural programming languages, such as the “C” programming language or similar programming languages. The computer readable program code may execute entirely on the user's computer, partly on the user's computer, as a stand-alone software package, partly on the user's computer and partly on a remote computer or entirely on the remote computer or server. In the latter scenario, the remote computer may be connected to the user's computer through any type of network, including a local area network (LAN) or a wide area network (WAN), or the connection may be made to an external computer (for example, through the Internet using an Internet Service Provider).</div>
<div class="description-paragraph" id="p-0051" num="0050">Reference throughout this specification to “one embodiment,” “an embodiment,” or similar language means that a particular feature, structure, or characteristic described in connection with the embodiment is included in at least one embodiment. Thus, appearances of the phrases “in one embodiment,” “in an embodiment,” and similar language throughout this specification may, but do not necessarily, all refer to the same embodiment, but mean “one or more but not all embodiments” unless expressly specified otherwise. The terms “including,” “comprising,” “having,” and variations thereof mean “including but not limited to,” unless expressly specified otherwise. An enumerated listing of items does not imply that any or all of the items are mutually exclusive, unless expressly specified otherwise. The terms “a,” “an,” and “the” also refer to “one or more” unless expressly specified otherwise.</div>
<div class="description-paragraph" id="p-0052" num="0051">Furthermore, the described features, structures, or characteristics of the embodiments may be combined in any suitable manner. In the following description, numerous specific details are provided, such as examples of programming, software modules, user selections, network transactions, database queries, database structures, hardware modules, hardware circuits, hardware chips, etc., to provide a thorough understanding of embodiments. One skilled in the relevant art will recognize, however, that embodiments may be practiced without one or more of the specific details, or with other methods, components, materials, and so forth. In other instances, well-known structures, materials, or operations are not shown or described in detail to avoid obscuring aspects of an embodiment.</div>
<div class="description-paragraph" id="p-0053" num="0052">Aspects of the embodiments are described below with reference to schematic flowchart diagrams and/or schematic block diagrams of methods, apparatuses, systems, and computer program products according to embodiments of the invention. It will be understood that each block of the schematic flowchart diagrams and/or schematic block diagrams, and combinations of blocks in the schematic flowchart diagrams and/or schematic block diagrams, can be implemented by computer readable program code. These computer readable program code may be provided to a processor of a general purpose computer, special purpose computer, sequencer, or other programmable data processing apparatus to produce a machine, such that the instructions, which execute via the processor of the computer or other programmable data processing apparatus, create means for implementing the functions/acts specified in the schematic flowchart diagrams and/or schematic block diagrams block or blocks</div>
<div class="description-paragraph" id="p-0054" num="0053">The computer readable program code may also be stored in a computer readable medium that can direct a computer, other programmable data processing apparatus, or other devices to function in a particular manner, such that the instructions stored in the computer readable medium produce an article of manufacture including instructions which implement the function/act specified in the schematic flowchart diagrams and/or schematic block diagrams block or blocks.</div>
<div class="description-paragraph" id="p-0055" num="0054">The computer readable program code may also be loaded onto a computer, other programmable data processing apparatus, or other devices to cause a series of operational steps to be performed on the computer, other programmable apparatus or other devices to produce a computer implemented process such that the program code which execute on the computer or other programmable apparatus provide processes for implementing the functions/acts specified in the flowchart and/or block diagram block or blocks.</div>
<div class="description-paragraph" id="p-0056" num="0055">The schematic flowchart diagrams and/or schematic block diagrams in the Figures illustrate the architecture, functionality, and operation of possible implementations of apparatuses, systems, methods and computer program products according to various embodiments of the present invention. In this regard, each block in the schematic flowchart diagrams and/or schematic block diagrams may represent a module, segment, or portion of code, which comprises one or more executable instructions of the program code for implementing the specified logical function(s).</div>
<div class="description-paragraph" id="p-0057" num="0056">It should also be noted that, in some alternative implementations, the functions noted in the block may occur out of the order noted in the Figures. For example, two blocks shown in succession may, in fact, be executed substantially concurrently, or the blocks may sometimes be executed in the reverse order, depending upon the functionality involved. Other steps and methods may be conceived that are equivalent in function, logic, or effect to one or more blocks, or portions thereof, of the illustrated Figures.</div>
<div class="description-paragraph" id="p-0058" num="0057">Although various arrow types and line types may be employed in the flowchart and/or block diagrams, they are understood not to limit the scope of the corresponding embodiments. Indeed, some arrows or other connectors may be used to indicate only the logical flow of the depicted embodiment. For instance, an arrow may indicate a waiting or monitoring period of unspecified duration between enumerated steps of the depicted embodiment. It will also be noted that each block of the block diagrams and/or flowchart diagrams, and combinations of blocks in the block diagrams and/or flowchart diagrams, can be implemented by special purpose hardware-based systems that perform the specified functions or acts, or combinations of special purpose hardware and computer readable program code.</div>
<div class="description-paragraph" id="h-0008" num="0000">Caching System</div>
<div class="description-paragraph" id="p-0059" num="0058"> <figref idrefs="DRAWINGS">FIG. 1A</figref> depicts one embodiment of a system <b>100</b> for destaging cached data in accordance with the present invention. The system <b>100</b>, in the depicted embodiment, includes a cache <b>102</b> a host device <b>114</b>, a direct cache module <b>116</b>, and a backing store <b>118</b>. The cache <b>102</b>, in the depicted embodiment, includes a solid-state storage controller <b>104</b>, a write data pipeline <b>106</b>, a read data pipeline <b>108</b>, and a solid-state storage media <b>110</b>. In general, the system <b>100</b> caches data for the backing store <b>118</b> in the cache <b>102</b> and the direct cache module <b>116</b> destages or writes cached data from the cache <b>102</b> to the backing store <b>118</b>.</div>
<div class="description-paragraph" id="p-0060" num="0059">In the depicted embodiment, the system <b>100</b> includes a single cache <b>102</b>. In another embodiment, the system <b>100</b> may include two or more caches <b>102</b>. For example, in various embodiments, the system <b>100</b> may mirror cached data between several caches <b>102</b>, may virtually stripe cached data across multiple caches <b>102</b>, or otherwise cache data in more than one cache <b>102</b>. In general, the cache <b>102</b> serves as a read and/or a write cache for the backing store <b>118</b> and the backing store <b>118</b> is a storage device that serves as a backing store or the cache <b>102</b>. In one embodiment, the cache <b>102</b> operates in a write-back mode and the direct cache module <b>116</b> destages cached write data to the backing store <b>118</b> opportunistically after caching the write data in the cache <b>102</b>. In certain embodiments, the cache <b>102</b> may operate, at least temporarily, in another mode, such as a write-back mode, a write-around mode, or the like, and the direct cache module <b>116</b> may write data to the backing store <b>118</b> substantially simultaneously with caching the data in the cache <b>102</b> or without caching the data in the cache <b>102</b>. The direct cache module <b>116</b> destages data from the cache <b>102</b> to the backing store <b>118</b> to persist the data in the backing store <b>118</b>. The direct cache module <b>116</b>, in one embodiment, destages dirty write data that the backing store <b>118</b> does not yet store to clean the data. Destaging the dirty write data to the backing store <b>118</b> converts the state of the dirt write data from dirty to clean data.</div>
<div class="description-paragraph" id="p-0061" num="0060">In the depicted embodiment, the cache <b>102</b> is embodied by a non-volatile, solid-state storage device, with a solid-state storage controller <b>104</b> and non-volatile, solid-state storage media <b>110</b>. The non-volatile, solid-state storage media <b>110</b> may include flash memory, nano random access memory (“nano RAM or NRAM”), magneto-resistive RAM (“MRAM”), dynamic RAM (“DRAM”), phase change RAM (“PRAM”), racetrack memory, memristor memory, nanocrystal wire-based memory, silicon-oxide based sub-10 nanometer process memory, graphene memory, silicon-oxide-nitride-oxide-silicon (“SON OS”) memory, resistive random-access memory (“RRAM”), programmable metallization cell (“PMC”), conductive-bridging RAM (“CBRAM”), or the like. Embodiments of the cache <b>102</b> that include a solid-state storage controller <b>104</b> and solid-state storage media <b>110</b> are described in more detail with respect to <figref idrefs="DRAWINGS">FIGS. 2 and 3</figref>. In further embodiments, the cache <b>102</b> may include other types of non-volatile and/or volatile data storage, such as dynamic RAM (“DRAM”), static RAM (“SRAM”), magnetic data storage, optical data storage, and/or other data storage technologies.</div>
<div class="description-paragraph" id="p-0062" num="0061">The cache <b>102</b>, in one embodiment, stores or preserves data in a log. The log, in a further embodiment, comprises a sequential, append-only log-based structure, or the like. The cache <b>102</b> stores at least a portion of the log on the solid-state storage media <b>110</b>. The cache <b>102</b>, in certain embodiments, may store a portion of the log, metadata for the log, or the like in volatile memory, such as RAM, and may store at least enough data of the log in the solid-state storage media <b>110</b> to recreate the log structure after an improper shutdown or other failure.</div>
<div class="description-paragraph" id="p-0063" num="0062">In one embodiment, the log includes a head at an append point and a tail at an end of the log with the oldest data (data written earliest in time). In certain embodiments, the log may include multiple append points, multiple sub-logs, or the like. In a further embodiment, the cache <b>102</b> may store or preserve data in multiple logs. The direct cache module <b>116</b>, in one embodiment, destages data from the cache <b>102</b> in a cache log order. A cache log order, as used herein, is an order in which data was appended to (or is organized within) the log of the cache <b>102</b>. The cache log order may be from older data toward newer data (e.g., tail to head order), from newer data toward older data (e.g., head to tail order), or the like.</div>
<div class="description-paragraph" id="p-0064" num="0063">In one embodiment, destaging in a tail to head cache log order may reduce write amplification by destaging older data that is more likely to be stored in a region that is a candidate for grooming/garbage collection. Write amplification is the rewriting or moving of data during a grooming or garbage collection process, causing data originally written in response to a user write request to be written more than once. Write amplification can increase the number of writes of a storage device, consume write bandwidth of a storage device, reduce a usable lifetime of a storage device, and otherwise reduce performance of a storage device. Once the direct cache module <b>116</b> has destaged dirty write data to the backing store <b>118</b>, the data is clean and the direct cache module <b>116</b> may selectively clear, invalidate, and/or evict the data, which is now clean, from the cache <b>102</b>, instead of writing the data forward during a grooming/garbage collection operation.</div>
<div class="description-paragraph" id="p-0065" num="0064">In general, the cache <b>102</b> caches data for the backing store <b>118</b>. The backing store <b>118</b>, in one embodiment, is a backing store associated with the cache <b>102</b> and/or with the direct cache module <b>116</b>. The backing store <b>118</b> may include a hard disk drive, an optical drive with optical media, a magnetic tape drive, or another type of storage device. In one embodiment, the backing store <b>118</b> may have a greater data storage capacity than the cache <b>102</b>. In another embodiment, the backing store <b>118</b> may have a higher latency, a lower throughput, or the like, than the cache <b>102</b>.</div>
<div class="description-paragraph" id="p-0066" num="0065">The backing store <b>118</b> may have a higher latency, a lower throughput, or the like due to properties of the backing store <b>118</b> itself, or due to properties of a connection to the backing store <b>118</b>. For example, in one embodiment, the cache <b>102</b> and the backing store <b>118</b> may each include non-volatile, solid-state storage media <b>110</b> with similar properties, but the backing store <b>118</b> may be in communication with the host device <b>114</b> over a data network, while the cache <b>102</b> may be directly connected to the host device <b>114</b>, causing the backing store <b>118</b> to have a higher latency relative to the host <b>114</b> than the cache <b>102</b>.</div>
<div class="description-paragraph" id="p-0067" num="0066">In one embodiment, the backing store <b>118</b> stores data more quickly and/or more efficiently when the backing store <b>118</b> receives the data in a sequential, backing store address order than when the backing store <b>118</b> receives the data out of order. In a further embodiment, the backing store <b>118</b> has a higher write rate writing data in a backing store address order than when writing data out of backing store address order. For example, in certain embodiments, the backing store <b>118</b> may include a magnetic or optical read/write head and may incur a seek time from moving the head between out of order addresses. The backing store, in one embodiment, may incur little or no seek time when writing data that is sequentially ordered by backing store address.</div>
<div class="description-paragraph" id="p-0068" num="0067">Data in a backing store address order, in one embodiment, includes a range of data that is ordered by a logical or physical address of the data, in either increasing or decreasing order. The logical or physical address of the data, in a further embodiment, is an address associated with the backing store <b>118</b>. In one embodiment, a range of data in backing store address order includes a continuous, unbroken range of data with consecutive backing store addresses. In another embodiment, a range of data in backing store address order may have one or more holes or gaps, with missing addresses, such that the backing store addresses of the range of data are ordered, but may not necessarily be consecutive.</div>
<div class="description-paragraph" id="p-0069" num="0068">As described above, in one embodiment, the direct cache module <b>116</b> destages data from the cache <b>102</b> to the backing store <b>118</b> in a cache log order. In another embodiment, the direct cache module <b>116</b> destages data from the cache <b>102</b> to the backing store <b>118</b> in a sequential backing store address order. In a further embodiment, the direct cache module <b>116</b> destages data from the cache <b>102</b> in both a cache log order and a backing store address order.</div>
<div class="description-paragraph" id="p-0070" num="0069">In certain embodiments, the direct cache module <b>116</b> can provide a higher destage rate destaging data in a sequential backing store address order than in a cache log order, due to more efficient write rates of the backing store <b>118</b>, or the like. The direct cache module <b>116</b>, in other embodiments, can destage data more efficiently for the cache <b>102</b> in a cache log order, as described above. The direct cache module <b>116</b>, in one embodiment, balances destaging in cache log order and in backing store address order.</div>
<div class="description-paragraph" id="p-0071" num="0070">In various embodiments, the direct cache module <b>116</b> may destage data in both a cache log order and a backing store address order in parallel, by alternating between a cache log order and a backing store address order, or the like. The direct cache module <b>116</b>, in one embodiment, determines a ratio, duty cycle, or the like defining an amount of data to destage in cache log order and an amount of data to destage in backing store address order. In a further embodiment, the direct cache module <b>116</b> destages data in a backing store address order in response to a destaging rate of log order destaging failing to satisfy a target destage rate, or the like. For example, the direct cache module <b>116</b> may balance destaging orders to satisfy a destaging pressure or a target destage rate, to maximize destaging efficiency of the cache <b>102</b>, or the like.</div>
<div class="description-paragraph" id="p-0072" num="0071">In one embodiment, the direct cache module <b>116</b> destages dirty data from the cache <b>102</b> to the backing store <b>118</b> in an order that favors operation of the cache <b>102</b>, the order selected such that cache operation is more efficient following destaging. One example of an order that favors operation of the cache <b>102</b> is cache log order destaging in the same order in which the data was appended to the log of the cache <b>102</b>. Log ordered destaging is one embodiment of an elevator algorithm. The direct cache module <b>116</b> may use other kinds of elevator algorithms that may produce higher destage rates and still favor operation of the cache <b>102</b>.</div>
<div class="description-paragraph" id="p-0073" num="0072">In one embodiment, the direct cache module <b>116</b> destages dirty data from the cache <b>102</b> to the backing store <b>118</b> in an order that favors operation of the backing store, the order selected such that backing store operation is more efficient during and/or following the destaging. One example of an order that favors operation of the backing store is destaging in a sequential backing store address order.</div>
<div class="description-paragraph" id="p-0074" num="0073">In one embodiment, the direct cache module <b>116</b> destages dirty data from the cache <b>102</b> to the backing store <b>118</b> in an order that favors management of the nonvolatile solid-state storage media, the order selected such that management of the nonvolatile solid-state storage media is more efficient following the destaging. For example, the direct cache module <b>116</b> may destage first a region of the cache <b>102</b>, such as an erase block, with the least amount of dirty data or may destage first a region of the cache <b>102</b>, such as an erase block, that will have the lowest grooming cost once the region is cleaned. In one embodiment, the direct cache module <b>116</b> may destage data from the cache <b>102</b> in an order based on the amount of dirty data (for example the least dirty data may have a lowest cost and may have the highest likelihood of being groomed next). For logical erase blocks (“LEBs”) or other regions not in the hot zone (i.e. at least X LEBS from an append point of the log) the amount of dirty data then, in certain embodiments, may be a dominant factor in determining destaging order. In another embodiment, the direct cache module <b>116</b> may destage data from the cache <b>102</b> in an order of the LEBs based on the cost to groom the LEB if only the LEB were clean (so the LEB is known to have dirty data)—assuming the LEB was all clean. In this embodiment, the direct cache module <b>116</b> may then order destaging in a manner that gets a next best grooming candidate ready the fastest.</div>
<div class="description-paragraph" id="p-0075" num="0074">The direct cache module <b>116</b>, in one embodiment, may use a combination of several destaging orders or destaging order algorithms and/or may alternate between destaging orders to favor operation of the cache <b>102</b>, to favor operation of the backing store, and/or to favor operation of the solid-state storage media <b>110</b> to various degrees or at various times. In other embodiments, the direct cache module <b>116</b> may use a single destaging order, or the like.</div>
<div class="description-paragraph" id="p-0076" num="0075">In one embodiment, the cache <b>102</b> and/or the backing store <b>118</b> are in communication with a processor of the host device <b>114</b> over one or more communications buses. In the depicted embodiment, the cache <b>102</b> and the backing store <b>118</b> are in communication with the host device <b>114</b> through the direct cache module <b>116</b>. The cache <b>102</b> and/or the backing store <b>118</b>, in one embodiment, may be direct attached storage (“DAS”) of the host device <b>114</b>. DAS, as used herein, is data storage that is connected to a device, either internally or externally, without a storage network in between.</div>
<div class="description-paragraph" id="p-0077" num="0076">In one embodiment, the cache <b>102</b> and/or the backing store <b>118</b> are internal to the host device <b>114</b> and are connected using a system bus, such as a peripheral component interconnect express (“PCI-e”) bus, a Serial Advanced Technology Attachment (“SATA”) bus, or the like. In another embodiment, the cache <b>102</b> and/or the backing store <b>118</b> may be external to the host device <b>114</b> and may be connected using a universal serial bus (“USB”) connection, an Institute of Electrical and Electronics Engineers (“IEEE”) 1394 bus (“Fire Wire”), an external SATA (“eSATA”) connection, or the like. In other embodiments, the cache <b>102</b> and/or the backing store <b>118</b> may be connected to the host device <b>114</b> using a peripheral component interconnect (“PCI”) express bus using external electrical or optical bus extension or bus networking solution such as Infiniband or PCI Express Advanced Switching (“PCIe-AS”), or the like.</div>
<div class="description-paragraph" id="p-0078" num="0077">In various embodiments, the cache <b>102</b> and/or the backing store <b>118</b> may be in the form of a dual-inline memory module (“DIMM”), a daughter card, or a micro-module. In another embodiment, the cache <b>102</b> and/or the backing store <b>118</b> may be elements within a rack-mounted blade. In another embodiment, the cache <b>102</b> and/or the backing store <b>118</b> may be contained within packages that are integrated directly onto a higher level assembly (e.g. mother board, lap top, graphics processor). In another embodiment, individual components comprising the cache <b>102</b> and/or the backing store <b>118</b> are integrated directly onto a higher level assembly without intermediate packaging.</div>
<div class="description-paragraph" id="p-0079" num="0078">In the depicted embodiment, the cache <b>102</b> includes one or more solid-state storage controllers <b>104</b> with a write data pipeline <b>106</b> and a read data pipeline <b>108</b>, and a solid-state storage media <b>110</b>, which are described in more detail below with respect to <figref idrefs="DRAWINGS">FIGS. 2 and 3</figref>. The backing store <b>118</b>, in the depicted embodiment, includes a backing store controller <b>120</b>. The solid-state storage controller <b>104</b> and the backing store controller</div>
<div class="description-paragraph" id="p-0080" num="0079"> <b>120</b>, in certain embodiments, may receive storage requests, perform management functions and the like for the cache <b>102</b> and the backing store <b>118</b>, or perform other functions. The solid-state storage controller <b>104</b> and/or the backing store controller <b>120</b>, in various embodiments, may comprise one or more device drivers installed on the host device <b>114</b>, logic hardware or firmware of the cache <b>102</b> and/or the backing store <b>118</b>, a combination of one or more device drivers and logic hardware or firmware, or the like.</div>
<div class="description-paragraph" id="p-0081" num="0080">In a further embodiment, instead of being connected directly to the host device <b>114</b> as DAS, the cache <b>102</b> and/or the backing store <b>118</b> may be connected to the host device <b>114</b> over a data network. For example, the cache <b>102</b> and/or the backing store <b>118</b> may include a storage area network (“SAN”) storage device, a network attached storage (“NAS”) device, a network share, or the like. In one embodiment, the system <b>100</b> may include a data network, such as the Internet, a wide area network (“WAN”), a metropolitan area network (“MAN”), a local area network (“LAN”), a token ring, a wireless network, a fiber channel network, a SAN, a NAS, ESCON, or the like, or any combination of networks. A data network may also include a network from the IEEE 802 family of network technologies, such Ethernet, token ring, Wi-Fi, Wi-Max, and the like. A data network may include servers, switches, routers, cabling, radios, and other equipment used to facilitate networking between the host device <b>114</b> and the cache <b>102</b> and/or the backing store <b>118</b>.</div>
<div class="description-paragraph" id="p-0082" num="0081">In one embodiment, at least the cache <b>102</b> is connected directly to the host device <b>114</b> as a DAS device. In a further embodiment, the cache <b>102</b> is directly connected to the host device <b>114</b> as a DAS device and the backing store <b>118</b> is directly connected to the cache <b>102</b>. For example, the cache <b>102</b> may be connected directly to the host device <b>114</b>, and the backing store <b>118</b> may be connected directly to the cache <b>102</b> using a direct, wire-line connection, such as a PCI express bus, an SATA bus, a USB connection, an IEEE 1394 connection, an eSATA connection, a proprietary direct connection, an external electrical or optical bus extension or bus networking solution such as Infiniband or PCIe-AS, or the like. One of skill in the art, in light of this disclosure, will recognize other arrangements and configurations of the host device <b>114</b>, the cache <b>102</b>, and the backing store <b>118</b> suitable for use in the system <b>100</b>.</div>
<div class="description-paragraph" id="p-0083" num="0082">The system <b>100</b> includes the host device <b>114</b> in communication with the cache <b>102</b> and the backing store <b>118</b> through the direct cache module <b>116</b>. A host device <b>114</b> may be a host, a server, a storage controller of a SAN, a workstation, a personal computer, a laptop computer, a handheld computer, a supercomputer, a computer cluster, a network switch, router, or appliance, a database or storage appliance, a data acquisition or data capture system, a diagnostic system, a test system, a robot, a portable electronic device, a wireless device, or the like.</div>
<div class="description-paragraph" id="p-0084" num="0083">In the depicted embodiment, the host device <b>114</b> is in communication with the direct cache module <b>116</b>. The direct cache module <b>116</b>, in general, receives or otherwise detects read and write requests from the host device <b>114</b> directed to the backing store <b>118</b> and manages the caching of data in the cache <b>102</b> and destaging of cached data to the backing store <b>118</b>. In one embodiment, the direct cache module <b>116</b> comprises a software application, file system filter driver, combination of filter drivers, or the like on the host device <b>114</b>.</div>
<div class="description-paragraph" id="p-0085" num="0084">In another embodiment, the direct cache module <b>116</b> comprises one or more storage controllers, such as the solid-state storage controller <b>104</b> of the cache <b>102</b> and/or the backing store controller <b>120</b> of the backing store <b>118</b>. <figref idrefs="DRAWINGS">FIG. 1B</figref> depicts a system <b>101</b> that is substantially similar to the system <b>100</b> of <figref idrefs="DRAWINGS">FIG. 1A</figref>, but with the storage controller <b>104</b> and the backing store controller <b>120</b> integrated with the direct cache module <b>116</b> as device drivers and/or filter drivers on the host device <b>114</b>. The storage controller <b>104</b> and the backing store controller <b>120</b> may be integrated with the direct cache module <b>116</b> as device drivers on the host device <b>114</b>, as dedicated hardware logic circuits or firmware of the cache <b>102</b> and/or the backing store <b>118</b>, as a combination of one or more device drivers and dedicated hardware, or the like. In a further embodiment, the direct cache module <b>116</b> comprises a combination of one or more software drivers of the host device <b>114</b> and one or more storage controllers, or the like. The direct cache module <b>116</b>, in various software, hardware, and combined software and hardware embodiments, may generally be referred to as a cache controller.</div>
<div class="description-paragraph" id="p-0086" num="0085">In one embodiment, the host device <b>114</b> loads one or more device drivers for the cache <b>102</b> and/or the backing store <b>118</b> and the direct cache module <b>116</b> communicates with the one or more device drivers on the host device <b>114</b>. As described above, in certain embodiments, the solid-state storage controller <b>104</b> of the cache <b>102</b> and/or the backing store controller <b>120</b> may comprise device drivers on the host device <b>114</b>. In another embodiment, the direct cache module <b>116</b> may communicate directly with a hardware interface of the cache <b>102</b> and/or the backing store <b>118</b>. In a further embodiment, the direct cache module <b>116</b> may be integrated with the cache <b>102</b> and/or the backing store <b>118</b>.</div>
<div class="description-paragraph" id="p-0087" num="0086">In one embodiment, the cache <b>102</b> and/or the backing store <b>118</b> have block device interfaces that support block device commands. For example, the cache <b>102</b> and/or the backing store <b>118</b> may support the standard block device interface, the A TA interface standard, the ATA Packet Interface (“ATAPI”) standard, the small computer system interface (“SCSI”) standard, and/or the Fibre Channel standard which are maintained by the InterNational Committee for Information Technology Standards (“INCITS”). The direct cache module <b>116</b> may interact with the cache <b>102</b> and/or the backing store <b>118</b> using block device commands to read, write, and clear (or trim) data. In one embodiment, the solid-state storage controller <b>104</b> and/or the backing store controller <b>120</b> provide block device interfaces to the direct cache module <b>116</b>.</div>
<div class="description-paragraph" id="p-0088" num="0087">In one embodiment, the direct cache module <b>116</b> serves as a proxy for the backing store <b>118</b>, receiving read and write requests for the backing store <b>118</b> directly from the host device <b>114</b>. The direct cache module <b>116</b> may represent itself to the host device <b>114</b> as a storage device having a capacity similar to and/or matching the capacity of the backing store <b>118</b>. The direct cache module <b>116</b>, upon receiving a read request or write request from the host device <b>114</b>, in one embodiment, fulfills the request by caching write data in the cache <b>102</b> or by retrieving read data from one of the cache <b>102</b> and the backing store <b>118</b> and returning the read data to the host device <b>114</b>.</div>
<div class="description-paragraph" id="p-0089" num="0088">Data caches are typically organized into cache lines which divide up the physical capacity of the cache, these cache lines may be divided into several sets. A cache line is typically larger than a block or sector of a backing store associated with a data cache, to provide for prefetching of additional blocks or sectors and to reduce cache misses and increase the cache hit rate. Data caches also typically evict an entire, fixed size, cache line at a time to make room for newly requested data in satisfying a cache miss. Data caches may be direct mapped, fully associative, N-way set associative, or the like.</div>
<div class="description-paragraph" id="p-0090" num="0089">In a direct mapped cache, each block or sector of a backing store has a one-to-one mapping to a cache line in the direct mapped cache. For example, if a direct mapped cache has T number of cache lines, the backing store associated with the direct mapped cache may be divided into T sections, and the direct mapped cache caches data from a section exclusively in the cache line corresponding to the section. Because a direct mapped cache always caches a block or sector in the same location or cache line, the mapping between a block or sector address and a cache line can be a simple manipulation of an address of the block or sector.</div>
<div class="description-paragraph" id="p-0091" num="0090">In a fully associative cache, any cache line can store data from any block or sector of a backing store. A fully associative cache typically has lower cache miss rates than a direct mapped cache, but has longer hit times (i.e., it takes longer to locate data in the cache) than a direct mapped cache. To locate data in a fully associative cache, either cache tags of the entire cache can be searched, a separate cache index can be used, or the like.</div>
<div class="description-paragraph" id="p-0092" num="0091">In an N-way set associative cache, each sector or block of a backing store may be cached in any of a set of N different cache lines. For example, in a 2-way set associative cache, either of two different cache lines may cache data for a sector or block. In an N-way set associative cache, both the cache and the backing store are typically divided into sections or sets, with one or more sets of sectors or blocks of the backing store assigned to a set of N cache lines. To locate data in an N-way set associative cache, a block or sector address is typically mapped to a set of cache lines, and cache tags of the set of cache lines are searched, a separate cache index is searched, or the like to determine which cache line in the set is storing data for the block or sector. An N-way set associative cache typically has miss rates and hit rates between those of a direct mapped cache and those of a fully associative cache.</div>
<div class="description-paragraph" id="p-0093" num="0092">The cache <b>102</b>, in one embodiment, may have characteristics of both a directly</div>
<div class="description-paragraph" id="p-0094" num="0093">mapped cache and a fully associative cache. A logical address space of the cache <b>102</b>, in one embodiment, is directly mapped to an address space of the backing store <b>118</b> while the physical storage media <b>110</b> of the cache <b>102</b> is fully associative with regard to the backing store <b>118</b>. In other words, each block or sector of the backing store <b>118</b>, in one embodiment, is directly mapped to a single logical address of the cache <b>102</b> while any portion of the physical storage media <b>110</b> of the cache <b>102</b> may store data for any block or sector of the backing store <b>118</b>. In one embodiment, a logical address is an identifier of a block of data and is distinct from a physical address of the block of data, but may be mapped to the physical address of the block of data. Examples of logical addresses, in various embodiments, include logical block addresses (“LBAs”), logical identifiers, object identifiers, pointers, references, and the like.</div>
<div class="description-paragraph" id="p-0095" num="0094">Instead of traditional cache lines, in one embodiment, the cache <b>102</b> has logical or physical cache data blocks associated with logical addresses that are equal in size to a block or sector of the backing store <b>118</b>. In a further embodiment, the cache <b>102</b> caches ranges and/or sets of ranges of blocks or sectors for the backing store <b>118</b> at a time, providing dynamic or variable length cache line functionality. A range or set of ranges of blocks or sectors, in a further embodiment, may include a mixture of contiguous and/or noncontiguous blocks. For example, the cache <b>102</b>, in one embodiment, supports block device requests that include a mixture of contiguous and/or noncontiguous blocks and that may include “holes” or intervening blocks that the cache <b>102</b> does not cache or otherwise store.</div>
<div class="description-paragraph" id="p-0096" num="0095">In one embodiment, one or more groups of logical addresses of the cache <b>102</b> are directly mapped to corresponding logical addresses of the backing store <b>118</b>. Directly mapping logical addresses of the cache <b>102</b> to logical addresses of the backing store <b>118</b>, in one embodiment, provides a one-to-one relationship between the logical addresses of the backing store <b>118</b> and the logical addresses of the cache <b>102</b>. Directly mapping logical addresses of the cache <b>102</b> to the logical or physical address space of the backing store <b>118</b>, in one embodiment, precludes the use of an extra translation layer in the direct cache module <b>116</b>, such as the use of cache tags, a cache index, the maintenance of a translation data structure, or the like. In one embodiment, while the logical address space of the cache <b>102</b> may be larger than a logical address space of the backing store <b>118</b>, both logical address spaces include at least logical addresses 0-N. In a further embodiment, at least a portion of the logical address space of the cache <b>102</b> represents or appears as the logical address space of the backing store <b>118</b> to a client, such as the host device <b>114</b>.</div>
<div class="description-paragraph" id="p-0097" num="0096">Alternatively, in certain embodiments where physical blocks or sectors of the backing store <b>118</b> are directly accessible using physical addresses, at least a portion of logical addresses in a logical address space of the cache <b>102</b> may be mapped to physical addresses of the backing store <b>118</b>. At least a portion of the logical address space of the cache <b>102</b>, in one embodiment, may correspond to the physical address space of the backing store <b>118</b>. At least a subset of the logical addresses of the cache <b>102</b>, in this embodiment, is directly mapped to corresponding physical addresses of the backing store <b>118</b>.</div>
<div class="description-paragraph" id="p-0098" num="0097">In one embodiment, the logical address space of the cache <b>102</b> is a sparse address space that is either as large as or is larger than the physical storage capacity of the cache <b>102</b>. This allows the backing store <b>118</b> to have a larger storage capacity than the cache <b>102</b>, while maintaining a direct mapping between the logical addresses of the cache <b>102</b> and logical or physical addresses of the backing store <b>118</b>. The sparse logical address space may be thinly provisioned, in one embodiment. In a further embodiment, as the direct cache module <b>116</b> writes data to the cache <b>102</b> using logical addresses, the cache <b>102</b> directly maps the logical addresses to distinct physical addresses or locations on the solid-state storage media <b>110</b> of the cache <b>102</b>, such that the physical addresses or locations of data on the solid-state storage media <b>110</b> are fully associative with the backing store <b>118</b>.</div>
<div class="description-paragraph" id="p-0099" num="0098">In one embodiment, the direct cache module <b>116</b> and/or the cache <b>102</b> use the same mapping structure to map addresses (either logical or physical) of the backing store <b>118</b> to logical addresses of the cache <b>102</b> and to map logical addresses of the cache <b>102</b> to locations/physical addresses of a block or sector (or range of blocks or sectors) on the physical solid-state storage media <b>110</b>. In one embodiment, using a single mapping structure for both functions eliminates the need for a separate cache map, cache index, cache tags, or the like, decreasing access times of the cache <b>102</b>. In a further embodiment, the direct cache module <b>116</b> uses the same mapping structure that maps logical addresses of the cache <b>102</b> to locations on the physical solid-state storage media <b>110</b> to locate ranges of data to destage in a backing store address order.</div>
<div class="description-paragraph" id="p-0100" num="0099">Once the direct cache module <b>116</b> has destaged dirty data from the cache <b>102</b>, the data is clean and the direct cache module <b>116</b> may clear, trim, replace, expire, and/or evict the data from the cache <b>102</b> and the physical addresses and associated physical storage media <b>110</b> may be freed to store data for other logical addresses. In one embodiment, as described above, the solid state storage controller <b>104</b> stores data at physical addresses using a log-based, append-only writing structure such that data evicted from the cache <b>102</b> or overwritten by a subsequent write request invalidates other data in the log. Consequently, a garbage collection or grooming process recovers the physical capacity of the invalid data in the log. One embodiment of the log-based, append only writing structure is logically ring-like data structure, as new data is appended to the log-based writing structure, previously used physical capacity is reused in a circular, theoretically infinite manner.</div>
<div class="description-paragraph" id="h-0009" num="0000">Solid-State Storage Device</div>
<div class="description-paragraph" id="p-0101" num="0100"> <figref idrefs="DRAWINGS">FIG. 2</figref> is a schematic block diagram illustrating one embodiment <b>201</b> of a solid-state storage device controller <b>202</b> that includes a write data pipeline <b>106</b> and a read data pipeline <b>108</b> in a cache <b>102</b> in accordance with the present invention. The solid-state storage device controller <b>202</b> may be embodied as hardware, as software, or as a combination of hardware and software.</div>
<div class="description-paragraph" id="p-0102" num="0101">The solid-state storage device controller <b>202</b> may include a number of solid-state storage controllers 0-N <b>104</b> <i>a</i>-<i>n</i>, each controlling solid-state storage media <b>110</b>. In the depicted embodiment, two solid-state controllers are shown: solid-state controller 0 <b>104</b> <i>a </i>and solid-state storage controller N <b>104</b> <i>n</i>, and each controls solid-state storage media <b>110</b> <i>a</i>-<i>n</i>. In the depicted embodiment, solid-state storage controller O <b>104</b> <i>a </i>controls a data channel so that the attached solid-state storage media <b>111</b> <i>a </i>stores data. Solid-state storage controller N <b>104</b> <i>n </i>controls an index metadata channel associated with the stored data and the associated solid-state storage media <b>110</b> <i>n</i>, stores index metadata. In an alternate embodiment, the solid-state storage device controller <b>202</b> includes a single solid-state controller <b>104</b> <i>a </i>with a single solid-state storage media <b>110</b> <i>a</i>. In another embodiment, there are a plurality of solid-state storage controllers <b>104</b> <i>a</i>-<i>n </i>and associated solid-state storage media <b>110</b> <i>a</i>-<i>n</i>. In one embodiment, one or more solid-state controllers <b>104</b> <i>a</i>-<b>104</b> <i>n</i>-<b>1</b>, coupled to their associated solid-state storage media <b>110</b> <i>a</i>-<b>110</b> <i>n</i>-<b>1</b>, control data while at least one solid-state storage controller <b>104</b> <i>n</i>, coupled to its associated solid-state storage media <b>110</b> <i>n</i>, controls index metadata.</div>
<div class="description-paragraph" id="p-0103" num="0102">In one embodiment, at least one solid-state controller <b>104</b> is field-programmable gate array (“FPGA”) and controller functions are programmed into the FPGA. In a particular embodiment, the FPGA is a Xilinx® FPGA. In another embodiment, the solid-state storage controller <b>104</b> comprises components specifically designed as a solid-state storage controller <b>104</b>, such as an application-specific integrated circuit (“ASIC”) or custom logic solution. Each solid-state storage controller <b>104</b> typically includes a write data pipeline <b>106</b> and a read data pipeline <b>108</b>, which are describe further in relation to <figref idrefs="DRAWINGS">FIG. 3</figref>. In another embodiment, at least one solid-state storage controller <b>104</b> is made up of a combination FPGA, ASIC, and custom logic components.</div>
<div class="description-paragraph" id="h-0010" num="0000">Solid-State Storage</div>
<div class="description-paragraph" id="p-0104" num="0103">The solid-state storage media <b>110</b> is an array of non-volatile solid-state storage elements <b>216</b>, <b>218</b>, <b>220</b>, arranged in banks <b>214</b>, and accessed in parallel through a bi-directional storage input/output (“I/O”) bus <b>210</b>. The storage I/O bus <b>210</b>, in one embodiment, is capable of unidirectional communication at any one time. For example, when data is being written to the solid-state storage media <b>110</b>, data cannot be read from the solid-state storage media <b>110</b>. In another embodiment, data can flow both directions simultaneously. However bi-directional, as used herein with respect to a data bus, refers to a data pathway that can have data flowing in only one direction at a time, but when data flowing one direction on the bi-directional data bus is stopped, data can flow in the opposite direction on the bi-directional data bus.</div>
<div class="description-paragraph" id="p-0105" num="0104">A solid-state storage element (e.g. SSS 0.0 <b>216</b> <i>a</i>) is typically configured as a chip (a package of one or more dies) or a die on a circuit board. As depicted, a solid-state storage element (e.g. <b>216</b> <i>a</i>) operates independently or semi-independently of other solid-state storage elements (e.g. <b>218</b> <i>a</i>) even if these several elements are packaged together in a chip package, a stack of chip packages, or some other package element. As depicted, a column of solid-state storage elements <b>216</b>, <b>218</b>, <b>220</b> is designated as a bank <b>214</b>. As depicted, there may be “n” banks <b>214</b> <i>a</i>-<i>n </i>and “m” solid-state storage elements <b>216</b> <i>a</i>-<i>m</i>, <b>218</b> <i>a</i>-<i>m</i>, <b>220</b> <i>a</i>-<i>m </i>per bank in an array of n×m solid-state storage elements <b>216</b>, <b>218</b>, <b>220</b> in a solid-state storage media <b>110</b>. In one embodiment, a solid-state storage media <b>110</b> <i>a </i>includes twenty solid-state storage elements per bank (e.g. <b>216</b> <i>a</i>-<i>m </i>in bank <b>214</b> <i>a</i>, <b>218</b> <i>a</i>-min bank <b>214</b> <i>b</i>, <b>220</b> <i>a</i>-min bank <b>214</b> <i>n</i>, where m=22) with eight banks (e.g. <b>214</b> <i>a</i>-<i>n </i>where n==8) and a solid-state storage media <b>110</b> <i>n </i>includes two solid-state storage elements (e.g. <b>216</b> <i>a</i>-<i>m </i>where m=2) per bank <b>214</b> with one bank <b>214</b> <i>a</i>. There is no requirement that two solid-state storage media <b>110</b> <i>a</i>, <b>110</b> <i>n </i>have the same number of solid-state storage elements and/or same number of banks <b>214</b>. In one embodiment, each solid-state storage element <b>216</b>, <b>218</b>, <b>220</b> is comprised of a single-level cell (“SLC”) devices. In another embodiment, each solid-state storage element <b>216</b>, <b>218</b>, <b>220</b> is comprised of multi-level cell (“MLC”) devices.</div>
<div class="description-paragraph" id="p-0106" num="0105">In one embodiment, solid-state storage elements for multiple banks that share a common storage I/O bus <b>210</b> <i>a </i>row (e.g. <b>216</b> <i>b</i>, <b>218</b> <i>b</i>, <b>220</b> <i>b</i>) are packaged together. In one embodiment, a solid-state storage element <b>216</b>, <b>218</b>, <b>220</b> may have one or more dies per chip with one or more chips stacked vertically and each die may be accessed independently. In another embodiment, a solid-state storage element (e.g. SSS 0.0 <b>216</b> <i>a</i>) may have one or more virtual dies per die and one or more dies per chip and one or more chips stacked vertically and each virtual die may be accessed independently. In another embodiment, a solid-state storage element SSS 0.0 <b>216</b> <i>a </i>may have one or more virtual dies per die and one or more dies per chip with some or all of the one or more dies stacked vertically and each virtual die may be accessed independently.</div>
<div class="description-paragraph" id="p-0107" num="0106">In one embodiment, two dies are stacked vertically with four stacks per group to form eight storage elements (e.g. SSS 0.0-SSS 0.8) <b>216</b> <i>a</i>-<b>220</b> <i>a</i>, each in a separate bank <b>214</b> <i>a</i>-<i>n</i>. In another embodiment, 20 storage elements (e.g. SSS 0.0-SSS 20.0) <b>216</b> form a virtual bank <b>214</b> <i>a </i>so that each of the eight virtual banks has 20 storage elements (e.g. SSS0.0-SSS 20.8). Data is sent to the solid-state storage media <b>110</b> over the storage I/O bus <b>210</b> to all storage elements of a particular group of storage elements (SSS 0.0-SSS 0.8) <b>216</b> <i>a</i>, <b>218</b> <i>a</i>, <b>220</b> <i>a</i>. The storage control bus <b>212</b> <i>a </i>is used to select a particular bank (e.g. Bank-0 <b>214</b> <i>a</i>) so that the data received over the storage I/O bus <b>210</b> connected to all banks <b>214</b> is written just to the selected bank <b>214</b> <i>a. </i> </div>
<div class="description-paragraph" id="p-0108" num="0107">In certain embodiments, the storage control bus <b>212</b> and storage I/O bus <b>210</b> are used together by the solid-state controller <b>104</b> to communicate addressing information, storage element command information, and data to be stored. Those of skill in the art recognize that this address, data, and command information may be communicated using one or the other of these buses <b>212</b>, <b>210</b>, or using separate buses for each type of control information. In one embodiment, addressing information, storage element command information, and storage data travel on the storage I/O bus <b>210</b> and the storage control bus <b>212</b> carries signals for activating a bank as well as identifying whether the data on the storage I/O bus <b>210</b> lines constitute addressing information, storage element command information, or storage data.</div>
<div class="description-paragraph" id="p-0109" num="0108">For example, a control signal on the storage control bus <b>212</b> such as “command enable” may indicate that the data on the storage I/O bus <b>210</b> lines is a storage element command such as program, erase, reset, read, and the like. A control signal on the storage control bus <b>212</b> such as “address enable” may indicate that the data on the storage I/O bus <b>210</b> lines is addressing information such as erase block identifier, page identifier, and optionally offset within the page within a particular storage element. Finally, an absence of a control signal on the storage control bus <b>212</b> for both “command enable” and “address enable” may indicate that the data on the storage I/O bus <b>210</b> lines is storage data that is to be stored on the storage element at a previously addressed erase block, physical page, and optionally offset within the page of a particular storage element.</div>
<div class="description-paragraph" id="p-0110" num="0109">In one embodiment, the storage I/O bus <b>210</b> is comprised of one or more independent I/O buses (“IIOBa-m” comprising <b>210</b> <i>a.a</i>-<i>m</i>, <b>210</b> <i>n.a</i>-<i>m</i>) wherein the solid-state storage elements within each row share one of the independent I/O buses across each solid-state storage element <b>216</b>, <b>218</b>, <b>220</b> in parallel so that all banks <b>214</b> are accessed simultaneously. For example, one IIOB <b>210</b> <i>a.a </i>of the storage I/O bus <b>210</b> may access a first solid-state storage element <b>216</b> <i>a</i>, <b>218</b> <i>a</i>, <b>220</b> <i>a </i>of each bank <b>214</b> <i>a</i>-<i>n </i>simultaneously. A second IIOB <b>210</b> <i>a.b </i>of the storage I/O bus <b>210</b> may access a second solid-state storage element <b>216</b> <i>b</i>, <b>218</b> <i>b</i>, <b>220</b> <i>b </i>of each bank <b>214</b> <i>a</i>-<i>n </i>simultaneously. Each row of solid-state storage elements <b>216</b>, <b>218</b>, <b>220</b> is accessed simultaneously. In one embodiment, where solid-state storage elements <b>216</b>, <b>218</b>, <b>220</b> are multi-level (physically stacked), all physical levels of the solid-state storage elements <b>216</b>, <b>218</b>, <b>220</b> are accessed simultaneously. As used herein, “simultaneously” also includes near simultaneous access where devices are accessed at slightly different intervals to avoid switching noise. Simultaneously is used in this context to be distinguished from a sequential or serial access wherein commands and/or data are sent individually one after the other.</div>
<div class="description-paragraph" id="p-0111" num="0110">Typically, banks <b>214</b> <i>a</i>-<i>n </i>are independently selected using the storage control bus <b>212</b>. In one embodiment, a bank <b>214</b> is selected using a chip enable or chip select. Where both chip select and chip enable are available, the storage control bus <b>212</b> may select one level of a multi-level solid-state storage element <b>216</b>, <b>218</b>, <b>220</b> using either of the chip select signal and the chip enable signal. In other embodiments, other commands are used by the storage control bus <b>212</b> to individually select one level of a multi-level solid-state storage element <b>216</b>, <b>218</b>, <b>220</b>. Solid-state storage elements <b>216</b>, <b>218</b>, <b>220</b> may also be selected through a combination of control and of address information transmitted on storage I/O bus <b>210</b> and the storage control bus <b>212</b>.</div>
<div class="description-paragraph" id="p-0112" num="0111">In one embodiment, each solid-state storage element <b>216</b>, <b>218</b>, <b>220</b> is partitioned into erase blocks and each erase block is partitioned into pages. A typical page is 2000 bytes (“2 kB”). In one example, a solid-state storage element (e.g. SSS0.0) includes two registers and can program two pages so that a two-register solid-state storage element has a page size of 4 kB. A single bank <b>214</b> <i>a </i>of 20 solid-state storage elements <b>216</b> <i>a</i>-<i>m </i>would then have an 80 kB capacity of pages accessed with the same address going out of the storage I/O bus <b>210</b>.</div>
<div class="description-paragraph" id="p-0113" num="0112">This group of pages in a bank <b>214</b> of solid-state storage elements <b>216</b>, <b>218</b>, <b>220</b> of 80 kB may be called a logical or virtual page. Similarly, an erase block of each storage element <b>216</b> <i>a</i>-<i>m </i>of a bank <b>214</b> <i>a </i>may be grouped to form a logical erase block. In one embodiment, erasing a logical erase block causes a physical erase block (“PEB”) of each storage element <b>216</b> <i>a</i>-<i>m </i>of a bank <b>214</b> <i>a </i>to be erased. In one embodiment, an erase block of pages within a solid-state storage element <b>216</b>, <b>218</b>, <b>220</b> is erased when an erase command is received within a solid-state storage element <b>216</b>, <b>218</b>, <b>220</b>. In another embodiment, a single physical erase block on each storage element (e.g. SSS M.N) collectively forms a logical erase block for the solid-state storage media <b>110</b> <i>a</i>. In such an embodiment, erasing a logical erase block comprises erasing an erase block at the same address within each storage element (e.g. SSS M.N) in the solid-state storage media <b>110</b> <i>a</i>. Whereas the size and number of erase blocks, pages, planes, or other logical and physical divisions within a solid-state storage element <b>216</b>, <b>218</b>, <b>220</b> may change over time with advancements in technology, it is to be expected that many embodiments consistent with new configurations are possible and are consistent with the general description herein.</div>
<div class="description-paragraph" id="p-0114" num="0113">In one embodiment, data is written in packets to the storage elements. The solid-state controller <b>104</b> uses the storage I/O bus <b>210</b> and storage control bus <b>212</b> to address a particular bank <b>214</b>, storage element <b>216</b>, <b>218</b>, <b>220</b>, physical erase block, physical page, and optionally offset within a physical page for writing the data packet. In one embodiment, the solid-state controller <b>104</b> sends the address information for the data packet by way of the storage I/O bus <b>210</b> and signals that the data on the storage I/O bus <b>210</b> is address data by way of particular signals set on the storage control bus <b>212</b>. The solid-state controller <b>104</b> follows the transmission of the address information with transmission of the data packet of data that is to be stored. The physical address contains enough information for the solid-state storage element <b>216</b>, <b>218</b>, <b>220</b> to direct the data packet to the designated location within the page.</div>
<div class="description-paragraph" id="p-0115" num="0114">In one embodiment, the storage I/O bus <b>210</b> <i>a.a </i>connects to each storage element in a row of storage elements (e.g. SSS 0.0-SSS O.N <b>216</b> <i>a</i>, <b>218</b> <i>a</i>, <b>220</b> <i>a</i>). In such an embodiment, the solid-state controller <b>104</b> <i>a </i>activates a desired bank <b>214</b> <i>a </i>using the storage control bus <b>212</b> <i>a</i>, such that data on storage I/O bus <b>210</b> <i>a.a </i>reaches the proper page of a single storage element (e.g. SSS 0.0 <b>216</b> <i>a</i>).</div>
<div class="description-paragraph" id="p-0116" num="0115">In addition, in certain embodiments, the solid-state controller <b>104</b> <i>a </i>simultaneously activates the same bank <b>214</b> <i>a </i>using the storage control bus <b>212</b> <i>a</i>, such that different data (a different data packet) on storage I/O bus <b>210</b> <i>a</i>. breaches the proper page of a single storage element on another row (e.g. SSS 1.0 <b>216</b> <i>b</i>). In this manner, multiple physical pages of multiple storage elements <b>216</b>, <b>218</b>, <b>220</b> may be written to simultaneously within a single bank <b>214</b> to store a logical page.</div>
<div class="description-paragraph" id="p-0117" num="0116">Similarly, a read command may require a command on the storage control bus <b>212</b> to select a single bank <b>214</b> <i>a </i>and the appropriate page within that bank <b>214</b> <i>a</i>. In one embodiment, a read command reads an entire physical page from each storage element, and because there are multiple solid-state storage elements <b>216</b>, <b>218</b>, <b>220</b> in parallel in a bank <b>214</b>, an entire logical page is read with a read command. However, the read command may be broken into subcommands, as will be explained below with respect to bank interleave. A logical page may also be accessed in a write operation.</div>
<div class="description-paragraph" id="p-0118" num="0117">In one embodiment, a solid-state controller <b>104</b> may send an erase block erase command over all the lines of the storage I/O bus <b>210</b> to erase a physical erase block having a particular erase block address. In addition, the solid-state controller <b>104</b> may simultaneously activate a single bank <b>214</b> using the storage control bus <b>212</b> such that each physical erase block in the single activated bank <b>214</b> is erased as part of a logical erase block.</div>
<div class="description-paragraph" id="p-0119" num="0118">In another embodiment, the solid-state controller <b>104</b> may send an erase block erase command over all the lines of the storage I/O bus <b>210</b> to erase a physical erase block having a particular erase block address on each storage element <b>216</b>, <b>218</b>, <b>220</b> (SSS 0.0-SSS M.N). These particular physical erase blocks together may form a logical erase block. Once the address of the physical erase blocks is provided to the storage elements <b>216</b>, <b>218</b>, <b>220</b>, the solid-state controller <b>104</b> may initiate the erase command on a bank <b>214</b> <i>a </i>by bank <b>214</b> <i>b </i>by bank <b>214</b> <i>n </i>basis (either in order or based on some other sequence). Other commands may also be sent to a particular location using a combination of the storage I/O bus <b>210</b> and the storage control bus <b>212</b>. One of skill in the art will recognize other ways to select a particular storage location using the bi-directional storage I/O bus <b>210</b> and the storage control bus <b>212</b>.</div>
<div class="description-paragraph" id="p-0120" num="0119">In one embodiment, the storage controller <b>104</b> sequentially writes data on the solid-state storage media <b>110</b> in a log structured format and within one or more physical structures of the storage elements, the data is sequentially stored on the solid-state storage media <b>110</b>. Sequentially writing data involves the storage controller <b>104</b> streaming data packets into storage write buffers for storage elements, such as a chip (a package of one or more dies) or a die on a circuit board. When the storage write buffers are full, the data packets are programmed to a designated virtual or logical page (“LP”). Data packets then refill the storage write buffers and, when full, the data packets are written to the next LP. The next virtual page may be in the same bank <b>214</b> <i>a </i>or another bank (e.g. <b>214</b> <i>b</i>). This process continues, LP after LP, typically until a virtual or logical erase block (“LEB”) is filled. LPs and LEBs are described in more detail below.</div>
<div class="description-paragraph" id="p-0121" num="0120">In another embodiment, the streaming may continue across LEB boundaries with the process continuing, LEB after LEB. Typically, the storage controller <b>104</b> sequentially stores data packets in an LEB by order of processing. In one embodiment, where a write data pipeline <b>106</b> is used, the storage controller <b>104</b> stores packets in the order that they come out of the write data pipeline <b>106</b>. This order may be a result of data segments arriving from a requesting device mixed with packets of valid data that are being read from another storage location as valid data is being recovered from another LEB during a recovery operation.</div>
<div class="description-paragraph" id="p-0122" num="0121">The sequentially stored data, in one embodiment, can serve as a log to reconstruct data indexes and other metadata using information from data packet headers. For example, in one embodiment, the storage controller <b>104</b> may reconstruct a storage index by reading headers to determine the data structure to which each packet belongs and sequence information to determine where in the data structure the data or metadata belongs. The storage controller <b>104</b>, in one embodiment, uses physical address information for each packet and timestamp or sequence information to create a mapping between the physical locations of the packets and the data structure identifier and data segment sequence. Timestamp or sequence information is used by the storage controller <b>104</b> to replay the sequence of changes made to the index and thereby reestablish the most recent state.</div>
<div class="description-paragraph" id="p-0123" num="0122">In one embodiment, erase blocks are time stamped or given a sequence number as packets are written and the timestamp or sequence information of an erase block is used along with information gathered from container headers and packet headers to reconstruct the storage index. In another embodiment, timestamp or sequence information is written to an erase block when the erase block is recovered.</div>
<div class="description-paragraph" id="p-0124" num="0123">In a read, modify, write operation, data packets associated with the logical structure are located and read in a read operation. Data segments of the modified structure that have been modified are not written to the location from which they are read. Instead, the modified data segments are again converted to data packets and then written to the next available location in the virtual page currently being written. Index entries for the respective data packets are modified to point to the packets that contain the modified data segments. The entry or entries in the index for data packets associated with the same logical structure that have not been modified will include pointers to original location of the unmodified data packets. Thus, if the original logical structure is maintained, for example to maintain a previous version of the logical structure, the original logical structure will have pointers in the index to all data packets as originally written. The new logical structure will have pointers in the index to some of the original data packets and pointers to the modified data packets in the virtual page that is currently being written.</div>
<div class="description-paragraph" id="p-0125" num="0124">In a copy operation, the index includes an entry for the original logical structure mapped to a number of packets stored on the solid-state storage media <b>110</b>. When a copy is made, a new logical structure is created and a new entry is created in the index mapping the new logical structure to the original packets. The new logical structure is also written to the solid-state storage media <b>110</b> with its location mapped to the new entry in the index. The new logical structure packets may be used to identify the packets within the original logical structure that are referenced in case changes have been made in the original logical structure that have not been propagated to the copy and the index is lost or corrupted. In another embodiment, the index includes a logical entry for a logical block.</div>
<div class="description-paragraph" id="p-0126" num="0125">Beneficially, sequentially writing packets facilitates a more even use of the solid-state storage media <b>110</b> and allows the solid-storage device controller <b>202</b> to monitor storage hot spots and level usage of the various virtual pages in the solid-state storage media <b>110</b>. Sequentially writing packets also facilitates a powerful, efficient garbage collection system, which is described in detail below. One of skill in the art will recognize other benefits of sequential storage of data packets.</div>
<div class="description-paragraph" id="p-0127" num="0126">The system <b>100</b> may comprise a log-structured storage system or log-structured array similar to a log-structured file system and the order that data is stored may be used to recreate an index. Typically an index that includes a logical-to-physical mapping is stored in volatile memory. If the index is corrupted or lost, the index may be reconstructed by addressing the solid-state storage media <b>110</b> in the order that the data was written. Within a logical erase block (“LEB”), data is typically stored sequentially by filling a first logical page, then a second logical page, etc. until the LEB is filled. The solid-state storage controller <b>104</b> then chooses another LEB and the process repeats. By maintaining an order that the LEBs were written to and by knowing that each LEB is written sequentially, the index can be rebuilt by traversing the solid-state storage media <b>110</b> in order from beginning to end. In other embodiments, if part of the index is stored in non-volatile memory, such as on the solid-state storage media <b>110</b>, the solid-state storage controller <b>104</b> may only need to replay a portion of the solid-state storage media <b>110</b> to rebuild a portion of the index that was not stored in non-volatile memory. One of skill in the art will recognize other benefits of sequential storage of data packets.</div>
<div class="description-paragraph" id="h-0011" num="0000">Solid-State Storage Device Controller</div>
<div class="description-paragraph" id="p-0128" num="0127">In various embodiments, the solid-state storage device controller <b>202</b> also includes a data bus <b>204</b>, a local bus <b>206</b>, a buffer controller <b>208</b>, buffers 0-N <b>222</b> <i>a</i>-<i>n</i>, a master controller <b>224</b>, a direct memory access (“DMA”) controller <b>226</b>, a memory controller <b>228</b>, a dynamic memory array <b>230</b>, a static random memory array <b>232</b>, a management controller <b>234</b>, a management bus <b>236</b>, a bridge <b>238</b> to a system bus <b>240</b>, and miscellaneous logic <b>242</b>, which are described below. In other embodiments, the system bus <b>240</b> is coupled to one or more network interface cards (“NICs”) <b>244</b>, some of which may include remote DMA (“RDMA”) controllers <b>246</b>, one or more central processing unit (“CPU”) <b>248</b>, one or more external memory controllers <b>250</b> and associated external memory arrays <b>252</b>, one or more storage controllers <b>254</b>, peer controllers <b>256</b>, and application specific processors <b>258</b>, which are described below. The components <b>244</b>-<b>258</b> connected to the system bus <b>240</b> may be located in the host device <b>114</b> or may be other devices.</div>
<div class="description-paragraph" id="p-0129" num="0128">In one embodiment, the solid-state storage controller(s) <b>104</b> communicate data to the solid-state storage media <b>110</b> over a storage I/O bus <b>210</b>. In a certain embodiment where the solid-state storage is arranged in banks <b>214</b> and each bank <b>214</b> includes multiple storage elements <b>216</b>, <b>218</b>, <b>220</b> accessible in parallel, the storage I/O bus <b>210</b> comprises an array of busses, one for each row of storage elements <b>216</b>, <b>218</b>, <b>220</b> spanning the banks <b>214</b>. As used herein, the term “storage I/O bus” may refer to one storage I/O bus <b>210</b> or an array of data independent busses <b>204</b>. In one embodiment, each storage I/O bus <b>210</b> accessing a row of storage elements (e.g. <b>216</b> <i>a</i>, <b>218</b> <i>a</i>, <b>220</b> <i>a</i>) may include a logical-to-physical mapping for storage divisions (e.g. erase blocks) accessed in a row of storage elements <b>216</b> <i>a</i>, <b>218</b> <i>a</i>, <b>220</b> <i>a</i>. This mapping allows a logical address mapped to a physical address of a storage division to be remapped to a different storage division if the first storage division fails, partially fails, is inaccessible, or has some other problem. Remapping is explained further in relation to the remapping module <b>430</b> of <figref idrefs="DRAWINGS">FIG. 4</figref>.</div>
<div class="description-paragraph" id="p-0130" num="0129">Data may also be communicated to the solid-state storage controller(s) <b>104</b> from a requesting device <b>155</b> through the system bus <b>240</b>, bridge <b>238</b>, local bus <b>206</b>, buffer(s) <b>222</b>, and finally over a data bus <b>204</b>. The data bus <b>204</b> typically is connected to one or more buffers <b>222</b> <i>a</i>-<i>n </i>controlled with a buffer controller <b>208</b>. The buffer controller <b>208</b> typically controls transfer of data from the local bus <b>206</b> to the buffers <b>222</b> and through the data bus <b>204</b> to the pipeline input buffer <b>306</b> and output buffer <b>330</b>. The buffer controller <b>208</b> typically controls how data arriving from a requesting device <b>155</b> can be temporarily stored in a buffer <b>222</b> and then transferred onto a data bus <b>204</b>, or vice versa, to account for different clock domains, to prevent data collisions, etc. The buffer controller <b>208</b> typically works in conjunction with the master controller <b>224</b> to coordinate data flow. As data arrives, the data will arrive on the system bus <b>240</b>, be transferred to the local bus <b>206</b> through a bridge <b>238</b>.</div>
<div class="description-paragraph" id="p-0131" num="0130">Typically the data is transferred from the local bus <b>206</b> to one or more data buffers <b>222</b> as directed by the master controller <b>224</b> and the buffer controller <b>208</b>. The data then flows out of the buffer(s) <b>222</b> to the data bus <b>204</b>, through a solid-state controller <b>104</b>, and on to the solid-state storage media <b>110</b> such as NAND flash or other storage media. In one embodiment, data and associated out-of-band metadata (“metadata”) arriving with the data is communicated using one or more data channels comprising one or more solid-state storage controllers <b>104</b> <i>a</i>-<b>104</b> <i>n</i>-<b>1</b> and associated solid-state storage media <b>110</b> <i>a</i>-<b>110</b> <i>n</i>-<b>1</b> while at least one channel (solid-state storage controller <b>104</b> <i>n</i>, solid-state storage media <b>110</b> <i>n</i>) is dedicated to in-band metadata, such as index information and other metadata generated internally to the cache <b>102</b>.</div>
<div class="description-paragraph" id="p-0132" num="0131">The local bus <b>206</b> is typically a bidirectional bus or set of busses that allows for communication of data and commands between devices internal to the solid-state storage device controller <b>202</b> and between devices internal to the cache <b>102</b> and devices <b>244</b>-<b>258</b> connected to the system bus <b>240</b>. The bridge <b>238</b> facilitates communication between the local bus <b>206</b> and system bus <b>240</b>. One of skill in the art will recognize other embodiments such as ring structures or switched star configurations and functions of buses <b>240</b>, <b>206</b>, <b>204</b> and bridges <b>238</b>.</div>
<div class="description-paragraph" id="p-0133" num="0132">The system bus <b>240</b> is typically a bus of a host device <b>114</b> or other device in which the cache <b>102</b> is installed or connected. In one embodiment, the system bus <b>240</b> may be a PCI-e bus, a Serial Advanced Technology Attachment (“serial ATA”) bus, parallel A TA, or the like. In another embodiment, the system bus <b>240</b> is an external bus such as small computer system interface (“SCSI”), Fire Wire, Fiber Channel, USB, PCIe-AS, or the like. The cache <b>102</b> may be packaged to fit internally to a device or as an externally connected device.</div>
<div class="description-paragraph" id="p-0134" num="0133">The solid-state storage device controller <b>202</b> includes a master controller <b>224</b> that controls higher-level functions within the cache <b>102</b>. The master controller <b>224</b>, in various embodiments, controls data flow by interpreting requests, directs creation of indexes to map identifiers associated with data to physical locations of associated data, coordinating DMA requests, etc. Many of the functions described herein are controlled wholly or in part by the master controller <b>224</b>.</div>
<div class="description-paragraph" id="p-0135" num="0134">In one embodiment, the master controller <b>224</b> uses embedded controller(s). In another embodiment, the master controller <b>224</b> uses local memory such as a dynamic memory array <b>230</b> (dynamic random access memory “DRAM”), a static memory array <b>232</b> (static random access memory “SRAM”), etc. In one embodiment, the local memory is controlled using the master controller <b>224</b>. In another embodiment, the master controller <b>224</b> accesses the local memory via a memory controller <b>228</b>. In another embodiment, the master controller <b>224</b> runs a Linux server and may support various common server interfaces, such as the World Wide Web, hyper-text markup language (“HTML”), etc. In another embodiment, the master controller <b>224</b> uses a nano-processor. The master controller <b>224</b> may be constructed using programmable or standard logic, or any combination of controller types listed above. The master controller <b>224</b> may be embodied as hardware, as software, or as a combination of hardware and software. One skilled in the art will recognize many embodiments for the master controller <b>224</b>.</div>
<div class="description-paragraph" id="p-0136" num="0135">In one embodiment, where the storage controller <b>152</b>/solid-state storage device controller <b>202</b> manages multiple data storage devices/solid-state storage media <b>110</b> <i>a</i>-<i>n</i>, the master controller <b>224</b> divides the work load among internal controllers, such as the solid-state storage controllers <b>104</b> <i>a</i>-<i>n</i>. For example, the master controller <b>224</b> may divide a data structure to be written to the data storage devices (e.g. solid-state storage media <b>110</b> <i>a</i>-<i>n</i>) so that a portion of the data structure is stored on each of the attached data storage devices. This feature is a performance enhancement allowing quicker storage and access to a data structure. In one embodiment, the master controller <b>224</b> is implemented using an FPGA. In another embodiment, the firmware within the master controller <b>224</b> may be updated through the management bus <b>236</b>, the system bus <b>240</b> over a network connected to a NIC <b>244</b> or other device connected to the system bus <b>240</b>.</div>
<div class="description-paragraph" id="p-0137" num="0136">In one embodiment, the master controller <b>224</b> emulates block storage such that a host device <b>114</b> or other device connected to the storage device/cache <b>102</b> views the storage device/cache <b>102</b> as a block storage device and sends data to specific physical or logical addresses in the storage device/cache <b>102</b>. The master controller <b>224</b> then divides up the blocks and stores the data blocks. The master controller <b>224</b> then maps the blocks and physical or logical address sent with the block to the actual locations determined by the master controller <b>224</b>. The mapping is stored in the index. Typically, for block emulation, a block device application program interface (“API”) is provided in a driver in the host device <b>114</b>, or other device wishing to use the storage device/cache <b>102</b> as a block storage device.</div>
<div class="description-paragraph" id="p-0138" num="0137">In another embodiment, the master controller <b>224</b> coordinates with NIC controllers <b>244</b> and embedded RDMA controllers <b>246</b> to deliver just-in-time RDMA transfers of data and command sets. NIC controller <b>244</b> may be hidden behind a non-transparent port to enable the use of custom drivers. Also, a driver on a host device <b>114</b> may have access to a computer network through an I/O memory driver using a standard stack API and operating in conjunction with NICs <b>244</b>.</div>
<div class="description-paragraph" id="p-0139" num="0138">In one embodiment, the master controller <b>224</b> is also a redundant array of independent drive (“RAID”) controller. Where the data storage device/cache <b>102</b> is networked with one or more other data storage devices, the master controller <b>224</b> may be a RAID controller for single tier RAID, multi-tier RAID, progressive RAID, etc. The master controller <b>224</b> may also allows some objects and other data structures to be stored in a RAID array and other data structures to be stored without RAID. In another embodiment, the master controller <b>224</b> may be a distributed RAID controller element. In another embodiment, the master controller <b>224</b> may comprise many RAID, distributed RAID, and other functions as described elsewhere.</div>
<div class="description-paragraph" id="p-0140" num="0139">In one embodiment, the master controller <b>224</b> coordinates with single or redundant network managers (e.g. switches) to establish routing, to balance bandwidth utilization, failover, etc. In another embodiment, the master controller <b>224</b> coordinates with integrated application specific logic (via local bus <b>206</b>) and associated driver software. In another embodiment, the master controller <b>224</b> coordinates with attached application specific processors <b>258</b> or logic (via the external system bus <b>240</b>) and associated driver software. In another embodiment, the master controller <b>224</b> coordinates with remote application specific logic (via a computer network) and associated driver software. In another embodiment, the master controller <b>224</b> coordinates with the local bus <b>206</b> or external bus attached hard disk drive (“HDD”) storage controller.</div>
<div class="description-paragraph" id="p-0141" num="0140">In one embodiment, the master controller <b>224</b> communicates with one or more storage controllers <b>254</b> where the storage device/cache <b>102</b> may appear as a storage device connected through a SCSI bus, Internet SCSI (“iSCSI”), fiber channel, etc. Meanwhile the storage device/cache <b>102</b> may autonomously manage objects or other data structures and may appear as an object file system or distributed object file system. The master controller <b>224</b> may also be accessed by peer controllers <b>256</b> and/or application specific processors <b>258</b>.</div>
<div class="description-paragraph" id="p-0142" num="0141">In another embodiment, the master controller <b>224</b> coordinates with an autonomous integrated management controller to periodically validate FPGA code and/or controller software, validate FPGA code while running (reset) and/or validate controller software during power on (reset), support external reset requests, support reset requests due to watchdog timeouts, and support voltage, current, power, temperature, and other environmental measurements and setting of threshold interrupts. In another embodiment, the master controller <b>224</b> manages garbage collection to free erase blocks for reuse. In another embodiment, the master controller <b>224</b> manages wear leveling. In another embodiment, the master controller <b>224</b> allows the data storage device/cache <b>102</b> to be partitioned into multiple virtual devices and allows partition-based media encryption. In yet another embodiment, the master controller <b>224</b> supports a solid-state storage controller <b>104</b> with advanced, multi-bit ECC correction. One of skill in the art will recognize other features and functions of a master controller <b>224</b> in a storage controller <b>152</b>, or more specifically in a cache <b>102</b>.</div>
<div class="description-paragraph" id="p-0143" num="0142">In one embodiment, the solid-state storage device controller <b>202</b> includes a memory controller <b>228</b> which controls a dynamic random memory array <b>230</b> and/or a static random memory array <b>232</b>. As stated above, the memory controller <b>228</b> may be independent or integrated with the master controller <b>224</b>. The memory controller <b>228</b> typically controls volatile memory of some type, such as DRAM (dynamic random memory array <b>230</b>) and SRAM (static random memory array <b>232</b>). In other examples, the memory controller <b>228</b> also controls other memory types such as electrically erasable programmable read only memory (“EEPROM”), etc.</div>
<div class="description-paragraph" id="p-0144" num="0143">In other embodiments, the memory controller <b>228</b> controls two or more memory types and the memory controller <b>228</b> may include more than one controller. Typically, the memory controller <b>228</b> controls as much SRAM <b>232</b> as is feasible and by DRAM <b>230</b> to supplement the SR AM <b>232</b>.</div>
<div class="description-paragraph" id="p-0145" num="0144">In one embodiment, the logical-to-physical index is stored in memory <b>230</b>, <b>232</b> and then periodically off-loaded to a channel of the solid-state storage media <b>110</b> <i>n </i>or other non-volatile memory. One of skill in the art will recognize other uses and configurations of the memory controller <b>228</b>, dynamic memory array <b>230</b>, and static memory array <b>232</b>.</div>
<div class="description-paragraph" id="p-0146" num="0145">In one embodiment, the solid-state storage device controller <b>202</b> includes a DMA controller <b>226</b> that controls DMA operations between the storage device/cache <b>102</b> and one or more external memory controllers <b>250</b> and associated external memory arrays <b>252</b> and CPUs <b>248</b>. Note that the external memory controllers <b>250</b> and external memory arrays <b>252</b> are called external because they are external to the storage device/cache <b>102</b>. In addition the DMA controller <b>226</b> may also control RDMA operations with requesting devices through a NIC <b>244</b> and associated RDMA controller <b>246</b>.</div>
<div class="description-paragraph" id="p-0147" num="0146">In one embodiment, the solid-state storage device controller <b>202</b> includes a management controller <b>234</b> connected to a management bus <b>236</b>. Typically the management controller <b>234</b> manages environmental metrics and status of the storage device/cache <b>102</b>. The management controller <b>234</b> may monitor device temperature, fan speed, power supply settings, etc. over the management bus <b>236</b>. The management controller <b>234</b> may support the reading and programming of erasable programmable read only memory (“EEPROM”) for storage of FPGA code and controller software. Typically the management bus <b>236</b> is connected to the various components within the storage device/cache <b>102</b>. The management controller <b>234</b> may communicate alerts, interrupts, etc. over the local bus <b>206</b> or may include a separate connection to a system bus <b>240</b> or other bus. In one embodiment the management bus <b>236</b> is an Inter-Integrated Circuit (“I2C”) bus. One of skill in the art will recognize other related functions and uses of a management controller <b>234</b> connected to components of the storage device/cache <b>102</b> by a management bus <b>236</b>.</div>
<div class="description-paragraph" id="p-0148" num="0147">In one embodiment, the solid-state storage device controller <b>202</b> includes miscellaneous logic <b>242</b> that may be customized for a specific application. Typically where the solid-state device controller <b>202</b> or master controller <b>224</b> is/are configured using a FPGA or other configurable controller, custom logic may be included based on a particular application, customer requirement, storage requirement, etc.</div>
<div class="description-paragraph" id="h-0012" num="0000">Data Pipeline</div>
<div class="description-paragraph" id="p-0149" num="0148"> <figref idrefs="DRAWINGS">FIG. 3</figref> is a schematic block diagram illustrating one embodiment <b>300</b> of a solid-state storage controller <b>104</b> with a write data pipeline <b>106</b> and a read data pipeline <b>108</b> in a cache <b>102</b> in accordance with the present invention. The embodiment <b>300</b> includes a data bus <b>204</b>, a local bus <b>206</b>, and buffer control <b>208</b>, which are substantially similar to those described in relation to the solid-state storage device controller <b>202</b> of <figref idrefs="DRAWINGS">FIG. 2</figref>. The write data pipeline <b>106</b> includes a packetizer <b>302</b> and an error-correcting code (“ECC”) generator <b>304</b>. In other embodiments, the write data pipeline <b>106</b> includes an input buffer <b>306</b>, a write synchronization buffer <b>308</b>, a write program module <b>310</b>, a compression module <b>312</b>, an encryption module <b>314</b>, a garbage collector bypass <b>316</b> (with a portion within the read data pipeline <b>108</b>), a media encryption module <b>318</b>, and a write buffer <b>320</b>. The read data pipeline <b>108</b> includes a read synchronization buffer <b>328</b>, an ECC correction module <b>322</b>, a depacketizer <b>324</b>, an alignment module <b>326</b>, and an output buffer <b>330</b>. In other embodiments, the read data pipeline <b>108</b> may include a media decryption module <b>332</b>, a portion of the garbage collector bypass <b>316</b>, a decryption module <b>334</b>, a decompression module <b>336</b>, and a read program module <b>338</b>. The solid-state storage controller <b>104</b> may also include control and status registers <b>340</b> and control queues <b>342</b>, a bank interleave controller <b>344</b>, a synchronization buffer <b>346</b>, a storage bus controller <b>348</b>, and a multiplexer (“MUX”) <b>350</b>. The components of the solid-state controller <b>104</b> and associated write data pipeline <b>106</b> and read data pipeline <b>108</b> are described below. In other embodiments, synchronous solid-state storage media <b>110</b> may be used and synchronization buffers <b>308</b> <b>328</b> may be eliminated.</div>
<div class="description-paragraph" id="h-0013" num="0000">Write Data Pipeline</div>
<div class="description-paragraph" id="p-0150" num="0149">The write data pipeline <b>106</b> includes a packetizer <b>302</b> that receives a data or metadata segment to be written to the solid-state storage, either directly or indirectly through another write data pipeline <b>106</b> stage, and creates one or more packets sized for the solid-state storage media <b>110</b>. The data or metadata segment is typically part of a data structure such as an object, but may also include an entire data structure. In another embodiment, the data segment is part of a block of data, but may also include an entire block of data. Typically, a set of data such as a data structure is received from a computer such as the host device <b>114</b>, or other computer or device and is transmitted to the cache <b>102</b> in data segments streamed to the cache <b>102</b> and/or the host device <b>114</b>. A data segment may also be known by another name, such as data parcel, but as referenced herein includes all or a portion of a data structure or data block.</div>
<div class="description-paragraph" id="p-0151" num="0150">Each data structure is stored as one or more packets. Each data structure may have one or more container packets. Each packet contains a header. The header may include a header type field. Type fields may include data, attribute, metadata, data segment delimiters (multi-packet), data structures, data linkages, and the like. The header may also include information regarding the size of the packet, such as the number of bytes of data included in the packet. The length of the packet may be established by the packet type. The header may include information that establishes the relationship of the packet to a data structure. An example might be the use of an offset in a data packet header to identify the location of the data segment within the data structure. One of skill in the art will recognize other information that may be included in a header added to data by a packetizer <b>302</b> and other information that may be added to a data packet.</div>
<div class="description-paragraph" id="p-0152" num="0151">Each packet includes a header and possibly data from the data or metadata segment. The header of each packet includes pertinent information to relate the packet to the data structure to which the packet belongs. For example, the header may include an object identifier or other data structure identifier and offset that indicate the data segment, object, data structure or data block from which the data packet was formed. The header may also include a logical address used by the storage bus controller <b>348</b> to store the packet. The header may also include information regarding the size of the packet, such as the number of bytes included in the packet. The header may also include a sequence number that identifies where the data segment belongs with respect to other packets within the data structure when reconstructing the data segment or data structure. The header may include a header type field. Type fields may include data, data structure attributes, metadata, data segment delimiters (multi-packet), data structure types, data structure linkages, and the like. One of skill in the art will recognize other information that may be included in a header added to data or metadata by a packetizer <b>302</b> and other information that may be added to a packet.</div>
<div class="description-paragraph" id="p-0153" num="0152">The write data pipeline <b>106</b> includes an ECC generator <b>304</b> that that generates one or more error-correcting codes (“ECC”) for the one or more packets received from the packetizer <b>302</b>. The ECC generator <b>304</b> typically uses an error correcting algorithm to generate ECC check bits which are stored with the one or more data packets. The ECC codes generated by the ECC generator <b>304</b> together with the one or more data packets associated with the ECC codes comprise an ECC chunk. The ECC data stored with the one or more data packets is used to detect and to correct errors introduced into the data through transmission and storage. In one embodiment, packets are streamed into the ECC generator <b>304</b> as un-encoded blocks of length N. A syndrome of length S is calculated, appended and output as an encoded block of length N+S. The value of N and S are dependent upon the characteristics of the algorithm which is selected to achieve specific performance, efficiency, and robustness metrics. In one embodiment, there is no fixed relationship between the ECC blocks and the packets; the packet may comprise more than one ECC block; the ECC block may comprise more than one packet; and a first packet may end anywhere within the ECC block and a second packet may begin after the end of the first packet within the same ECC block. In one embodiment, ECC algorithms are not dynamically modified. In one embodiment, the ECC data stored with the data packets is robust enough to correct errors in more than two bits.</div>
<div class="description-paragraph" id="p-0154" num="0153">Beneficially, using a robust ECC algorithm allowing more than single bit correction or even double bit correction allows the life of the solid-state storage media <b>110</b> to be extended. For example, if flash memory is used as the storage medium in the solid-state storage media <b>110</b>, the flash memory may be written approximately 100,000 times without error per erase cycle. This usage limit may be extended using a robust ECC algorithm. Having the ECC generator <b>304</b> and corresponding ECC correction module <b>322</b> onboard the cache <b>102</b>, the cache <b>102</b> can internally correct errors and has a longer useful life than if a less robust ECC algorithm is used, such as single bit correction. However, in other embodiments the ECC generator <b>304</b> may use a less robust algorithm and may correct single-bit or double-bit errors. In another embodiment, the solid-state storage device <b>110</b> may comprise less reliable storage such as multi-level cell (“MLC”) flash in order to increase capacity, which storage may not be sufficiently reliable without more robust ECC algorithms.</div>
<div class="description-paragraph" id="p-0155" num="0154">In one embodiment, the write pipeline <b>106</b> includes an input buffer <b>306</b> that receives a data segment to be written to the solid-state storage media <b>110</b> and stores the incoming data segments until the next stage of the write data pipeline <b>106</b>, such as the packetizer <b>302</b> (or other stage for a more complex write data pipeline <b>106</b>) is ready to process the next data segment. The input buffer <b>306</b> typically allows for discrepancies between the rate data segments are received and processed by the write data pipeline <b>106</b> using an appropriately sized data buffer. The input buffer <b>306</b> also allows the data bus <b>204</b> to transfer data to the write data pipeline <b>106</b> at rates greater than can be sustained by the write data pipeline <b>106</b> in order to improve efficiency of operation of the data bus <b>204</b>. Typically when the write data pipeline <b>106</b> does not include an input buffer <b>306</b>, a buffering function is performed elsewhere, such as in the cache <b>102</b>, but outside the write data pipeline <b>106</b>, in the host device <b>114</b>, such as within a network interface card (“NIC”), or at another device, for example when using remote direct memory access (“RDMA”).</div>
<div class="description-paragraph" id="p-0156" num="0155">In another embodiment, the write data pipeline <b>106</b> also includes a write synchronization buffer <b>308</b> that buffers packets received from the ECC generator <b>304</b> prior to writing the packets to the solid-state storage media <b>110</b>. The write synch buffer <b>308</b> is located at a boundary between a local clock domain and a solid-state storage clock domain and provides buffering to account for the clock domain differences. In other embodiments, synchronous solid-state storage media <b>110</b> may be used and synchronization buffers <b>308</b> <b>328</b> may be eliminated.</div>
<div class="description-paragraph" id="p-0157" num="0156">In one embodiment, the write data pipeline <b>106</b> also includes a media encryption module <b>318</b> that receives the one or more packets from the packetizer <b>302</b>, either directly or indirectly, and encrypts the one or more packets using an encryption key unique to the cache <b>102</b> prior to sending the packets to the ECC generator <b>304</b>. Typically, the entire packet is encrypted, including the headers. In another embodiment, headers are not encrypted. In this document, encryption key is understood to mean a secret encryption key that is managed externally from a solid-state storage controller <b>104</b>.</div>
<div class="description-paragraph" id="p-0158" num="0157">The media encryption module <b>318</b> and corresponding media decryption module <b>332</b> provide a level of security for data stored in the solid-state storage media <b>110</b>. For example, where data is encrypted with the media encryption module <b>318</b>, if the solid-state storage media <b>110</b> is connected to a different solid-state storage controller <b>104</b>, cache <b>102</b>, or server, the contents of the solid-state storage media <b>110</b> typically could not be read without use of the same encryption key used during the write of the data to the solid-state storage media <b>110</b> without significant effort.</div>
<div class="description-paragraph" id="p-0159" num="0158">In a typical embodiment, the cache <b>102</b> does not store the encryption key in non-volatile storage and allows no external access to the encryption key. The encryption key is provided to the solid-state storage controller <b>104</b> during initialization. The cache <b>102</b> may use and store a non-secret cryptographic nonce that is used in conjunction with an encryption key. A different nonce may be stored with every packet. Data segments may be split between multiple packets with unique nonces for the purpose of improving protection by the encryption algorithm.</div>
<div class="description-paragraph" id="p-0160" num="0159">The encryption key may be received from a host device <b>114</b>, a server, key manager, or other device that manages the encryption key to be used by the solid-state storage controller <b>104</b>. In another embodiment, the solid-state storage media <b>110</b> may have two or more partitions and the solid-state storage controller <b>104</b> behaves as though it was two or more solid-state storage controllers <b>104</b>, each operating on a single partition within the solid-state storage media <b>110</b>. In this embodiment, a unique media encryption key may be used with each partition.</div>
<div class="description-paragraph" id="p-0161" num="0160">In another embodiment, the write data pipeline <b>106</b> also includes an encryption module <b>314</b> that encrypts a data or metadata segment received from the input buffer <b>306</b>, either directly or indirectly, prior sending the data segment to the packetizer <b>302</b>, the data segment encrypted using an encryption key received in conjunction with the data segment. The encryption keys used by the encryption module <b>314</b> to encrypt data may not be common to all data stored within the cache <b>102</b> but may vary on an per data structure basis and received in conjunction with receiving data segments as described below. For example, an encryption key for a data segment to be encrypted by the encryption module <b>314</b> may be received with the data segment or may be received as part of a command to write a data structure to which the data segment belongs. The cache <b>102</b> may use and store a non-secret cryptographic nonce in each data structure packet that is used in conjunction with the encryption key. A different nonce may be stored with every packet. Data segments may be split between multiple packets with unique nonces for the purpose of improving protection by the encryption algorithm.</div>
<div class="description-paragraph" id="p-0162" num="0161">The encryption key may be received from a host device <b>114</b>, a computer, key manager, or other device that holds the encryption key to be used to encrypt the data segment. In one embodiment, encryption keys are transferred to the solid-state storage controller <b>104</b> from one of a cache <b>102</b>, a computer, a host device <b>114</b>, or other external agent which has the ability to execute industry standard methods to securely transfer and protect private and public keys.</div>
<div class="description-paragraph" id="p-0163" num="0162">In one embodiment, the encryption module <b>314</b> encrypts a first packet with a first encryption key received in conjunction with the packet and encrypts a second packet with a second encryption key received in conjunction with the second packet. In another embodiment, the encryption module <b>314</b> encrypts a first packet with a first encryption key received in conjunction with the packet and passes a second data packet on to the next stage without encryption. Beneficially, the encryption module <b>314</b> included in the write data pipeline <b>106</b> of the cache <b>102</b> allows data structure-by-data structure or segment-by-segment data encryption without a single file system or other external system to keep track of the different encryption keys used to store corresponding data structures or data segments. Each requesting device <b>155</b> or related key manager independently manages encryption keys used to encrypt only the data structures or data segments sent by the requesting device <b>155</b>.</div>
<div class="description-paragraph" id="p-0164" num="0163">In one embodiment, the encryption module <b>314</b> may encrypt the one or more packets using an encryption key unique to the cache <b>102</b>. The encryption module <b>314</b> may perform this media encryption independently, or in addition to the encryption described above. Typically, the entire packet is encrypted, including the headers. In another embodiment, headers are not encrypted. The media encryption by the encryption module <b>314</b> provides a level of security for data stored in the solid-state storage media <b>110</b>. For example, where data is encrypted with media encryption unique to the specific cache <b>102</b> if the solid-state storage media <b>110</b> is connected to a different solid-state storage controller <b>104</b>, cache <b>102</b>, or host device <b>114</b>, the contents of the solid-state storage media <b>110</b> typically could not be read without use of the same encryption key used during the write of the data to the solid-state storage media <b>110</b> without significant effort.</div>
<div class="description-paragraph" id="p-0165" num="0164">In another embodiment, the write data pipeline <b>106</b> includes a compression module <b>312</b> that compresses the data for metadata segment prior to sending the data segment to the packetizer <b>302</b>. The compression module <b>312</b> typically compresses a data or metadata segment using a compression routine known to those of skill in the art to reduce the storage size of the segment. For example, if a data segment includes a string of 512 zeros, the compression module <b>312</b> may replace the 512 zeros with code or token indicating the 512 zeros where the code is much more compact than the space taken by the 512 zeros.</div>
<div class="description-paragraph" id="p-0166" num="0165">In one embodiment, the compression module <b>312</b> compresses a first segment with a first compression routine and passes along a second segment without compression. In another embodiment, the compression module <b>312</b> compresses a first segment with a first compression routine and compresses the second segment with a second compression routine. Having this flexibility within the cache <b>102</b> is beneficial so that the host device <b>114</b> or other devices writing data to the cache <b>102</b> may each specify a compression routine or so that one can specify a compression routine while another specifies no compression. Selection of compression routines may also be selected according to default settings on a per data structure type or data structure class basis. For example, a first data structure of a specific data structure may be able to override default compression routine settings and a second data structure of the same data structure class and data structure type may use the default compression routine and a third data structure of the same data structure class and data structure type may use no compression.</div>
<div class="description-paragraph" id="p-0167" num="0166">In one embodiment, the write data pipeline <b>106</b> includes a garbage collector bypass <b>316</b> that receives data segments from the read data pipeline <b>108</b> as part of a data bypass in a garbage collection system. A garbage collection system typically marks packets that are no longer valid, typically because the packet is marked for deletion or has been modified and the modified data is stored in a different location. At some point, the garbage collection system determines that a particular section of storage may be recovered. This determination may be due to a lack of available storage capacity, the percentage of data marked as invalid reaching a threshold, a consolidation of valid data, an error detection rate for that section of storage reaching a threshold, or improving performance based on data distribution, etc. Numerous factors may be considered by a garbage collection algorithm to determine when a section of storage is to be recovered.</div>
<div class="description-paragraph" id="p-0168" num="0167">Once a section of storage has been marked for recovery, valid packets in the section typically must be relocated. The garbage collector bypass <b>316</b> allows packets to be read into the read data pipeline <b>108</b> and then transferred directly to the write data pipeline <b>106</b> without being routed out of the solid-state storage controller <b>104</b>. In one embodiment, the garbage collector bypass <b>316</b> is part of an autonomous garbage collector system that operates within the cache <b>102</b>. This allows the cache <b>102</b> to manage data so that data is systematically spread throughout the solid-state storage media <b>110</b> to improve performance, data reliability and to avoid overuse and underuse of any one location or area of the solid-state storage media <b>110</b> and to lengthen the useful life of the solid-state storage media <b>110</b>.</div>
<div class="description-paragraph" id="p-0169" num="0168">The garbage collector bypass <b>316</b> coordinates insertion of segments into the write data pipeline <b>106</b> with other segments being written by a host device <b>114</b> or other devices. In the depicted embodiment, the garbage collector bypass <b>316</b> is before the packetizer <b>302</b> in the write data pipeline <b>106</b> and after the depacketizer <b>324</b> in the read data pipeline <b>108</b>, but may also be located elsewhere in the read and write data pipelines <b>106</b>, <b>108</b>. The garbage collector bypass <b>316</b> may be used during a flush of the write pipeline <b>108</b> to fill the remainder of the virtual page in order to improve the efficiency of storage within the solid-state storage media <b>110</b> and thereby reduce the frequency of garbage collection.</div>
<div class="description-paragraph" id="p-0170" num="0169">In one embodiment, the write data pipeline <b>106</b> includes a write buffer <b>320</b> that buffers data for efficient write operations. Typically, the write buffer <b>320</b> includes enough capacity for packets to fill at least one virtual page in the solid-state storage media <b>110</b>. This allows a write operation to send an entire page of data to the solid-state storage media <b>110</b> without interruption. By sizing the write buffer <b>320</b> of the write data pipeline <b>106</b> and buffers within the read data pipeline <b>108</b> to be the same capacity or larger than a storage write buffer within the solid-state storage media <b>110</b>, writing and reading data is more efficient since a single write command may be crafted to send a fill virtual page of data to the solid-state storage media <b>110</b> instead of multiple commands.</div>
<div class="description-paragraph" id="p-0171" num="0170">While the write buffer <b>320</b> is being filled, the solid-state storage media <b>110</b> may be used for other read operations. This is advantageous because other solid-state devices with a smaller write buffer or no write buffer may tie up the solid-state storage when data is written to a storage write buffer and data flowing into the storage write buffer stalls. Read operations will be blocked until the entire storage write buffer is filled and programmed. Another approach for systems without a write buffer or a small write buffer is to flush the storage write buffer that is not full in order to enable reads. Again this is inefficient because multiple write/program cycles are required to fill a page.</div>
<div class="description-paragraph" id="p-0172" num="0171">For depicted embodiment with a write buffer <b>320</b> sized larger than a virtual page, a single write command, which includes numerous subcommands, can then be followed by a single program command to transfer the page of data from the storage write buffer in each solid-state storage element <b>216</b>, <b>218</b>, <b>220</b> to the designated page within each solid-state storage element <b>216</b>, <b>218</b>, <b>220</b>. This technique has the benefits of eliminating partial page programming, which is known to reduce data reliability and durability and freeing up the destination bank for reads and other commands while the buffer fills.</div>
<div class="description-paragraph" id="p-0173" num="0172">In one embodiment, the write buffer <b>320</b> is a ping-pong buffer where one side of the buffer is filled and then designated for transfer at an appropriate time while the other side of the ping-pong buffer is being filled. In another embodiment, the write buffer <b>320</b> includes a first-in first-out (“FIFO”) register with a capacity of more than a virtual page of data segments. One of skill in the art will recognize other write buffer <b>320</b> configurations that allow a virtual page of data to be stored prior to writing the data to the solid-state storage media <b>110</b>.</div>
<div class="description-paragraph" id="p-0174" num="0173">In another embodiment, the write buffer <b>320</b> is sized smaller than a virtual page so that less than a page of information could be written to a storage write buffer in the solid-state storage media <b>110</b>. In the embodiment, to prevent a stall in the write data pipeline <b>106</b> from holding up read operations, data is queued using the garbage collection system that needs to be moved from one location to another as part of the garbage collection process. In case of a data stall in the write data pipeline <b>106</b>, the data can be fed through the garbage collector bypass <b>316</b> to the write buffer <b>320</b> and then on to the storage write buffer in the solid-state storage media. <b>110</b> to fill the pages of a virtual page prior to programming the data. In this way a data stall in the write data pipeline <b>106</b> would not stall reading from the cache <b>102</b>.</div>
<div class="description-paragraph" id="p-0175" num="0174">In another embodiment, the write data pipeline <b>106</b> includes a write program module <b>310</b> with one or more user-definable functions within the write data pipeline <b>106</b>. The write program module <b>310</b> allows a user to customize the write data pipeline <b>106</b>. A user may customize the write data pipeline <b>106</b> based on a particular data requirement or application. Where the solid-state storage controller <b>104</b> is an FPGA, the user may program the write data pipeline <b>106</b> with custom commands and functions relatively easily. A user may also use the write program module <b>310</b> to include custom functions with an ASIC, however, customizing an ASIC may be more difficult than with an FPGA. The write program module <b>310</b> may include buffers and bypass mechanisms to allow a first data segment to execute in the write program module <b>310</b> while a second data segment may continue through the write data pipeline <b>106</b>. In another embodiment, the write program module <b>310</b> may include a processor core that can be programmed through software.</div>
<div class="description-paragraph" id="p-0176" num="0175">Note that the write program module <b>310</b> is shown between the input buffer <b>306</b> and the compression module <b>312</b>, however, the write program module <b>310</b> could be anywhere in the write data pipeline <b>106</b> and may be distributed among the various stages <b>302</b>-<b>320</b>. In addition, there may be multiple write program modules <b>310</b> distributed among the various states <b>302</b>-<b>320</b> that are programmed and operate independently. In addition, the order of the stages <b>302</b>-<b>320</b> may be altered. One of skill in the art will recognize workable alterations to the order of the stages <b>302</b>-<b>320</b> based on particular user requirements.</div>
<div class="description-paragraph" id="h-0014" num="0000">Read Data Pipeline</div>
<div class="description-paragraph" id="p-0177" num="0176">The read data pipeline <b>108</b> includes an ECC correction module <b>322</b> that determines if a data error exists in ECC blocks a requested packet received from the solid-state storage media <b>110</b> by using ECC stored with each ECC block of the requested packet. The ECC correction module <b>322</b> then corrects any errors in the requested packet if any error exists and the errors are correctable using the FCC. For example, if the ECC can detect an error in six bits but can only correct three bit errors, the ECC correction module <b>322</b> corrects ECC blocks of the requested packet with up to three bits in error. The ECC correction module <b>322</b> corrects the bits in error by changing the bits in error to the correct one or zero state so that the requested data packet is identical to when it was written to the solid-state storage media <b>110</b> and the ECC was generated for the packet.</div>
<div class="description-paragraph" id="p-0178" num="0177">If the ECC correction module <b>322</b> determines that the requested packets contains more bits in error than the ECC can correct, the ECC correction module <b>322</b> cannot correct the errors in the corrupted ECC blocks of the requested packet and sends an interrupt. In one embodiment, the ECC correction module <b>322</b> sends an interrupt with a message indicating that the requested packet is in error. The message may include information that the ECC correction module <b>322</b> cannot correct the errors or the inability of the ECC correction module <b>322</b> to correct the errors may be implied. In another embodiment, the ECC correction module <b>322</b> sends the corrupted ECC blocks of the requested packet with the interrupt and/or the message.</div>
<div class="description-paragraph" id="p-0179" num="0178">In one embodiment, a corrupted ECC block or portion of a corrupted ECC block of the requested packet that cannot be corrected by the ECC correction module <b>322</b> is read by the master controller <b>224</b>, corrected, and returned to the ECC correction module <b>322</b> for further processing by the read data pipeline <b>108</b>. In one embodiment, a corrupted ECC block or portion of a corrupted ECC block of the requested packet is sent to the device requesting the data. The requesting device <b>155</b> may correct the ECC block or replace the data using another copy, such as a backup or mirror copy, and then may use the replacement data of the requested data packet or return it to the read data pipeline <b>108</b>. The requesting device <b>155</b> may use header information in the requested packet in error to identify data required to replace the corrupted requested packet or to replace the data structure to which the packet belongs. In another embodiment, the solid-state storage controller <b>104</b> stores data using some type of RAID and is able to recover the corrupted data. In another embodiment, the ECC correction module <b>322</b> sends an interrupt and/or message and the receiving device fails the read operation associated with the requested data packet. One of skill in the art will recognize other options and actions to be taken as a result of the ECC correction module <b>322</b> determining that one or more ECC blocks of the requested packet are corrupted and that the ECC correction module <b>322</b> cannot correct the errors.</div>
<div class="description-paragraph" id="p-0180" num="0179">The read data pipeline <b>108</b> includes a depacketizer <b>324</b> that receives ECC blocks of the requested packet from the ECC correction module <b>322</b>, directly or indirectly, and checks and removes one or more packet headers. The depacketizer <b>324</b> may validate the packet headers by checking packet identifiers, data length, data location, etc. within the headers. In one embodiment, the header includes a hash code that can be used to validate that the packet delivered to the read data pipeline <b>108</b> is the requested packet. The depacketizer <b>324</b> also removes the headers from the requested packet added by the packetizer <b>302</b>. The depacketizer <b>324</b> may directed to not operate on certain packets but pass these forward without modification. An example might be a container label that is requested during the course of a rebuild process where the header information is required for index reconstruction. Further examples include the transfer of packets of various types destined for use within the cache <b>102</b>. In another embodiment, the depacketizer <b>324</b> operation may be packet type dependent.</div>
<div class="description-paragraph" id="p-0181" num="0180">The read data pipeline <b>108</b> includes an alignment module <b>326</b> that receives data from the depacketizer <b>324</b> and removes unwanted data. In one embodiment, a read command sent to the solid-state storage media <b>110</b> retrieves a packet of data. A device requesting the data may not require all data within the retrieved packet and the alignment module <b>326</b> removes the unwanted data. If all data within a retrieved page is requested data, the alignment module <b>326</b> does not remove any data.</div>
<div class="description-paragraph" id="p-0182" num="0181">The alignment module <b>326</b> re-formats the data as data segments of a data structure in a form compatible with a device requesting the data segment prior to forwarding the data segment to the next stage. Typically, as data is processed by the read data pipeline <b>108</b>, the size of data segments or packets changes at various stages. The alignment module <b>326</b> uses received data to format the data into data segments suitable to be sent to the requesting device <b>155</b> and joined to form a response. For example, data from a portion of a first data packet may be combined with data from a portion of a second data packet. If a data segment is larger than a data requested by the requesting device <b>155</b>, the alignment module <b>326</b> may discard the unwanted data.</div>
<div class="description-paragraph" id="p-0183" num="0182">In one embodiment, the read data pipeline <b>108</b> includes a read synchronization buffer <b>328</b> that buffers one or more requested packets read from the solid-state storage media <b>110</b> prior to processing by the read data pipeline <b>108</b>. The read synchronization buffer <b>328</b> is at the boundary between the solid-state storage clock domain and the local bus clock domain and provides buffering to account for the clock domain differences.</div>
<div class="description-paragraph" id="p-0184" num="0183">In another embodiment, the read data pipeline <b>108</b> includes an output buffer <b>330</b> that receives requested packets from the alignment module <b>326</b> and stores the packets prior to transmission to the requesting device <b>155</b>. The output buffer <b>330</b> accounts for differences between when data segments are received from stages of the read data pipeline <b>108</b> and when the data segments are transmitted to other parts of the solid-state storage controller <b>104</b> or to the requesting device <b>155</b>. The output buffer <b>330</b> also allows the data bus <b>204</b> to receive data from the read data pipeline <b>108</b> at rates greater than can be sustained by the read data pipeline <b>108</b> in order to improve efficiency of operation of the data bus <b>204</b>.</div>
<div class="description-paragraph" id="p-0185" num="0184">In one embodiment, the read data pipeline <b>108</b> includes a media decryption module <b>332</b> that receives one or more encrypted requested packets from the ECC correction module <b>322</b> and decrypts the one or more requested packets using the encryption key unique to the cache <b>102</b> prior to sending the one or more requested packets to the depacketizer <b>324</b>. Typically the encryption key used to decrypt data by the media decryption module <b>332</b> is identical to the encryption key used by the media encryption module <b>318</b>. In another embodiment, the solid-state storage media <b>110</b> may have two or more partitions and the solid-state storage controller <b>104</b> behaves as though it was two or more solid-state storage controllers <b>104</b> each operating on a single partition within the solid-state storage media <b>110</b>. In this embodiment, a unique media encryption key may be used with each partition.</div>
<div class="description-paragraph" id="p-0186" num="0185">In another embodiment, the read data pipeline <b>108</b> includes a decryption module <b>334</b> that decrypts a data segment formatted by the depacketizer <b>324</b> prior to sending the data segment to the output buffer <b>330</b>. The data segment may be decrypted using an encryption key received in conjunction with the read request that initiates retrieval of the requested packet received by the read synchronization buffer <b>328</b>. The decryption module <b>334</b> may decrypt a first packet with an encryption key received in conjunction with the read request for the first packet and then may decrypt a second packet with a different encryption key or may pass the second packet on to the next stage of the read data pipeline <b>108</b> without decryption. When the packet was stored with a non-secret cryptographic nonce, the nonce is used in conjunction with an encryption key to decrypt the data packet. The encryption key may be received from a host device <b>114</b>, a computer, key manager, or other device that manages the encryption key to be used by the solid-state storage controller <b>104</b>.</div>
<div class="description-paragraph" id="p-0187" num="0186">In another embodiment, the read data pipeline <b>108</b> includes a decompression module <b>336</b> that decompresses a data segment formatted by the depacketizer <b>324</b>. In one embodiment, the decompression module <b>336</b> uses compression information stored in one or both of the packet header and the container label to select a complementary routine to that used to compress the data by the compression module <b>312</b>. In another embodiment, the decompression routine used by the decompression module <b>336</b> is dictated by the device requesting the data segment being decompressed. In another embodiment, the decompression module <b>336</b> selects a decompression routine according to default settings on a per data structure type or data structure class basis. A first packet of a first object may be able to override a default decompression routine and a second packet of a second data structure of the same data structure class and data structure type may use the default decompression routine and a third packet of a third data structure of the same data structure class and data structure type may use no decompression.</div>
<div class="description-paragraph" id="p-0188" num="0187">In another embodiment, the read data pipeline <b>108</b> includes a read program module <b>338</b> that includes one or more user-definable functions within the read data pipeline <b>108</b>. The read program module <b>338</b> has similar characteristics to the write program module <b>310</b> and allows a user to provide custom functions to the read data pipeline <b>108</b>. The read program module <b>338</b> may be located as shown in <figref idrefs="DRAWINGS">FIG. 3</figref>, may be located in another position within the read data pipeline <b>108</b>, or may include multiple parts in multiple locations within the read data pipeline <b>108</b>. Additionally, there may be multiple read program modules <b>338</b> within multiple locations within the read data pipeline <b>108</b> that operate independently. One of skill in the art will recognize other forms of a read program module <b>338</b> within a read data pipeline <b>108</b>. As with the write data pipeline <b>106</b>, the stages of the read data pipeline <b>108</b> may be rearranged and one of skill in the art will recognize other orders of stages within the read data pipeline <b>108</b>.</div>
<div class="description-paragraph" id="p-0189" num="0188">The solid-state storage controller <b>104</b> includes control and status registers <b>340</b> and corresponding control queues <b>342</b>. The control and status registers <b>340</b> and control queues <b>342</b> facilitate control and sequencing commands and subcommands associated with data processed in the write and read data pipelines <b>106</b>, <b>108</b>. For example, a data segment in the packetizer <b>302</b> may have one or more corresponding control commands or instructions in a control queue <b>342</b> associated with the ECC generator <b>304</b>. As the data segment is packetized, some of the instructions or commands may be executed within the packetizer <b>302</b>. Other commands or instructions may be passed to the next control queue <b>342</b> through the control and status registers <b>340</b> as the newly formed data packet created from the data segment is passed to the next stage.</div>
<div class="description-paragraph" id="p-0190" num="0189">Commands or instructions may be simultaneously loaded into the control queues <b>342</b> for a packet being forwarded to the write data pipeline <b>106</b> with each pipeline stage pulling the appropriate command or instruction as the respective packet is executed by that stage. Similarly, commands or instructions may be simultaneously loaded into the control queues <b>342</b> for a packet being requested from the read data pipeline <b>108</b> with each pipeline stage pulling the appropriate command or instruction as the respective packet is executed by that stage. One of skill in the art will recognize other features and functions of control and status registers <b>340</b> and control queues <b>342</b>.</div>
<div class="description-paragraph" id="p-0191" num="0190">The solid-state storage controller <b>104</b> and or the cache <b>102</b> may also include a bank interleave controller <b>344</b>, a synchronization buffer <b>346</b>, a storage bus controller <b>348</b>, and a multiplexer (“MUX”) <b>350</b>, which are described in relation to <figref idrefs="DRAWINGS">FIG. 4</figref>.</div>
<div class="description-paragraph" id="h-0015" num="0000">Bank Interleave</div>
<div class="description-paragraph" id="p-0192" num="0191"> <figref idrefs="DRAWINGS">FIG. 4</figref> is a schematic block diagram illustrating one embodiment <b>400</b> of a bank interleave controller <b>344</b> in the solid-state storage controller <b>104</b> in accordance with the present invention. The bank interleave controller <b>344</b> is connected to the control and status registers <b>340</b> and to the storage I/O bus <b>210</b> and storage control bus <b>212</b> through the MUX <b>350</b>, storage bus controller <b>348</b>, and synchronization buffer <b>346</b>, which are described below. The bank interleave controller <b>344</b> includes a read agent <b>402</b>, a write agent <b>404</b>, an erase agent <b>406</b>, a management agent <b>408</b>, read queues <b>410</b> <i>a</i>-<i>n</i>, write queues <b>412</b> <i>a</i>-<i>n</i>, erase queues <b>414</b> <i>a</i>-<i>n</i>, and management queues <b>416</b> <i>a</i>-<i>n </i>for the banks <b>214</b> in the solid-state storage media <b>110</b>, bank controllers <b>418</b> <i>a</i>-<i>n</i>, a bus arbiter <b>420</b>, and a status MUX <b>422</b>, which are described below. The storage bus controller <b>348</b> includes a mapping module <b>424</b> with a remapping module <b>430</b>, a status capture module <b>426</b>, and a NAND) bus controller <b>428</b>, which are described below.</div>
<div class="description-paragraph" id="p-0193" num="0192">The bank interleave controller <b>344</b> directs one or more commands to two or more queues in the bank interleave controller <b>104</b> and coordinates among the banks <b>214</b> of the solid-state storage media <b>110</b> execution of the commands stored in the queues, such that a command of a first type executes on one bank <b>214</b> <i>a </i>while a command of a second type executes on a second bank <b>214</b> <i>b</i>. The one or more commands are separated by command type into the queues. Each bank <b>214</b> of the solid-state storage media <b>110</b> has a corresponding set of queues within the bank interleave controller <b>344</b> and each set of queues includes a queue for each command type.</div>
<div class="description-paragraph" id="p-0194" num="0193">The bank interleave controller <b>344</b> coordinates among the banks <b>214</b> of the solid-state storage media <b>110</b> execution of the commands stored in the queues. For example, a command of a first type executes on one bank <b>214</b> <i>a </i>while a command of a second type executes on a second bank <b>214</b> <i>b</i>. Typically the command types and queue types include read and write commands and queues <b>410</b>, <b>412</b>, but may also include other commands and queues that are storage media specific. For example, in the embodiment depicted in <figref idrefs="DRAWINGS">FIG. 4</figref>, erase and management queues <b>414</b>, <b>416</b> are included and would be appropriate for flash memory, N RAM, MRAM, DRAM, PRAM, etc.</div>
<div class="description-paragraph" id="p-0195" num="0194">For other types of solid-state storage media <b>110</b>, other types of commands and corresponding queues may be included without straying from the scope of the invention. The flexible nature of an FPGA solid-state storage controller <b>104</b> allows flexibility in storage media. If flash memory were changed to another solid-state storage type, the bank interleave controller <b>344</b>, storage bus controller <b>348</b>, and MUX <b>350</b> could be altered to accommodate the media type without significantly affecting the data pipelines <b>106</b>, <b>108</b> and other solid-state storage controller <b>104</b> functions.</div>
<div class="description-paragraph" id="p-0196" num="0195">In the embodiment depicted in <figref idrefs="DRAWINGS">FIG. 4</figref>, the bank interleave controller <b>344</b> includes, for each bank <b>214</b>, a read queue <b>410</b> for reading data from the solid-state storage media <b>110</b>, a write queue <b>412</b> for write commands to the solid-state storage media <b>110</b>, an erase queue <b>414</b> for erasing an erase block in the solid-state storage, an a management queue <b>416</b> for management commands. The bank interleave controller <b>344</b> also includes corresponding read, write, erase, and management agents <b>402</b>, <b>404</b>, <b>406</b>, <b>408</b>. In another embodiment, the control and status registers <b>340</b> and control queues <b>342</b> or similar components queue commands for data sent to the banks <b>214</b> of the solid-state storage media <b>110</b> without a bank interleave controller <b>344</b>.</div>
<div class="description-paragraph" id="p-0197" num="0196">The agents <b>402</b>, <b>404</b>, <b>406</b>, <b>408</b>, in one embodiment, direct commands of the appropriate type destined for a particular bank <b>214</b> <i>a </i>to the correct queue for the bank <b>214</b> <i>a</i>. For example, the read agent <b>402</b> may receive a read command for bank-1 <b>214</b> <i>b </i>and directs the read command to the bank-1 read queue <b>410</b> <i>b</i>. The write agent <b>404</b> may receive a write command to write data to a location in bank-0 <b>214</b> <i>a </i>of the solid-state storage media <b>110</b> and will then send the write command to the bank-0 write queue <b>412</b> <i>a</i>. Similarly, the erase agent <b>406</b> may receive an erase command to erase an erase block in bank-1 <b>214</b> <i>b </i>and will then pass the erase command to the bank-1 erase queue <b>414</b> <i>b</i>. The management agent <b>408</b> typically receives management commands, status requests, and the like, such as a reset command or a request to read a configuration register of a bank <b>214</b>, such as bank-0 <b>214</b> <i>a</i>. The management agent <b>408</b> sends the management command to the bank-0 management queue <b>416</b> <i>a. </i> </div>
<div class="description-paragraph" id="p-0198" num="0197">The agents <b>402</b>, <b>404</b>, <b>406</b>, <b>408</b> typically also monitor status of the queues <b>410</b>,<b>412</b>, <b>414</b>, <b>416</b> and send status, interrupt, or other messages when the queues <b>410</b>, <b>412</b>, <b>414</b>, <b>416</b> are full, nearly full, non-functional, etc. In one embodiment, the agents <b>402</b>, <b>404</b>, <b>406</b>, <b>408</b> receive commands and generate corresponding sub-commands. In one embodiment, the agents <b>402</b>, <b>404</b>, <b>406</b>, <b>408</b> receive commands through the control &amp; status registers <b>340</b> and generate corresponding sub-commands which are forwarded to the queues <b>410</b>, <b>412</b>, <b>414</b>, <b>416</b>. One of skill in the art will recognize other functions of the agents <b>402</b>, <b>404</b>, <b>406</b>, <b>408</b>.</div>
<div class="description-paragraph" id="p-0199" num="0198">The queues <b>410</b>, <b>412</b>, <b>414</b>, <b>416</b> typically receive commands and store the commands until required to be sent to the solid-state storage banks <b>214</b>. In a typical embodiment, the queues <b>410</b>, <b>412</b>, <b>414</b>, <b>416</b> are first-in, first-out (“FIFO”) registers or a similar component that operates as a FIFO. In another embodiment, the queues <b>410</b>, <b>412</b>, <b>414</b>, <b>416</b> store commands in an order that matches data, order of importance, or other criteria.</div>
<div class="description-paragraph" id="p-0200" num="0199">The bank controllers <b>418</b> typically receive commands from the queues <b>410</b>, <b>412</b>, <b>414</b>, <b>416</b> and generate appropriate subcommands. For example, the bank-0 write queue <b>412</b> <i>a </i>may receive a command to write a page of data packets to bank-0 <b>214</b> <i>a</i>. The bank-0 controller <b>418</b> <i>a </i>may receive the write command at an appropriate time and may generate one or more write subcommands for each data packet stored in the write buffer <b>320</b> to be written to the page in bank-0 <b>214</b> <i>a</i>. For example, bank-0 controller <b>418</b> <i>a </i>may generate commands to validate the status of bank O <b>214</b> <i>a </i>and the solid-state storage array <b>216</b>, select the appropriate location for writing one or more data packets, clear the input buffers within the solid-state storage memory array <b>216</b>, transfer the one or more data packets to the input buffers, program the input buffers into the selected location, verify that the data was correctly programmed, and if program failures occur do one or more of interrupting the master controller <b>224</b>, retrying the write to the same physical location, and retrying the write to a different physical location. Additionally, in conjunction with example write command, the storage bus controller <b>348</b> will cause the one or more commands to multiplied to each of the each of the storage I/O buses <b>210</b> <i>a</i>-<i>n </i>with the logical address of the command mapped to a first physical addresses for storage I/O bus <b>210</b> <i>a</i>, and mapped to a second physical address for storage I/O bus <b>210</b> <i>b</i>, and so forth as further described below.</div>
<div class="description-paragraph" id="p-0201" num="0200">Typically, bus arbiter <b>420</b> selects from among the bank controllers <b>418</b> and pulls subcommands from output queues within the bank controllers <b>418</b> and forwards these to the Storage Bus Controller <b>348</b> in a sequence that optimizes the performance of the banks <b>214</b>. In another embodiment, the bus arbiter <b>420</b> may respond to a high level interrupt and modify the normal selection criteria. In another embodiment, the master controller <b>224</b> can control the bus arbiter <b>420</b> through the control and status registers <b>340</b>. One of skill in the art will recognize other means by which the bus arbiter <b>420</b> may control and interleave the sequence of commands from the bank controllers <b>418</b> to the solid-state storage media <b>110</b>.</div>
<div class="description-paragraph" id="p-0202" num="0201">The bus arbiter <b>420</b> typically coordinates selection of appropriate commands, and corresponding data when required for the command type, from the bank controllers <b>418</b> and sends the commands and data to the storage bus controller <b>348</b>. The bus arbiter <b>420</b> typically also sends commands to the storage control bus <b>212</b> to select the appropriate bank <b>214</b>. For the case of flash memory or other solid-state storage media <b>110</b> with an asynchronous, bi-directional serial storage I/O bus <b>210</b>, only one command (control information) or set of data can be transmitted at a time. For example, when write commands or data are being transmitted to the solid-state storage media <b>110</b> on the storage I/O bus <b>210</b>, read commands, data being read, erase commands, management commands, or other status commands cannot be transmitted on the storage I/O bus <b>210</b>. For example, when data is being read from the storage I/O bus <b>210</b>, data cannot be written to the solid-state storage media <b>110</b>.</div>
<div class="description-paragraph" id="p-0203" num="0202">For example, during a write operation on bank-0 the bus arbiter <b>420</b> selects the bank-0 controller <b>418</b> <i>a </i>which may have a write command or a series of write sub-commands on the top of its queue which cause the storage bus controller <b>348</b> to execute the following sequence. The bus arbiter <b>420</b> forwards the write command to the storage bus controller <b>348</b>, which sets up a write command by selecting bank-0 <b>214</b> <i>a </i>through the storage control bus <b>212</b>, sending a command to clear the input buffers of the solid-state storage elements <b>110</b> associated with the bank-0 <b>214</b> <i>a</i>, and sending a command to validate the status of the solid-state storage elements <b>216</b>, <b>218</b>, <b>220</b> associated with the bank-0 <b>214</b> <i>a</i>. The storage bus controller <b>348</b> then transmits a write subcommand on the storage I/O bus <b>210</b>, which contains the physical addresses including the address of the logical erase block for each individual physical erase solid-stage storage element <b>216</b> <i>a</i>-<i>m </i>as mapped from the logical erase block address. The storage bus controller <b>348</b> then muxes the write buffer <b>320</b> through the write sync buffer <b>308</b> to the storage <b>1</b>/O bus <b>210</b> through the MUX <b>350</b> and streams write data to the appropriate page. When the page is full, then storage bus controller <b>348</b> causes the solid-state storage elements <b>216</b> <i>a</i>-<i>m </i>associated with the bank-0 <b>214</b> <i>a </i>to program the input buffer to the memory cells within the solid-state storage elements <b>216</b> <i>a</i>-<i>m</i>. Finally, the storage bus controller <b>348</b> validates the status to ensure that page was correctly programmed.</div>
<div class="description-paragraph" id="p-0204" num="0203">A read operation is similar to the write example above. During a read operation, typically the bus arbiter <b>420</b>, or other component of the bank interleave controller <b>344</b>, receives data and corresponding status information and sends the data to the read data pipeline <b>108</b> while sending the status information on to the control and status registers <b>340</b>. Typically, a read data command forwarded from bus arbiter <b>420</b> to the storage bus controller <b>348</b> will cause the MUX <b>350</b> to gate the read data on storage I/O bus <b>210</b> to the read data pipeline <b>108</b> and send status information to the appropriate control and status registers <b>340</b> through the status MUX <b>422</b>.</div>
<div class="description-paragraph" id="p-0205" num="0204">The bus arbiter <b>420</b> coordinates the various command types and data access modes so that only an appropriate command type or corresponding data is on the bus at any given time. If the bus arbiter <b>420</b> has selected a write command, and write subcommands and corresponding data are being written to the solid-state storage media <b>110</b>, the bus arbiter <b>420</b> will not allow other command types on the storage I/O bus <b>210</b>. Beneficially, the bus arbiter <b>420</b> uses timing information, such as predicted command execution times, along with status information received concerning bank <b>214</b> status to coordinate execution of the various commands on the bus with the goal of minimizing or eliminating idle time of the busses.</div>
<div class="description-paragraph" id="p-0206" num="0205">The master controller <b>224</b> through the bus arbiter <b>420</b> typically uses expected completion times of the commands stored in the queues <b>410</b>, <b>412</b>, <b>414</b>, <b>416</b>, along with status information, so that when the subcommands associated with a command are executing on one bank <b>214</b> <i>a</i>, other subcommands of other commands are executing on other banks <b>214</b> <i>b</i>-<i>n</i>. When one command is fully executed on a bank <b>214</b> <i>a</i>, the bus arbiter <b>420</b> directs another command to the bank <b>214</b> <i>a</i>. The bus arbiter <b>420</b> may also coordinate commands stored in the queues <b>410</b>, <b>412</b>, <b>414</b>, <b>416</b> with other commands that are not stored in the queues <b>410</b>, <b>412</b>, <b>414</b>, <b>416</b>.</div>
<div class="description-paragraph" id="p-0207" num="0206">For example, an erase command may be sent out to erase a group of erase blocks within the solid-state storage media <b>110</b>. An erase command may take 10 to 1000 times more time to execute than a write or a read command or 10 to 100 times more time to execute than a program command. For N banks <b>214</b>, the bank interleave controller <b>344</b> may split the erase command into N commands, each to erase a virtual erase block of a bank <b>214</b> <i>a</i>. While bank-0 <b>214</b> <i>a </i>is executing an erase command, the bus arbiter <b>420</b> may select other commands for execution on the other banks <b>214</b> <i>b</i>-<i>n</i>. The bus arbiter <b>420</b> may also work with other components, such as the storage bus controller <b>348</b>, the master controller <b>224</b>, etc., to coordinate command execution among the buses. Coordinating execution of commands using the bus arbiter <b>420</b>, bank controllers <b>418</b>, queues <b>410</b>, <b>412</b>, <b>414</b>, <b>416</b>, and agents <b>402</b>, <b>404</b>, <b>406</b>, <b>408</b> of the bank interleave controller <b>344</b> can dramatically increase performance over other solid-state storage systems without a bank interleave function.</div>
<div class="description-paragraph" id="p-0208" num="0207">In one embodiment, the solid-state controller <b>104</b> includes one bank interleave controller <b>344</b> that serves all of the storage elements <b>216</b>, <b>218</b>, <b>220</b> of the solid-state storage media <b>110</b>. In another embodiment, the solid-state controller <b>104</b> includes a bank interleave controller <b>344</b> for each column of storage elements <b>216</b> <i>a</i>-<i>m</i>, <b>218</b> <i>a</i>-<i>m</i>, <b>220</b> <i>a</i>-<i>m</i>. For example, one bank interleave controller <b>344</b> serves one column of storage elements SSS 0.0-SSS M.0 <b>216</b> <i>a</i>, <b>216</b> <i>b</i>, . . . <b>216</b> <i>m</i>, a second bank interleave controller <b>344</b> serves a second column of storage elements SSS 0.1-SSS M.1 <b>218</b> <i>a</i>, <b>218</b> <i>b</i>, . . . <b>218</b> <i>m </i>etc.</div>
<div class="description-paragraph" id="h-0016" num="0000">Storage-Specific Components</div>
<div class="description-paragraph" id="p-0209" num="0208">The solid-state storage controller <b>104</b> includes a synchronization buffer <b>346</b> that buffers commands and status messages sent and received from the solid-state storage media <b>110</b>. The synchronization buffer <b>346</b> is located at the boundary between the solid-state storage clock domain and the local bus clock domain and provides buffering to account for the clock domain differences. The synchronization buffer <b>346</b>, write synchronization buffer <b>308</b>, and read synchronization buffer <b>328</b> may be independent or may act together to buffer data, commands, status messages, etc. In one embodiment, the synchronization buffer <b>346</b> is located where there are the fewest number of signals crossing the clock domains. One skilled in the art will recognize that synchronization between clock domains may be arbitrarily moved to other locations within the cache <b>102</b> in order to optimize some aspect of design implementation.</div>
<div class="description-paragraph" id="p-0210" num="0209">The solid-state storage controller <b>104</b> includes a storage bus controller <b>348</b> that interprets and translates commands for data sent to and read from the solid-state storage media <b>110</b> and status messages received from the solid-state storage media <b>110</b> based on the type of solid-state storage media <b>110</b>. For example, the storage bus controller <b>348</b> may have different timing requirements for different types of storage, storage with different performance characteristics, storage from different manufacturers, etc. The storage bus controller <b>348</b> also sends control commands to the storage control bus <b>212</b>.</div>
<div class="description-paragraph" id="p-0211" num="0210">In one embodiment, the solid-state storage controller <b>104</b> includes a MUX <b>350</b> that comprises an array of multiplexers <b>350</b> <i>a</i>-<i>n </i>where each multiplexer is dedicated to a row in the solid-state storage array <b>110</b>. For example, multiplexer <b>350</b> <i>a </i>is associated with solid-state storage elements <b>216</b> <i>a</i>, <b>218</b> <i>a</i>, <b>220</b> <i>a</i>. MUX <b>350</b> routes the data from the write data pipeline <b>106</b> and commands from the storage bus controller <b>348</b> to the solid-state storage media <b>110</b> via the storage I/O bus <b>210</b> and routes data and status messages from the solid-state storage media <b>110</b> via the storage I/O bus <b>210</b> to the read data pipeline <b>108</b> and the control and status registers <b>340</b> through the storage bus controller <b>348</b>, synchronization buffer <b>346</b>, and bank interleave controller <b>344</b>.</div>
<div class="description-paragraph" id="p-0212" num="0211">In one embodiment, the solid-state storage controller <b>104</b> includes a MUX <b>350</b> for each row of solid-state storage elements (e.g. SSS 0.1 <b>216</b> <i>a</i>, SSS 0.2 <b>218</b> <i>a</i>, SSS O.N <b>220</b> <i>a</i>). A MUX <b>350</b> combines data from the write data pipeline <b>106</b> and commands sent to the solid-state storage media <b>110</b> via the storage I/O bus <b>210</b> and separates data to be processed by the read data pipeline <b>108</b> from commands. Packets stored in the write buffer <b>320</b> are directed on busses out of the write buffer <b>320</b> through a write synchronization buffer <b>308</b> for each row of solid-state storage elements (SSS x.O to SSS x.N <b>216</b>, <b>218</b>, <b>220</b>) to the MUX <b>350</b> for each row of solid-state storage elements (SSS x.O to SSS x.N <b>216</b>, <b>218</b>, <b>220</b>). The commands and read data are received by the MUXes <b>350</b> from the storage I/O bus <b>210</b>. The MUXes <b>350</b> also direct status messages to the storage bus controller <b>348</b>.</div>
<div class="description-paragraph" id="p-0213" num="0212">The storage bus controller <b>348</b> includes a mapping module <b>424</b>. The mapping module <b>424</b> maps a logical address of an erase block to one or more physical addresses of an erase block. For example, a solid-state storage media <b>110</b> with an array of twenty storage elements (e.g. SSS 0.0 to SSS M.0 <b>216</b>) per block <b>214</b> <i>a </i>may have a logical address for a particular erase block mapped to twenty physical addresses of the erase block, one physical address per storage element. Because the storage elements are accessed in parallel, erase blocks at the same position in each storage element in a row of storage elements <b>216</b> <i>a</i>, <b>218</b> <i>a</i>, <b>220</b> <i>a </i>will share a physical address. To select one erase block (e.g. in storage element SSS 0.0 <b>216</b> <i>a</i>) instead of all erase blocks in the row (e.g. in storage elements SSS 0.0, 0.1, . . . O.N <b>216</b> <i>a</i>, <b>218</b> <i>a</i>, <b>220</b> <i>a</i>), one bank (in this case bank-0 <b>214</b> <i>a</i>) is selected.</div>
<div class="description-paragraph" id="p-0214" num="0213">This logical-to-physical mapping for erase blocks is beneficial because if one erase block becomes damaged or inaccessible, the mapping can be changed to map to another erase block. This mitigates the loss of losing an entire virtual erase block when one element's erase block is faulty. The remapping module <b>430</b> changes a mapping of a logical address of an erase block to one or more physical addresses of a virtual erase block (spread over the array of storage elements). For example, virtual erase block 1 may be mapped to erase block 1 of storage element SSS 0.0 <b>216</b> <i>a</i>, to erase block 1 of storage element SSS 1.0 <b>216</b> <i>b</i>, . . . , and to storage element M.0 <b>216</b> <i>m</i>, virtual erase block 2 may be mapped to erase block 2 of storage element SSS 0.1 <b>218</b> <i>a</i>, to erase block 2 of storage element SSS 1.1 <b>218</b> <i>b</i>, . . . , and to storage element M.1 <b>218</b> <i>m</i>, etc. Alternatively, virtual erase block 1 may be mapped to one erase block from each storage element in an array such that virtual erase block 1 includes erase block 1 of storage element SSS 0.0 <b>216</b> <i>a </i>to erase block 1 of storage element SSS 1.0 <b>216</b> <i>b </i>to storage element M.0 <b>216</b> <i>m</i>, and erase block 1 of storage element SSS 0.1 <b>218</b> <i>a </i>to erase block 1 of storage element SSS 1.1 <b>218</b> <i>b</i>, . . . , and to storage element M.1 <b>218</b> <i>m</i>, for each storage element in the array up to erase block 1 of storage element M.N <b>220</b> <i>m. </i> </div>
<div class="description-paragraph" id="p-0215" num="0214">If erase block 1 of a storage element SSS0.0 <b>216</b> <i>a </i>is damaged, experiencing errors due to wear, etc., or cannot be used for some reason, the remapping module <b>430</b> could change the logical-to-physical mapping for the logical address that pointed to erase block 1 of virtual erase block 1. If a spare erase block (call it erase block <b>221</b>) of storage element SSS O.0 <b>216</b> <i>a </i>is available and currently not mapped, the remapping module <b>430</b> could change the mapping of virtual erase block 1 to point to erase block <b>221</b> of storage element SSS 0.0 <b>216</b> <i>a</i>, while continuing to point to erase block 1 of storage element SSS 1.0 <b>216</b> <i>b</i>, erase block 1 of storage element SSS 2.0 (not shown) . . . , and to storage element M.0 <b>216</b> <i>m</i>. The mapping module <b>424</b> or remapping module <b>430</b> could map erase blocks in a prescribed order (virtual erase block 1 to erase block 1 of the storage elements, virtual erase block 2 to erase block 2 of the storage elements, etc.) or may map erase blocks of the storage elements <b>216</b>, <b>218</b>, <b>220</b> in another order based on some other criteria.</div>
<div class="description-paragraph" id="p-0216" num="0215">In one embodiment, the erase blocks could be grouped by access time. Grouping by access time, meaning time to execute a command, such as programming (writing) data into pages of specific erase blocks, can level command completion so that a command executed across the erase blocks of a virtual erase block is not limited by the slowest erase block. In other embodiments, the erase blocks may be grouped by wear level, health, etc. One of skill in the art will recognize other factors to consider when mapping or remapping erase blocks.</div>
<div class="description-paragraph" id="p-0217" num="0216">In one embodiment, the storage bus controller <b>348</b> includes a status capture module <b>426</b> that receives status messages from the solid-state storage media <b>110</b> and sends the status messages to the status MUX <b>422</b>. In another embodiment, when the solid-state storage media <b>110</b> is flash memory, the storage bus controller <b>348</b> includes a NAND bus controller <b>428</b>. The NAND bus controller <b>428</b> directs commands from the read and write data pipelines <b>106</b>, <b>108</b> to the correct location in the solid-state storage media <b>110</b>, coordinates timing of command execution based on characteristics of the flash memory, etc. If the solid-state storage media <b>110</b> is another solid-state storage type, the NAND bus controller <b>428</b> would be replaced by a bus controller specific to the storage type. One of skill in the art will recognize other functions of a NAND) bus controller <b>428</b>.</div>
<div class="description-paragraph" id="h-0017" num="0000">Data Caching</div>
<div class="description-paragraph" id="p-0218" num="0217"> <figref idrefs="DRAWINGS">FIG. 5</figref> depicts one embodiment of a host device <b>114</b>. The host device <b>114</b> may be similar, in certain embodiments, to the host device <b>114</b> depicted in <figref idrefs="DRAWINGS">FIGS. 1A and 1B</figref>. The depicted embodiment includes a user application <b>502</b> in communication with a storage client <b>504</b>. The storage client <b>504</b> is in communication with a direct cache module <b>116</b>, which, in one embodiment, is substantially similar to the direct cache module <b>116</b> of <figref idrefs="DRAWINGS">FIGS. 1A and 1B</figref>, described above. The direct cache module <b>116</b>, in the depicted embodiment, is in communication with the cache <b>102</b> and the backing store <b>118</b> through the storage controller <b>104</b> and the backing store controller <b>120</b>.</div>
<div class="description-paragraph" id="p-0219" num="0218">In one embodiment, the user application <b>502</b> is a software application operating on or in conjunction with the storage client <b>504</b>. The storage client <b>504</b> manages file systems, files, data, and the like and utilizes the functions and features of the direct cache module <b>116</b>, the cache <b>102</b>, and the backing store <b>118</b>. Representative examples of storage clients include, but are not limited to, a server, a file system, an operating system, a database management system (“DBMS”), a volume manager, and the like.</div>
<div class="description-paragraph" id="p-0220" num="0219">In the depicted embodiment, the storage client <b>504</b> is in communication with the direct cache module <b>116</b>. In a further embodiment, the storage client <b>504</b> may also be in communication with the cache <b>102</b> and/or the backing store <b>118</b> directly. The storage client <b>504</b>, in one embodiment, reads data from and writes data to the backing store <b>118</b> through the direct cache module <b>116</b>, which uses the cache <b>102</b> to cache read data and/or write data for the backing store <b>118</b>. In a further embodiment, the direct cache module <b>116</b> caches data in a manner that is substantially transparent to the storage client <b>504</b>, with the storage client <b>504</b> sending read requests and write requests directly to the direct cache module <b>116</b>.</div>
<div class="description-paragraph" id="p-0221" num="0220">In one embodiment, the direct cache module <b>116</b> has exclusive access to and/or control over the cache <b>102</b> and the backing store <b>118</b>. The direct cache module <b>116</b> may represent itself to the storage client <b>504</b> as a storage device. For example, the direct cache module <b>116</b> may represent itself as a conventional block storage device, or the like. In a particular embodiment, the direct cache module <b>116</b> may represent itself to the storage client <b>504</b> as a storage device having the same number of logical blocks (0 to N) as the backing store <b>118</b>. In another embodiment, the direct cache module <b>116</b> may represent itself to the storage client <b>504</b> as a storage device have the more logical blocks (0 to N+X) as the backing store <b>118</b>, where X=the number of logical blocks addressable by the direct cache module <b>116</b> beyond N. In certain embodiments, X=264−N.</div>
<div class="description-paragraph" id="p-0222" num="0221">As described above with regard to the direct cache module <b>116</b> depicted in the embodiments of <figref idrefs="DRAWINGS">FIGS. 1A and 1B</figref>, in various embodiments, the direct cache module <b>116</b> may be embodied by one or more of a storage controller <b>104</b> of the cache <b>102</b> and/or a backing store controller <b>120</b> of the backing store <b>118</b>; a separate hardware controller device that interfaces with the cache <b>102</b> and the backing store <b>118</b>; a device driver loaded on the host device <b>114</b>; and the like.</div>
<div class="description-paragraph" id="p-0223" num="0222">In one embodiment, the host device <b>114</b> loads a device driver for the direct cache module <b>116</b>. In a further embodiment, the host device <b>114</b> loads device drivers for the cache <b>102</b> and/or the backing store <b>118</b>, such as one or more device drivers of the storage controller <b>104</b> and/or the backing store controller <b>120</b>. The direct cache module <b>116</b> may communicate with the cache <b>102</b> and/or the backing store <b>118</b> through device drivers loaded on the host device <b>114</b>, through the storage controller <b>104</b> of the cache <b>102</b> and/or through the backing store controller <b>120</b> of the backing store <b>118</b>, or the like.</div>
<div class="description-paragraph" id="p-0224" num="0223">In one embodiment, the storage client <b>504</b> communicates with the direct cache module <b>116</b> through an Input/Output (“I/O”) interface represented by a block I/O emulation layer <b>506</b>. In certain embodiments, the fact that the direct cache module <b>116</b> is providing caching services in front of one or more caches <b>102</b>, and/or one or more backing stores, such as the backing store <b>118</b>, may be transparent to the storage client <b>504</b>. In such an embodiment, the direct cache module <b>116</b> may present (i.e., identify itself as) a conventional block device to the storage client <b>504</b>.</div>
<div class="description-paragraph" id="p-0225" num="0224">In a further embodiment, the cache <b>102</b> and/or the backing store <b>118</b> either include a distinct block I/O emulation layer <b>506</b> or may be conventional block storage devices. Certain conventional block storage devices divide the storage media into volumes or partitions. Each volume or partition may include a plurality of sectors. One or more sectors are organized into a logical block. In certain storage systems, such as those interfacing with the Windows-® operating systems, the logical blocks are referred to as clusters. In other storage systems, such as those interfacing with UNIX, Linux, or similar operating systems, the logical blocks are referred to simply as blocks. A logical block or cluster represents a smallest physical amount of storage space on the storage media that is addressable by the storage client <b>504</b>. A block storage device may associate n logical blocks available for user data storage across the storage media with a logical block address, numbered from 0 to n. In certain block storage devices, the logical block addresses may range from 0 to r per volume or partition. In conventional block storage devices, a logical block address maps directly to a particular logical block. In conventional block storage devices, each logical block maps to a particular set of physical sectors on the storage media.</div>
<div class="description-paragraph" id="p-0226" num="0225">However, the direct cache module <b>116</b>, the cache <b>102</b> and/or the backing store <b>118</b>, in certain embodiments, may not directly or necessarily associate logical block addresses with particular physical blocks. The direct cache module <b>116</b>, the cache <b>102</b>, and/or the backing store <b>118</b> may emulate a conventional block storage interface to maintain compatibility with block storage clients <b>504</b> and with conventional block storage commands and protocols.</div>
<div class="description-paragraph" id="p-0227" num="0226">When the storage client <b>504</b> communicates through the block I/O emulation layer <b>506</b>, the direct cache module <b>116</b> appears to the storage client <b>504</b> as a conventional block storage device. In one embodiment, the direct cache module <b>116</b> provides the block I/O emulation layer <b>506</b> which serves as a block device interface, or API In this embodiment, the storage client <b>504</b> communicates with the direct cache module <b>116</b> through this block device interface. In one embodiment, the block I/O emulation layer <b>506</b> receives commands and logical block addresses from the storage client <b>504</b> in accordance with this block device interface. As a result, the block I/O emulation layer <b>506</b> provides the direct cache module <b>116</b> compatibility with block storage clients <b>504</b>. In a further embodiment, the direct cache module <b>116</b> may communicate with the cache <b>102</b> and/or the backing store <b>118</b> using corresponding block device interfaces.</div>
<div class="description-paragraph" id="p-0228" num="0227">In one embodiment, a storage client <b>504</b> communicates with the direct cache module <b>116</b> through a direct interface layer <b>508</b>. In this embodiment, the direct cache module <b>116</b> directly exchanges information specific to the cache <b>102</b> and/or the backing store <b>118</b> with the storage client <b>504</b>. Similarly, the direct cache module <b>116</b>, in one embodiment, may communicate with the cache <b>102</b> and/or the backing store <b>118</b> through direct interface layers <b>508</b>.</div>
<div class="description-paragraph" id="p-0229" num="0228">A direct cache module <b>116</b> using the direct interface <b>508</b> may store data on the cache <b>102</b> and/or the backing store <b>118</b> as blocks, sectors, pages, logical blocks, logical pages, erase blocks, logical erase blocks, ECC chunks or in any other format or structure advantageous to the technical characteristics of the cache <b>102</b> and/or the backing store <b>118</b>. For example, in one embodiment, the backing store <b>118</b> comprises a hard disk drive and the direct cache module <b>116</b> stores data on the backing store <b>118</b> as contiguous sectors of 512 bytes, or the like, using physical cylinder-head-sector addresses for each sector, logical block addresses for each sector, or the like. The direct cache module <b>116</b> may receive a logical address and a command from the storage client <b>504</b> and perform the corresponding operation in relation to the cache <b>102</b>, and/or the backing store <b>118</b>. The direct cache module <b>116</b>, the cache <b>102</b>, and/or the backing store <b>118</b> may support a block I/O emulation layer <b>506</b>, a direct interface <b>508</b>, or both a block I/O emulation layer <b>506</b> and a direct interface <b>508</b>.</div>
<div class="description-paragraph" id="p-0230" num="0229">As described above, certain storage devices, while appearing to a storage client <b>504</b> to be a block storage device, do not directly associate particular logical block addresses with particular physical blocks, also referred to in the art as sectors. Such storage devices may use a logical-to-physical translation layer <b>510</b>. In the depicted embodiment, the cache <b>102</b> includes a logical-to-physical translation layer <b>510</b>. In a further embodiment, the backing store <b>118</b> may also include a logical-to-physical translation layer <b>510</b>. In another embodiment, the direct cache module <b>116</b> maintains a single logical-to-physical translation layer <b>510</b> for the cache <b>102</b> and the backing store <b>118</b>. In another embodiment, the direct cache module <b>116</b> maintains a distinct logical-to-physical translation layer <b>510</b> for each of the cache <b>102</b> and the backing store <b>118</b>.</div>
<div class="description-paragraph" id="p-0231" num="0230">The logical-to-physical translation layer <b>510</b> provides a level of abstraction between the logical block addresses used by the storage client <b>504</b> and the physical block addresses at which the cache <b>102</b> and/or the backing store <b>118</b> store the data. In the depicted embodiment, the logical-to-physical translation layer <b>510</b> maps logical block addresses to physical block addresses of data stored on the media of the cache <b>102</b>. This mapping allows data to be referenced in a logical address space using logical identifiers, such as a logical block address. A logical identifier does not indicate the physical location of data in the cache <b>102</b>, but is an abstract reference to the data. The mapping module <b>424</b> and the remapping module <b>430</b> of <figref idrefs="DRAWINGS">FIG. 4</figref>, discussed above, are one example of a logical-to-physical translation layer <b>510</b>. One further example of a logical-to-physical translation layer <b>510</b> includes the direct mapping module <b>706</b> of <figref idrefs="DRAWINGS">FIG. 7</figref> discussed below.</div>
<div class="description-paragraph" id="p-0232" num="0231">In the depicted embodiment, the cache <b>102</b> and the backing store <b>118</b> separately manage physical block addresses in the distinct, separate physical address spaces of the cache <b>102</b> and the backing store <b>118</b>. In one example, contiguous logical block addresses may in fact be stored in non-contiguous physical block addresses as the logical-to-physical translation layer <b>510</b> determines the location on the physical media <b>110</b> of the cache <b>102</b> at which to perform data operations.</div>
<div class="description-paragraph" id="p-0233" num="0232">Furthermore, in one embodiment, the logical address space of the cache <b>102</b> is substantially larger than the physical address space or storage capacity of the cache <b>102</b>. This “thinly provisioned” or “sparse address space” embodiment, allows the number of logical addresses for data references to greatly exceed the number of possible physical addresses. A thinly provisioned and/or sparse address space also allows the cache <b>102</b> to cache data for a backing store <b>118</b> with a larger address space (i.e., a larger storage capacity) than the physical address space of the cache <b>102</b>.</div>
<div class="description-paragraph" id="p-0234" num="0233">In one embodiment, the logical-to-physical translation layer <b>510</b> includes a map or index that maps logical block addresses to physical block addresses. The map or index may be in the form of a B-tree, a content addressable memory (“CAM”), a binary tree, and/or a hash table, and the like. In certain embodiments, the logical-to-physical translation layer <b>510</b> is a tree with nodes that represent logical block addresses and include references to corresponding physical block addresses. Example embodiments of B-tree mapping structure are described below with regard to <figref idrefs="DRAWINGS">FIGS. 8, 9, and 10</figref>. In certain embodiments, the direct cache module <b>116</b> uses a mapping structure of the logical-to-physical translation layer <b>510</b> to locate ranges of data to destage in backing store address order. The direct cache module <b>116</b>, in one embodiment, requests data for destaging, in cache log order, backing store address order, or the like, from the storage controller <b>104</b> and the storage controller <b>104</b> returns the data (or references to the data) to the direct cache module <b>116</b> for destaging.</div>
<div class="description-paragraph" id="p-0235" num="0234">As stated above, in conventional block storage devices, a logical block address maps directly to a particular physical block. When a storage client <b>504</b> communicating with the conventional block storage device deletes data for a particular logical block address, the storage client <b>504</b> may note that the particular logical block address is deleted and can re-use the physical block associated with that deleted logical block address without the need to perform any other action.</div>
<div class="description-paragraph" id="p-0236" num="0235">Conversely, when a storage client <b>504</b>, communicating with a storage controller <b>104</b> or device driver with a logical-to-physical translation layer <b>510</b> (a storage controller <b>104</b> or device driver that does not map a logical block address directly to a particular physical block), deletes data of a logical block address, the corresponding physical block address may remain allocated because the storage client <b>504</b> may not communicate the change in used blocks to the storage controller <b>104</b> or device driver. The storage client <b>504</b> may not be configured to communicate changes in used blocks (also referred to herein as “data block usage information”). Because the storage client <b>504</b>, in one embodiment, uses the block I/O emulation <b>506</b> layer, the storage client <b>504</b> may erroneously believe that the direct cache module <b>116</b>, the cache <b>102</b>, and/or the backing store <b>118</b> is a conventional block storage device that would not utilize the data block usage information. Or, in certain embodiments, other software layers between the storage client <b>504</b> and the direct cache module <b>116</b>, the cache <b>102</b>, and/or the backing store <b>118</b> may fail to pass on data block usage information.</div>
<div class="description-paragraph" id="p-0237" num="0236">Consequently, the storage controller <b>104</b> or device driver may preserve the relationship between the logical block address and a physical address and the data on the cache <b>102</b> and/or the backing store <b>118</b> corresponding to the physical block. As the number of allocated blocks increases, the performance of the cache <b>102</b> and/or the backing store <b>118</b> may suffer depending on the configuration of the cache <b>102</b> and/or the backing store <b>118</b>.</div>
<div class="description-paragraph" id="p-0238" num="0237">Specifically, in certain embodiments, the cache <b>102</b>, and/or the backing store <b>118</b> are configured to store data sequentially, using an append-only writing process, and use a storage space recovery process that re-uses non-volatile storage media storing deallocated/unused logical blocks. Specifically, as described above, the cache <b>102</b>, and/or the backing store <b>118</b> may sequentially write data on the solid-state storage media <b>110</b> in a log structured format and within one or more physical structures of the storage elements, the data is sequentially stored on the solid-state storage media <b>110</b>. Those of skill in the art will recognize that other embodiments that include several caches <b>102</b> can use the same append-only writing process and storage space recovery process.</div>
<div class="description-paragraph" id="p-0239" num="0238">As a result of storing data sequentially and using an append-only writing process, the cache <b>102</b> and/or the backing store <b>118</b> achieve a high write throughput and a high number of I/O operations per second (“IOPS”). The cache <b>102</b> and/or the backing store <b>118</b> may include a storage space recovery, or garbage collection process that re-uses data storage cells to provide sufficient storage capacity. The storage space recovery process reuses storage cells for logical blocks marked as deallocated, invalid, unused, or otherwise designated as available for storage space recovery in the logical-physical translation layer <b>510</b>. In one embodiment, the direct cache module <b>116</b> marks logical blocks as deallocated or invalid based on a cache eviction policy, to recover storage capacity for caching additional data for the backing store <b>118</b>. The direct cache module <b>116</b>, in certain embodiments, selects data that is either cached read data or destaged, cleaned write data to clear, invalidate, or evict. The storage space recovery process is described in greater detail below with regard to the garbage collection module <b>714</b> of <figref idrefs="DRAWINGS">FIG. 7</figref>.</div>
<div class="description-paragraph" id="p-0240" num="0239">As described above, the storage space recovery process determines that a particular section of storage may be recovered. Once a section of storage has been marked for recovery, the cache <b>102</b> and/or the backing store <b>118</b> may relocate valid blocks (e.g. packets, pages, sectors, etc.) in the section. The storage space recovery process, when relocating valid blocks, copies the packets and writes them to another location so that the particular section of storage may be reused as available storage space, typically after an erase operation on the particular section. The cache <b>102</b> and/or the backing store <b>118</b> may then use the available storage space to continue sequentially writing data in an append-only fashion. Consequently, the storage controller <b>104</b> expends resources and overhead in preserving data in valid blocks. Therefore, physical blocks corresponding to deleted logical blocks may be unnecessarily preserved by the storage controller <b>104</b>, which expends unnecessary resources in relocating the physical blocks during storage space recovery.</div>
<div class="description-paragraph" id="p-0241" num="0240">The direct cache module <b>116</b>, in certain embodiments, decreases write amplification caused by relocating and copying data forward by destaging data in a cache log order and clearing, invalidating, or evicting certain clean, destaged data instead of copying the data forward. In a further embodiment, the direct cache module <b>116</b> balances destaging of data in cache log order with destaging of data in backing store address order to satisfy a destaging pressure or target destaging rate while managing write amplification.</div>
<div class="description-paragraph" id="p-0242" num="0241">Some storage devices are configured to receive messages or commands notifying the storage device of these unused logical blocks so that the storage device may deallocate the corresponding physical blocks (e.g. the physical storage media <b>110</b> storing the unused packets, pages, sectors, etc.). As used herein, to deallocate a physical block includes marking the physical block as invalid, unused, or otherwise designating the physical block as available for storage space recovery, its contents on storage media no longer needing to be preserved by the storage device. Data block usage information may also refer to information maintained by a storage device regarding which physical blocks are allocated and/or deal located/unallocated and changes in the allocation of physical blocks and/or logical-to-physical block mapping information. Data block usage information may also refer to information maintained by a storage device regarding which blocks are in use and which blocks are not in use by a storage client <b>504</b>. Use of a block may include storing of data in the block on behalf of the storage client <b>504</b>, reserving the block for use by the storage client <b>504</b>, and the like.</div>
<div class="description-paragraph" id="p-0243" num="0242">While physical blocks may be deallocated, in certain embodiments, the cache <b>102</b> and/or the backing store <b>118</b> may not immediately erase the data on the storage media. An erase operation may be performed later in time. In certain embodiments, the data in a deallocated physical block may be marked as unavailable by the cache <b>102</b> and/or the backing store <b>118</b> such that subsequent requests for data in the physical block return a null result or an empty set of data. In certain embodiments, the direct cache module <b>116</b> evicts and/or invalidates data by deallocating physical blocks corresponding to the data in the cache <b>102</b>.</div>
<div class="description-paragraph" id="p-0244" num="0243">One example of a command or message for such deallocation is the “TRIM” function is described in greater detail in U.S. patent application Ser. No. 12/711,113 entitled “APPARATUS, SYSTEM, AND METHOD FOR DATA BLOCK USAGE INFORMATION SYNCHRONIZATION FOR A NON-VOLATILE STORAGE VOLUME” and filed on Feb. 23, 2010 and in U.S. patent application Ser. No. 11/952,113 entitled “APPARATUS, SYSTEM, AND METHOD FOR MANAGING DATA IN A STORAGE DEVICE WITH AN EMPTY DATA TOKEN DIRECTIVE” and filed on Dec. 6, 2007, which are incorporated herein by reference. A storage device, upon receiving a TRIM command, may deallocate physical blocks for logical blocks whose data is no longer needed by the storage client <b>504</b>. A storage device that deallocates physical blocks may achieve better performance and increased storage space, especially storage devices that write data using certain processes and/or use a similar data storage recovery process as that described above.</div>
<div class="description-paragraph" id="p-0245" num="0244">Consequently, the performance of the storage device is enhanced as physical blocks are deallocated when they are no longer needed such as through the TRIM command or other similar deallocation commands issued to the cache <b>102</b> and/or the backing store <b>118</b>. In one embodiment, the direct cache module <b>116</b> clears, trims, and/or evicts cached data from the cache <b>102</b> based on a cache eviction policy, or the like. As used herein, clearing, trimming, or evicting data includes deallocating physical media associated with the data, marking the data as invalid or unused (using either a logical or physical address of the data), erasing physical media associated with the data, overwriting the data with different data, issuing a TRIM command or other deallocation command relative to the data, or otherwise recovering storage capacity of physical storage media corresponding to the data. Cleating cached data from the cache <b>102</b> based on a cache eviction policy frees storage capacity in the cache <b>102</b> to cache more data for the backing store <b>118</b>.</div>
<div class="description-paragraph" id="p-0246" num="0245">The direct cache module <b>116</b>, in various embodiments, may represent itself, the cache <b>102</b>, and the backing store <b>118</b> to the storage client <b>504</b> in different configurations. In one embodiment, the direct cache module <b>116</b> may represent itself to the storage client <b>504</b> as a single storage device (e.g., as the backing store <b>118</b>, as a storage device with a similar physical capacity as the backing store <b>118</b>, or the like) and the cache <b>102</b> may be transparent or invisible to the storage client <b>504</b>. In another embodiment, the direct cache module <b>116</b> may represent itself to the direct cache module <b>116</b> as a cache device (e.g., as the cache <b>102</b>, as a cache device with certain cache functions or APIs available, or the like) and the backing store <b>118</b> may be separately visible and/or available to the storage client <b>504</b> (with part of the physical capacity of the backing store <b>118</b> reserved for the cache <b>201</b>). In a further embodiment, the direct cache module <b>116</b> may represent itself to the storage client <b>504</b> as a hybrid cache/storage device including both the cache <b>102</b> and the backing store <b>118</b>.</div>
<div class="description-paragraph" id="p-0247" num="0246">Depending on the configuration, the direct cache module <b>116</b> may pass certain commands down to the cache <b>102</b> and/or to the backing store <b>118</b> and may not pass down other commands. In a further embodiment, the direct cache module <b>116</b> may support certain custom or new block I/O commands. In one embodiment, the direct cache module <b>116</b> supports a deallocation or trim command that clears corresponding data from both the cache <b>102</b> and the backing store <b>118</b>, i.e., the direct cache module <b>116</b> passes the command to both the cache <b>102</b> and the backing store <b>118</b>. In a further embodiment, the direct cache module <b>116</b> supports a flush type trim or deallocation command that ensures that corresponding data is stored in the backing store <b>118</b> (i.e., that the corresponding data in the cache <b>102</b> is clean) and clears the corresponding data from the cache <b>102</b>, without clearing the corresponding data from the backing store <b>118</b>. In another embodiment, the direct cache module <b>116</b> supports an evict type trim or deallocation command that evicts corresponding data from the cache <b>102</b>, marks corresponding data for eviction in the cache <b>102</b>, or the like, without clearing the corresponding data from the backing store <b>118</b>.</div>
<div class="description-paragraph" id="p-0248" num="0247">In a further embodiment, the direct cache module <b>116</b> may receive, detect, and/or intercept one or more predefined commands that a storage client <b>504</b> or another storage manager sent to the backing store <b>118</b>, that a storage manager sends to a storage client <b>504</b>, or the like. For example, in various embodiments, the direct cache module <b>116</b> or a portion of the direct cache module <b>116</b> may be part of a filter driver that receives or detects the predefined commands, the direct cache module <b>116</b> may register with an event server to receive a notification of the predefined commands, or the like. The direct cache module <b>116</b>, in one embodiment, performs one or more actions on the cache <b>102</b> in response to detecting the one or more predefined commands for the backing store <b>118</b>, such as writing or flushing data related to a command from the cache <b>102</b> to the backing store <b>118</b>, evicting data related to a command from the cache <b>102</b>, switching from a write back policy to a write through policy for data related to a command, or the like.</div>
<div class="description-paragraph" id="p-0249" num="0248">One example of predefined commands that the direct cache module <b>116</b> may intercept or respond to, in one embodiment, includes a “freeze/thaw” commands. “Freeze/thaw” commands are used in SAN s, storage arrays, and the like, to suspend storage access, such as access to the backing store <b>118</b> or the like, to take an snapshot or backup of the storage without interrupting operation of the applications using the storage. “Freeze/thaw” commands alert a storage client <b>504</b> that a snapshot is about to take place, the storage client <b>504</b> flushes pending operations, for example in-flight transactions, or data cached in volatile memory, the snapshot takes place while the storage client <b>504</b> use of the storage is in a “frozen” or ready state, and once the snapshot is complete the storage client <b>504</b> continues normal use of the storage in response to a thaw command.</div>
<div class="description-paragraph" id="p-0250" num="0249">The direct cache module <b>116</b>, in one embodiment, flushes or cleans dirty data from the cache <b>102</b> to the backing store <b>118</b> in response to detecting a “freeze/thaw” command. In a further embodiment, the direct cache module <b>116</b> suspends access to the backing store <b>118</b> during a snapshot or other backup of a detected “freeze/thaw” command and resumes access in response to a completion of the snapshot or other backup. In another embodiment, the direct cache module <b>116</b> may cache data for the backing store <b>118</b> during a snapshot or other backup without interrupting the snapshot or other backup procedure. In other words, rather than the backup/snapshot software signaling the application to quiesce I/O operations, the direct cache module <b>116</b> receives and responds to the freeze/thaw commands.</div>
<div class="description-paragraph" id="p-0251" num="0250"> <figref idrefs="DRAWINGS">FIG. 6</figref> depicts one embodiment of the direct cache module <b>116</b> <i>a</i>. In the depicted embodiment, the direct cache module <b>116</b> <i>a </i>includes a write request module <b>602</b>, a cache write module <b>604</b>, and a destage module <b>606</b>. The direct cache module <b>116</b> <i>a </i>of FIG.</div>
<div class="description-paragraph" id="p-0252" num="0251"> <b>6</b>, in one embodiment, is substantially similar to the direct cache module <b>116</b> described above with regard to <figref idrefs="DRAWINGS">FIG. 1A</figref>, <figref idrefs="DRAWINGS">FIG. 1B</figref> and/or <figref idrefs="DRAWINGS">FIG. 5</figref>. In general, the direct cache module <b>116</b> <i>a </i>caches data for the backing store <b>118</b> and destages cached write data to the backing store <b>118</b> in cache log order and/or in backing store address order.</div>
<div class="description-paragraph" id="p-0253" num="0252">In one embodiment, the write request module <b>602</b> detects one or more write requests to store data on the backing store <b>118</b>. The write request module <b>602</b> may detect a write request by receiving the write request directly, detecting a write request sent to a different module or entity (such as detecting a write request sent directly to the backing store <b>118</b>), or the like. In one embodiment, the host device <b>114</b> sends the write request. The direct cache module <b>116</b> <i>a</i>, in one embodiment, represents itself to the host device <b>114</b> as a storage device, and the host device <b>114</b> sends write requests directly to the write request module <b>602</b>.</div>
<div class="description-paragraph" id="p-0254" num="0253">A write request, in one embodiment, includes data that is not stored on the backing store <b>118</b>. Data that is not stored on the backing store <b>118</b>, in various embodiments, includes new data not yet stored on the backing store <b>118</b>, modifications to data that is stored on the backing store <b>118</b>, and the like. The write request, in various embodiments, may directly include the data, may include a reference, a pointer, or an address for the data, or the like. For example, in one embodiment, the write request includes a range of addresses indicating data to be stored on the backing store <b>118</b> by way of a Direct Memory Access (“DMA”) or Remote DMA (“RDMA”) operation. In a further embodiment, a single write request may include several different contiguous and/or noncontiguous ranges of addresses or blocks. In a further embodiment, the write request includes one or more destination addresses for the data, such as logical and/or physical addresses for the data on the cache <b>102</b> and/or on the backing store <b>118</b>. The write request module <b>602</b> and/or another cooperating module, in various embodiments, may retrieve the data of a write request directly from the write request itself, from a storage location referenced by a write request (i.e., from a location in system memory or other data storage referenced in a DMA or RDMA request), or the like.</div>
<div class="description-paragraph" id="p-0255" num="0254">The cache write module <b>604</b>, in one embodiment, writes data of a write request to the cache <b>102</b> to cache the data in the cache <b>102</b>. The cache write module <b>604</b>, in another embodiment, caches the data of the write request to the cache <b>102</b> at one or more logical addresses of the cache <b>102</b> corresponding to one or more backing store addresses of the write request. In one embodiment, the cache write module <b>604</b> caches the data to the cache <b>102</b> by appending the data to a sequential, log-based writing structure preserved in the physical storage media <b>110</b> of the cache <b>102</b> at an append point. The cache write module <b>604</b>, in one embodiment, returns one or more physical addresses corresponding to a location of the append point at which the data was appended to a direct mapping module such as the direct mapping module <b>706</b> described below with regard to <figref idrefs="DRAWINGS">FIG. 7</figref>, which maps the one or more logical addresses of the cache <b>102</b> to the one or more physical addresses corresponding to the append point.</div>
<div class="description-paragraph" id="p-0256" num="0255">The destage module <b>606</b>, in one embodiment, destages cached data from the cache <b>102</b> to the backing store <b>118</b>. The destage module <b>606</b> destages data to the backing store <b>118</b> by copying, writing, storing, or otherwise persisting the data in the backing store <b>118</b>. The destage module <b>606</b> destages dirty write data that the backing store <b>118</b> does not yet store. Data that is stored in the cache <b>102</b> that is not yet stored in the backing store <b>118</b> is referred to as “dirty” data. Once the backing store <b>118</b> stores data, the data is referred to as “clean.” The destage module <b>606</b> destages or cleans data in the cache <b>102</b> by writing the data to the backing store <b>118</b>. In certain embodiments, the destage module <b>606</b> may also destage some clean data that the backing store <b>118</b> already stores, such as clean data that is within a range of dirty data, an entire region or block of data to obviate the need for tracking clean and dirty data within the region, or the like.</div>
<div class="description-paragraph" id="p-0257" num="0256">In one embodiment, the destage module <b>606</b> destages data from the cache <b>102</b> to the backing store <b>118</b> in a cache log order. As described above with regard to the cache <b>102</b> of <figref idrefs="DRAWINGS">FIGS. 1A and 1B</figref>, the cache log order is an order in which data is organized within or appended to a log of the cache <b>102</b>, such as oldest to newest or the like. In a further embodiment, the destage module <b>606</b> destages data from the cache <b>102</b> to the backing store <b>118</b> in a sequential backing store address order. As described above with regard to the cache <b>102</b> of <figref idrefs="DRAWINGS">FIGS. 1A and 1B</figref>, the sequential backing store address order, in certain embodiments, comprises one or more ranges of address contiguous data that is sequentially ordered by backing store address.</div>
<div class="description-paragraph" id="p-0258" num="0257">In one embodiment, the destage module <b>606</b> traverses a log of the cache <b>102</b> to select data to destage in cache log order. The destage module <b>606</b>, in certain embodiments, destages data in cache log order region by region. A region may include an erase block such as an LEB, a page, a block, a sector, a packet, or another discrete segment of data. The destage module <b>606</b>, in one embodiment, traverses the log from one region to a neighboring region. In various embodiments, regions in a log may be physical neighbors in the storage media <b>110</b>, logical neighbors in a data structure of the log, or the like. For example, the destage module <b>606</b>, in one embodiment, may follow logical pointers or references from region to neighboring region in cache log order to destage data. As discussed below with regard to the re-order module <b>726</b> of <figref idrefs="DRAWINGS">FIG. 7</figref>, in one embodiment, the destage module <b>606</b> may re-order data within a region for destaging. For example, the destage module <b>606</b> may destage regions in cache log order, but may re-order data within the region by backing store address order.</div>
<div class="description-paragraph" id="p-0259" num="0258">In one embodiment, the destage module <b>606</b> traverses a mapping structure or cache index that maps backing store addresses to locations on physical storage media of the cache <b>102</b> to select ranges of data to destage in backing store address order. The destage module <b>606</b>, in one embodiment, may traverse the mapping structure in backing store address order and destage ranges of data in backing store address order. In another embodiment, the destage module <b>606</b> may traverse the mapping structure in a different order, such as by level in a tree, by frequency of access in order to destage the least accessed or “coldest” data first, or in another order. For example, in certain embodiments, the destage module <b>606</b> may access the mapping structure in an order that is based on access statistics for the mapping structure, or the like.</div>
<div class="description-paragraph" id="p-0260" num="0259">In one embodiment, the destage module <b>606</b> selects a destage data range of data for destaging in a backing store address order such that members of the data range have contiguous and sequential backing store addresses. In a further embodiment, the destage module <b>606</b> selects destage data ranges based on a predefined sequential threshold, selecting destage data ranges that have at least the predefined sequential threshold amount of contiguous and sequential backing store addresses for destaging.</div>
<div class="description-paragraph" id="p-0261" num="0260">As discussed in greater detail below with regard to the dirty indicator module <b>712</b> of <figref idrefs="DRAWINGS">FIG. 7</figref>, in certain embodiments, the destage module <b>606</b> accesses one or more dirty data indicators to determine which data in the cache <b>102</b> is dirty and a candidate for destaging. In one embodiment, the destage module <b>606</b> destages in a cache log order, and the dirty data indicator includes a pointer indicating a current destage point, and user write data to one side of the pointer is clean while user write data to the other side of the pointer is dirty. One example of a pointer indicating a current destage point is described below with regard to <figref idrefs="DRAWINGS">FIG. 10</figref>. In other embodiments, a dirty data indicator may include one or more flags, one or more bit fields, one or more bit arrays, or the like.</div>
<div class="description-paragraph" id="p-0262" num="0261">Dirty data indicators, in various embodiments, may be stored in a mapping structure, in a reverse map, in volatile memory of the cache <b>102</b> or the host device <b>114</b>, in a region of data such as an erase block or a packet, and/or in other data storage accessible to the destage module <b>606</b>. In a further embodiment, the destage module <b>606</b> may store dirty indicators on volatile memory and may also store at least enough information to reconstruct the dirty indicators in the storage media <b>110</b> of the cache <b>102</b>. In one embodiment, the destage module <b>606</b> updates one or more dirty data indicators in response to successfully destaging data to the backing store <b>118</b> so that the one or more dirty data indicators indicate that the destaged data is clean.</div>
<div class="description-paragraph" id="p-0263" num="0262">The destage module <b>606</b>, in one embodiment, may determine an address for selected destage data in the backing store <b>118</b> based on a write request corresponding to the data. In a further embodiment, the destage module <b>606</b> determines an address for destage data in the backing store <b>118</b> based on a logical address of the data in the cache <b>102</b>, based on a cache index, a mapping structure, or the like. In another embodiment, the destage module <b>606</b> uses a reverse map or the like to determine an address for destage data in the backing store <b>118</b> based on a physical address of the data in the cache <b>102</b>.</div>
<div class="description-paragraph" id="p-0264" num="0263">The destage module <b>606</b>, in one embodiment, writes data to the backing store <b>118</b> based on a write policy. In one embodiment, the destage module <b>606</b> uses a write-back write policy, and does not immediately write data of a write request to the backing store <b>118</b> upon detecting the write request. Instead, the destage module <b>606</b>, in one embodiment, performs an opportunistic or “lazy” write, destaging data to the backing store <b>118</b> when the data is evicted from the cache <b>102</b>, when the cache <b>102</b> and/or the direct cache module <b>116</b> has a light load, when available storage capacity in the cache <b>102</b> falls below a threshold, to satisfy a destaging pressure or target destage rate, or the like. In certain write-back embodiments, the destage module <b>606</b> may read data from the cache <b>102</b> and write the data to the backing store <b>118</b>.</div>
<div class="description-paragraph" id="p-0265" num="0264">In another embodiment, instead of cleaning data according to a write-back write policy, the destage module <b>606</b> uses a write-through policy, performing a synchronous write to the backing store <b>118</b> for each write request that the write request module <b>602</b> receives. The destage module <b>606</b>, in one embodiment, transitions from a write-back to a write-through write policy in response to a predefined error condition, such as an error or failure of the cache <b>102</b>, or the like.</div>
<div class="description-paragraph" id="p-0266" num="0265">In one embodiment, the destage module <b>606</b> does not invalidate or evict destaged data from the cache <b>102</b>, but destaged data remains in the cache <b>102</b> to service read requests until the destaged data is evicted from the cache by a separate eviction process. In a further embodiment, the destage module <b>606</b> may invalidate, clear, or evict destaged data from the cache <b>102</b> once the backing store <b>118</b> stores the data. In certain embodiments, evicting data upon destaging may lead to an increase in cache misses, but may also increase a speed or efficiency of garbage collection/grooming of the cache <b>102</b>.</div>
<div class="description-paragraph" id="p-0267" num="0266">In one embodiment, described in greater detail below with regard to <figref idrefs="DRAWINGS">FIG. 7</figref>, the destage module <b>606</b> destages a portion of dirty data in cache log order and a portion of dirty data in backing store address order. The destage module <b>606</b>, in a further embodiment, adjusts a ratio, duty cycle, or the like of destaging in cache log order and in backing store address order to satisfy a destaging pressure, a destaging target, or the like.</div>
<div class="description-paragraph" id="p-0268" num="0267"> <figref idrefs="DRAWINGS">FIG. 7</figref> depicts another embodiment of the direct cache module <b>116</b> <i>b</i>. In the depicted embodiment, the direct cache module <b>116</b> <i>b </i>includes the block I/O emulation layer <b>508</b>, the direct interface layer <b>510</b>, the write request module <b>602</b>, the cache write module <b>604</b>, and the destage module <b>606</b>, substantially as described above with regard to <figref idrefs="DRAWINGS">FIGS. 5 and 6</figref>. The direct cache module <b>116</b> <i>b</i>, in the depicted embodiment, further includes a destaging pressure module <b>702</b>, a destaging rate module <b>704</b>, a direct mapping module <b>706</b>, a recent data module <b>708</b>, a multiple append point module <b>710</b>, a dirty indicator module <b>712</b>, a garbage collection module <b>714</b>, an eviction module <b>716</b>, a read request module <b>718</b>, and a backing store interface module <b>720</b>. The direct cache module <b>116</b> <i>b</i>, in certain embodiments, may be substantially similar to the direct cache module <b>116</b> of <figref idrefs="DRAWINGS">FIGS. 1A and 1B</figref>, the direct cache module <b>116</b> of <figref idrefs="DRAWINGS">FIG. 5</figref>, and the direct cache module <b>116</b> <i>b </i>of <figref idrefs="DRAWINGS">FIG. 6</figref>.</div>
<div class="description-paragraph" id="p-0269" num="0268">In the depicted embodiment, the destage module <b>606</b> includes a log order module <b>722</b>, an address order module <b>724</b>, and a re-order module <b>726</b>. In one embodiment, the log order module <b>722</b> destages at least a portion of dirty data from the cache <b>102</b> to the backing store <b>118</b> in a cache log order and the address order module <b>724</b> destages at least a portion of dirty data from the cache <b>102</b> to the backing store <b>118</b> in a backing store address order.</div>
<div class="description-paragraph" id="p-0270" num="0269">In a cache log order, the backing store addresses of the data may be somewhat random or unorganized due to an order in which the host device <b>114</b>, a user application <b>502</b>, a storage client <b>504</b>, or the like writes the data. As described above with regard to <figref idrefs="DRAWINGS">FIGS. 1A and 1B</figref>, in certain embodiments, the backing store <b>118</b> may have a lower write rate when destaging in cache log order than when destaging in backing store address order, due to write head seek times, or the like. Destaging in cache log order, however, in certain embodiments, may be more efficient for the cache <b>102</b>. For example, older data on the log may be more likely to be groomed by the garbage collection module <b>714</b> and destaging the data prior to grooming may reduce write amplification because destaged data may be selectively evicted from the cache <b>102</b> instead of written forward on the log.</div>
<div class="description-paragraph" id="p-0271" num="0270">To take advantage of these different efficiencies, in certain embodiments, the destage module <b>606</b> manages a ratio, duty cycle, or the like of destaging in cache log order using the log order module <b>722</b> and destaging in backing store address order using the address order module <b>724</b>. As described below with regard to the destaging pressure module <b>702</b> and the destaging rate module <b>704</b>, in one embodiment, the destage module <b>606</b> uses the log order module <b>722</b> and/or the address order module <b>724</b> to destage data at a destaging rate that satisfies a destaging pressure for the cache <b>102</b>.</div>
<div class="description-paragraph" id="p-0272" num="0271">In one embodiment, the log order module <b>722</b> destages dirty data region by region with the regions ordered in cache log order. In a further embodiment, the regions are the same size, granularity, or type of region groomed by the garbage collection module <b>714</b>, such as erase blocks (e.g. LEBs, PEBs, etc.), pages, packets, or the like. Destaging data at the same size granularity as the units the garbage collection module <b>714</b> uses to groom the data, in certain embodiments, may be more efficient for the grooming process. In certain embodiments, reasons for this improved efficiency are because the log order module <b>722</b> destages dirty data for an entire region, write amplification may be decreased, and the like. In another embodiment, the log order module <b>722</b> may destage regions of a different size, granularity, or type than the garbage collection module <b>714</b> grooms.</div>
<div class="description-paragraph" id="p-0273" num="0272">In one embodiment, once the log order module <b>722</b> selects a region for destaging in cache log order, the re-order module <b>726</b> re-orders data stored in the selected region to a backing store address order. Re-ordering (or pre-ordering) data of a region for destaging, in certain embodiments, may increase the write rate of the backing store <b>118</b> when destaging the region, even if the log order module <b>722</b> destages the regions themselves in cache log order. The re-order module <b>726</b>, in one embodiment, sends the data of a region to a buffer, a queue, or the like in backing store address order for destaging to the backing store <b>118</b>.</div>
<div class="description-paragraph" id="p-0274" num="0273">As described above, in one embodiment, the type of region is selected based on a size, granularity, or type of region that the garbage collection module <b>714</b> grooms. In another embodiment, the re-order module <b>726</b> re-orders a region of a different size, granularity, or type than the garbage collection module <b>714</b> grooms. For example, in one embodiment, the log order module <b>722</b> may select a region of the log to destage, such as a quarter, a half or another portion of the log, and the re-order module <b>726</b> may re-order dirty data for the selected region. The re-order module <b>726</b>, in various embodiments, may re-order data of a region by packet, by sector, by block, by page, or the like.</div>
<div class="description-paragraph" id="p-0275" num="0274">In one embodiment, the destaging pressure module <b>702</b> determines a destaging pressure for the cache <b>102</b>. The destaging pressure, in one embodiment, is a level of demand for destaging cached data of the cache <b>102</b>. The destaging pressure module <b>702</b>, in a further embodiment, determines a destaging pressure based on a difference between an actual amount of dirty write data in the cache <b>102</b> and a target amount of dirty write data in the cache <b>102</b>. For example, the destaging pressure module <b>702</b> may use a predefined destaging pressure scale with a higher destaging pressure as the actual amount of dirty write data approaches, exceeds, or has another predefined relationship with the target amount of dirty write data. Destaging pressure comprises a level of importance or priority for performing destaging operations.</div>
<div class="description-paragraph" id="p-0276" num="0275">The target amount of dirty write data, in various embodiments, may be a maximum amount of dirty write data, an optimal or desired amount of dirty write data, or the like. In one embodiment, the target amount of dirty write data is user defined. In another embodiment, the direct cache module <b>116</b> <i>b </i>dynamically adjusts the target amount of dirty write data based on usage conditions of the cache <b>102</b> such as hit and miss rates for read data, write data, dirty data, clean data, and the like, a percentage of the cache <b>102</b> that stores data, or on other usage conditions. In one embodiment, the destaging pressure module <b>702</b> maintains a count or tally of the actual amount of dirty write data for the cache <b>102</b>, adding to the count or tally as write data is cached in the cache <b>102</b> and subtracting from the count or tally as data is destaged to the backing store <b>118</b>. In other embodiments, the destaging pressure module <b>702</b> may receive an indicator of the actual amount of dirty write data from the dirty indicator module <b>712</b>, from the storage controller <b>104</b>, or the like.</div>
<div class="description-paragraph" id="p-0277" num="0276">In one embodiment, the destaging rate module <b>704</b> adjusts a destaging rate for the cache <b>102</b> by adjusting a ratio of data to destage in cache log order using the log order module <b>722</b> to data to destage in backing store address order using the address order module <b>724</b>. In a further embodiment, the destaging rate module <b>704</b> adjusts the destaging rate to satisfy the destaging pressure determined by the destaging pressure module <b>702</b>. The destaging rate of the cache <b>102</b>, in one embodiment, is the amount of data (in bytes or other data units) that the destage module <b>606</b> destages to the backing store <b>118</b> per unit of time.</div>
<div class="description-paragraph" id="p-0278" num="0277">In various embodiments, the destaging rate may satisfy the destaging pressure by moving the actual amount of dirty write data below the target amount of dirty write data, moving the actual amount of dirty write data within or below the target amount of dirty write data, or causing the actual amount of dirty write data to have another predefined relationship with respect to the target amount of dirty write data. In one embodiment, the destaging pressure module <b>702</b> and the destaging rate module <b>704</b> comprise a difference controller that receives the actual amount of dirty data and the target amount of dirty data and adjusts the destaging rate based on the difference between the actual and target amounts.</div>
<div class="description-paragraph" id="p-0279" num="0278">The destaging rate module <b>704</b>, in one embodiment, toggles or alternates between destaging in cache log order using the log order module <b>722</b> and destaging in backing store address order using the address order module <b>724</b>, to satisfy the destaging pressure. In an embodiment where the destaging rate module <b>704</b> toggles or alternates between destaging orders, the ratio of the orders may determine the duty cycle of time (number of cycles) spent destaging in cache log order and in backing store address order. In another embodiment where the destaging rate module <b>704</b> toggles or alternates between destaging orders, proportions of time spent destaging in cache log order versus destaging in backing store address order may be varied. For example, the time spent during destaging using one destaging order versus another order may be divided 60% to 40%. In addition, or alternatively, the destaging rate module <b>704</b> may change this proportion based on characteristics of the cache <b>102</b>, the flash solid-state storage media <b>110</b>, or the backing store <b>118</b>.</div>
<div class="description-paragraph" id="p-0280" num="0279">In another embodiment, the destaging rate module <b>704</b> may determine a ratio that satisfies the destaging pressure by destaging in cache log order using the log order module <b>722</b> until the destaging pressure satisfies a predefined destaging pressure criteria, and destaging in the backing store address order using the address order module <b>724</b> in response to the destaging pressure satisfying the predefined destaging pressure criteria, or the like. A predefined destaging pressure criteria, in one embodiment, includes one or more rules, policies, and/or thresholds that trigger a change in the destaging operations of the cache <b>102</b>. A predefined destaging pressure criteria may be based on a destage rate or other property associated with a destaging algorithm, characteristics of the cache <b>102</b>, characteristics of the solid-state storage media <b>110</b>, characteristics of the backing store <b>118</b>, or the like. In one embodiment, the destaging rate module <b>704</b> may destage in backing store address order using the address order module <b>724</b> until the destaging pressure exceeds, falls below or otherwise satisfies the predefined destaging pressure criteria or threshold. The destaging rate module <b>704</b> may use the destaging pressure itself, a scaled or weighted product of the destaging pressure, an integral of the destaging pressure, or another criteria related to the destaging pressure as a predefined destaging pressure criteria.</div>
<div class="description-paragraph" id="p-0281" num="0280">In one embodiment, the destaging rate module <b>704</b> may destage data in cache log order using the log order module <b>722</b> and in backing store address order using the address order module <b>724</b> simultaneously in parallel. In a simultaneous or parallel embodiment, the destaging rate module <b>704</b> may adjust the ratio of cache log order destaging to backing store address order destaging by adjusting or throttling individual destaging rates of the log order module <b>722</b> and the address order module <b>724</b>, or the like.</div>
<div class="description-paragraph" id="p-0282" num="0281">The direct mapping module <b>706</b>, in one embodiment, directly maps logical or physical addresses of the backing store <b>118</b> (“backing store addresses”) to logical addresses of the cache <b>102</b> and directly maps logical addresses of the cache <b>102</b> to the backing store addresses of the backing store <b>118</b>. As used herein, direct mapping of addresses means that for a given address in a first address space there is exactly one corresponding address in a second address space with no translation or manipulation of the address to get from an address in the first address space to the corresponding address in the second address space. The direct mapping module <b>706</b>, in a further embodiment, maps backing store addresses to logical addresses of the cache <b>102</b> such that each backing store <b>118</b> address has a one to one relationship with a logical address of the cache <b>102</b>. As described above, in one embodiment, the logical addresses of the cache <b>102</b> are independent of the physical addresses of the physical storage media <b>110</b> for the cache <b>102</b> and the physical addresses of the physical storage media <b>110</b> of the cache <b>102</b> are fully associative with backing store addresses of the backing store <b>118</b>.</div>
<div class="description-paragraph" id="p-0283" num="0282">In one embodiment, the direct mapping module <b>706</b> maps the backing store addresses directly to logical addresses of the cache <b>102</b> so that the backing store addresses of the backing store <b>118</b> and the logical addresses of the cache <b>102</b> are equal or equivalent. In one example of this embodiment, the backing store addresses and the logical addresses of the cache <b>102</b> share a lower range of the logical address space of the cache <b>102</b>, such as addresses between about 0-232, or the like.</div>
<div class="description-paragraph" id="p-0284" num="0283">In one embodiment, the direct mapping module <b>706</b> directly maps logical addresses of the cache <b>102</b> to physical addresses and/or locations on the physical storage media <b>110</b> of the cache <b>102</b>. In a further embodiment, the direct mapping module <b>706</b> uses a single mapping structure to map backing store addresses to logical addresses of the cache <b>102</b> and to map logical addresses of the cache <b>102</b> to locations on the physical storage media <b>110</b> of the cache <b>102</b>. The mapping structure, in various embodiments, may include a B-tree, B*-tree, B+-tree, a CAM, a binary tree, a hash table, an index, an array, a linked-list, a look-up table, or another mapping data structure.</div>
<div class="description-paragraph" id="p-0285" num="0284">Use of a B-tree as the mapping structure m certain embodiments, is particularly advantageous where the logical address space presented to the client is a very large address space (such as 264 addressable blocks or the like—which may or may not be sparsely populated). Because B-trees maintain an ordered structure, searching such a large space remains very fast. Example embodiments of a B-tree as a mapping structure are described in greater detail with regard to <figref idrefs="DRAWINGS">FIGS. 8, 9, and 10</figref>. For example, in one embodiment, the mapping structure includes a B-tree with multiple nodes and each node may store several entries. In the example embodiment, each entry may map a variable sized range of logical addresses of the cache <b>102</b> to a location (such as a starting location) on the physical storage media <b>110</b> of the cache <b>102</b>. Furthermore, the number of nodes in the B-tree may vary as the B-tree grows wider and/or deeper.</div>
<div class="description-paragraph" id="p-0286" num="0285">In one embodiment, the mapping structure of the direct mapping module <b>706</b> only includes a node or entry for logical addresses of the cache <b>102</b> that are associated with currently cached data in the cache <b>102</b>. In this embodiment, membership in the mapping structure represents membership in the cache <b>102</b>. The direct mapping module <b>706</b>, in one embodiment, adds entries, nodes, and the like to the mapping structure as data is stored in the cache and removes entries, nodes, and the like from the mapping structure in response to data being evicted, cleared, trimmed, or otherwise removed from the cache <b>102</b>.</div>
<div class="description-paragraph" id="p-0287" num="0286">Similarly, membership in the mapping structure may represent valid allocated blocks on the solid-state storage media <b>110</b>. The solid-state storage controller <b>104</b> (and/or the direct mapping module <b>706</b>), in one embodiment, adds entries, nodes, and the like to the mapping structure as data is stored on the solid-state storage media <b>110</b> and removes entries, nodes, and the like from the mapping structure in response to data being invalidated cleared, trimmed, or otherwise removed from the solid-state storage media <b>110</b>. In the case where the mapping structure is shared for both cache management and data storage management on the solid-state storage media <b>110</b>, the dirty indicator module <b>712</b> described below, in certain embodiments, may also track whether the data is dirty or not to determine whether the data is persisted on the backing store <b>118</b>. The address order module <b>724</b>, in a further embodiment, may also traverse the mapping structure to locate ranges of data in backing store address order, may request ranges of data in backing store address order from the direct mapping module <b>706</b>, or the like.</div>
<div class="description-paragraph" id="p-0288" num="0287">In a further embodiment, the mapping structure of the direct mapping module <b>706</b> may include one or more nodes or entries for logical addresses of the cache <b>102</b> that are not associated with data currently stored in the cache <b>102</b>, but that are associated with addresses of the backing store <b>118</b> that currently store data. The nodes or entries for logical addresses of the cache <b>102</b> that are not associated with data currently stored in the cache <b>102</b>, in one embodiment, are not mapped to locations on the physical storage media <b>110</b> of the cache <b>102</b>, but include a reference or indicator that the cache <b>102</b> does not store data corresponding to the logical addresses. The nodes or entries, in a further embodiment, may include information that the data resides in the backing store <b>118</b>. For example, in certain embodiments, the mapping structure of the direct mapping module <b>706</b> may include nodes or entries for read misses, data of which the backing store <b>118</b> stores but the cache <b>102</b> does not currently store.</div>
<div class="description-paragraph" id="p-0289" num="0288">Nodes, entries, records, or the like of the mapping structure, in one embodiment, may include information (such as physical addresses, offsets, indicators, etc.) directly, as part of the mapping structure, or may include pointers, references, or the like for locating information in memory, in a table, or in another data structure. The direct mapping module <b>706</b>, in one embodiment, optimizes the mapping structure by monitoring the shape of the mapping structure, monitoring the size of the mapping structure, balancing the mapping structure, enforcing one or more predefined rules with regard to the mapping structure, ensuring that leaf nodes of the mapping structure are at the same depth, combining nodes, splitting nodes, and/or otherwise optimizing the mapping structure.</div>
<div class="description-paragraph" id="p-0290" num="0289">The direct mapping module <b>706</b>, in one embodiment, stores the mapping structure on the solid-state storage media <b>110</b> of the cache <b>102</b>. By storing the mapping structure on the cache <b>102</b>, in a further embodiment, the mapping of addresses of the backing store <b>118</b> to the logical addresses of the cache <b>102</b> and/or the mapping of the logical addresses of the cache <b>102</b> to locations on the physical storage media <b>110</b> of the cache <b>102</b> are persistent, even if the cache <b>102</b> is subsequently paired with a different host device <b>114</b>. In one embodiment, the backing store <b>118</b> is also subsequently paired with the different host device <b>114</b>. In a further embodiment, the cache <b>102</b> rebuilds or restores at least a portion of data from the backing store <b>118</b> on a new backing store storage device associated with the different host device <b>114</b>, based on the mapping structure and data stored on the cache <b>102</b>.</div>
<div class="description-paragraph" id="p-0291" num="0290">In one embodiment, the recent data module <b>708</b> prevents destaging of data within a predefined distance of an append point of the log of the cache <b>102</b>. In certain embodiments, data within a predefined distance of the append point (e.g. the newest data on the log) may be more likely to be invalidated by a new write to the same corresponding backing store addresses. Data that is invalidated or logically overwritten by a subsequent write does not need to be destaged. The recent data module <b>708</b>, in one embodiment, prevents the destage module <b>606</b> from destaging data within a most recent region adjacent to the append point to reduce the amount of dirty data that is destaged and subsequently invalidated or logically overwritten by a subsequent write. The recent data module <b>708</b>, in various embodiments, may prevent the destage module <b>606</b> from destaging data within a predefined distance of an append point by verifying data eligibility for destaging for the destage module <b>606</b> prior to the destage module <b>606</b> destaging the data, by locking data from destaging using a locking data structure, by maintaining a destagable data table or other data structure for the destage module <b>606</b>, or the like.</div>
<div class="description-paragraph" id="p-0292" num="0291">In one embodiment, the multiple append point module <b>710</b> maintains multiple append points for the log (or maintains multiple logs) of the cache <b>102</b>. The multiple append points, in various embodiments, may be for a single log, for multiple sub-logs, for multiple separate logs, or the like. In a further embodiment, at least one append point is for writing dirty data and at least one append point is for writing clean data. In one embodiment, once the destage module <b>606</b> destages data from a dirty data portion of a log or a dirty data sub-log, the eviction module <b>716</b> may invalidate or evict the clean data and reclaim the corresponding storage space. Alternatively, the garbage collection module <b>714</b> may copy the clean data to a clean data portion of the log or a clean data sub-log. By using separate append points and/or logs to store dirty data and clean data, in certain embodiments, tracking dirty and clean data is simplified and may use little or no metadata overhead, or tracking data structures.</div>
<div class="description-paragraph" id="p-0293" num="0292">In one embodiment, the dirty indicator module <b>712</b> sets an indicator that the destage module <b>606</b> has destaged data to the backing store <b>118</b>, such as the dirty data indicator described above with regard to <figref idrefs="DRAWINGS">FIG. 6</figref> or the like, to track which data is clean and which data is dirty. The dirty indicator module <b>712</b>, in one embodiment, sets the indicator that the backing store <b>118</b> stores the data once the destage module <b>606</b> has successfully written the data to the backing store <b>118</b>. Setting the indicator (dirty/clean indicator) that the backing store <b>118</b> stores the data, in one embodiment, prevents the destage module <b>606</b> from destaging data a second time once the destage module <b>606</b> has already destaged the data. In a further embodiment, setting the indicator that the backing store <b>118</b> stores the data may alert a garbage collection or grooming process, such as the garbage collection module <b>714</b>, that the data may be cleared from the cache <b>102</b> and/or alert an eviction process, such as the eviction module <b>716</b>, that the data may be evicted from the cache <b>102</b>, or the like.</div>
<div class="description-paragraph" id="p-0294" num="0293">In one embodiment, the dirty indicator module <b>712</b> sets an indicator that the backing store <b>118</b> stores data by marking the data as clean in the cache <b>102</b>. In a further embodiment, the dirty indicator module <b>712</b> may set an indicator that the backing store <b>118</b> stores data by communicating an address of the data to the direct mapping module <b>706</b> or by sending a request to the direct mapping module <b>706</b> to update an indicator in a logical to physical mapping or other mapping structure. In another embodiment, the dirty indicator module <b>712</b> may set an indicator that the backing store <b>118</b> stores data by updating one or more indicators for a region of data in the cache <b>102</b>, or the like. For example, in certain embodiments, the dirty indicator module <b>712</b> may maintain a bit field or bit array for one or more regions of the cache <b>102</b> representing which data is dirty and which data is clean within the one or more regions. In the bit fields or bit arrays, in one embodiment, each bit represents a packet, a page, a sector, a block, a range of data, or the like within a region, with one binary state indicating that the packet, page, sector, block, or range of data is dirty and the other binary state representing that the packet, page, sector, block, or range of data is clean.</div>
<div class="description-paragraph" id="p-0295" num="0294">In one embodiment, where the destage module <b>606</b> uses the log order module <b>722</b> for destaging data in a cache log order, the dirty indicator module <b>712</b> may set an indicator that the backing store <b>118</b> stores data by moving a pointer representing a current destage point in the log. As described above, in certain embodiments, user write data to one side of the pointer may be dirty and user write data to the other side of the pointer may be clean. One embodiment of a pointer for a current destage point is depicted in <figref idrefs="DRAWINGS">FIG. 10</figref>. One of skill in the art, in light of this disclosure, will recognize other manners in which the dirty indicator module <b>712</b> may track dirty and clean data.</div>
<div class="description-paragraph" id="p-0296" num="0295">In one embodiment, the garbage collection module <b>714</b> recovers storage capacity of physical storage media <b>110</b> corresponding to data that is marked as invalid, such as data destaged by the destage module <b>606</b> and/or evicted by the eviction module <b>716</b>. The garbage collection module <b>714</b>, in one embodiment, recovers storage capacity of physical storage media corresponding to data that the destage module <b>606</b> has cleaned and that the eviction module <b>716</b> has evicted, or that has been otherwise marked as invalid. In one embodiment, the garbage collection module <b>714</b> allows clean data to remain in the cache <b>102</b> as long as possible until the eviction module <b>716</b> evicts the data or the data is otherwise marked as invalid, to decrease the number of cache misses.</div>
<div class="description-paragraph" id="p-0297" num="0296">In one embodiment, the garbage collection module <b>714</b> recovers storage capacity of physical storage media corresponding to invalid data opportunistically. For example, the garbage collection module <b>714</b> may recover storage capacity in response to a lack of available storage capacity, a percentage of data marked as invalid reaching a predefined threshold level, a consolidation of valid data, an error detection rate for a section of physical storage media reaching a threshold value, performance crossing a threshold value, a scheduled garbage collection cycle, identifying a section of the physical storage media <b>110</b> with a high amount of invalid data, identifying a section of the physical storage media <b>110</b> with a low amount of wear, or the like.</div>
<div class="description-paragraph" id="p-0298" num="0297">In one embodiment, the garbage collection module <b>714</b> relocates valid data that is in a section of the physical storage media <b>110</b> in the cache <b>102</b> that the garbage collection module <b>714</b> is recovering to preserve the valid data. The garbage collection module <b>714</b>, in a further embodiment, relocates or copies forward dirty data that has not been destaged upon grooming the dirty data to preserve the dirty data. In another embodiment, the garbage collection module <b>714</b> may selectively relocate or copy forward clean data that has already been destaged. In one embodiment, the eviction module <b>716</b> determines which clean data to relocate and which clean data to erase without relocating. Erasing data without relocating the data evicts the data from the cache <b>102</b>. The garbage collection module <b>714</b> and/or the eviction module <b>716</b>, in one embodiment, may select which clean data remains in the cache and which clean data is evicted based on an eviction policy, a determined cost for the clean data, a frequency of use for the clean data, or the like. In another embodiment, the garbage collection module <b>714</b> clears or erases all clean data in a section of the physical storage media <b>110</b> that the garbage collection module <b>714</b> has selected for grooming.</div>
<div class="description-paragraph" id="p-0299" num="0298">In one embodiment, the garbage collection module <b>714</b> is part of an autonomous garbage collector system that operates within the cache <b>102</b>. This allows the cache <b>102</b> to manage data so that data is systematically spread throughout the solid-state storage media <b>110</b>, or other physical storage media, to improve performance, data reliability and to avoid overuse and underuse of any one location or area of the solid-state storage media <b>110</b> and to lengthen the useful life of the solid-state storage media <b>110</b>.</div>
<div class="description-paragraph" id="p-0300" num="0299">The garbage collection module <b>714</b>, upon recovering a section of the physical storage media <b>110</b>, allows the cache <b>102</b> to re-use the section of the physical storage media <b>110</b> to store different data. In one embodiment, the garbage collection module <b>714</b> adds the recovered section of physical storage media to an available storage pool for the cache <b>102</b>, or the like. The garbage collection module <b>714</b>, in one embodiment, erases existing data in a recovered section. In a further embodiment, the garbage collection module <b>714</b> allows the cache <b>102</b> to overwrite existing data in a recovered section. Whether or not the garbage collection module <b>714</b>, in one embodiment, erases existing data in a recovered section may depend on the nature of the physical storage media. For example, Flash media requires that cells be erased prior to reuse where magnetic media such as hard drives does not have that requirement. In an embodiment where the garbage collection module <b>714</b> does not erase data in a recovered section, but allows the cache <b>102</b> to overwrite data in the recovered section, the garbage collection module <b>714</b>, in certain embodiments, may mark the data in the recovered section as unavailable to service read requests so that subsequent requests for data in the recovered section return a null result or an empty set of data until the cache <b>102</b> overwrites the data.</div>
<div class="description-paragraph" id="p-0301" num="0300">In one embodiment, the garbage collection module <b>714</b> recovers storage capacity of the cache <b>102</b> one or more storage divisions at a time. A storage division, in one embodiment, is an erase block or other predefined division. For flash memory, an erase operation on an erase block writes ones to every bit in the erase block. This is a lengthy process compared to a program operation which starts with a location being all ones, and as data is written, some bits are changed to zero. However, where the solid-state storage <b>110</b> is not flash memory or has flash memory where an erase cycle takes a similar amount of time as other operations, such as a read or a program, the eviction module <b>716</b> may erase the data of a storage division as it evicts data, instead of the garbage collection module <b>714</b>.</div>
<div class="description-paragraph" id="p-0302" num="0301">In one embodiment, allowing the eviction module <b>716</b> to mark data as invalid rather than actually erasing the data and allowing the garbage collection module <b>714</b> to recover the physical media associated with invalid data, increases efficiency because, as mentioned above, for flash memory and other similar storage an erase operation takes a significant amount of time. Allowing the garbage collection module <b>714</b> to operate autonomously and opportunistically within the cache <b>102</b> provides a way to separate erase operations from reads, writes, and other faster operations so that the cache <b>102</b> operates very efficiently.</div>
<div class="description-paragraph" id="p-0303" num="0302">In one embodiment, the garbage collection module <b>714</b> is integrated with and/or works in conjunction with the destage module <b>606</b> and/or the eviction module <b>716</b>. For example, the garbage collection module <b>714</b>, in one embodiment, clears data from the cache <b>102</b> in response to an indicator that the storage device stores the data (i.e., that the destage module <b>606</b> has cleaned the data) based on a cache eviction policy (i.e., in response to the eviction module <b>716</b> evicting the data). The eviction module <b>716</b>, in one embodiment, evicts data by marking the data as invalid. In other embodiments, the eviction module <b>716</b> may evict data by erasing the data, overwriting the data, trimming the data, deallocating physical storage media associated with the data, or otherwise clearing the data from the cache <b>102</b>.</div>
<div class="description-paragraph" id="p-0304" num="0303">The eviction module <b>716</b>, in one embodiment, evicts data from the cache <b>102</b> based on a cache eviction policy. The cache eviction policy, in one embodiment, is based on a combination or a comparison of one or more cache eviction factors. In one embodiment, the cache eviction factors include wear leveling of the physical storage media <b>110</b>. In another embodiment, the cache eviction factors include a determined reliability of a section of the physical storage media <b>110</b>. In a further embodiment, the cache eviction factors include a failure of a section of the physical storage media <b>110</b>. The cache eviction factors, in one embodiment, include a least recently used (“LRU”) block of data. In another embodiment, the cache eviction factors include a frequency of access of a block of data, i.e., how “hot” or “cold” a block of data is. In one embodiment, the cache eviction factors include a position of a block of data in the physical storage media <b>110</b> relative to other “hot” data. In a further embodiment, the cache eviction factors include an amount of read data in a block, an amount of write data in a block, an amount of dirty data in a block, an amount of clean data in a block, and/or other data composition factors for a block. One of skill in the art, in light of this disclosure, will recognize other cache eviction factors suitable for use in the cache eviction policy.</div>
<div class="description-paragraph" id="p-0305" num="0304">In one embodiment, the direct mapping module <b>706</b> determines one or more of the cache eviction factors based on a history of access to the mapping structure. The direct mapping module <b>706</b>, in a further embodiment, identifies areas of high frequency, “hot,” use and/or low frequency, “cold,” use by monitoring accesses of branches or nodes in the mapping structure. The direct mapping module <b>706</b>, in a further embodiment, determines a count or frequency of access to a branch, directed edge, or node in the mapping structure. In one embodiment, a count associated with each node of ab-tree like mapping structure may be incremented for each I/O read operation and/or each I/O write operation that visits the node in a traversal of the mapping structure. Of course, separate read counts and write counts may be maintained for each node. Certain counts may be aggregated to different levels in the mapping structure in other embodiments. The eviction module <b>716</b>, in one embodiment, evicts data from the cache <b>102</b> intelligently and/or opportunistically based on activity in the mapping structure monitored by the direct mapping module <b>706</b>, based on information about the physical storage media <b>110</b>, and/or based on other cache eviction factors.</div>
<div class="description-paragraph" id="p-0306" num="0305">In a further embodiment, the eviction module <b>716</b> combines one or more cache eviction factors for a block of data or region of the cache <b>102</b> to form an eviction cost for the block, and compares the eviction costs of different blocks to determine which block to evict. In one embodiment, the eviction module <b>716</b> interfaces with the garbage collection module <b>714</b>, and sends a selected block (such as an LEB, or the like) to the garbage collection module <b>714</b> for grooming, as described above. In another embodiment, the garbage collection module <b>714</b> erases clean data that has already been destaged in a block received from the eviction module <b>716</b> without relocating the clean data, effectively evicting the clean data of the selected block from the cache <b>102</b>.</div>
<div class="description-paragraph" id="p-0307" num="0306">The direct mapping module <b>706</b>, the eviction module <b>716</b>, and/or the garbage collection module <b>714</b>, in one embodiment, share information to increase the efficiency of the cache <b>102</b>, to reduce cache misses, to make intelligent eviction decisions, and the like. In one embodiment, the direct mapping module <b>706</b> tracks or monitors a frequency that I/O requests access logical addresses in the mapping structure. The direct mapping module <b>706</b>, in a further embodiment, stores the access frequency information in the mapping structure, communicates the access frequency information to the eviction module <b>716</b> and/or to the garbage collection module <b>714</b>, or the like. The direct mapping module <b>706</b>, in another embodiment, may track, collect, or monitor other usage/access statistics relating to the logical to physical mapping of addresses for the cache <b>102</b> and/or relating to the mapping between the logical address space of the cache <b>102</b> and the address space of the backing store <b>118</b>, and may share that data with the eviction module <b>716</b> and/or with the garbage collection module <b>714</b>.</div>
<div class="description-paragraph" id="p-0308" num="0307">One example of a benefit of sharing information between the destage module <b>606</b>, the direct mapping module <b>706</b>, the eviction module <b>716</b>, and the garbage collection module <b>714</b>, in certain embodiments, is that write amplification can be reduced. As described above, in one embodiment, the garbage collection module <b>714</b> copies any valid data in an erase block forward to the current append point of the log-based append-only writing structure of the cache <b>102</b> before recovering the physical storage capacity of the erase block. By cooperating with the destage module <b>606</b>, the direct mapping module <b>706</b>, and/or with the eviction module <b>716</b>, in one embodiment, the garbage collection module <b>714</b> may clear certain valid data from an erase block without copying the data forward (for example because the replacement algorithm for the eviction module <b>716</b> indicates that the valid data is unlikely to be re-requested soon), reducing write amplification, increasing available physical storage capacity and efficiency. The garbage collection module <b>714</b> can even clear valid user write data from an erase block, so long as the destage module <b>606</b> has destaged the data to the backing store <b>118</b>.</div>
<div class="description-paragraph" id="p-0309" num="0308">For example, m one embodiment, the garbage collection module <b>714</b> preserves valid data with an access frequency in the mapping structure that is above a predefined threshold, and clears valid data from an erase block if the valid data has an access frequency below the predefined threshold. In a further embodiment, the eviction module <b>716</b> may mark certain data as conditionally evictable, conditionally invalid, or the like, and the garbage collection module <b>714</b> may evict the conditionally invalid data based on an access frequency or other data that the direct mapping module <b>706</b> provides. In another example, the destage module <b>606</b>, the direct mapping module <b>706</b>, the eviction module <b>716</b>, and the garbage collection module <b>714</b> may cooperate such that valid data that is in the cache <b>102</b> and is dirty gets stored on the backing store <b>118</b> by the destage module <b>606</b> rather than copied to the front of the log, because the eviction module <b>716</b> indicated that it is more advantageous to do so, or the like.</div>
<div class="description-paragraph" id="p-0310" num="0309">Those of skill in the art will appreciate a variety of other examples and scenarios in which the modules responsible for managing the non-volatile storage media <b>110</b> that uses a log-based append-only writing structure can leverage the information available in the direct cache module <b>116</b> <i>b</i>. Furthermore, those of skill in the art will appreciate a variety of other examples and scenarios in which the modules responsible for managing the cache <b>102</b> (destage module <b>606</b>, direct cache module <b>116</b> <i>b</i>, garbage collection module <b>714</b>, and/or eviction module <b>716</b>) can leverage the information available in solid-state controller <b>104</b> regarding the condition of the non-volatile storage media <b>110</b>.</div>
<div class="description-paragraph" id="p-0311" num="0310">In another example, the destage module <b>606</b>, the direct mapping module <b>706</b>, the eviction module <b>716</b>, and/or the garbage collection module <b>714</b>, in one embodiment, cooperate such that selection of one or more blocks of data by the eviction module <b>716</b> is influenced by the Uncorrectable Bit Error Rates (“UBER”), Correctable Bit Error Rates (“BER”), Program I Erase (“PE”) cycle counts, read frequency, or other non-volatile solid state storage specific attributes of the region of the solid-state storage media <b>110</b> in the cache <b>102</b> that presently holds the valid data. High BER, UBER, PEs may be used as factors to increase the likelihood that the eviction module <b>716</b> will evict a particular block range stored on media having those characteristics.</div>
<div class="description-paragraph" id="p-0312" num="0311">In one embodiment, the read request module <b>718</b> services read requests for data stored in the cache <b>102</b> and/or the backing store <b>118</b>. The read request module <b>718</b>, in one embodiment, detects a read request to retrieve requested data from the backing store <b>118</b>. In a further embodiment, the read request module <b>718</b> receives read requests from the host device <b>114</b>. A read request is a read command with an indicator, such as a logical address or range of logical addresses, of the data being requested. In one embodiment, the read request module <b>718</b> supports read requests with several contiguous and/or noncontiguous ranges of logical addresses, as discussed above with regard to the write request module <b>602</b>.</div>
<div class="description-paragraph" id="p-0313" num="0312">In the depicted embodiment, the read request module <b>718</b> includes a read miss module <b>728</b> and a read retrieve module <b>730</b>. The read miss module <b>728</b>, in one embodiment, determines whether or not requested data is stored in the cache <b>102</b>. The read miss module <b>728</b> may query the cache <b>102</b> directly, query the direct mapping module <b>706</b>, query the mapping structure of the direct mapping module <b>706</b>, or the like to determine whether or not requested data is stored in the cache <b>102</b>.</div>
<div class="description-paragraph" id="p-0314" num="0313">The read retrieve module <b>730</b>, in one embodiment, returns requested data to the requesting entity, such as the host device <b>114</b>. If the read miss module <b>728</b> determines that the cache <b>102</b> stores the requested data, in one embodiment, the read retrieve module <b>730</b> reads the requested data from the cache <b>102</b> and returns the data to the requesting entity. The direct mapping module <b>706</b>, in one embodiment, provides the read retrieve module <b>730</b> with one or more physical addresses of the requested data in the cache <b>102</b> by mapping one or more logical addresses of the requested data to the one or more physical addresses of the requested data.</div>
<div class="description-paragraph" id="p-0315" num="0314">If the read miss module <b>728</b> determines that the cache <b>102</b> does not store the requested data, in one embodiment, the read retrieve module <b>730</b> reads the requested data from the backing store <b>118</b>, writes the requested data to the cache <b>102</b>, and returns the requested data to the requesting entity. In one embodiment, the read retrieve module <b>730</b> writes the requested data to the cache <b>102</b> by appending the requested data to an append point of a log-based writing structure of the cache <b>102</b>. In a further embodiment, the read retrieve module <b>730</b> provides one or more physical addresses corresponding to the append point to the direct mapping module <b>706</b> with the one or more logical addresses of the requested data and the direct mapping module <b>706</b> adds and/or updates the mapping structure with the mapping of logical and physical addresses for the requested data. The read retrieve module <b>30</b>, in one embodiment, writes the requested data to the cache <b>102</b> using and/or in conjunction with the cache write module <b>604</b>.</div>
<div class="description-paragraph" id="p-0316" num="0315">In one embodiment, the read miss module <b>728</b> detects a partial miss, where the cache <b>102</b> stores one portion of the requested data but does not store another. A partial miss, in various embodiments, may be the result of eviction of the unstored data, a block I/O request for noncontiguous data, or the like. The read miss module <b>728</b>, in one embodiment, reads the missing data or “hole” data from the backing store <b>118</b> and returns both the portion of the requested data from the cache <b>102</b> and the portion of the requested data from the backing store <b>118</b> to the requesting entity. In one embodiment, the read miss module <b>728</b> stores the missing data retrieved from the backing store <b>118</b> in the cache <b>102</b>.</div>
<div class="description-paragraph" id="p-0317" num="0316">In one embodiment, the backing store interface module <b>720</b> provides an interface between the direct cache module <b>116</b> <i>b</i>, the cache <b>102</b>, and/or the backing store <b>118</b>. As described above with regard to <figref idrefs="DRAWINGS">FIG. 5</figref>, in various embodiments, the direct cache module <b>116</b> <i>b </i>interact with the cache <b>102</b> and/or the backing store <b>118</b> through a block device interface, a direct interface, a device driver on the host device <b>114</b>, a storage controller, or the like. In one embodiment, the backing store interface module <b>720</b> provides the direct cache module <b>116</b> <i>b </i>with access to one or more of these interfaces. For example, the backing store interface module <b>720</b> may receive read commands, write commands, and clear (or TRIM) commands from one or more of the cache write module <b>604</b>, the direct mapping module <b>706</b>, the read request module <b>718</b>, the destage module <b>606</b>, the garbage collection module <b>714</b>, and the like and relay the commands to the cache <b>102</b> and/or the backing store <b>118</b>. In a further embodiment, the backing store interface module <b>720</b> may translate or format a command into a format compatible with an interface for the cache <b>102</b> and/or the backing store <b>118</b>.</div>
<div class="description-paragraph" id="p-0318" num="0317">In one embodiment, the backing store interface module <b>720</b> has exclusive ownership over the backing store <b>118</b> and the direct cache module <b>116</b> <i>b </i>is an exclusive gateway to accessing the backing store <b>118</b>. Providing the backing store interface module <b>720</b> with exclusive ownership over the backing store <b>118</b> and preventing access to the backing store <b>118</b> by other routes obviates stale data issues and cache coherency requirements, because all changes to data in the backing store <b>118</b> are processed by the direct cache module <b>116</b> <i>b. </i> </div>
<div class="description-paragraph" id="p-0319" num="0318">In a further embodiment, the backing store interface module <b>720</b> does not have exclusive ownership of the backing store <b>118</b>, and the backing store interface module <b>720</b> manages cache coherency for the cache <b>102</b>. For example, in various embodiments, the backing store interface module <b>720</b> may access a common directory with other users of the backing store <b>118</b> to maintain coherency, may monitor write operations from other users of the backing store <b>118</b>, may participate in a predefined coherency protocol with other users of the backing store <b>118</b>, or the like.</div>
<div class="description-paragraph" id="p-0320" num="0319"> <figref idrefs="DRAWINGS">FIG. 8</figref> a schematic block diagram of an example of a forward map <b>804</b> and a reverse map <b>822</b> in accordance with the present invention. Typically, the direct cache module <b>116</b> detects and/or receives a storage request, such as storage request to read an address. For example, the direct cache module <b>116</b> may receive a logical block storage request <b>802</b> to start reading read address “182” and read 3 blocks. Typically the forward map <b>804</b> stores logical block addresses as virtual/logical addresses along with other virtual/logical addresses so the direct mapping module <b>706</b> uses forward map <b>804</b> to identify a physical address from the virtual/logical address “182” of the storage request <b>802</b>. In the example, for simplicity, only logical addresses that are numeric are shown, but one of skill in the art will recognize that any logical address may be used and represented in the forward map <b>804</b>. A forward map <b>804</b>, in other embodiments, may include alpha-numerical characters, hexadecimal characters, and the like. The forward map <b>804</b> is one embodiment of a mapping structure described above with regard to the direct mapping module <b>706</b>.</div>
<div class="description-paragraph" id="p-0321" num="0320">In the example, the forward map <b>804</b> is a simple B-tree. In other embodiments, the forward map <b>804</b> may be a CAM, a binary tree, a hash table, or other data structure known to those of skill in the art. In the depicted embodiment, a B-Tree includes nodes (e.g. the root node <b>808</b>) that may include entries of two logical addresses. Each entry, in one embodiment, may include a range of logical addresses. For example, a logical address may be in the form of a logical identifier with a range (e.g. offset and length) or may represent a range using a first and a last address or location. In a further embodiment, each entry may include an indicator of whether the included range of data is dirty or clean (not shown).</div>
<div class="description-paragraph" id="p-0322" num="0321">Where a single logical address or range of logical addresses is included at a particular node, such as the root node <b>808</b>, if a logical address <b>806</b> being searched is lower than the logical address or addresses of the node, the search will continue down a directed edge <b>810</b> to the left of the node <b>808</b>. If the searched logical address <b>806</b> matches the current node <b>808</b> (i.e., is located within the range identified in the node), the search stops and the pointer, link, physical address, etc. at the current node <b>808</b> is identified. If the searched logical address <b>806</b> is greater than the range of the current node <b>808</b>, the search continues down directed edge <b>812</b> to the right of the current node <b>808</b>. Where a node includes two logical addresses or ranges of logical addresses and a searched logical address <b>806</b> falls between the listed logical addresses of the node, the search continues down a center directed edge (not shown) to nodes with logical addresses that fall between the two logical addresses or ranges of logical addresses of the current node <b>808</b>. A search continues down the B-tree until either locating a desired logical address or determining that the searched logical address <b>806</b> does not exist in the B-tree. As described above, in one embodiment, membership in the B-tree denotes membership in the cache <b>102</b>, and determining that the searched logical address <b>806</b> is not in the B-tree is a cache miss.</div>
<div class="description-paragraph" id="p-0323" num="0322">In the example depicted in <figref idrefs="DRAWINGS">FIG. 8</figref>, the direct mapping module <b>706</b> searches for logical address “182” <b>806</b> starting at the root node <b>808</b>. Since the searched logical address <b>806</b> is lower than the logical address of 205-212 in the root node <b>808</b>, the direct mapping module <b>706</b> searches down the directed edge <b>810</b> to the left to the next node <b>814</b>. The searched logical address “182” <b>806</b> is greater than the logical address (072-083) stored in the next node <b>814</b> so the direct mapping module <b>706</b> searches down a directed edge <b>816</b> to the right of the node <b>814</b> to the next node <b>818</b>. In this example, the next node <b>818</b> includes a logical address of 178-192 so that the searched logical address “182” <b>806</b> matches the logical address 178-192 of this node <b>818</b> because the searched logical address “182” <b>806</b> falls within the range 178-192 of the node <b>818</b>.</div>
<div class="description-paragraph" id="p-0324" num="0323">Once the direct mapping module <b>706</b> determines a match in the forward map <b>804</b>, the direct mapping module <b>706</b> returns a physical address, either found within the node <b>818</b> or linked to the node <b>818</b>. In the depicted example, the node <b>818</b> identified by the direct mapping module <b>706</b> as containing the searched logical address <b>806</b> includes a link “f’ that maps to an entry <b>820</b> in the reverse map <b>822</b>.</div>
<div class="description-paragraph" id="p-0325" num="0324">In the depicted embodiment, for each entry <b>820</b> in the reverse map <b>822</b> (depicted as a row in a table), the reverse map <b>822</b> includes an entry ID <b>824</b>, a physical address <b>826</b>, a data length <b>828</b> associated with the data stored at the physical address <b>826</b> on the solid-state storage media <b>110</b> (in this case the data is compressed), a valid tag <b>830</b>, a logical address <b>832</b> (optional), a data length <b>834</b> (optional) associated with the logical address <b>832</b>, and other miscellaneous data <b>836</b>. In a further embodiment, the reverse map <b>822</b> may include an indicator of whether the physical address <b>826</b> stores dirty or clean data, or the like. The reverse map <b>822</b> is organized into erase blocks (erase regions). In this example, the entry <b>820</b> that corresponds to the selected node <b>818</b> is located in erase block n <b>838</b>. Erase block n <b>838</b> is preceded by erase block n−1 <b>840</b> and followed by erase block n+1 <b>842</b> (the contents of erase blocks n−1 and n+1 are not shown). An erase block may be some erase region that includes a predetermined number of pages. An erase region is an area in the solid-state storage media <b>110</b> erased together in a storage recovery operation.</div>
<div class="description-paragraph" id="p-0326" num="0325">While the entry ID <b>824</b> is shown as being part of the reverse map <b>822</b>, the entry ID) <b>824</b> may be an address, a virtual link, or other means to tie an entry in the reverse map <b>822</b> to a node in the forward map <b>804</b>. The physical address <b>826</b> is an address in the solid-state storage media <b>110</b> where data that corresponds to the searched logical address <b>806</b> resides. The data length <b>828</b> associated with the physical address <b>826</b> identifies a length of the data packet stored at the physical address <b>826</b>. (Together the physical address <b>826</b> and data length <b>828</b> may be called destination parameters <b>844</b> and the logical address <b>832</b> and associated data length <b>834</b> may be called source parameters <b>846</b> for convenience.) In the example, the data length <b>828</b> of the destination parameters <b>844</b> is different from the data length <b>834</b> of the source parameters <b>846</b> in one embodiment compression the data packet stored on the solid-state storage media <b>110</b> was compressed prior to storage. For the data associated with the entry <b>820</b>, the data was highly compressible and was compressed from 64 blocks t 1 block.</div>
<div class="description-paragraph" id="p-0327" num="0326">The valid tag <b>830</b> indicates if the data mapped to the entry <b>820</b> is valid or not. In this case, the data associated with the entry <b>820</b> is valid and is depicted in <figref idrefs="DRAWINGS">FIG. 8</figref> as a “Y” in the row of the entry <b>820</b>. Typically the reverse map <b>822</b> tracks both valid and invalid data and the forward map <b>804</b> tracks valid data. In the example, entry “c” <b>848</b> indicates that data associated with the entry <b>848</b> is invalid. Note that the forward map <b>804</b> does not include logical addresses associated with entry “c” <b>848</b>. The reverse map <b>822</b> typically maintains entries for invalid data so that valid and invalid data can be quickly distinguished during a storage recovery operation. In certain embodiments, the forward map <b>804</b> and/or the reverse map <b>822</b> may track dirty and clean data in a similar manner to distinguish dirty data from clean data.</div>
<div class="description-paragraph" id="p-0328" num="0327">The depicted reverse map <b>822</b> includes source parameters <b>846</b> for convenience, but the reverse map <b>822</b> may or may not include the source parameters <b>846</b>. For example, if the source parameters <b>846</b> are stored with the data, possibly in a header of the stored data, the reverse map <b>822</b> could identify a logical address indirectly by including a physical address <b>826</b> associated with the data and the source parameters <b>846</b> could be identified from the stored data. One of skill in the art will recognize when storing source parameters <b>846</b> in a reverse map <b>822</b> would be beneficial.</div>
<div class="description-paragraph" id="p-0329" num="0328">The reverse map <b>822</b> may also include other miscellaneous data <b>836</b>, such as a file name, object name, source data, etc. One of skill in the art will recognize other information useful in a reverse map <b>822</b>. While physical addresses <b>826</b> are depicted in the reverse map <b>822</b>, in other embodiments, physical addresses <b>826</b>, or other destination parameters <b>844</b>, may be included in other locations, such as in the forward map <b>804</b>, an intermediate table or data structure, etc.</div>
<div class="description-paragraph" id="p-0330" num="0329">Typically, the reverse map <b>822</b> is arranged by erase block or erase region so that traversing a section of the map associated with an erase block (e.g. erase block n <b>838</b>) allows the garbage collection module <b>714</b> to identify valid data in the erase block <b>838</b> and to quantify an amount of valid data, or conversely invalid data, in the erase block <b>838</b>. Similarly, the destage module <b>606</b>, in certain embodiments, may traverse the reverse map <b>822</b> and/or the forward map <b>804</b> to locate dirty data for destaging, to quantify an amount of dirty data and/or clean data, or the like. Arranging an index into a forward map <b>804</b> that can be quickly searched to identify a physical address <b>826</b> from a logical address <b>806</b> and a reverse map <b>822</b> that can be quickly searched to identify valid data and quantity of valid data (and/or dirty data) in an erase block <b>838</b> is beneficial because the index may be optimized for searches, storage recovery, and/or destaging operations. One of skill in the art will recognize other benefits of an index with a forward map <b>804</b> and a reverse map <b>822</b>.</div>
<div class="description-paragraph" id="p-0331" num="0330"> <figref idrefs="DRAWINGS">FIG. 9</figref> depicts one embodiment of a mapping structure <b>900</b>, a logical address space <b>920</b> of the cache <b>102</b>, a combined logical address space <b>919</b> that is accessible to a storage client, a sequential, log-based, append-only writing structure <b>940</b>, and a storage device address space <b>970</b> of the backing store <b>118</b>. The mapping structure <b>900</b>, in one embodiment, is maintained by the direct mapping module <b>706</b>. The mapping structure <b>900</b>, in the depicted embodiment, is a B-tree that is substantially similar to the forward map <b>804</b> described above with regard to <figref idrefs="DRAWINGS">FIG. 8</figref>, with several additional entries. Further, instead of links that map to entries in a reverse map <b>822</b>, the nodes of the mapping structure <b>900</b> include direct references to physical locations in the cache <b>102</b>. The mapping structure <b>900</b>, in various embodiments, may be used either with or without a reverse map <b>822</b>. As described above with regard to the forward map <b>804</b> of <figref idrefs="DRAWINGS">FIG. 8</figref>, in other embodiments, the references in the mapping structure <b>900</b> may include alpha-numerical characters, hexadecimal characters, pointers, links, and the like.</div>
<div class="description-paragraph" id="p-0332" num="0331">The mapping structure <b>900</b>, in the depicted embodiment, includes a plurality of nodes. Each node, in the depicted embodiment, is capable of storing two entries. In other embodiments, each node may be capable of storing a greater number of entries, the number of entries at each level may change as the mapping structure <b>900</b> grows or shrinks through use, or the like. In a further embodiment, each entry may store one or more indicators of whether the data corresponding to the entry is clean or dirty, valid or invalid, read data or write data, or the like.</div>
<div class="description-paragraph" id="p-0333" num="0332">Each entry, in the depicted embodiment, maps a variable length range of logical addresses of the cache <b>102</b> to a physical location in the storage media <b>110</b> for the cache <b>102</b>. Further, while variable length ranges of logical addresses, in the depicted embodiment, are represented by a starting address and an ending address, in other embodiments, a variable length range of addresses may be represented by a starting address and a length or by another representation. In one embodiment, the capital letters ‘A’ through ‘M’ represent a logical or physical erase block in the physical storage media <b>110</b> of the cache <b>102</b> that stores the data of the corresponding range of logical addresses. In other embodiments, the capital letters may represent other physical addresses or locations of the cache <b>102</b>. In the depicted embodiment, the capital letters ‘A’ through ‘M’ are also depicted in the writing structure <b>940</b> which represents the physical storage media <b>110</b> of the cache <b>102</b>. Although each range of logical addresses maps simply to an entire erase block, in the depicted embodiment, for simplicity of description, in other embodiments, a single erase block may store a plurality of ranges of logical addresses, ranges of logical addresses may cross erase block boundaries, and the like.</div>
<div class="description-paragraph" id="p-0334" num="0333">In the depicted embodiment, membership in the mapping structure <b>900</b> denotes membership (or storage) in the cache <b>102</b>. In another embodiment, an entry may further include an indicator of whether the cache <b>102</b> stores data corresponding to a logical block within the range of logical addresses, data of the reverse map <b>822</b> described above, and/or other data. For example, in one embodiment, the mapping structure <b>900</b> may also map logical addresses of the backing store <b>118</b> to physical addresses or locations within the backing store <b>118</b>, and an entry may include an indicator that the cache <b>102</b> does not store the data and a physical address or location for the data on the backing store <b>118</b>. The mapping structure <b>900</b>, in the depicted embodiment, is accessed and traversed in a similar manner as that described above with regard to the forward map <b>804</b>.</div>
<div class="description-paragraph" id="p-0335" num="0334">In the depicted embodiment, the root node <b>808</b> includes entries <b>902</b>, <b>904</b> with noncontiguous ranges of logical addresses. A “hole” exists at logical address “208” between the two entries <b>902</b>, <b>904</b> of the root node. In one embodiment, a “hole” indicates that the cache <b>102</b> does not store data corresponding to one or more logical addresses corresponding to the “hole.” In one embodiment, a “hole” may exist because the eviction module <b>716</b> evicted data corresponding to the “hole” from the cache <b>102</b>. If the eviction module <b>716</b> evicted data corresponding to a “hole,” in one embodiment, the backing store <b>118</b> still stores data corresponding to the “hole.” In another embodiment, the cache <b>102</b> and/or the backing store <b>118</b> supports block I/O requests (read, write, trim, etc.) with multiple contiguous and/or noncontiguous ranges of addresses (i.e., ranges that include one or more “holes” in them). A “hole,” in one embodiment, may be the result of a single block I/O request with two or more noncontiguous ranges of addresses. In a further embodiment, a “hole” may be the result of several different block I/O requests with address ranges bordering the “hole.”</div>
<div class="description-paragraph" id="p-0336" num="0335">In <figref idrefs="DRAWINGS">FIG. 8</figref>, the root node <b>808</b> includes a single entry with a logical address range of “205-212,” without the hole at logical address “208.” If the entry of the root node <b>908</b> were a fixed size cache line of a traditional cache, the entire range of logical addresses “205-212” would be evicted together. Instead, in the embodiment depicted in <figref idrefs="DRAWINGS">FIG. 9</figref>, the eviction module <b>716</b> evicts data of a single logical address “208” and splits the range of logical addresses into two separate entries <b>902</b>, <b>904</b>. In one embodiment, the direct mapping module <b>706</b> may rebalance the mapping structure <b>900</b>, adjust the location of a directed edge, root node, or child node, or the like in response to splitting a range of logical addresses. Similarly, in one embodiment, each range of logical addresses may have a dynamic and/or variable length, allowing the cache <b>102</b> to store dynamically selected and/or variable lengths of logical block ranges.</div>
<div class="description-paragraph" id="p-0337" num="0336">In the depicted embodiment, similar “holes” or noncontiguous ranges of logical addresses exist between the entries <b>906</b>, <b>908</b> of the node <b>814</b>, between the entries <b>910</b>, <b>912</b> of the left child node of the node <b>814</b>, between entries <b>914</b>, <b>916</b> of the node <b>818</b>, and between entries of the node <b>918</b>. In one embodiment, similar “holes” may also exist between entries in parent nodes and child nodes. For example, in the depicted embodiment, a “hole” of logical addresses “060-071” exists between the left entry <b>906</b> of the node <b>814</b> and the right entry <b>912</b> of the left child node of the node <b>814</b>.</div>
<div class="description-paragraph" id="p-0338" num="0337">The “hole” at logical address “003,” in the depicted embodiment, can also be seen in the logical address space <b>920</b> of the cache <b>102</b> at logical address “003” <b>930</b>. The hash marks at logical address “003” <b>940</b> represent an empty location, or a location for which the cache <b>102</b> does not store data. In the depicted embodiment, storage device address “003” <b>980</b> of the storage device address space <b>970</b> does store data (identified as ‘b’), indicating that the eviction module <b>716</b> evicted data from logical address “003” <b>930</b> of the cache <b>102</b>. The “hole” at logical address <b>934</b> in the logical address space <b>920</b>, however, has no corresponding data in storage device address <b>984</b>, indicating that the “hole” is due to one or more block I/O requests with noncontiguous ranges, a trim or other deallocation command to both the cache <b>102</b> and the backing store <b>118</b>, or the like.</div>
<div class="description-paragraph" id="p-0339" num="0338">The “hole” at logical address “003” <b>930</b> of the logical address space <b>920</b>, however, in one embodiment, is not viewable or detectable to a storage client. In the depicted embodiment, the combined logical address space <b>919</b> represents the data that is available to a storage client, with data that is stored in the cache <b>102</b> and data that is stored in the backing store <b>118</b> but not in the cache <b>102</b>. As described above, the read miss module <b>728</b> of <figref idrefs="DRAWINGS">FIG. 7</figref> handles misses and returns requested data to a requesting entity. In the depicted embodiment, if a storage client requests data at logical address “003” <b>930</b>, the read miss module <b>728</b> will retrieve the data from the backing store <b>118</b>, as depicted at address “003” <b>980</b> of the storage device address space <b>970</b>, and return the requested data to the storage client. The requested data at logical address “003” <b>930</b> may then also be placed back in the cache <b>102</b> and thus logical address <b>930</b> would indicate ‘b’ as present in the cache <b>102</b>.</div>
<div class="description-paragraph" id="p-0340" num="0339">For a partial miss, the read miss module <b>728</b> may return a combination of data from both the cache <b>102</b> and the backing store <b>118</b>. For this reason, the combined logical address space <b>919</b> includes data ‘b’ at logical address “003” <b>930</b> and the “hole” in the logical address space <b>920</b> of the cache <b>102</b> is transparent. In the depicted embodiment, the combined logical address space <b>919</b> is the size of the logical address space <b>920</b> of the cache <b>102</b> and is larger than the storage device address space <b>980</b>. In another embodiment, the direct cache module <b>116</b> may size the combined logical address space <b>919</b> as the size of the storage device address space <b>980</b>, or as another size.</div>
<div class="description-paragraph" id="p-0341" num="0340">The logical address space <b>920</b> of the cache <b>102</b>, in the depicted embodiment, is larger than the physical storage capacity and corresponding storage device address space <b>970</b> of the backing store <b>118</b>. In the depicted embodiment, the cache <b>102</b> has a 64 bit logical address space <b>920</b> beginning at logical address “O” <b>922</b> and extending to logical address “264-1” <b>926</b>. The storage device address space <b>970</b> begins at storage device address “O” <b>972</b> and extends to storage device address “N” <b>974</b>. Storage device address “N” <b>974</b>, in the depicted embodiment, corresponds to logical address “N” <b>924</b> in the logical address space <b>920</b> of the cache <b>102</b>. Because the storage device address space <b>970</b> corresponds to only a subset of the logical address space <b>920</b> of the cache <b>102</b>, the rest of the logical address space <b>920</b> may be shared with an additional cache <b>102</b>, may be mapped to a different backing store <b>118</b>, may store data in the cache <b>102</b> (such as a Non-volatile memory cache) that is not stored in the storage device <b>970</b>, or the like.</div>
<div class="description-paragraph" id="p-0342" num="0341">For example, in the depicted embodiment, the first range of logical addresses “000-002” <b>928</b> stores data corresponding to the first range of storage device addresses “000-002” <b>978</b>. Data corresponding to logical address “003” <b>930</b>, as described above, was evicted from the cache <b>102</b> forming a “hole” and a potential cache miss. The second range of logical addresses “004-059” <b>932</b> corresponds to the second range of storage device addresses “004-059” <b>982</b>. However, the final range of logical addresses <b>936</b> extending from logical address “N” <b>924</b> extends beyond storage device address “N” <b>974</b>. No storage device address in the storage device address space <b>970</b> corresponds to the final range of logical addresses <b>936</b>. The cache <b>102</b> may store the data corresponding to the final range of logical addresses <b>936</b> until the data backing store <b>118</b> is replaced with larger storage or is expanded logically, until an additional data backing store <b>118</b> is added, simply use the non-volatile storage capability of the cache <b>102</b> to indefinitely provide storage capacity directly to a storage client <b>504</b> independent of a backing store <b>118</b>, or the like. In a further embodiment, the direct cache module <b>116</b> alerts a storage client <b>504</b>, an operating system, a user application <b>502</b>, or the like in response to detecting a write request with a range of addresses, such as the final range of logical addresses <b>936</b>, that extends beyond the storage device address space <b>970</b>. The user may then perform some maintenance or other remedial operation to address the situation. Depending on the nature of the data, no further action may be taken. For example, the data may represent temporary data which if lost would cause no ill effects.</div>
<div class="description-paragraph" id="p-0343" num="0342">The sequential, log-based, append-only writing structure <b>940</b>, in the depicted embodiment, is a logical representation of the log preserved in the physical storage media <b>110</b> of the cache <b>102</b>. In a further embodiment, the backing store <b>118</b> may use a substantially similar sequential, log-based, append-only writing structure <b>940</b>. In certain embodiments, the cache <b>102</b> stores data sequentially, appending data to the writing structure <b>940</b> at an append point <b>944</b>. The cache <b>102</b>, in a further embodiment, uses a storage space recovery process, such as the garbage collection module <b>714</b> that re-uses non-volatile storage media <b>110</b> storing deallocated, unused, or evicted logical blocks. Non-volatile storage media <b>110</b> storing deallocated, unused, or evicted logical blocks, in the depicted embodiment, is added to an available storage pool <b>946</b> for the cache <b>102</b>. By evicting and clearing certain data from the cache <b>102</b>, as described above, and adding the physical storage capacity corresponding to the evicted and/or cleared data back to the available storage pool <b>946</b>, in one embodiment, the writing structure <b>940</b> is ring-like and has a theoretically infinite capacity.</div>
<div class="description-paragraph" id="p-0344" num="0343">In the depicted embodiment, the append point <b>944</b> progresses around the log-based, append-only writing structure <b>940</b> in a circular pattern <b>942</b>. In one embodiment, the circular pattern <b>942</b> wear balances the solid-state storage media <b>110</b>, increasing a usable life of the solid-state storage media <b>110</b>. In the depicted embodiment, the eviction module <b>716</b> and/or the garbage collection module <b>714</b> have marked several blocks <b>948</b>, <b>950</b>, <b>952</b>, <b>954</b> as invalid, represented by an “X” marking on the blocks <b>948</b>, <b>950</b>, <b>952</b>, <b>954</b>. The garbage collection module <b>714</b>, in one embodiment, will recover the physical storage capacity of the invalid blocks <b>948</b>, <b>950</b>, <b>952</b>, <b>954</b> and add the recovered capacity to the available storage pool <b>946</b>. In the depicted embodiment, modified versions of the blocks <b>948</b>, <b>950</b>, <b>952</b>, <b>954</b> have been appended to the writing structure <b>940</b> as new blocks <b>956</b>, <b>958</b>, <b>960</b>, <b>962</b> in a read, modify, write operation or the like, allowing the original blocks <b>948</b>, <b>950</b>, <b>952</b>, <b>954</b> to be recovered. In further embodiments, the garbage collection module <b>714</b> may copy forward to the append point <b>944</b> any dirty data and selectively any valid data that the blocks <b>948</b>, <b>950</b>, <b>952</b>, <b>954</b> store, if any.</div>
<div class="description-paragraph" id="p-0345" num="0344"> <figref idrefs="DRAWINGS">FIG. 10</figref> depicts one embodiment of address order destaging using a mapping structure <b>1000</b> and log order destaging using a log <b>1020</b> in accordance with the present invention. As described above, in various embodiments, the destage module <b>606</b> may destage data from the cache <b>102</b> to the backing store <b>118</b> in cache log order using the log order module <b>722</b>, in backing store address order using the address order module <b>724</b>, or both.</div>
<div class="description-paragraph" id="p-0346" num="0345">In one embodiment, the address order module <b>724</b> traverses the mapping structure <b>1000</b> to locate one or more ranges of backing store addresses for destaging. As described above with regard to the mapping structure <b>900</b> of <figref idrefs="DRAWINGS">FIG. 9</figref>, the logical addresses of the cache <b>102</b> stored in the mapping structure <b>1000</b>, in the depicted embodiment, are also the backing store addresses of the backing store <b>118</b>. Destaging data in logical address order based on ranges of logical addresses in the mapping structure <b>1000</b>, in certain embodiments, is also destaging in backing store address order, as the logical addresses are directly mapped to backing store addresses. In another embodiment where backing store addresses are not directly mapped to logical addresses of the cache <b>102</b>, the mapping structure <b>1000</b> may map backing store addresses to logical and/or physical addresses of the cache <b>102</b>.</div>
<div class="description-paragraph" id="p-0347" num="0346">The address order module <b>724</b>, in one embodiment, locates ranges of backing store addresses for destaging that correspond to dirty write data that has not yet been destaged to the backing store <b>118</b>. Entries of the mapping structure <b>1000</b>, in one embodiment, include indicators of whether the data corresponding to the entry is clean or dirty. In a further embodiment, the address order module <b>724</b> may reference a reverse map, a data structure of the dirty indicator module <b>712</b>, or the like to determine whether data corresponding to an entry in the mapping structure <b>1000</b> is clean or dirty. In one embodiment, the direct mapping module <b>706</b> may maintain separate mapping structures for dirty data and for clean data, and the mapping structure <b>1000</b> may include just entries for dirty data.</div>
<div class="description-paragraph" id="p-0348" num="0347">In one embodiment, the address order module <b>724</b> selects one or more ranges of backing store addresses for destaging that include at least a predefined sequential threshold number of contiguous, sequential backing store addresses. For example, in the depicted embodiment, entries <b>1002</b>, <b>1004</b>, <b>1006</b>, <b>1008</b>, <b>1010</b>, <b>1012</b>, and <b>1014</b> each include a range of five or more sequential, contiguous, backing store addresses, and would satisfy a predefined sequential threshold of five.</div>
<div class="description-paragraph" id="p-0349" num="0348">In a further embodiment, the address order module <b>724</b> may combine entries to form a range of backing store addresses with one or more holes. For example, in certain embodiments, the address order module <b>724</b> may combine adjacent entries <b>1018</b> with backing store addresses “000-002” and “004-059” and/or may combine adjacent entries <b>1019</b> with backing store addresses “205-207” and “209-212” for destaging, even with holes at backing store address “003” and backing store address “208.” In other embodiments, a hole may comprise multiple backing store addresses. In one embodiment, the address order module <b>724</b> combines ranges of backing store addresses that together include at least a predefined sequential threshold amount of sequential backing store addresses, even if the ranges are not contiguous.</div>
<div class="description-paragraph" id="p-0350" num="0349">The address order module <b>724</b>, in one embodiment, may traverse the mapping structure <b>1000</b> in backing store address order and destage ranges of data in backing store address order. For example, in the depicted embodiment, with a predefined sequential threshold amount of five and without combining ranges, the address order module <b>724</b> may destage entry “004-059” <b>1002</b>, destage entry “072-076” <b>1004</b>, destage entry “186-192” <b>1006</b>, destage entry “213-250” <b>1008</b>, destage entry “291-347” <b>1010</b>, destage entry “257-280” <b>1012</b>, destage entry “454-477” <b>1014</b>, and destage entry “535-598” <b>1016</b> in numerical address order, provided the entries <b>1002</b>, <b>1004</b>, <b>1006</b>, <b>1008</b>, <b>1010</b>, <b>1012</b>, <b>1014</b>, <b>1016</b> each comprise dirty write data.</div>
<div class="description-paragraph" id="p-0351" num="0350">In another embodiment, the address order module <b>724</b> may traverse the mapping structure <b>1000</b> in a different order, such as by level in the B-tree of the mapping structure <b>1000</b>, from top to bottom, bottom to top, or the like. In a further embodiment, the address order module <b>724</b> may traverse the mapping structure <b>1000</b> based on one or more statistics of the mapping structure <b>1000</b>. For example, in one embodiment, the direct mapping module <b>706</b> may maintain access statistics for each entry, node, and/or branch of the mapping structure <b>1000</b>, and the address order module <b>724</b> may begin destaging ranges of backing store addresses that are least accessed or “coldest,” and proceed with destaging in a least accessed order, or the like. In another embodiment, the address order module <b>724</b> may select ranges of backing store addresses for destaging from the largest range toward the smallest range, destaging larger ranges of backing store addresses first.</div>
<div class="description-paragraph" id="p-0352" num="0351">In one embodiment, the log order module <b>722</b> destages in cache log order along the log <b>1020</b>. In the depicted embodiment, the log order module <b>722</b> maintains a pointer <b>1022</b> indicating a position in the log <b>1020</b> at which the log order module <b>722</b> is currently destaging data. The log order module <b>722</b>, in the depicted embodiment, moves the pointer <b>1022</b> along the log <b>1020</b> in a logically circular pattern <b>1024</b>, moving from the tail of the log <b>1020</b> (i.e., the oldest data) toward the head of the log <b>1020</b> (i.e. the newest data). The log order module <b>722</b>, in one embodiment, skips data, such as the depicted block “A,” that has been invalidated by a subsequent write, such as the depicted block “A′,” by being copied forward on the log <b>1020</b>, or the like. In one embodiment, the recent data module <b>708</b> prevents the log order module <b>722</b> and/or the address order module <b>724</b> from destaging data within a predefined distance <b>1026</b> of the append point <b>944</b>.</div>
<div class="description-paragraph" id="p-0353" num="0352">The log order module <b>722</b>, in various embodiments, may determine what data in a block is dirty using a reverse map, a data structure of the dirty indicator module <b>712</b>, data stored with each block of the log <b>1020</b>, or the like to determine what data of each region or block of the log <b>1020</b> is dirty. In one embodiment, the cache <b>102</b> may include one or more separate append points <b>944</b> and/or separate logs <b>1020</b> for dirty data, and the log <b>1020</b> may include just dirty data for destaging, or the like. In certain embodiments, as the log order module <b>722</b> selects regions or blocks of the log <b>1020</b> for destaging, the re-order module <b>726</b> re-orders data within the region or block into backing store address order for destaging.</div>
<div class="description-paragraph" id="p-0354" num="0353"> <figref idrefs="DRAWINGS">FIG. 11</figref> depicts one embodiment of a method <b>1100</b> for destaging cached data in accordance with the present invention. The method <b>1100</b> begins, and the cache write module <b>604</b> caches <b>1102</b> data corresponding to the write request in the cache <b>102</b>. The destage module <b>606</b>, in the depicted embodiment, de stages <b>1104</b> data, such as dirty write data or the like, from the cache <b>102</b> to the backing store <b>118</b> in cache log order. The write request module <b>602</b>, in the depicted embodiment, continues to detect <b>1102</b> write requests.</div>
<div class="description-paragraph" id="p-0355" num="0354"> <figref idrefs="DRAWINGS">FIG. 12</figref> depicts another embodiment of a method <b>1200</b> for destaging cached data in accordance with the present invention. The method <b>1200</b> begins, and the write request module <b>602</b> detects <b>1202</b> whether or not a write request has been made to store data on the backing store <b>118</b>. If the write request module <b>602</b> does not detect <b>1202</b> a write request, the write request module <b>602</b> continues to detect <b>1202</b> write requests.</div>
<div class="description-paragraph" id="p-0356" num="0355">In the depicted embodiment, if the write request module <b>602</b> detects <b>1202</b> a write request, the cache write module <b>604</b> caches <b>1204</b> data corresponding to the write request in the cache <b>102</b>. The destaging pressure module <b>702</b>, in the depicted embodiment, determines <b>1206</b> a destaging pressure for the cache <b>102</b>. As described above with regard to the destaging pressure module <b>702</b> of <figref idrefs="DRAWINGS">FIG. 7</figref>, a de staging pressure is a level of demand for destaging cached data of the cache <b>102</b>. The destaging rate module <b>704</b>, in the depicted embodiment, determines <b>1208</b> whether the destaging pressure satisfies a predefined destaging pressure criteria. If the destaging rate module <b>704</b> determines <b>1208</b> that the destaging pressure does not satisfy the predefined destaging pressure criteria, the log order module <b>722</b> destages <b>1210</b> at least a portion of dirty write data in the cache <b>102</b> in a cache log order. If the destaging rate module <b>704</b> determines <b>1208</b> that the destaging pressure does satisfy the predefined destaging pressure criteria, the address order module <b>724</b> destages <b>1212</b> at least a portion of dirty write data in the cache <b>102</b> in a backing store address order.</div>
<div class="description-paragraph" id="p-0357" num="0356">The recent data module <b>708</b>, in the depicted embodiment, prevents <b>1214</b> the log order module <b>722</b> and the address order module <b>724</b> from destaging data near or within a predefined distance of an append point of the log. The write request module <b>602</b>, in the depicted embodiment, continues to detect <b>1202</b> write requests.</div>
<div class="description-paragraph" id="p-0358" num="0357"> <figref idrefs="DRAWINGS">FIG. 13</figref> depicts one embodiment of a method <b>1300</b> for log order destaging in accordance with the present invention. In the depicted embodiment, the method <b>1300</b> begins, and the direct cache module <b>116</b>, also referred to as a cache controller, makes a log order destaging request <b>1302</b> to the storage controller <b>104</b> of the cache <b>102</b>. In response to the log order destaging request <b>1302</b>, the storage controller <b>104</b> finds <b>1304</b> an oldest region in a log of the cache <b>102</b> that has dirty data. The storage controller <b>104</b>, in the depicted embodiment, copies <b>1306</b> the dirty data from the oldest region in the log to a buffer. In one embodiment, the storage controller <b>104</b> may copy <b>1306</b> the dirty data in backing store address order, re-ordering the dirty data if necessary. In certain embodiments, the storage controller <b>104</b> may lock the selected region, mark the selected region as not groomable, or the like. In a further embodiment, the storage controller <b>104</b> may allow the selected region to be groomed during the destaging process, since the storage controller <b>104</b> copied the dirty data to the buffer for destaging and the dirty data remains accessible to the direct cache module <b>116</b> in the buffer.</div>
<div class="description-paragraph" id="p-0359" num="0358">The direct cache module <b>116</b>, in the depicted embodiment, destages <b>1308</b> the dirty data of the oldest region with dirty data from the buffer to the backing store <b>118</b>. The direct cache module <b>116</b>, in one embodiment, destages <b>1308</b> or writes the dirty data of the oldest region to the backing store <b>118</b> in cache log order, the order in which data was stored to the region. In a further embodiment, the direct cache module <b>116</b> may destage <b>1308</b> or write the dirty data of the oldest region to the backing store <b>118</b> re-ordered in a backing store address order.</div>
<div class="description-paragraph" id="p-0360" num="0359">In one embodiment, the destaging <b>1308</b> is canceled or stopped in response to receiving a write request writing data to addresses selected for destaging. In a further embodiment, the destaging <b>1308</b> may continue in response to an invalidating write request and the new write data is subsequently destaged <b>1308</b> to overwrite the invalid data. In response to the direct cache module <b>116</b> completing the destaging <b>1308</b>, the storage controller <b>104</b> marks <b>1310</b> the destaged data as clean in the cache <b>102</b>, unless the data has been invalidated by a subsequent write, or the like. In certain embodiments, if a grooming process of the storage controller <b>104</b> copied the data forward during the destaging <b>1308</b>, the storage controller <b>104</b> marks the data clean in the data's new location. In the depicted embodiment, the method <b>1300</b> continues and the direct cache module <b>116</b> requests <b>1302</b> additional log order destaging.</div>
<div class="description-paragraph" id="p-0361" num="0360"> <figref idrefs="DRAWINGS">FIG. 14</figref> depicts one embodiment of a method <b>1400</b> for address order destaging in accordance with the present invention. In the depicted embodiment, the direct cache module <b>116</b>, or cache controller, sends <b>1402</b> a request for backing store address order destaging to the storage controller <b>104</b> of the cache <b>102</b>. The storage controller <b>104</b>, in the depicted embodiment, finds <b>1404</b> a range of dirty data in backing store address order in a mapping structure of the cache <b>102</b> and sends the range of dirty data (or associated addresses) to the direct cache module <b>116</b>, to data storage accessible to the direct cache module <b>116</b>, or the like.</div>
<div class="description-paragraph" id="p-0362" num="0361">The direct cache module <b>116</b>, in the depicted embodiment, destages <b>1406</b> the dirty data of the selected range to the backing store <b>118</b>. In certain embodiments, the storage controller <b>104</b> may queue any invalidating write requests that overlap the selected range of dirty data until the direct cache module <b>116</b> completes the destaging <b>1406</b>, or the like. The storage controller <b>104</b>, in the depicted embodiment, marks <b>1408</b> the destaged data as clean in the cache <b>102</b>. In certain embodiments, if a grooming process of the storage controller <b>104</b> copied the data forward during the destaging <b>1406</b>, the storage controller <b>104</b> marks the data clean in the data's new location. In the depicted embodiment, the method <b>1400</b> continues and the direct cache module <b>116</b> requests <b>1402</b> additional backing store address order destaging.</div>
<div class="description-paragraph" id="p-0363" num="0362">The present invention may be embodied in other specific forms without departing from its spirit or essential characteristics. The described embodiments are to be considered in all respects only as illustrative and not restrictive. The scope of the invention is, therefore, indicated by the appended claims rather than by the foregoing description. All changes which come within the meaning and range of equivalency of the claims are to be embraced within their scope.</div>
</div>
</div>
</section><section itemprop="claims" itemscope="">
<h2>Claims (<span itemprop="count">19</span>)</h2>
<div html="" itemprop="content"><div class="claims" lang="EN" load-source="patent-office" mxw-id="PCLM191010497">
<claim-statement>What is claimed is:</claim-statement>
<div class="claim"> <div class="claim" id="CLM-00001" num="00001">
<div class="claim-text">1. A method for destaging cached data, the method comprising:
<div class="claim-text">caching data corresponding to write requests pertaining to a backing store, the data written to cache storage as dirty cache data, the cache storage comprising an ordered log maintained on a nonvolatile storage medium; and</div>
<div class="claim-text">performing destage operations to destage cache to the backing store, wherein performing a destage operation comprises:
<div class="claim-text">determining a destage metric for the cache storage, the destage metric corresponding to an amount of dirty cache data within the cache storage;</div>
<div class="claim-text">using a first criterion to select data from the cache storage for the destage operation in response to the destage metric exceeding a threshold, the first criterion corresponding to a sequential backing store address order; and</div>
<div class="claim-text">using a second criterion, different from the first criterion, to select the data from the cache storage for the destage operation in response to the destage metric failing to exceed the threshold, the second criterion corresponding to a log order.</div>
</div>
</div>
</div>
</div> <div class="claim-dependent"> <div class="claim" id="CLM-00002" num="00002">
<div class="claim-text">2. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein determining the destage metric comprises maintaining a tally of the amount of dirty cache data currently stored within the cache storage.</div>
</div>
</div> <div class="claim-dependent"> <div class="claim" id="CLM-00003" num="00003">
<div class="claim-text">3. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein performing the destage operations further comprises:
<div class="claim-text">adjusting a destaging rate of the destage operations based on the destage metric.</div>
</div>
</div>
</div> <div class="claim-dependent"> <div class="claim" id="CLM-00004" num="00004">
<div class="claim-text">4. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein:
<div class="claim-text">the first criterion corresponds to a sequential backing store address order, such that data are selected from the cache storage based on backing store addresses associated with the data.</div>
</div>
</div>
</div> <div class="claim-dependent"> <div class="claim" id="CLM-00005" num="00005">
<div class="claim-text">5. The method of <claim-ref idref="CLM-00004">claim 4</claim-ref>, wherein selecting data using the first criterion comprises traversing a mapping structure that maps backing store addresses to locations on physical storage media of the nonvolatile storage medium and selecting a destage data range such that members of the data range have contiguous and sequential backing store addresses.</div>
</div>
</div> <div class="claim-dependent"> <div class="claim" id="CLM-00006" num="00006">
<div class="claim-text">6. The method of <claim-ref idref="CLM-00004">claim 4</claim-ref>, wherein:
<div class="claim-text">each backing store address directly maps to a logical address of a logical address space associated with the nonvolatile storage medium; and</div>
<div class="claim-text">logical addresses of the cache storage are independent of physical storage media addresses of the nonvolatile storage medium.</div>
</div>
</div>
</div> <div class="claim-dependent"> <div class="claim" id="CLM-00007" num="00007">
<div class="claim-text">7. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein a destage rate for destage operations using the first criterion is higher than a destage rate for destage operations using the second criterion.</div>
</div>
</div> <div class="claim-dependent"> <div class="claim" id="CLM-00008" num="00008">
<div class="claim-text">8. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, further comprising preventing destaging of data within a predefined distance of an append point of the ordered log of the nonvolatile solid-state cache.</div>
</div>
</div> <div class="claim-dependent"> <div class="claim" id="CLM-00009" num="00009">
<div class="claim-text">9. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein determining the destage metric comprises determining a difference between an actual amount of dirty cache data within the cache storage and a target amount of dirty cache data.</div>
</div>
</div> <div class="claim-dependent"> <div class="claim" id="CLM-00010" num="00010">
<div class="claim-text">10. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein:
<div class="claim-text">the second criterion corresponds to a log order; and</div>
<div class="claim-text">selecting data using the second criterion comprises selecting data within one of a plurality of regions of the ordered log, the regions ordered in the log order.</div>
</div>
</div>
</div> <div class="claim-dependent"> <div class="claim" id="CLM-00011" num="00011">
<div class="claim-text">11. The method of <claim-ref idref="CLM-00010">claim 10</claim-ref>, further comprising:
<div class="claim-text">destaging data from a designated region of the ordered log; and</div>
<div class="claim-text">re-ordering data of the designated region to a backing store address order.</div>
</div>
</div>
</div> <div class="claim-dependent"> <div class="claim" id="CLM-00012" num="00012">
<div class="claim-text">12. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the ordered log comprises multiple append points for writing cached data, the multiple append points comprising at least one append point for writing dirty cache data and at least one append point for writing clean cache data, the clean cache data comprising cache data that is stored in the backing store.</div>
</div>
</div> <div class="claim"> <div class="claim" id="CLM-00013" num="00013">
<div class="claim-text">13. An apparatus, comprising:
<div class="claim-text">means for sending write requests pertaining to a backing store to a storage controller, the storage controller configured to cache data associated with the write requests in a log, the log comprising a sequential, log-based structure stored within a nonvolatile solid-state storage device;</div>
<div class="claim-text">means for determining an amount of data cached within the nonvolatile solid-state storage device that has not been destaged to the backing store;</div>
<div class="claim-text">means for using a sequential backing store address order for the destaging when the determined amount exceeds a threshold; and</div>
<div class="claim-text">means for using a log order for the destaging when the determined amount does not exceed the threshold.</div>
</div>
</div>
</div> <div class="claim-dependent"> <div class="claim" id="CLM-00014" num="00014">
<div class="claim-text">14. The apparatus of <claim-ref idref="CLM-00013">claim 13</claim-ref>, wherein the threshold defines a target amount of dirty write data to cache within the nonvolatile solid-state storage device.</div>
</div>
</div> <div class="claim-dependent"> <div class="claim" id="CLM-00015" num="00015">
<div class="claim-text">15. The apparatus of <claim-ref idref="CLM-00013">claim 13</claim-ref>, further comprising means for determining the amount of data cached within the nonvolatile solid-state storage device that has not been destaged to the backing store.</div>
</div>
</div> <div class="claim-dependent"> <div class="claim" id="CLM-00016" num="00016">
<div class="claim-text">16. The apparatus of <claim-ref idref="CLM-00013">claim 13</claim-ref>, wherein the means for sending, the means for determining, the means for using a sequential backing store address order, and the means for using a log order comprise a device driver executing on a host computer system.</div>
</div>
</div> <div class="claim"> <div class="claim" id="CLM-00017" num="00017">
<div class="claim-text">17. A system, comprising:
<div class="claim-text">a cache comprising a log maintained within a nonvolatile solid-state storage device, the log comprising a sequential, log-based structure;</div>
<div class="claim-text">a backing store;</div>
<div class="claim-text">a cache controller configured to admit dirty write data into the cache in response to write requests pertaining to the backing store, the dirty write data appended to the log maintained within the nonvolatile solid-state storage device; and</div>
<div class="claim-text">a destage module configured to destage dirty write data from the cache to the backing store, by:
<div class="claim-text">determining a destage pressure on the cache, the determined destage pressure corresponding to an amount of dirty write data cached within the log;</div>
<div class="claim-text">destaging dirty write data from the cache according to a sequential backing store address ordering of the dirty write data within the cache in response to the determined destage pressure satisfying a threshold; and</div>
<div class="claim-text">destaging dirty write data from the cache according to a log ordering of the dirty write data within the cache in response to the determined destage pressure failing to exceed the threshold.</div>
</div>
</div>
</div>
</div> <div class="claim-dependent"> <div class="claim" id="CLM-00018" num="00018">
<div class="claim-text">18. The system of <claim-ref idref="CLM-00017">claim 17</claim-ref>, wherein the cache controller is further configured to receive at least a portion of the dirty write data from the nonvolatile solid-state storage device in the sequential backing store address order and to destage the received dirty write data to the backing store in the sequential backing store address order.</div>
</div>
</div> <div class="claim-dependent"> <div class="claim" id="CLM-00019" num="00019">
<div class="claim-text">19. The system of <claim-ref idref="CLM-00017">claim 17</claim-ref>, further comprising a host computer system comprising a processor, wherein the cache controller and comprises a device driver executing on the processor of the host computer system.</div>
</div>
</div> </div>
</div>
</section>
                </article>
            </search-app>
        </body>
    </html>
    