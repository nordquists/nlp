
    <html>
        <body>
            <search-app>
                <article class="result" itemscope="" itemtype="http://schema.org/ScholarlyArticle">
    <h1 itemprop="pageTitle">US10511768B2 - Preview image acquisition user interface for linear panoramic image stitching 
        - Google Patents</h1><section itemprop="abstract" itemscope="">
<h2>Abstract</h2>
<div html="" itemprop="content"><abstract lang="EN" load-source="patent-office" mxw-id="PA351676032">
<div class="abstract" id="p-0001" num="0000">A system and method that allows the capture of a series of images to create a single linear panoramic image is disclosed. The method includes capturing an image, dynamically comparing a previously captured image with a preview image on a display of a capture device until a predetermined overlap threshold is satisfied, generating a user interface to provide feedback on the display of the capture device to guide a movement of the capture device, and capturing the preview image with enough overlap with the previously captured image with little to no tilt for creating a linear panorama.</div>
</abstract>
</div>
</section><section itemprop="description" itemscope="">
<h2>Description</h2>
<div html="" itemprop="content"><div class="description" lang="EN" load-source="patent-office" mxw-id="PDES229419759">
<heading id="h-0001">CROSS REFERENCE TO RELATED APPLICATIONS</heading>
<div class="description-paragraph" id="p-0002" num="0001">This application is a continuation of and claims priority to U.S. application Ser. No. 15/715,081, filed Sep. 25, 2017, titled “Preview Image Acquisition User Interface for Linear Panoramic Image Stitching,” which is a continuation of and claims priority to U.S. application Ser. No. 15/467,093, filed Mar. 23, 2017, titled “Preview Image Acquisition User Interface for Linear Panoramic Image Stitching,” which is a continuation of U.S. application Ser. No. 14/791,376, filed Jul. 3, 2015, titled “Preview Image Acquisition User Interface for Linear Panoramic Image Stitching,” which claims priority, under 35 U.S.C. § 119, to U.S. Provisional Patent Application No. 62/105,189, filed Jan. 19, 2015, entitled “Image Acquisition User Interface for Linear Panoramic Image Stitching,” and to U.S. Provisional Patent Application No. 62/127,750, filed Mar. 3, 2015, entitled “Image Acquisition User Interface for Linear Panoramic Image Stitching,” which are incorporated by reference in their entirety.</div>
<heading id="h-0002">BACKGROUND</heading>
<div class="description-paragraph" id="p-0003" num="0002">Field of the Invention</div>
<div class="description-paragraph" id="p-0004" num="0003">The specification generally relates to providing a user interface for guiding the user to capture a series of images to create a single linear panoramic image. In particular, the specification relates to a system and method for generating one or more user interface elements that provide instantaneous feedback to guide the user in capturing the series of images to create the single linear panoramic image.</div>
<div class="description-paragraph" id="p-0005" num="0004">Description of the Background Art</div>
<div class="description-paragraph" id="p-0006" num="0005">A planogram is a visual representation of products in a retail environment. For example, a planogram may describe where in the retail environment and in what quantity products should be located. Such planograms are known to be effective tools for increasing sales, managing inventory and otherwise ensuring that the desired quantity and sizes of an item are placed to optimize profits or other parameters. However, presentation and maintenance of adequate levels of stock on shelves, racks and display stands is a labor-intensive effort, thereby making enforcement of planograms difficult. While the location and quantity of products in retail stores can be manually tracked by a user, attempts are being made to automatically recognize the products and automatically or semi-automatically obtain information about the state of products.</div>
<div class="description-paragraph" id="p-0007" num="0006">Previous attempts at recognizing products have deficiencies. For example, one method to achieve the goal of recognizing multiple products from multiple images is through image stitching. Unfortunately, existing image stitching techniques can lead to artifacts and can interfere with the optimal operation of recognition.</div>
<heading id="h-0003">SUMMARY</heading>
<div class="description-paragraph" id="p-0008" num="0007">The techniques introduced herein overcome the deficiencies and limitations of the prior art, at least in part, with a system and method for capturing a series of images to create a linear panorama. In one embodiment, the system includes an image recognition application. The image recognition application is configured to receive an image of a portion of an object of interest from a capture device and to determine the features of the image. The image recognition application is further configured to generate a user interface including a current preview image of the object of interest on a display of the capture device and to compare dynamically the features of the image with the current preview image of the object of interest on the display of the capture device to determine overlap. The image recognition application is further configured to update the user interface to include a first visually distinct indicator to guide a movement of the capture device to produce the overlap and to determine whether the overlap between the image and the current preview image satisfies a predetermined overlap threshold. The image recognition application is further configured to capture a next image of the portion of the object of interest using the capture device based on the overlap satisfying the predetermined overlap threshold.</div>
<div class="description-paragraph" id="p-0009" num="0008">Other aspects include corresponding methods, systems, apparatuses, and computer program products for these and other innovative aspects.</div>
<div class="description-paragraph" id="p-0010" num="0009">The features and advantages described herein are not all-inclusive and many additional features and advantages will be apparent to one of ordinary skill in the art in view of the figures and description. Moreover, it should be noted that the language used in the specification has been principally selected for readability and instructional purposes and not to limit the scope of the techniques described.</div>
<description-of-drawings>
<heading id="h-0004">BRIEF DESCRIPTION OF THE DRAWINGS</heading>
<div class="description-paragraph" id="p-0011" num="0010">The techniques introduced herein are illustrated by way of example, and not by way of limitation in the figures of the accompanying drawings in which like reference numerals are used to refer to similar elements.</div>
<div class="description-paragraph" id="p-0012" num="0011"> <figref idrefs="DRAWINGS">FIG. 1</figref> is a high-level block diagram illustrating one embodiment of a system for capturing a series of images to create a linear panorama.</div>
<div class="description-paragraph" id="p-0013" num="0012"> <figref idrefs="DRAWINGS">FIG. 2</figref> is a block diagram illustrating one embodiment of a computing device including an image recognition application.</div>
<div class="description-paragraph" id="p-0014" num="0013"> <figref idrefs="DRAWINGS">FIG. 3</figref> is a flow diagram illustrating one embodiment of a method for capturing a series of images of an object of interest under a guidance of direction for a single linear panoramic image.</div>
<div class="description-paragraph" id="p-0015" num="0014"> <figref idrefs="DRAWINGS">FIGS. 4A-4B</figref> are flow diagrams illustrating one embodiment of a method for capturing a series of images of an object of interest in a directionally guided pattern for generating a single linear panoramic image.</div>
<div class="description-paragraph" id="p-0016" num="0015"> <figref idrefs="DRAWINGS">FIGS. 5A-5B</figref> are flow diagrams illustrating another embodiment of a method for capturing a series of images of an object of interest in a directionally guided pattern for generating a single linear panoramic image.</div>
<div class="description-paragraph" id="p-0017" num="0016"> <figref idrefs="DRAWINGS">FIGS. 6A-6B</figref> are flow diagrams illustrating one embodiment of a method for realigning the current preview image with a previously captured image of an object of interest.</div>
<div class="description-paragraph" id="p-0018" num="0017"> <figref idrefs="DRAWINGS">FIG. 7A</figref> is a graphical representation of an embodiment of a user interface for capturing an image of a shelf.</div>
<div class="description-paragraph" id="p-0019" num="0018"> <figref idrefs="DRAWINGS">FIG. 7B</figref> is a graphical representation of another embodiment of the user interface for capturing an image of a shelf.</div>
<div class="description-paragraph" id="p-0020" num="0019"> <figref idrefs="DRAWINGS">FIG. 8</figref> is a graphical representation of one embodiment of an overlap between images captured of an object of interest.</div>
<div class="description-paragraph" id="p-0021" num="0020"> <figref idrefs="DRAWINGS">FIG. 9</figref> is a graphical representation of one embodiment of the image matching process for generating the visually distinct indicator for overlap</div>
<div class="description-paragraph" id="p-0022" num="0021"> <figref idrefs="DRAWINGS">FIGS. 10A-10D</figref> are graphical representations of embodiments of the user interface displaying a visually distinct indicator for overlap when the client device moves in a left-to-right direction.</div>
<div class="description-paragraph" id="p-0023" num="0022"> <figref idrefs="DRAWINGS">FIGS. 11A-11D</figref> are graphical representations of embodiments of the user interface displaying a visually distinct indicator for overlap when the client device moves in a bottom-to-top direction.</div>
<div class="description-paragraph" id="p-0024" num="0023"> <figref idrefs="DRAWINGS">FIGS. 12A-12C</figref> are graphical representations of embodiments of the user interface displaying a visually distinct indicator for tilt when the client device is rolling about the Z axis.</div>
<div class="description-paragraph" id="p-0025" num="0024"> <figref idrefs="DRAWINGS">FIGS. 13A-13C</figref> are graphical representations of embodiments of the user interface displaying a visually distinct indicator for tilt when the client device is pitching about the X axis.</div>
<div class="description-paragraph" id="p-0026" num="0025"> <figref idrefs="DRAWINGS">FIGS. 14A-14B</figref> are graphical representations of embodiments of the user interface displaying visually distinct indicator for tilt when the client device is tilting in both X and Z axes.</div>
<div class="description-paragraph" id="p-0027" num="0026"> <figref idrefs="DRAWINGS">FIG. 15</figref> is a graphical representation of one embodiment of the realignment process for generating the visually distinct indicator for realignment.</div>
<div class="description-paragraph" id="p-0028" num="0027"> <figref idrefs="DRAWINGS">FIGS. 16A-16D</figref> are graphical representations of embodiments of the user interface displaying realigning current preview image displayed on a client device with a previously captured image.</div>
<div class="description-paragraph" id="p-0029" num="0028"> <figref idrefs="DRAWINGS">FIGS. 17A-17F</figref> are graphical representations illustrating another set of embodiments of the user interface displaying realigning current preview image displayed on a client device with a previously captured image.</div>
<div class="description-paragraph" id="p-0030" num="0029"> <figref idrefs="DRAWINGS">FIG. 18</figref> is a graphical representation of one embodiment of the serpentine scan pattern of image capture.</div>
<div class="description-paragraph" id="p-0031" num="0030"> <figref idrefs="DRAWINGS">FIG. 19</figref> is a graphical representation of one embodiment of constructing a mosaic preview using images of a shelving unit.</div>
<div class="description-paragraph" id="p-0032" num="0031"> <figref idrefs="DRAWINGS">FIGS. 20A-20I</figref> are graphical representations of embodiments of the user interface displaying visually distinct indicator for direction of movement of the client device.</div>
<div class="description-paragraph" id="p-0033" num="0032"> <figref idrefs="DRAWINGS">FIG. 21</figref> is a graphical representation of another embodiment of the user interface displaying visually distinct indicator for direction of movement of the client device.</div>
<div class="description-paragraph" id="p-0034" num="0033"> <figref idrefs="DRAWINGS">FIGS. 22A-22B</figref> are graphical representation of embodiments of the user interface previewing the set of captured images in a mosaic.</div>
</description-of-drawings>
<heading id="h-0005">DETAILED DESCRIPTION</heading>
<div class="description-paragraph" id="p-0035" num="0034"> <figref idrefs="DRAWINGS">FIG. 1</figref> is a high-level block diagram illustrating one embodiment of a system <b>100</b> for capturing a series of images to create a linear panorama. The illustrated system <b>100</b> may have one or more client devices <b>115</b> <i>a </i>. . . <b>115</b> <i>n </i>that can be accessed by users and a recognition server <b>101</b>. In <figref idrefs="DRAWINGS">FIG. 1</figref> and the remaining figures, a letter after a reference number, e.g., “<b>115</b> <i>a</i>,” represents a reference to the element having that particular reference number. A reference number in the text without a following letter, e.g., “<b>115</b>,” represents a general reference to instances of the element bearing that reference number. In the illustrated embodiment, these entities of the system <b>100</b> are communicatively coupled via a network <b>105</b>.</div>
<div class="description-paragraph" id="p-0036" num="0035">The network <b>105</b> can be a conventional type, wired or wireless, and may have numerous different configurations including a star configuration, token ring configuration or other configurations. Furthermore, the network <b>105</b> may include a local area network (LAN), a wide area network (WAN) (e.g., the Internet), and/or other interconnected data paths across which multiple devices may communicate. In some embodiments, the network <b>105</b> may be a peer-to-peer network. The network <b>105</b> may also be coupled to or include portions of a telecommunications network for sending data in a variety of different communication protocols. In some embodiments, the network <b>105</b> may include Bluetooth communication networks or a cellular communications network for sending and receiving data including via short messaging service (SMS), multimedia messaging service (MMS), hypertext transfer protocol (HTTP), direct data connection, WAP, email, etc. Although <figref idrefs="DRAWINGS">FIG. 1</figref> illustrates one network <b>105</b> coupled to the client devices <b>115</b> and the recognition server <b>101</b>, in practice one or more networks <b>105</b> can be connected to these entities.</div>
<div class="description-paragraph" id="p-0037" num="0036">In some embodiments, the system <b>100</b> includes a recognition server <b>101</b> coupled to the network <b>105</b>. In some embodiments, the recognition server <b>101</b> may be either a hardware server, a software server, or a combination of software and hardware. The recognition server <b>101</b> may be, or may be implemented by, a computing device including a processor, a memory, applications, a database, and network communication capabilities. In the example of <figref idrefs="DRAWINGS">FIG. 1</figref>, the components of the recognition server <b>101</b> are configured to implement an image recognition application <b>103</b> <i>a </i>described in more detail below. In one embodiment, the recognition server <b>101</b> provides services to a consumer packaged goods firm for identifying products on shelves, racks, or displays. While the examples herein describe recognition of products in an image of shelves, such as a retail display, it should be understood that the image may include any arrangement of organized objects. For example, the image may be of a warehouse, stockroom, store room, cabinet, etc. Similarly, the objects, in addition to retail products, may be tools, parts used in manufacturing, construction or maintenance, medicines, first aid supplies, emergency or safety equipment, etc.</div>
<div class="description-paragraph" id="p-0038" num="0037">In some embodiments, the recognition server <b>101</b> sends and receives data to and from other entities of the system <b>100</b> via the network <b>105</b>. For example, the recognition server <b>101</b> sends and receives data including images to and from the client device <b>115</b>. The images received by the recognition server <b>101</b> can include an image captured by the client device <b>115</b>, an image copied from a website or an email, or an image from any other source. Although only a single recognition server <b>101</b> is shown in <figref idrefs="DRAWINGS">FIG. 1</figref>, it should be understood that there may be any number of recognition servers <b>101</b> or a server cluster. The recognition server <b>101</b> also includes a data storage <b>243</b>, which is described below in more detail with reference to <figref idrefs="DRAWINGS">FIG. 2</figref>.</div>
<div class="description-paragraph" id="p-0039" num="0038">The client device <b>115</b> may be a computing device that includes a memory, a processor and a camera, for example a laptop computer, a desktop computer, a tablet computer, a mobile telephone, a smartphone, a personal digital assistant (PDA), a mobile email device, a webcam, a user wearable computing device or any other electronic device capable of accessing a network <b>105</b>. The client device <b>115</b> provides general graphics and multimedia processing for any type of application. For example, the client device <b>115</b> may include a graphics processor unit (GPU) for handling graphics and multimedia processing. The client device <b>115</b> includes a display for viewing information provided by the recognition server <b>101</b>. While <figref idrefs="DRAWINGS">FIG. 1</figref> illustrates two client devices <b>115</b> <i>a </i>and <b>115</b> <i>n</i>, the disclosure applies to a system architecture having one or more client devices <b>115</b>.</div>
<div class="description-paragraph" id="p-0040" num="0039">The client device <b>115</b> is adapted to send and receive data to and from the recognition server <b>101</b>. For example, the client device <b>115</b> sends a query image to the recognition server <b>101</b> and the recognition server <b>101</b> provides data in JavaScript Object Notation (JSON) format about one or more objects recognized in the query image to the client device <b>115</b>. The client device <b>115</b> may support use of graphical application program interface (API) such as Metal on Apple iOS™ or RenderScript on Android™ for determination of feature location and feature descriptors on the client device <b>115</b>.</div>
<div class="description-paragraph" id="p-0041" num="0040">The image recognition application <b>103</b> may include software and/or logic to provide the functionality for capturing a series of images to create a linear panorama. In some embodiments, the image recognition application <b>103</b> can be implemented using programmable or specialized hardware, such as a field-programmable gate array (FPGA) or an application-specific integrated circuit (ASIC). In some embodiments, the image recognition application <b>103</b> can be implemented using a combination of hardware and software. In other embodiments, the image recognition application <b>103</b> may be stored and executed on a combination of the client devices <b>115</b> and the recognition server <b>101</b>, or by any one of the client devices <b>115</b> or recognition server <b>101</b>.</div>
<div class="description-paragraph" id="p-0042" num="0041">In some embodiments, the image recognition application <b>103</b> <i>b </i>may be a thin-client application with some functionality executed on the client device <b>115</b> and additional functionality executed on the recognition server <b>101</b> by image recognition application <b>103</b> <i>a</i>. For example, the image recognition application <b>103</b> <i>b </i>on the client device <b>115</b> could include software and/or logic for capturing the image, transmitting the image to the recognition server <b>101</b>, and displaying image recognition results. In another example, the image recognition application <b>103</b> <i>a </i>on the recognition server <b>101</b> could include software and/or logic for receiving the image, stitching the image to a mosaic view based on sufficient overlap with a previously received image and generating image recognition results. The image recognition application <b>103</b> <i>a </i>or <b>103</b> <i>b </i>may include further functionality described herein, such as, processing the image and performing feature identification.</div>
<div class="description-paragraph" id="p-0043" num="0042">In some embodiments, the image recognition application <b>103</b> receives an image of a portion of an object of interest from a capture device. The image recognition application <b>103</b> determines features of the image. The image recognition application <b>103</b> generates a user interface including a current preview image of the object of interest on a display of the capture device. The image recognition application <b>103</b> dynamically compares the features of the image with the current preview image of the object of interest to determine overlap. The image recognition application <b>103</b> updates the user interface to include a visually distinct indicator to guide a movement of the capture device to produce the desired or prescribed overlap and alignment between the images. The image recognition application <b>103</b> determines whether the overlap between the image and the current preview image satisfies a predetermined overlap and alignment thresholds. For example, an overlap threshold can be set at 60 percent between images to be stitched together to create a linear panorama. The image recognition application <b>103</b> captures the preview image of the portion of the object of interest based on the overlap satisfying the predetermined overlap threshold. The operation of the image recognition application <b>103</b> and the functions listed above are described below in more detail below with reference to <figref idrefs="DRAWINGS">FIGS. 3-15</figref>.</div>
<div class="description-paragraph" id="p-0044" num="0043"> <figref idrefs="DRAWINGS">FIG. 2</figref> is a block diagram illustrating one embodiment of a computing device <b>200</b> including an image recognition application <b>103</b>. The computing device <b>200</b> may also include a processor <b>235</b>, a memory <b>237</b>, an optional display device <b>239</b>, a communication unit <b>241</b>, data storage <b>243</b>, optional orientation sensors <b>245</b> and an optional capture device <b>247</b> according to some examples. The components of the computing device <b>200</b> are communicatively coupled by a bus <b>220</b>. The bus <b>220</b> may represent one or more buses including an industry standard architecture (ISA) bus, a peripheral component interconnect (PCI) bus, a universal serial bus (USB), or some other bus known in the art to provide similar functionality. In some embodiments, the computing device <b>200</b> may be the client device <b>115</b>, the recognition server <b>101</b>, or a combination of the client device <b>115</b> and the recognition server <b>101</b>. In such embodiments where the computing device <b>200</b> is the client device <b>115</b> or the recognition server <b>101</b>, it should be understood that the client device <b>115</b>, and the recognition server <b>101</b> may include other components described above but not shown in <figref idrefs="DRAWINGS">FIG. 2</figref>.</div>
<div class="description-paragraph" id="p-0045" num="0044">The processor <b>235</b> may execute software instructions by performing various input/output, logical, and/or mathematical operations. The processor <b>235</b> may have various computing architectures to process data signals including, for example, a complex instruction set computer (CISC) architecture, a reduced instruction set computer (RISC) architecture, and/or an architecture implementing a combination of instruction sets. The processor <b>235</b> may be physical and/or virtual, and may include a single processing unit or a plurality of processing units and/or cores. In some implementations, the processor <b>235</b> may be capable of generating and providing electronic, display signals to a display device, supporting the display of images, capturing and transmitting images, performing complex tasks including various types of feature extraction and sampling, etc. In some implementations, the processor <b>235</b> may be coupled to the memory <b>237</b> via the bus <b>220</b> to access data and instructions therefrom and store data therein. The bus <b>220</b> may couple the processor <b>235</b> to the other components of the computing device <b>200</b> including, for example, the memory <b>237</b>, the communication unit <b>241</b>, the image recognition application <b>103</b>, and the data storage <b>243</b>. It will be apparent to one skilled in the art that other processors, operating systems, sensors, displays and physical configurations are possible.</div>
<div class="description-paragraph" id="p-0046" num="0045">The memory <b>237</b> may store and provide access to data for the other components of the computing device <b>200</b>. The memory <b>237</b> may be included in a single computing device or distributed among a plurality of computing devices as discussed elsewhere herein. In some implementations, the memory <b>237</b> may store instructions and/or data that may be executed by the processor <b>235</b>. The instructions and/or data may include code for performing the techniques described herein. For example, in one embodiment, the memory <b>237</b> may store the image recognition application <b>103</b>. The memory <b>237</b> is also capable of storing other instructions and data, including, for example, an operating system, hardware drivers, other software applications, databases, etc. The memory <b>237</b> may be coupled to the bus <b>220</b> for communication with the processor <b>235</b> and the other components of the computing device <b>200</b>.</div>
<div class="description-paragraph" id="p-0047" num="0046">The memory <b>237</b> may include one or more non-transitory computer-usable (e.g., readable, writeable) device, a static random access memory (SRAM) device, an embedded memory device, a discrete memory device (e.g., a PROM, FPROM, ROM), a hard disk drive, an optical disk drive (CD, DVD, Blu-ray™, etc.) mediums, which can be any tangible apparatus or device that can contain, store, communicate, or transport instructions, data, computer programs, software, code, routines, etc., for processing by or in connection with the processor <b>235</b>. In some implementations, the memory <b>237</b> may include one or more of volatile memory and non-volatile memory. For example, the memory <b>237</b> may include, but is not limited to, one or more of a dynamic random access memory (DRAM) device, a static random access memory (SRAM) device, an embedded memory device, a discrete memory device (e.g., a PROM, FPROM, ROM), a hard disk drive, an optical disk drive (CD, DVD, Blu-ray™ etc.). It should be understood that the memory <b>237</b> may be a single device or may include multiple types of devices and configurations.</div>
<div class="description-paragraph" id="p-0048" num="0047">The display device <b>239</b> is a liquid crystal display (LCD), light emitting diode (LED) or any other similarly equipped display device, screen or monitor. The display device <b>239</b> represents any device equipped to display user interfaces, electronic images and data as described herein. In different embodiments, the display is binary (only two different values for pixels), monochrome (multiple shades of one color), or allows multiple colors and shades. The display device <b>239</b> is coupled to the bus <b>220</b> for communication with the processor <b>235</b> and the other components of the computing device <b>200</b>. It should be noted that the display device <b>239</b> is shown in <figref idrefs="DRAWINGS">FIG. 2</figref> with dashed lines to indicate it is optional. For example, where the computing device <b>200</b> is the recognition server <b>101</b>, the display device <b>239</b> is not part of the system, where the computing device <b>200</b> is the client device <b>115</b>, the display device <b>239</b> is included and is used to display the user interfaces described below with reference to <figref idrefs="DRAWINGS">FIGS. 7A, 7B, 9A-15B, 17A-17I, 18 and 22A-22F</figref>.</div>
<div class="description-paragraph" id="p-0049" num="0048">The communication unit <b>241</b> is hardware for receiving and transmitting data by linking the processor <b>235</b> to the network <b>105</b> and other processing systems. The communication unit <b>241</b> receives data such as requests from the client device <b>115</b> and transmits the requests to the controller <b>201</b>, for example a request to process an image. The communication unit <b>241</b> also transmits information including recognition results to the client device <b>115</b> for display, for example, in response to processing the image. The communication unit <b>241</b> is coupled to the bus <b>220</b>. In one embodiment, the communication unit <b>241</b> may include a port for direct physical connection to the client device <b>115</b> or to another communication channel. For example, the communication unit <b>241</b> may include an RJ45 port or similar port for wired communication with the client device <b>115</b>. In another embodiment, the communication unit <b>241</b> may include a wireless transceiver (not shown) for exchanging data with the client device <b>115</b> or any other communication channel using one or more wireless communication methods, such as IEEE 802.11, IEEE 802.16, Bluetooth® or another suitable wireless communication method.</div>
<div class="description-paragraph" id="p-0050" num="0049">In yet another embodiment, the communication unit <b>241</b> may include a cellular communications transceiver for sending and receiving data over a cellular communications network such as via short messaging service (SMS), multimedia messaging service (MMS), hypertext transfer protocol (HTTP), direct data connection, WAP, e-mail or another suitable type of electronic communication. In still another embodiment, the communication unit <b>241</b> may include a wired port and a wireless transceiver. The communication unit <b>241</b> also provides other conventional connections to the network <b>105</b> for distribution of files and/or media objects using standard network protocols such as TCP/IP, HTTP, HTTPS and SMTP as will be understood to those skilled in the art.</div>
<div class="description-paragraph" id="p-0051" num="0050">The data storage <b>243</b> is a non-transitory memory that stores data for providing the functionality described herein. The data storage <b>243</b> may be a dynamic random access memory (DRAM) device, a static random access memory (SRAM) device, flash memory or some other memory devices. In some embodiments, the data storage <b>243</b> also may include a non-volatile memory or similar permanent storage device and media including a hard disk drive, a floppy disk drive, a CD-ROM device, a DVD-ROM device, a DVD-RAM device, a DVD-RW device, a flash memory device, or some other mass storage device for storing information on a more permanent basis.</div>
<div class="description-paragraph" id="p-0052" num="0051">In the illustrated embodiment, the data storage <b>243</b> is communicatively coupled to the bus <b>220</b>. The data storage <b>243</b> stores data for analyzing a received image and results of the analysis and other functionality as described herein. For example, the data storage <b>243</b> may store an image overlap threshold for capturing optimal overlapping images. The data storage <b>243</b> may similarly store a captured image and the set of features determined for the captured image. Additionally, the data storage <b>243</b> may store a stitched linear panoramic image. The data stored in the data storage <b>243</b> is described below in more detail.</div>
<div class="description-paragraph" id="p-0053" num="0052">The orientation sensors <b>245</b> may be hardware-based or software-based, or a combination of hardware and software for determining position or motion of the computing device <b>200</b>. In some embodiments, the orientation sensors <b>245</b> may include an accelerometer, a gyroscope, a proximity sensor, a geomagnetic field sensor, etc. In different embodiments, the orientation sensors <b>245</b> may provide acceleration force data for the three coordinate axes, rate of rotation data for the three coordinate axes (e.g., yaw, pitch and roll values), proximity data indicating a distance of an object, etc. It should be noted that the orientation sensors <b>245</b> are shown in <figref idrefs="DRAWINGS">FIG. 2</figref> with dashed lines to indicate it is optional. For example, where the computing device <b>200</b> is the recognition server <b>101</b>, the orientation sensors <b>245</b> are not part of the system, where the computing device <b>200</b> is the client device <b>115</b>, the orientation sensors <b>245</b> are included and are used to provide sensor information for various motion or position determination events of the client device <b>200</b> described herein.</div>
<div class="description-paragraph" id="p-0054" num="0053">The capture device <b>247</b> may be operable to capture an image or data digitally of an object of interest. For example, the capture device <b>247</b> may be a high definition (HD) camera, a regular 2D camera, a multi-spectral camera, a structured light 3D camera, a time-of-flight 3D camera, a stereo camera, a standard smartphone camera or a wearable computing device. The capture device <b>247</b> is coupled to the bus to provide the images and other processed metadata to the processor <b>235</b>, the memory <b>237</b> or the data storage <b>243</b>. It should be noted that the capture device <b>247</b> is shown in <figref idrefs="DRAWINGS">FIG. 2</figref> with dashed lines to indicate it is optional. For example, where the computing device <b>200</b> is the recognition server <b>101</b>, the capture device <b>247</b> is not part of the system, where the computing device <b>200</b> is the client device <b>115</b>, the capture device <b>247</b> is included and is used to provide images and other metadata information described below with reference to <figref idrefs="DRAWINGS">FIGS. 7A, 7B, 9A-15B, 17A-17I, 18 and 22A-22F</figref>.</div>
<div class="description-paragraph" id="p-0055" num="0054">In some embodiments, the image recognition application <b>103</b> may include a controller <b>201</b>, a feature extraction module <b>203</b>, an alignment module <b>205</b>, a user guidance module <b>207</b>, a stitching module <b>209</b> and a user interface module <b>211</b>. The components of the image recognition application <b>103</b> are communicatively coupled via the bus <b>220</b>.</div>
<div class="description-paragraph" id="p-0056" num="0055">The controller <b>201</b> may include software and/or logic to control the operation of the other components of the image recognition application <b>103</b>. The controller <b>201</b> controls the other components of the image recognition application <b>103</b> to perform the methods described below with reference to <figref idrefs="DRAWINGS">FIGS. 3-6</figref>. The controller <b>201</b> may also include software and/or logic to provide the functionality for handling communications between the image recognition application <b>103</b> and other components of the computing device <b>200</b> as well as between the components of the image recognition application <b>103</b>. In some embodiments, the controller <b>201</b> can be implemented using programmable or specialized hardware including a field-programmable gate array (FPGA) or an application-specific integrated circuit (ASIC). In some embodiments, the controller <b>201</b> can be implemented using a combination of hardware and software executable by processor <b>235</b>. In some embodiments, the controller <b>201</b> is a set of instructions executable by the processor <b>235</b>. In some implementations, the controller <b>201</b> is stored in the memory <b>237</b> and is accessible and executable by the processor <b>235</b>. In some implementations, the controller <b>201</b> is adapted for cooperation and communication with the processor <b>235</b>, the memory <b>237</b> and other components of the image recognition application <b>103</b> via the bus <b>220</b>.</div>
<div class="description-paragraph" id="p-0057" num="0056">In some embodiments, the controller <b>201</b> sends and receives data, via the communication unit <b>241</b>, to and from one or more of the client device <b>115</b> and the recognition server <b>101</b>. For example, the controller <b>201</b> receives, via the communication unit <b>241</b>, an image from a client device <b>115</b> operated by a user and sends the image to the feature extraction module <b>203</b>. In another example, the controller <b>201</b> receives data for providing a graphical user interface to a user from the user interface module <b>211</b> and sends the data to a client device <b>115</b>, causing the client device <b>115</b> to present the user interface to the user.</div>
<div class="description-paragraph" id="p-0058" num="0057">In some embodiments, the controller <b>201</b> receives data from other components of the image recognition application <b>103</b> and stores the data in the data storage <b>243</b>. For example, the controller <b>201</b> receives data including features identified for an image from the feature extraction module <b>203</b> and stores the data in the data storage <b>243</b>. In other embodiments, the controller <b>201</b> retrieves data from the data storage <b>243</b> and sends the data to other components of the image recognition application <b>103</b>. For example, the controller <b>201</b> retrieves data including an overlap threshold from the data storage <b>243</b> and sends the retrieved data to the alignment module <b>205</b>.</div>
<div class="description-paragraph" id="p-0059" num="0058">The feature extraction module <b>203</b> may include software and/or logic to provide the functionality for receiving an image of an object of interest from the client device <b>115</b> and determining features for the image. In some embodiments, the feature extraction module <b>203</b> can be implemented using programmable or specialized hardware including a field-programmable gate array (FPGA) or an application-specific integrated circuit (ASIC). In some embodiments, the feature extraction module <b>203</b> can be implemented using a combination of hardware and software executable by processor <b>235</b>. In some embodiments, the feature extraction module <b>203</b> is a set of instructions executable by the processor <b>235</b>. In some implementations, the feature extraction module <b>203</b> is stored in the memory <b>237</b> and is accessible and executable by the processor <b>235</b>. In some implementations, the feature extraction module <b>203</b> is adapted for cooperation and communication with the processor <b>235</b>, the memory <b>237</b> and other components of the image recognition application <b>103</b> via the bus <b>220</b>.</div>
<div class="description-paragraph" id="p-0060" num="0059">In some embodiments, the feature extraction module <b>203</b> receives an image and determine features for the image. In some embodiments, the feature extraction module <b>203</b> receives a preview image of an object of interest from the alignment module <b>205</b> and determines a set of features for the image. For example, the feature extraction module <b>203</b> may determine a location, an orientation, and an image descriptor for each feature identified in the image. In some embodiments, the feature extraction module <b>203</b> uses corner detection algorithms such as, Shi-Tomasi corner detection algorithm, Harris and Stephens corner detection algorithm, etc., for determining feature location. In some embodiments, the feature extraction module <b>203</b> uses Binary Robust Independent Elementary Features (BRIEF) descriptor approach for determining efficient image feature descriptors. In some embodiments, the feature extraction module <b>203</b> sends the set of features for the images to the alignment module <b>205</b>. In other embodiments, the feature extraction module <b>203</b> identifies the image as a reference image and stores the set of features in the data storage <b>243</b>.</div>
<div class="description-paragraph" id="p-0061" num="0060">The alignment module <b>205</b> may include software and/or logic to provide the functionality for receiving a preview image of an object of interest from the client device <b>115</b> for realignment with a reference image, instructing the user interface module <b>211</b> to generate a user interface including the preview image and/or dynamically comparing features of the reference image and a preview image of an object of interest. In some embodiments, the alignment module <b>205</b> can be implemented using programmable or specialized hardware including a field-programmable gate array (FPGA) or an application-specific integrated circuit (ASIC). In some embodiments, the alignment module <b>205</b> can be implemented using a combination of hardware and software executable by processor <b>235</b>. In some embodiments, the alignment module <b>205</b> is a set of instructions executable by the processor <b>235</b>. In some implementations, the alignment module <b>205</b> is stored in the memory <b>237</b> and is accessible and executable by the processor <b>235</b>. In some implementations, the alignment module <b>205</b> is adapted for cooperation and communication with the processor <b>235</b>, the memory <b>237</b> and other components of the image recognition application <b>103</b> via the bus <b>220</b>.</div>
<div class="description-paragraph" id="p-0062" num="0061">In some embodiments, the alignment module <b>205</b> continuously receives preview images of an object of interest sampled by the capture device <b>247</b> and sends the preview images to the feature extraction module <b>203</b>. In other embodiments, the alignment module <b>205</b> instructs the user interface module <b>211</b> to generate a user interface for displaying the preview image on a display of the client device <b>115</b>. In some embodiments, the alignment module <b>205</b> may receive a user selection for realignment of images on the client device <b>115</b>. In some embodiments, the alignment module <b>205</b> receives features for the preview images from the feature extraction module <b>203</b> and dynamically compares the features of the reference image against the features of the preview images. In some embodiments, the alignment module <b>205</b> determines an overlap between images and instructs the user interface module <b>211</b> for generating visually distinct indicators on a user interface for guiding a movement of the client device <b>115</b> to produce a desired overlap. In other embodiments, the alignment module <b>205</b> determines whether the overlap satisfies a predetermined overlap threshold and sends instructions to the feature extraction module <b>203</b> to set the preview image as the reference image based on the predetermined overlap threshold being satisfied.</div>
<div class="description-paragraph" id="p-0063" num="0062">The user guidance module <b>207</b> may include software and/or logic to provide the functionality for guiding a movement of the client device <b>115</b> in a direction, guiding an orientation of the client device <b>115</b> in an axis of orientation and providing progress information through visually distinct indicators. In some embodiments, the user guidance module <b>207</b> can be implemented using programmable or specialized hardware including a field-programmable gate array (FPGA) or an application-specific integrated circuit (ASIC). In some embodiments, the user guidance module <b>207</b> can be implemented using a combination of hardware and software executable by processor <b>235</b>. In some embodiments, the user guidance module <b>207</b> is a set of instructions executable by the processor <b>235</b>. In some implementations, the user guidance module <b>207</b> is stored in the memory <b>237</b> and is accessible and executable by the processor <b>235</b>. In some implementations, the user guidance module <b>207</b> is adapted for cooperation and communication with the processor <b>235</b>, the memory <b>237</b> and other components of the image recognition application <b>103</b> via the bus <b>220</b>.</div>
<div class="description-paragraph" id="p-0064" num="0063">In some embodiments, the user guidance module <b>207</b> receives gyroscope sensor information from the orientation sensors <b>245</b> of the client device <b>115</b>. In some embodiments, the user guidance module <b>207</b> determines whether the client device <b>115</b> is tilting in one of the three axes of orientation based on the gyroscope sensor information. In other embodiments, the user guidance module <b>207</b> sends instructions to the user interface module <b>211</b> for generating visually distinct indicators on a user interface for guiding an orientation of the client device <b>115</b> to nullify the tilt. In some embodiments, the user guidance module <b>207</b> receives a selection of a pattern of image capture for receiving images of an object of interest from a client device <b>115</b>. In some embodiments, the user guidance module <b>207</b> sends instructions to the user interface module <b>211</b> for generating visually distinct indicators for directional movement of the client device based on the selected pattern of image capture. In other embodiments, the user guidance module <b>207</b> sends instructions to the user interface module <b>211</b> for generating a mosaic preview of images received for an object of interest on the user interface.</div>
<div class="description-paragraph" id="p-0065" num="0064">The stitching module <b>209</b> may include software and/or logic to provide the functionality for stitching a series of images into a single linear panoramic image. In some embodiments, the stitching module <b>209</b> can be implemented using programmable or specialized hardware including a field-programmable gate array (FPGA) or an application-specific integrated circuit (ASIC). In some embodiments, the stitching module <b>209</b> can be implemented using a combination of hardware and software executable by processor <b>235</b>. In some embodiments, the stitching module <b>209</b> is a set of instructions executable by the processor <b>235</b>. In some implementations, the stitching module <b>209</b> is stored in the memory <b>237</b> and is accessible and executable by the processor <b>235</b>. In some implementations, the stitching module <b>209</b> is adapted for cooperation and communication with the processor <b>235</b>, the memory <b>237</b> and other components of the image recognition application <b>103</b> via the bus <b>220</b>.</div>
<div class="description-paragraph" id="p-0066" num="0065">In some embodiments, the stitching module <b>209</b> receives the reference images of the object of interest from the feature extraction module <b>203</b>. In some embodiments, the stitching module <b>209</b> receives overlap information between the images being processed by the alignment module <b>205</b>. In some embodiments, where the computing device <b>200</b> is the client device <b>115</b>, the stitching module <b>209</b> of the image recognition application <b>103</b> sends the reference images of the object of interest, overlap information and other metadata information to the recognition server <b>101</b> for generating a single linear panoramic image. In some embodiments, where the computing device <b>200</b> is the recognition server <b>101</b>, the stitching module <b>209</b> of the image recognition application <b>103</b> generates the single linear panoramic image using the reference images of the object of interest, overlap information and other metadata information. In other embodiments, the stitching module <b>209</b> receives the linear panoramic image, stores the linear panoramic image in the data storage <b>243</b> and instructs the user interface module <b>211</b> to generate a user interface for displaying the linear panoramic image.</div>
<div class="description-paragraph" id="p-0067" num="0066">The user interface module <b>211</b> may include software and/or logic for providing user interfaces to a user. In some embodiments, the user interface module <b>211</b> can be implemented using programmable or specialized hardware including a field-programmable gate array (FPGA) or an application-specific integrated circuit (ASIC). In some embodiments, the user interface module <b>211</b> can be implemented using a combination of hardware and software executable by processor <b>235</b>. In some embodiments, the user interface module <b>211</b> is a set of instructions executable by the processor <b>235</b>. In some implementations, the user interface module <b>211</b> is stored in the memory <b>237</b> and is accessible and executable by the processor <b>235</b>. In some implementations, the user interface module <b>211</b> is adapted for cooperation and communication with the processor <b>235</b>, the memory <b>237</b> and other components of the image recognition application <b>103</b> via the bus <b>220</b>.</div>
<div class="description-paragraph" id="p-0068" num="0067">In some embodiments, the user interface module <b>211</b> receives instructions from the alignment module <b>205</b> to generate a graphical user interface that instructs the user on how to move the client device <b>115</b> to capture a next image that has a good overlap with the previously captured image. In some embodiments, the user interface module <b>211</b> receives instructions from the user guidance module <b>207</b> to generate a graphical user interface that guides the user to capture an overlapping image with little to no tilt in any of the axes of orientations (e.g., X, Y, or Z axis). In other embodiments, the user interface module <b>211</b> sends graphical user interface data to an application (e.g., a browser) in the client device <b>115</b> via the communication unit <b>241</b> causing the application to display the data as a graphical user interface.</div>
<div class="description-paragraph" id="h-0006" num="0000">Methods</div>
<div class="description-paragraph" id="p-0069" num="0068"> <figref idrefs="DRAWINGS">FIG. 3</figref> is a flow diagram illustrating one embodiment of a method <b>300</b> for capturing a series of images of an object of interest under guidance of direction for a single linear panoramic image. At <b>302</b>, the feature extraction module <b>203</b> receives an image of a portion of an object of interest from a client device <b>115</b> for serving as a reference image. For example, the image can be an image of a shelf, a region, an artwork, a landmark, a scenic location, outer space, etc. The image is processed and assuming it satisfies the criteria (location, orientation and alignment) for being the first image in the series of images needed to form the single linear panoramic image, it is identified as the reference image. At <b>304</b>, the alignment module <b>205</b> determines whether there are preview images being sampled by the client device <b>115</b>. If the preview images are being sampled, at <b>306</b>, the alignment module <b>205</b> receives a preview image of another portion of the object of interest from the client device <b>115</b>. At <b>308</b>, the user interface module <b>211</b> generates a user interface including a visually distinct indicator overlaid upon the preview image, the visually distinct indicator identifying a direction for guiding a movement of the client device. For example, the direction of movement can be in a north, south, east, or west direction. At <b>310</b>, the user interface module <b>211</b> adds to the user interface a progressively growing mosaic preview, the mosaic preview including a thumbnail representation of the reference image. At <b>312</b>, the alignment module <b>205</b> compares dynamically the reference image with the preview image to determine whether an overlap being detected between the reference image and the preview image satisfies a predetermined overlap threshold. For example, the predetermined overlap threshold can be set at 60 percent. At <b>314</b>, the alignment module <b>205</b> checks whether the overlap threshold is satisfied. If the overlap threshold is satisfied, at <b>316</b>, the feature extraction module <b>203</b> sets the preview image to be the reference image and the method <b>300</b> repeats the process from step <b>304</b>. If the overlap threshold is not satisfied, the method <b>300</b> repeats the process from step <b>304</b>. More images are received as preview images on the display of the capture device and the user interface is continuously updated until a preview image with sufficient overlap with the reference image is determined. If the preview images are not being sampled by the client device <b>115</b>, then at <b>318</b>, the stitching module <b>209</b> sends the images of the portion of the object of interest to generate a single linear panoramic image. In some embodiments, the alignment module <b>205</b> is responsive to user input and once the user stops providing preview images, the alignment module <b>205</b> sends an instruction to the stitching module <b>209</b>. The stitching module <b>209</b> sends the images of the object of interest to the recognition server <b>101</b> for the panoramic image to be generated. In some embodiments, the stitching module <b>209</b> provides feedback to the user as to whether enough images have been captured to form a panoramic image. In some embodiments, the user guidance module <b>207</b> may receive input as to the pattern of image capture and the user guidance module <b>207</b> may instruct the user interface module <b>211</b> to generate a user interface to guide the user as to the next image to preview or provide. In other words, the method may provide the user additional feedback as to what vertical and lateral movement to make to provide previews of images.</div>
<div class="description-paragraph" id="p-0070" num="0069"> <figref idrefs="DRAWINGS">FIGS. 4A-4B</figref> are flow diagrams illustrating one embodiment of a method <b>400</b> for capturing a series of images of an object of interest in a directionally guided pattern for generating a single linear panoramic image. At <b>402</b>, the user guidance module <b>207</b> receives a selection of a serpentine pattern of image capture for receiving images of an object of interest from a client device. At <b>404</b>, the feature extraction module <b>203</b> receives an image of a portion of the object of interest from the client device and identifies the image as a reference image. At <b>406</b>, the feature extraction module <b>203</b> determines features of the reference image. For example, the feature extraction module <b>203</b> determines an image descriptor for each feature identified for the reference image. The feature extraction module <b>203</b> uses Binary Robust Independent Elementary Features (BRIEF) descriptor approach for determining efficient image feature descriptors. The image descriptor can be a 256-bit bitmask which describes the image sub-region covered by the feature.</div>
<div class="description-paragraph" id="p-0071" num="0070">At <b>408</b>, the user guidance module <b>207</b> checks whether a lateral direction for the serpentine pattern is known. If the lateral direction for the serpentine pattern is known, at <b>414</b>, the alignment module <b>205</b> determines whether there are preview images being sampled by the client device <b>115</b>. For example, the current preview image can be the live preview generated on a display screen of the client device <b>115</b> by continuously receiving the image formed on the lens and processed by the image sensor included within the client device <b>115</b>. If the preview images are being sampled, at <b>416</b>, the alignment module <b>205</b> receives a preview image of another portion of the object of interest from the client device <b>115</b>. At <b>418</b>, the user interface module <b>211</b> generates a user interface including the preview image on a display of the client device. At <b>420</b>, the user interface module <b>211</b> adds to the user interface a visually distinct indicator identifying a direction for guiding a movement of the client device for receiving additional preview images from the client device <b>115</b> according to the serpentine pattern. For example, the visually distinct indicator can be a directional arrow pointing in east, west, north or south on the user interface. At <b>422</b>, the alignment module <b>205</b> compares dynamically the features of the reference image with the preview image to determine whether a desired overlap between the reference image and the preview image satisfies a predetermined overlap threshold. For example, the alignment module <b>205</b> uses Hamming distance to compare image descriptors (i.e., 256-bit bitmasks) of the features of the reference image and the preview image of the object of interest to determine the overlap. At <b>424</b>, the alignment module <b>205</b> checks whether the overlap threshold is satisfied. If the overlap threshold is satisfied, at <b>426</b>, the feature extraction module <b>203</b> sets the preview image to be the reference image and the method <b>400</b> repeats the process from step <b>406</b>. If the overlap threshold is not satisfied, the method <b>400</b> repeats the process from step <b>414</b>.</div>
<div class="description-paragraph" id="p-0072" num="0071">If the lateral direction for the serpentine pattern is not known, then at <b>410</b>, the user guidance module <b>207</b> checks whether the reference image is identified lateral to a previous reference image. If the reference image is identified lateral to the previous reference image, then at <b>412</b>, the user guidance module <b>207</b> identifies the lateral direction of the serpentine pattern for guiding the client device <b>115</b> linearly across the object of interest and the method <b>400</b> proceeds to execute step <b>414</b>. For example, if a subsequent image is captured to the left of a previously captured image, the user guidance module <b>207</b> determines that the lateral direction of the serpentine pattern is a right-to-left serpentine pattern for capturing images linearly across the object of interest. If the reference image is not lateral to the previous reference image, then the method <b>400</b> proceeds to execute step <b>414</b>. At <b>414</b>, the alignment module <b>205</b> determines whether there are preview images being sampled by the client device <b>115</b>. If the preview images are not being sampled by the client device <b>115</b>, then at <b>428</b>, the stitching module <b>209</b> sends the images of the portions of the object of interest to generate a single linear panoramic image.</div>
<div class="description-paragraph" id="p-0073" num="0072"> <figref idrefs="DRAWINGS">FIGS. 5A-5B</figref> are flow diagrams illustrating another embodiment of a method <b>500</b> for capturing a series of images of an object of interest in a directionally guided pattern for generating a single linear panoramic image. At <b>502</b>, the user guidance module <b>207</b> receives a selection of a serpentine pattern of image capture for receiving images of an object of interest from a client device. At <b>504</b>, the feature extraction module <b>203</b> receives an image of a portion of the object of interest from the client device and identifies the image as a reference image. For example, the image can be an image of a shelf, a region, an artwork, a landmark, a scenic location, outer space, etc. For example, the direction of movement can be in a north, south, east, or west direction. At <b>506</b>, the feature extraction module <b>203</b> determines features of the reference image. At <b>508</b>, the user guidance module <b>207</b> checks whether a lateral direction for the serpentine pattern is known.</div>
<div class="description-paragraph" id="p-0074" num="0073">If the lateral direction for the serpentine pattern is known, at <b>516</b>, the alignment module <b>205</b> determines whether there are preview images being sampled by the client device <b>115</b>. If the preview images are being sampled, at <b>518</b>, the alignment module <b>205</b> receives a preview image of another portion of the object of interest from the client device <b>115</b>. At <b>520</b>, the user interface module <b>211</b> generates a user interface including the preview image and a progressively growing mosaic preview on a display of the client device. For example, the mosaic preview provides progress information relating to the images received for the object of interest so far. At <b>522</b>, the user interface module <b>211</b> adds to the mosaic preview a thumbnail representation of the reference image and identifies at least one location in the mosaic preview where a subsequent image of the object of interest is to be placed according to the serpentine pattern. At <b>524</b>, the alignment module <b>205</b> compares dynamically the features of the reference image with the preview image to determine whether a desired overlap between the reference image and the preview image satisfies a predetermined overlap threshold. At <b>526</b>, the alignment module <b>205</b> checks whether the overlap threshold is satisfied. If the overlap threshold is satisfied, at <b>528</b>, the feature extraction module <b>203</b> sets the preview image to be the reference image and the method <b>500</b> repeats the process from step <b>506</b>. If the overlap threshold is not satisfied, the method <b>500</b> repeats the process from step <b>516</b>.</div>
<div class="description-paragraph" id="p-0075" num="0074">If the lateral direction for the serpentine pattern is not known, then at <b>510</b>, the user guidance module <b>207</b> checks whether the reference image is identified lateral to a previous reference image. If the reference image is lateral to the previous reference image, then at <b>512</b>, the user guidance module <b>207</b> identifies the lateral direction of the serpentine pattern for guiding the client device <b>115</b> linearly across the object of interest. At <b>514</b>, the user guidance module <b>207</b> slides the progressively growing mosaic preview in an opposite direction on the user interface and method <b>500</b> proceeds to execute step <b>516</b>. For example, the mosaic preview is slid to the left on the user interface if the lateral direction of the serpentine pattern of image capture is a left-to-right direction. If the reference image is not lateral to the previous reference image, then the method <b>500</b> proceeds to execute step <b>516</b>. At <b>516</b>, the alignment module <b>205</b> determines whether there are preview images being sampled by the client device <b>115</b>. If the preview images are not being sampled by the client device <b>115</b>, then at <b>530</b>, the stitching module <b>209</b> sends the images of the portions of the object of interest to generate a single linear panoramic image.</div>
<div class="description-paragraph" id="p-0076" num="0075"> <figref idrefs="DRAWINGS">FIGS. 6A-6B</figref> are flow diagrams illustrating one embodiment of a method <b>600</b> for realigning the current preview image with a previously captured image of an object of interest. At <b>602</b>, the feature extraction module <b>203</b> receives an image of a portion of an object of interest from a client device <b>115</b>. At <b>604</b>, the alignment module <b>205</b> determines whether realignment is needed. For example, the alignment module <b>205</b> may receive a user input to realign a preview image on the client device <b>115</b> with the previously captured image. If realignment is not needed, then the method <b>600</b> ends. If realignment is needed, then at <b>606</b>, the feature extraction module <b>203</b> identifies the image as a ghost image and determines features of the ghost image. At <b>608</b>, the alignment module <b>205</b> determines whether there are preview images being sampled by the client device <b>115</b>. If the preview images are not being sampled, the method <b>600</b> ends. If the preview images are being sampled, then at <b>610</b>, the alignment module <b>205</b> receives a preview image of another portion of the object of interest from the client device <b>115</b>. At <b>612</b>, the user interface module <b>211</b> generates a user interface overlaying the ghost image as a semi-transparent mask on top of the preview image on a display of the client device <b>115</b>. At <b>614</b>, the alignment module <b>205</b> compares dynamically the features of the ghost image with the preview image of the object of interest to determine a realignment between the ghost image and the preview image. At <b>616</b>, the user interface module <b>211</b> adds to the user interface a visually distinct indicator overlaid upon the preview image for guiding a movement of the client device <b>115</b> to produce a desired realignment. At <b>618</b>, the user interface module <b>211</b> updates a position of the visually distinct indicator relative to a target outline at a center of the preview image in the user interface based on the dynamic comparison, the position of the visually distinct indicator inside the target outline indicating that the realignment is successful. At <b>620</b>, the alignment module <b>205</b> checks whether the realignment is successful. If the realignment is successful, at <b>622</b>, the user interface module <b>211</b> updates the user interface to indicate the realignment is successful. If the realignment is not successful, the method <b>600</b> repeats the process from step <b>608</b>.</div>
<div class="description-paragraph" id="h-0007" num="0000">User Interfaces</div>
<div class="description-paragraph" id="p-0077" num="0076">In some embodiments, the alignment module <b>205</b> receives a request from a user of the client device <b>115</b> to capture an image of an object of interest. For example, the image can be an image of a shelf, a region, an artwork, a landmark, a scenic location, outer space, etc. In some embodiments, the alignment module <b>205</b> instructs the user interface module <b>211</b> to generate a user interface for including a preview image of the object of interest on a display of the client device <b>115</b>. The feature extraction module <b>203</b> receives the image captured by the client device <b>115</b> and extracts a set of features for the image. As shown in the example of <figref idrefs="DRAWINGS">FIG. 7A</figref>, the graphical representation illustrates an embodiment of the user interface <b>700</b> for capturing an image of a shelf. For example, the image of the shelf captures a state of the shelf at a retail store. The user interface <b>700</b> in the graphical representation includes a frame <b>701</b> defined by four corner markers <b>702</b> for aligning the client device <b>115</b> with the shelf for image capture, a pair of target outlines <b>703</b> and <b>704</b> of a concentric circles for centering the shelf at the middle of the display, a gyro horizon line <b>705</b> and a pair of tilt-reference arrows <b>709</b> <i>a</i>-<b>709</b> <i>b </i>and <b>711</b> <i>a</i>-<b>711</b> <i>b </i>on the periphery for indicating whether a preview image <b>707</b> of the shelf is off-center and/or tilting before capturing the image. The thin straight line <b>715</b> connecting the tilt reference arrows <b>709</b> <i>a</i>-<b>709</b> <i>b </i>may move laterally left and right in unison with the tilt-reference arrows <b>709</b> <i>a</i>-<b>709</b> <i>b </i>to indicate a tilting of the client device <b>115</b> in an axis of orientation. The thin straight line <b>717</b> connecting the tilt-reference arrows <b>711</b> <i>a</i>-<b>711</b> <i>b </i>may move up and down in unison with the tilt-reference arrows <b>711</b> <i>a</i>-<b>711</b> <i>b </i>to indicate a tilting of the client device <b>115</b> in another axis of orientation. The outer target outline <b>704</b> may include a pair of tilt-reference arrows <b>713</b> <i>a</i>-<b>713</b> <i>b </i>that provides the same functionality of the tilt-reference arrows <b>709</b> <i>a</i>-<b>709</b> <i>b </i>but in a different way. In another example, as shown in <figref idrefs="DRAWINGS">FIG. 7B</figref>, the graphical representation illustrates another embodiment of the user interface <b>750</b> for capturing an image of a shelf. The user interface <b>750</b> in the graphical representation is minimalistic. The tilt-reference arrows <b>709</b> <i>a</i>-<b>709</b> <i>b </i>from <figref idrefs="DRAWINGS">FIG. 7A</figref> are discarded in <figref idrefs="DRAWINGS">FIG. 7B</figref>. The tilt-reference arrows <b>713</b> <i>a</i>-<b>713</b> <i>b </i>placed inside the outer target outline <b>704</b> are made use of instead. The tilt-reference arrows <b>709</b> <i>a</i>-<b>709</b> <i>b </i>in conjunction with the gyro horizon line <b>705</b> may indicate whether the preview image <b>707</b> of the shelf is off-center and/or tilting. For example, the tilt-reference arrows <b>709</b> <i>a</i>-<b>709</b> <i>b </i>and the gyro horizon line <b>705</b> may rotate clockwise/anti-clockwise depending on a direction in which the client device <b>115</b> is rolling about the Z axis. The image of the shelf may be received for recognition and may include multiple items of interest. For example, the image can be an image of packaged products on a shelf (e.g., coffee packages, breakfast cereal boxes, soda bottles, etc.) in a retail store. The packaged product may include textual and pictorial information printed on its surface that distinguishes it from other items on the shelf. In one example, the display of the client device <b>115</b> may flash to indicate that the image was captured in response to the user tapping the screen.</div>
<div class="description-paragraph" id="p-0078" num="0077">In some embodiments, the feature extraction module <b>203</b> receives an image of a portion of an object of interest from the client device <b>115</b>, extracts a set of features from the image and sends the set of features to the alignment module <b>205</b>. The set of features extracted may be robust to variations in scale, rotation, ambient lighting, image acquisition parameters, etc. The feature extraction module <b>203</b> locates each feature in the set of features and determines a location, an orientation, and an image descriptor for each feature. The location may be a relative location to a point in the image (e.g., the location of one identified feature) where each feature occurs. In some embodiments, the feature extraction module <b>203</b> uses corner detection algorithms such as, Shi-Tomasi corner detection algorithm, Harris and Stephens corner detection algorithm, etc., for determining feature location. In some embodiments, the feature extraction module <b>203</b> uses Binary Robust Independent Elementary Features (BRIEF) descriptor approach for determining efficient image feature descriptors. An image descriptor of a feature may be a 256-bit bitmask which describes the image sub-region covered by the feature. In some embodiments, the feature extraction module <b>203</b> may compare each pair of 256 pixel pairs near the feature for intensity and based on each comparison, the feature extraction module <b>203</b> may set or clear one bit in the 256-bit bitmask. In some embodiments, the feature extraction module <b>203</b> determines whether the received image is optimal for image recognition and instructs the user interface module <b>211</b> to generate data for instructing the user to retake the image if a section of the image taken has limited information for complete recognition (e.g., a feature rich portion is cut off), the image is too blurry, the image has an illumination artifact (e.g., excessive reflection), etc. In some embodiments, the feature extraction module <b>203</b> identifies the image captured by the client device <b>115</b> as a reference image and stores the set of identified features for the reference image in a cache. For example, the feature extraction module <b>203</b> processes the image and determines whether it satisfies the criteria (location, orientation and alignment) for being the first image in the series of images needed to form the single linear panoramic image. If it does, then the feature extraction module <b>203</b> identifies the image as a reference image. In other embodiments, the feature extraction module <b>203</b> sends the image captured by the client device <b>115</b> to the stitching module <b>209</b>. In other embodiments, the feature extraction module <b>203</b> receives the preview images of an object of interest from the alignment module <b>205</b>, extracts a set of features from the preview image in real time and sends the set of features to the alignment module <b>205</b>.</div>
<div class="description-paragraph" id="p-0079" num="0078">For purposes of creating a linear panoramic image using a series of images, the user may move the client device <b>115</b> in any direction along the object of interest while remaining parallel to an object of interest for capturing subsequent images following a first image. For example, the user carrying the client device <b>115</b> can move in a north, south, east, or west direction from one point of location to another while remaining parallel to the shelving unit for capturing other images in the series. The images needed for creating the linear panoramic image of a lengthy shelving unit cannot be captured by the user of the client device <b>115</b> by remaining stationary at a fixed point of location. This is because, from a fixed point of location, the user can merely pivot vertically or horizontally for capturing surrounding images that connect to the first image. If the images of the shelf were to be captured in such a manner, the images cannot be stitched together without producing strange artifacts in the panoramic image at locations where two images are stitched together. In some embodiments, the user guidance module <b>207</b> receives a user selection of a pattern of image capture for capturing the series of images. The user guidance module <b>207</b> instructs the user interface module <b>211</b> to provide guidance to the user via the client device <b>115</b> on how to capture a next image in the series of images based on the selected pattern of image capture.</div>
<div class="description-paragraph" id="p-0080" num="0079">In some embodiments, the selected pattern of image capture may be a serpentine scan pattern. In the serpentine scan pattern, the sequence in image capture may alternate between the top and the bottom (or between the left and the right) while the client device <b>115</b> is moving parallel to the object of interest in a horizontal direction (or a vertical direction). The user guidance module <b>207</b> instructs the user interface module <b>211</b> to generate a user interface for guiding a movement of the client device <b>115</b> by the user based on the serpentine scan pattern. For example, the user interface may indicate that the client device <b>115</b> may move first down (or up) the object of interest, then to move to the right (or left) of the object of interest, then to move up (or down) the object of interest, then to move to the right (or left) of the object of interest, and again to move down (or up) the object of interest, in order to follow the serpentine scan pattern. The feature extraction module <b>203</b> receives an image of the object of interest captured by the client device <b>115</b> at the end of each movement.</div>
<div class="description-paragraph" id="p-0081" num="0080">In some embodiments, the selected pattern of image capture may be a raster scan pattern. The raster scan pattern covers the image capture of the object of interest by moving the client device <b>115</b> progressively along the object of interest, one line at a time. The user guidance module <b>207</b> instructs the user interface module <b>211</b> to generate a user interface for guiding a movement of the client device <b>115</b> by the user based on the raster scan pattern. For example, the user interface may indicate that the client device <b>115</b> may move from left-to-right (or right-to-left) of the object of interest in a line, then move down (or up) the object of interest at the end of line and start again from left-to-right (or right-to-left) of the object of interest in a next line, in order to follow the raster scan pattern. The feature extraction module <b>203</b> receives an image of the object of interest captured by the client device <b>115</b> at the end of each movement of the client device <b>115</b> from left-to-right (or right-to-left).</div>
<div class="description-paragraph" id="p-0082" num="0081">In other embodiments, the selected pattern of image capture may be an over-and-back scan pattern. The over-and-back scan pattern covers the image capture of the object of interest by moving the client device <b>115</b> over a portion of the object of interest in a horizontal (or vertical) direction to one end and then moving the client device <b>115</b> back to capture another portion of the object of interest that was not covered. The user guidance module <b>207</b> instructs the user interface module <b>211</b> to generate a user interface for guiding a movement of the client device <b>115</b> by the user based on the over-and-back scan pattern. For example, the user interface may indicate that the client device <b>115</b> may move from left-to-right (or right-to-left) of the object of interest to one end, then move down (or up) the object of interest, and to move from right-to-left (or left-to-right) back to the starting end, in order to follow the over and back scan pattern. The feature extraction module <b>203</b> receives an image of the object of interest captured by the client device <b>115</b> at the end of each movement of the client device <b>115</b> from left-to-right to one end and at the end of each movement of the client device <b>115</b> from right-to-left and back to the starting end.</div>
<div class="description-paragraph" id="p-0083" num="0082">As shown in the example of <figref idrefs="DRAWINGS">FIG. 8</figref>, the graphical representation <b>800</b> illustrates one embodiment of an overlap between images captured of an object of interest. The graphical representation <b>800</b> includes a first captured image <b>801</b> and a second captured image <b>803</b> of a shelving unit <b>805</b> in a retail store. The shelving unit <b>805</b> is stocked with consumer products. The graphical representation <b>800</b> illustrates the overlap <b>807</b> between the first captured image <b>801</b> and the second image <b>803</b>. In some embodiments, the alignment module <b>205</b> instructs the user interface module <b>211</b> to generate a user interface to guide movement of the client device <b>115</b> to capture a next image in the series of images that is overlapping with a previously captured image of the object of interest by a certain amount. The overlap may be computed in either the horizontal or vertical direction depending on which direction the user carrying the capture device moves the client device <b>115</b>. This overlap may be a threshold amount of overlap (e.g., approximately 60%) between the images expected by a stitching algorithm used for creating the linear panorama by stitching together each of the individually captured images in the series. In some embodiments, the image overlap threshold value may be tuned based on the stitching algorithm used by the recognition server <b>101</b>. For example, the stitching algorithm may be the Stitcher class included in the Open Source Computer Vision (OpenCV) package, where feature finding and description algorithms supporting the Stitcher class can be one or more from a group of Binary Robust Invariant Scalable Keypoints (BRISK) algorithm, Fast Retina Keypoint (FREAK) algorithm, Oriented FAST and Rotated BRIEF (ORB) algorithm, etc. In some embodiments, the image overlap threshold value may be other percentages. In some embodiments, the image overlap threshold value may have a range between 55% and 65%. As such, the client device <b>115</b> may tune parameters for capturing images that are compatible and improve the performance of the stitching algorithm</div>
<div class="description-paragraph" id="p-0084" num="0083">In some embodiments, the alignment module <b>205</b> continuously receives the current preview image of a portion of the object of interest as displayed by the client device <b>115</b> when the client device <b>115</b> is pointing at the object of interest. The current preview image can be the live preview generated on a display screen of the client device <b>115</b> by continuously receiving the image formed on the lens and processed by the image sensor included within the client device <b>115</b>. In some embodiments, the alignment module <b>205</b> sends the preview images for the object of interest that are being received continuously from the client device <b>115</b> to the feature extraction module <b>203</b> for extracting the image features. For example, the feature extraction module <b>203</b> determines image features for the images in the camera preview as the client device <b>115</b> moves along the object of interest.</div>
<div class="description-paragraph" id="p-0085" num="0084">In some embodiments, the alignment module <b>205</b> dynamically compares the identified features of a previously captured image of the object of interest with the features of the current preview image being displayed by the client device <b>115</b>. The alignment module <b>205</b> identifies distinctive features in the previously captured image and then efficiently matches them to the features of the current preview image to quickly establish a correspondence between the pair of images. For example, if the variable ‘i’ can be used to represent the most recent, previously captured image, then the image feature set may be represented as F<sub>i</sub>, and therefore the set of image features for the current image in the image pipeline may be represented by F<sub>i+1</sub>. The set of image features for the very first image in the sequence may be represented as F<sub>0</sub>. In some embodiments, the alignment module <b>205</b> determines a similarity function to compare the previously captured image F<sub>i </sub>to the current preview image F<sub>i+1</sub>, to generate a similarity measure S<sub>i</sub>. For example, the formula may be stated as sim (F<sub>i</sub>, F<sub>i+1</sub>)=S<sub>i</sub>. The value S<sub>i </sub>represents the amount of similarity between the previously captured image F<sub>i </sub>and the current preview image F<sub>i+1</sub>.</div>
<div class="description-paragraph" id="p-0086" num="0085">In some embodiments, the alignment module <b>205</b> uses the image overlap threshold as a parameter along with the dynamic feature comparison between the current preview image and the previously captured image for providing guidance and/or feedback to the user via a user interface on the client device <b>115</b>. For example, the alignment module <b>205</b> uses the image overlap threshold to set a similarity value ‘V’ at 0.6. In some embodiments, the alignment module <b>205</b> may receive data including movement of the client device <b>115</b> from the orientation sensors <b>245</b> when the user moves the client device <b>115</b> in one of the directions (e.g., north, south, east or west) parallel to the object of interest after capturing the previous image. In some embodiments, the alignment module <b>205</b> determines a direction of movement of the client device <b>115</b> based on the dynamic feature comparison between the previously captured image of the object of interest and the current preview image as displayed by the client device <b>115</b>. The dynamic feature comparison between the previously captured image and the current preview image determines an extent of the image differentiation. The alignment module <b>205</b> determines whether there is an existing overlap between the previously captured image and the current preview image in the direction of movement of the client device <b>115</b> and whether the existing overlap is approaching a predetermined image overlap threshold when the client device <b>115</b> is moving in the direction of movement. The alignment module <b>205</b> instructs the user interface module <b>211</b> to generate a visually distinct indicator for overlap on the user interface responsive to the determined overlap in the direction of the movement of the client device <b>115</b>. The visually distinct indicator for overlap may be overlaid upon the preview image displayed by the client device <b>115</b>. The visually distinct indicator for overlap can be visually distinct by one or more from the group of a shape, a size, a color, a position, an orientation, and shading.</div>
<div class="description-paragraph" id="p-0087" num="0086">The alignment module <b>205</b> couples the position of the visually distinct indicator for overlap on the user interface with the direction of movement of the client device <b>115</b>. For example, if the user carrying the client device <b>115</b> is moving from left-to-right, the visually distinct indicator for overlap may initially appear on the right side of the display and begin to move to the left side based on the dynamic feature comparison. In another example, if the user carrying the client device <b>115</b> is moving from right-to-left, the visually distinct indicator for overlap may initially appear on the left side of the display and begin to move to the right side based on the dynamic feature comparison. The alignment module <b>205</b> continues to dynamically compare the identified features of the previously captured image of the object of interest with the features of the current preview image in the direction of movement of the client device <b>115</b>. The alignment module <b>205</b> translates the dynamic comparison data in the direction of movement into changing the position of the visually distinct indicator on the user interface which provides the user with instantaneous feedback on how to move the client device <b>115</b> to achieve an optimal overlap satisfying the predetermined overlap threshold. For example, if the overlap between the previously captured image and the current preview image corresponds to a predetermined image overlap threshold (i.e., similarity value ‘V’=60%) in a direction of movement, then the position of the visually distinct indicator for overlap changes on the user interface to indicate that such a condition has been met. The visually distinct indicator for overlap may move into a bounded target outline of a geometric shape such as, a circle, a square, or a polygon overlaid upon the preview image at the center of the display of the client device <b>115</b> to illustrate the condition has been met for optimal overlap. In some embodiments, the alignment module <b>205</b> uses a tolerance value ‘T’ along with similarity value ‘V’ to compute when the visually distinct indicator for overlap is within range, for example, inside the geometric shape. In some embodiments, the alignment module <b>205</b> uses the tolerance value ‘T’ to allow a bit of fuzziness with respect to how much of the visually distinct indicator for overlap needs to be inside of the geometric shape before the image may be captured. In other words, the visually distinct indicator can be partially within the geometric shape and partially outside the geometric shape. The visually distinct indicator may not need to fit exactly within the geometric shape before the image can be captured. In some embodiments, the alignment module <b>205</b> instructs the user interface module <b>211</b> to generate a progress status bar on the user interface to indicate an extent of overlap occurring between the previously captured image and the current preview image until the image overlap threshold is met. For example, the progress status bar may show incremental progress in achieving the overlap. In other embodiments, the alignment module <b>205</b> sends a capture command to the client device <b>115</b> to capture the image responsive to the overlap satisfying the image overlap threshold, receives the image from the client device <b>115</b> and sends the image to the feature extraction module <b>203</b>.</div>
<div class="description-paragraph" id="p-0088" num="0087">In some embodiments, the alignment module <b>205</b> determines a distance measure function along with the similarity function for sending instructions to the user interface module <b>211</b>. For example, the instructions to the user interface module <b>211</b> may be instructions that drive the user interface for displaying the visually distinct indicator for overlap and determine when to capture the image. The distance measure function represents a sum of all similarity measures ‘S’ determined thus far, from image F<sub>0 </sub>(i.e., S<sub>0</sub>) to image F<sub>i </sub>(i.e., S<sub>i</sub>) and may be represented as dist (S<sub>i</sub>). The distance measure function determines how close the two images F<sub>0 </sub>and F<sub>i </sub>are to each other. The alignment module <b>205</b> determines whether the similarity measure S<sub>i </sub>is within the tolerance value ‘T’ of similarity value ‘V’ such that the condition (V−T)&lt;dist (S<sub>i</sub>)&lt;(V+T) is satisfied. If it is satisfied, then the alignment module <b>205</b> sends a capture command to the client device <b>115</b> to capture the image. As the distance measure function dist (S<sub>i</sub>) approaches to being within the tolerance value ‘T’, the alignment module <b>205</b> uses a value produced by the distance measure function dist (S<sub>i</sub>) to represent the visually distinct indicator for overlap getting closer to the geometric shape to fit within the bounded region of the geometric shape on the user interface. For example, this may translate into the visually distinct indicator for overlap appearing less and less transparent on the user interface of the client device <b>115</b>.</div>
<div class="description-paragraph" id="p-0089" num="0088">As shown in the example of <figref idrefs="DRAWINGS">FIG. 9</figref>, the graphical representation <b>900</b> illustrates an embodiment of the image matching process for generating the visually distinct indicator for overlap. In <figref idrefs="DRAWINGS">FIG. 9</figref>, the graphical representation <b>900</b> includes a camera preview frames <b>902</b> for changing image frames (F<sub>1 </sub>to F<sub>4</sub>) based on the user moving the client device <b>115</b> and receiving preview images on the display of the client device <b>115</b>. The graphical representation <b>900</b> also includes a similarity measure function <b>904</b> computed for every two image frames <b>902</b> and a distance measure function <b>906</b> computed for images frames <b>902</b> that have been received so far.</div>
<div class="description-paragraph" id="p-0090" num="0089">As shown in the example of <figref idrefs="DRAWINGS">FIGS. 10A-10D</figref>, the graphical representations illustrate embodiments of the user interface displaying a visually distinct indicator for overlap when the client device <b>115</b> moves in a left-to-right direction. In <figref idrefs="DRAWINGS">FIG. 10A</figref>, the graphical representation illustrates a user interface <b>1000</b> that includes a ball <b>1001</b> (shaded circle) and a pair of target outlines <b>1003</b> and <b>1003</b> of concentric circles over a current preview image <b>1005</b> of the shelf as displayed on the client device <b>115</b>. The ball <b>1001</b> serves as the visually distinct indicator for overlap and initially appears transparent and at the right edge of the display on the user interface <b>1000</b> because of an overlap starting to occur as the client device <b>115</b> is being moved from left-to-right of the shelf. The inner target outline <b>1003</b> of a circle serves as a target boundary region within which the ball <b>1001</b> may be positioned. In some embodiments, the ball <b>1001</b> and the pair of target outlines <b>1003</b> and <b>1003</b> can be customized to be of any color, shading, transparency, orientation, shape, symbol, etc. The aim for the user is to align and position the ball <b>1001</b> within the inner target outline <b>1003</b> on the user interface <b>1000</b> by moving the client device <b>115</b> from left-to-right of the shelf in order to capture an overlapping image being continuously previewed on the display. The alignment of the ball <b>1001</b> within the outer target outline <b>1003</b> but outside of the inner target outline <b>1003</b> signifies that the overlap is good but not enough. The alignment of the ball <b>1001</b> within the inner target outline <b>1003</b> signifies that the overlap between the current preview image <b>1005</b> and a previously captured image is enough to satisfy the image overlap threshold for capturing a next image. In <figref idrefs="DRAWINGS">FIGS. 10B and 10C</figref>, the respective graphical representations illustrate an updated user interfaces <b>1030</b> and <b>1060</b> that display the ball <b>1001</b> moving closer to the inner target outline <b>1003</b> and appearing less and less transparent in color to indicate the desired overlap being produced. In other embodiments, the appearance of the ball <b>1001</b> could be changed to visually indicate the degree of the overlap. For example, the ball <b>1001</b> may change color, shape, transparency, shading, orientation, etc. The position of the ball <b>1001</b>, as it is getting closer and closer to the inner target outline <b>1003</b>, indicates a progress associated with attaining the overlap between the current preview image <b>1005</b> and a previously captured image that corresponds to the image overlap threshold. In <figref idrefs="DRAWINGS">FIG. 10D</figref>, the graphical representation illustrates the user interface <b>1090</b> updated to display the ball <b>1001</b> centered within the inner target outline <b>1003</b> in a solid, non-transparent color. This indicates to the user that the image overlap threshold condition is satisfied for capturing the image. The satisfaction of the overlap threshold could be shown in various other ways by showing the ball <b>1001</b> in a visually distinct manner from its prior state such as, flashing, flashing in a different color, a change in shape (e.g., triangle, pentagon, etc.), a change in fill, etc. In some embodiments, the user interface <b>1090</b> may flash briefly with an audible shutter clicking sound on the client device <b>115</b> to indicate that the image has been captured. In <figref idrefs="DRAWINGS">FIG. 10D</figref>, the user interface <b>1090</b> may be reset and ball <b>1001</b> may disappear from the user interface <b>1090</b> after the image has been captured until the client device <b>115</b> starts to move again in one of the directions over the shelf.</div>
<div class="description-paragraph" id="p-0091" num="0090">In another example of <figref idrefs="DRAWINGS">FIGS. 11A-11D</figref>, the graphical representations illustrate embodiments of displaying a visually distinct indicator for overlap when the client device <b>115</b> moves in a bottom to top direction. In <figref idrefs="DRAWINGS">FIG. 11A</figref>, the graphical representation illustrates a user interface <b>1100</b> that includes a ball <b>1101</b> and a pair of target outlines <b>1103</b> and <b>1104</b> of concentric circles over a current preview image <b>1105</b> of the shelf as displayed on the client device <b>115</b>. The ball <b>1101</b> serves as the visually distinct indicator for overlap and initially appears transparent and at the top edge of the display on the user interface <b>1100</b> because of an overlap starting to occur as the client device <b>115</b> is being moved from bottom to top of the shelf. The aim for the user is to align and position the ball <b>1101</b> within the inner target outline <b>1103</b> on the user interface <b>1100</b> by moving the client device <b>115</b> from bottom to top of the shelf in order to capture an overlapping image being previewed on the display. The alignment of the ball <b>1101</b> within the inner target outline <b>1103</b> signifies that the overlap between the current preview image <b>1105</b> and a previously captured image satisfies the image overlap threshold for capturing a next image. In <figref idrefs="DRAWINGS">FIGS. 11B and 11C</figref>, the respective graphical representations illustrate an updated user interfaces <b>1130</b> and <b>1160</b> that displays the ball <b>1101</b> moving closer to the inner target outline <b>1103</b> and appearing less and less transparent in color. The position of the ball <b>1101</b>, as it is getting closer and closer to the inner target outline <b>1103</b>, indicates a progress associated with attaining the overlap between the current preview image <b>1105</b> and a previously captured image that corresponds to the image overlap threshold. In <figref idrefs="DRAWINGS">FIG. 11D</figref>, the graphical representation illustrates the user interface <b>1190</b> updated to display the ball <b>1101</b> centered within the target outline <b>1103</b> in a solid, non-transparent color. This indicates to the user that the image overlap threshold condition is satisfied for capturing the image. In some embodiments, the user interface <b>1190</b> may flash briefly with an audible shutter clicking sound on the client device <b>115</b> to indicate that the image has been captured. In <figref idrefs="DRAWINGS">FIG. 11D</figref>, the user interface <b>1190</b> may reset and the ball <b>1101</b> may disappear from the user interface <b>1190</b> after the image has been captured until the client device <b>115</b> starts to move again in one of the directions over the shelf.</div>
<div class="description-paragraph" id="p-0092" num="0091">In some embodiments, the feature extraction module <b>203</b> receives subsequent captured images following a first captured image of an object of interest with little to no tilt between the images. The user guidance module <b>207</b> instructs the user interface module <b>211</b> to generate a user interface to guide the user to capture an overlapping image with little to no tilt in any of the axis of orientations (e.g., X, Y, or Z axis). The overlapping images with little to no tilt may be expected by the stitching algorithm for creating a high resolution linear panoramic image which in turn may enable better image recognition. In some embodiments, the user guidance module <b>207</b> receives gyroscopic sensor data including tilting of the client device <b>115</b> in any of the three axes of orientation. The gyroscopic sensor data can be generated by the orientation sensors <b>245</b> included within the client device <b>115</b> that measure an angle of rotation in any of the three axes. For example, the angle of rotation in the X axis is defined by the pitch parameter, the angle of rotation in the Y axis is defined by the yaw parameter, and the angle of rotation in the Z axis is defined by the roll parameter. The user guidance module <b>207</b> determines whether the client device <b>115</b> is tilting in one of the axes of orientation when pointed at the object of interest based on the gyroscopic sensor data. The user guidance module <b>207</b> instructs the user interface module <b>211</b> to generate a visually distinct indicator for tilt on the user interface of the client device <b>115</b> responsive to the client device <b>115</b> tilting in one or more of the axes of orientation. The position and/or appearance of the visually distinct indicator for tilt on the user interface may be coupled to the tilting/orientation of the client device <b>115</b> in such a way that it can indicate through instantaneous feedback when there is a tilt associated with the client device <b>115</b> in any of the three axes of orientation. In one example, the visually distinct indicator for tilt can be a gradient-based indicator to show tilt feedback on the periphery of the user interface on the client device <b>115</b>. The gradient-based indicator can differ in colors for example, a red color for indicating roll, a blue color for indicating pitch, and a white color for indicating yaw. In another example, the visually distinct indicator for tilt can be a horizon line displayed at the center of the user interface on the client device <b>115</b>. In another example, the visually distinct indicator for tilt can be an angle offset indicator to show the angle of rotation about the X axis, Y axis, and Z axis of orientation on the user interface of the client device <b>115</b>. In another example, the visually distinct indicator for tilt can be a line connecting two arrow points on opposite sides of the user interface displayed on the client device <b>115</b>. The movement of the line connecting the two arrow points across the user interface may be configured to show tilt feedback on the user interface. In yet another example, the visually distinct indicator for tilt can be a combination of the gradient-based indicator, the horizon line, and the line connecting the two arrow points. In some embodiments, the user guidance module <b>207</b> instructs the user interface module <b>211</b> to generate a warning notification on the user interface to indicate to the user that the tilt has to be rectified first before the image of the object of interest can be captured.</div>
<div class="description-paragraph" id="p-0093" num="0092">As shown in the example of <figref idrefs="DRAWINGS">FIGS. 12A-12C</figref>, the graphical representations illustrate embodiments of the user interface displaying a visually distinct indicator for tilt when the client device <b>115</b> is rolling about the Z axis. In <figref idrefs="DRAWINGS">FIG. 12A</figref>, the graphical representation illustrates a user interface <b>1200</b> that includes a pair of roll reference arrows <b>1201</b> <i>a</i>-<b>1201</b> <i>b</i>, a pair of pitch reference arrows <b>1209</b> <i>a</i>-<b>1209</b> <i>b </i>and a horizon line <b>1203</b> over a current preview image <b>1205</b> of the shelf as displayed on the client device <b>115</b>. The roll reference arrows <b>1201</b> <i>a</i>-<b>1201</b> <i>b </i>are positioned at the top and the bottom peripheral portion of the user interface <b>1200</b>. They are connected by a thin straight line <b>1207</b> and may serve as the visually distinct indicator for rolling. The pitch reference arrows <b>1209</b> <i>a</i>-<b>1209</b> <i>b </i>are positioned on the left and the right peripheral portion of the user interface <b>1200</b>. They are connected by a thin straight line <b>1211</b> and may serve as the visually distinct indicator for pitching. In <figref idrefs="DRAWINGS">FIG. 12A</figref>, the roll reference arrows <b>1201</b> <i>a</i>-<b>1201</b> <i>b </i>connected by the thin straight line <b>1207</b>, the pitch reference arrows <b>1209</b> <i>a</i>-<b>1209</b> <i>b </i>connected by the thin straight line <b>1211</b> and the horizon line <b>1203</b> are in neutral roll position since the client device <b>115</b> is not tilted pointing at the shelf. In <figref idrefs="DRAWINGS">FIG. 12B</figref>, the graphical representation illustrates an updated user interface <b>1230</b> when the client device <b>115</b> is rolling to the left while being parallel to the shelf. The roll reference arrows <b>1201</b> <i>a</i>-<b>1201</b> <i>b </i>connected by the thin straight line <b>1207</b> move to the left of the user interface <b>1230</b> to indicate the extent of roll associated with the client device <b>115</b> pointing at the shelf. The pitch reference arrows <b>1209</b> <i>a</i>-<b>1209</b> <i>b </i>connected by the thin straight line <b>1211</b> do not change position since the client device <b>115</b> is not pitching. In addition to the roll reference arrows <b>1201</b> <i>a</i>-<b>1201</b> <i>b</i>, the user interface <b>1230</b> also includes a roll gradients <b>1213</b> <i>a </i>and <b>1213</b> <i>b </i>on the periphery of the user interface <b>1230</b> to serve as the visually distinct indicator for rolling. The roll gradients <b>1213</b> <i>a </i>and <b>1213</b> <i>b </i>indicates how off center the tilt is because of the roll to the left. The horizon line <b>1203</b> provides additional information about how far away the client device <b>115</b> is from the neutral roll position. In <figref idrefs="DRAWINGS">FIG. 12C</figref>, the graphical representation illustrates another updated user interface <b>1260</b> when the client device <b>115</b> is rolling to the right while being parallel to the shelf. The roll reference arrows <b>1201</b> <i>a</i>-<b>1201</b> <i>b </i>connected by the thin straight line <b>1207</b> move to the right of the user interface <b>1260</b> to indicate the extent of roll associated with the client device <b>115</b> pointing at the shelf. The roll gradients <b>1213</b> <i>a</i>-<b>1213</b> <i>b </i>again indicate how off center the tilt is because of the roll to the right and the horizon line <b>1203</b> shows how far away the client device <b>115</b> is from the neutral roll position. In some embodiments, the ball <b>1215</b> in the <figref idrefs="DRAWINGS">FIGS. 12B and 12C</figref> may turn a different color yellow to indicate that the client device <b>115</b> is rolling to the left or to the right. In some embodiments, the ball <b>1215</b> may become centered within the inner target outline <b>1217</b> when there is a decent overlap with a previously captured image. The user guidance module <b>207</b> instructs the user interface module <b>211</b> to generate a warning notification on the user interface to indicate to the user that the tilt has to be rectified first before the image can be captured. In some embodiments, the roll reference arrows <b>1201</b> <i>a</i>-<b>1201</b> <i>b </i>may be absent in the user interface. The user interface <b>1200</b> shown in the graphical representation of <figref idrefs="DRAWINGS">FIG. 12A</figref> may be updated to display horizontal grid lines (not shown) instead of the roll reference arrows <b>1201</b> <i>a</i>-<b>1201</b> <i>b</i>. The horizontal grid lines may be displayed visually over the current preview image <b>1205</b>. If tilt occurs when the client device <b>115</b> is rolling about the Z axis, the horizon line <b>1203</b> displayed over the current preview image <b>1205</b> in the user interface may disengage from the neutral roll position and rotate about the center in either clockwise or anticlockwise direction depending on whether the capture device is rolling left or rolling right. The position of the horizon line <b>1203</b> for roll tilt on the user interface may be coupled to the movement of the client device <b>115</b>. The aim for the user is to align and position the horizon line <b>1203</b> parallel to the grid lines by moving the client device <b>115</b>. This may be done to rectify the roll tilt before the image can be captured.</div>
<div class="description-paragraph" id="p-0094" num="0093">As shown in the example of <figref idrefs="DRAWINGS">FIGS. 13A-13C</figref>, the graphical representations illustrate embodiments of the user interface displaying a visually distinct indicator for tilt when the client device <b>115</b> is pitching about the X axis. In <figref idrefs="DRAWINGS">FIG. 13A</figref>, the graphical representation illustrates a user interface <b>1300</b> that includes a pair of pitch reference arrows <b>1301</b> <i>a</i>-<b>1301</b> <i>b </i>and a pair of roll reference arrows <b>1303</b> <i>a</i>-<b>1303</b> <i>b </i>over a current preview image <b>1305</b> of the shelf as displayed on the client device <b>115</b>. The pitch reference arrows <b>1301</b> <i>a</i>-<b>1301</b> <i>b </i>are positioned on the left and the right peripheral portion of the user interface <b>1300</b>. The pitch reference arrows <b>1301</b> <i>a</i>-<b>1301</b> <i>b </i>are connected by a thin straight line <b>1307</b> and may serve as the visually distinct indicator for pitch. In <figref idrefs="DRAWINGS">FIG. 13A</figref>, the pitch reference arrows <b>1301</b> <i>a</i>-<b>1301</b> <i>b </i>are in neutral pitch position since the client device <b>115</b> is not tilted pointing at the shelf. In <figref idrefs="DRAWINGS">FIG. 13B</figref>, the graphical representation illustrates an updated user interface <b>1330</b> when the client device <b>115</b> is pitching forward. The top of the client device <b>115</b> is closer to the top of the shelf and products toward the top of the shelf appear large on the current preview image <b>1205</b>. The pitch reference arrows <b>1301</b> <i>a</i>-<b>1301</b> <i>b </i>connected by the thin straight line <b>1307</b> move to the top of the user interface <b>1330</b> to indicate the extent of pitch associated with the client device <b>115</b> pointing at the shelf. The pair of roll reference arrows <b>1303</b> <i>a</i>-<b>1303</b> <i>b </i>connected by the thin straight line <b>1309</b> do not change position since the client device <b>115</b> is not rolling. In addition to the pitch reference arrows <b>1301</b> <i>a</i>-<b>1301</b> <i>b</i>, the user interface <b>1330</b> also includes a pitch gradients <b>1311</b> <i>a </i>and <b>1311</b> <i>b </i>on the periphery of the user interface to serve as the visually distinct indicator for pitching. The pitch gradients <b>1311</b> <i>a </i>and <b>1311</b> <i>b </i>indicate how much pitch is being sensed by the client device <b>115</b>. In <figref idrefs="DRAWINGS">FIG. 13C</figref>, the graphical representation illustrates another updated user interface <b>1360</b> when the client device <b>115</b> is pitching backward. The bottom of the client device <b>115</b> is closer to the bottom of the shelf and products towards the bottom of the shelf appear large on the current preview image <b>1305</b>. The pitch reference arrows <b>1301</b> <i>a</i>-<b>1301</b> <i>b </i>connected by the thin straight line <b>1307</b> move to the bottom of the user interface <b>1360</b> to indicate the extent of pitch associated with the client device <b>115</b> pointing at the shelf. The pitch gradients <b>1311</b> <i>a </i>and <b>1311</b> <i>b </i>again indicate how much pitch is being sensed by the client device <b>115</b> when it is pitching backward. In some embodiments, the ball <b>1313</b> in the <figref idrefs="DRAWINGS">FIGS. 13B and 13C</figref> may turn a different color to indicate that the client device <b>115</b> is pitching forward or backward.</div>
<div class="description-paragraph" id="p-0095" num="0094">As shown in the example of <figref idrefs="DRAWINGS">FIGS. 14A-14B</figref>, the graphical representations illustrate embodiments of the user interface displaying a visually distinct indicator for tilt when the client device <b>115</b> is tilting in both X and Z axes. In <figref idrefs="DRAWINGS">FIG. 14A</figref>, the graphical representation illustrates a user interface <b>1400</b> when the client device <b>115</b> is pitching forward and rolling to the left while being pointed at the shelf. The thin straight line <b>1415</b> connecting the roll reference arrows <b>1407</b> <i>a</i>-<b>1407</b> <i>b </i>and the thin straight line <b>1417</b> connecting the pitch reference arrows <b>1411</b> <i>a</i>-<b>1411</b> <i>b </i>cross each other outside the inner target outline <b>1403</b> to form the cross point <b>1401</b>. The position of the cross point <b>1401</b> outside the inner target outline <b>1403</b> may indicate to the user visually that the client device <b>115</b> is tilting in the X axis or in the Z axis or in both the X and Z axes. In <figref idrefs="DRAWINGS">FIG. 14B</figref>, the graphical representation illustrates another user interface <b>1450</b> when the client device <b>115</b> is pitching backward and rolling to the right while being pointed at the shelf. The cross point <b>1401</b> is again located outside the target outline <b>1403</b> which indicates to the user visually that the client device <b>115</b> is tilting in the X axis or in the Z axis or in both the X and Z axes. In <figref idrefs="DRAWINGS">FIGS. 14A and 14B</figref>, the peripheral portion of the user interfaces <b>1400</b> and <b>1450</b> including the gradient-based indicators (e.g., roll gradients <b>1409</b> <i>a</i>-<b>1409</b> <i>b</i>, pitch gradients <b>1413</b> <i>a</i>-<b>1413</b> <i>b</i>, etc.) may change color to indicate to the user visually that the client device <b>115</b> is tilting too much in one or more axes. The roll reference arrows <b>1407</b> <i>a</i>-<b>1407</b> <i>b </i>connected by the straight line <b>1415</b> glide left and right and the pitch reference arrows <b>1411</b> <i>a</i>-<b>1411</b> <i>b </i>connected by the straight line <b>1417</b> glide up and down on peripheral of the user interfaces <b>1400</b> and <b>1450</b> in conjunction with their corresponding roll gradients <b>1409</b> <i>a</i>-<b>1409</b> <i>b </i>in the roll (Z) axis and pitch gradients <b>1413</b> <i>a</i>-<b>1413</b> <i>b </i>in the pitch (X) axis to provide instantaneous feedback to the user regarding the tilt.</div>
<div class="description-paragraph" id="p-0096" num="0095">In some embodiments, the alignment module <b>205</b> receives a request from the user to align a current preview image of the object of interest as displayed by the client device <b>115</b> with a view point of a previously captured image after an interruption in the sequence of image capture pattern. For example, the user may get interrupted while capturing an image of a portion of object of interest and may have to leave the scene for a period of time. The user may then want to return to continue capturing subsequent images of the object of interest. In some cases, the user may not remember where they were interrupted in the image capture process. In the example of capturing images of a shelving unit in an aisle, it is critical to restart the image capture process at the same position more or less where the last image was captured before interruption. In some embodiments, the visually distinct indicators for overlap and/or direction may not function unless the user restarts the image capture process from a position of good overlap with the previously captured image. It is important to find a general area where the previous image of the object of interest was captured by the client device <b>115</b> before restarting the image capture process.</div>
<div class="description-paragraph" id="p-0097" num="0096">In some embodiments, the feature extraction module <b>203</b> identifies the previously captured image as a ghost image with which a realignment of the preview image is desired and sends the ghost image to the alignment module <b>205</b>. The alignment module <b>205</b> instructs the user interface module <b>211</b> to generate a user interface that places the previously captured image as a ghost image on top of the current preview image being displayed by the client device <b>115</b>. For example, the user may walk over to a location along the object of interest where they understand the last image was previously captured and use the overlay of the ghost image on top of the current preview image to start the realignment process. The ghost image may appear as a semi-transparent mask overlaid upon the preview image. The alignment module <b>205</b> instructs the user interface module <b>211</b> to update the user interface with a visually distinct indicator for guiding a movement of the client device <b>115</b> to produce a desired realignment. The visually distinct indicator for realignment can be visually distinct by one or more from the group of a shape, a size, a color, a position, an orientation, and shading. The feature extraction module <b>203</b> determines image features for the preview images in the camera preview as the client device <b>115</b> moves along the object of interest and sends the image features to the alignment module <b>205</b>. The alignment module <b>205</b> couples the position of the visually distinct indicator for realignment on the user interface with the movement of the client device <b>115</b>. The alignment module <b>205</b> dynamically compares the identified features of the previously captured image of the object of interest with the features of the current preview image in the direction of movement of the client device <b>115</b>. For example, the set of image features for the previously captured image may be represented as F<sub>0</sub>. The set of image features determined for a preview image frame may be represented by F<sub>i</sub>. As the client device <b>115</b> moves along the object of interest to realign with the previously captured image, the feature extraction module <b>203</b> generates image features for each preview image frame. If variable ‘i’ in F<sub>i </sub>is equal to five (i.e. five preview image frames have been captured not counting the previously captured image and the fifth preview image frame is F<sub>5</sub>), then the alignment module <b>205</b> determines a similarity function to compare the previously captured image F<sub>0 </sub>to the current preview image F<sub>5 </sub>to generate a similarity measure S<sub>5</sub>. For example, the similarity function can be represented as sim (F<sub>0</sub>, F<sub>5</sub>)=S<sub>5</sub>. This value S<sub>5 </sub>represents how similar the two images are to each other and indicates how far the user must move along the object of interest to realign with the previously captured image. The similarity measure S<sub>5 </sub>indicates a comparison with the previously captured image F<sub>0 </sub>serving as the reference image and not with the last image feature set F<sub>4 </sub>that precedes the image feature set F<sub>5</sub>. The alignment module <b>205</b> then translates the dynamic comparison in the direction of movement (i.e., similarity function) into changing the position of the visually distinct indicator on the user interface such that it provides the user with feedback on how to move the client device <b>115</b> to achieve a proper realignment with the previously captured image. In some embodiments, the alignment module <b>205</b> receives a confirmation from the user interface module <b>211</b> that the realignment is successful. In some embodiments, the alignment module <b>205</b> instructs the user interface module <b>211</b> to update the user interface to indicate that the realignment is successful and return the user interface from realignment mode to capture mode that can guide the user on how to capture the next image in the series of images.</div>
<div class="description-paragraph" id="p-0098" num="0097">As shown in the example of <figref idrefs="DRAWINGS">FIG. 15</figref>, the graphical representation <b>1500</b> illustrates an embodiment of the realignment process for generating the visually distinct indicator for realignment. In <figref idrefs="DRAWINGS">FIG. 15</figref>, the graphical representation <b>1500</b> includes camera preview frames <b>1504</b> for changing image frames (F<sub>1 </sub>to F<sub>4</sub>) based on the user moving the client device <b>115</b> along an object of interest. The graphical representation <b>1500</b> also includes a similarity measure function <b>1506</b> computed between features of each preview image frame <b>1504</b> and the features of the previously captured image <b>1502</b>. As described before, the similarity measure function <b>1506</b> represents how similar each preview image frame <b>1504</b> is to the previously captured image <b>1502</b> and indicates how the user must move the client device <b>115</b> along the object of interest to realign a preview image with the previously captured image <b>1502</b>.</div>
<div class="description-paragraph" id="p-0099" num="0098">As shown in the example of <figref idrefs="DRAWINGS">FIGS. 16A-16D</figref>, the graphical representations illustrate embodiment of the user interface displaying realigning current preview image displayed on a client device <b>115</b> with a previously captured image. In <figref idrefs="DRAWINGS">FIG. 16A</figref>, the graphical representation illustrates a user interface <b>1600</b> that includes a ball <b>1601</b> and a pair of target outlines <b>1603</b> and <b>1604</b> of concentric circles over a ghost image <b>1605</b> appearing on top the current preview image <b>1607</b> of the shelf as displayed by the client device <b>115</b>. The ball <b>1601</b> serves as the visually distinct indictor for realignment. The inner target outline <b>1603</b> may appear modified with an ‘X’ crosshair to indicate that the user interface is in realignment mode. The inner target outline <b>1603</b> assumes the same appearance as the align button <b>1609</b> which the user of the client device <b>115</b> selects to start the alignment. The inner target outline <b>1603</b> serves as a target boundary region within which to position the visually distinct indicator for realignment. The aim for the user is to align and position the ball <b>1601</b> within the target outline <b>1603</b> on the user interface <b>1600</b> by moving the client device <b>115</b> to achieve alignment with the ghost image <b>1605</b>. In <figref idrefs="DRAWINGS">FIG. 16B</figref>, the graphical representation illustrates an updated user interface <b>1630</b> that displays the ball <b>1601</b> moving closer to the inner target outline <b>1603</b> as the preview image <b>1607</b> is appearing to realign with the ghost image <b>1605</b>. In <figref idrefs="DRAWINGS">FIG. 16C</figref>, the graphical representation illustrates another user interface <b>1660</b> that displays an updated inner target outline <b>1603</b> to show realignment is almost complete and the ball <b>1601</b> is almost inside the inner target outline <b>1603</b>. The inner target outline <b>1603</b> is back to a regular crosshair. In <figref idrefs="DRAWINGS">FIG. 16D</figref>, the graphical representation illustrates the user interface <b>1690</b> updated to display the current preview image <b>1607</b> after realignment. The ghost image <b>1605</b> from <figref idrefs="DRAWINGS">FIG. 16C</figref> is no longer overlaid upon the preview image <b>1607</b> since the realignment is successful. This indicates to the user that the user interface <b>1690</b> is switched from realignment mode to capture mode and is now ready to capture a next image of the object of interest.</div>
<div class="description-paragraph" id="p-0100" num="0099">As shown in the example of <figref idrefs="DRAWINGS">FIGS. 17A-17F</figref>, the graphical representations illustrate another set of embodiments of the user interface displaying realigning current preview image displayed on a client device <b>115</b> with a previously captured image. In <figref idrefs="DRAWINGS">FIG. 17A</figref>, the graphical representation illustrates a user interface <b>1700</b> that includes an image <b>1702</b> of the shelf as being captured by the client device <b>115</b> when the ball <b>1712</b> gets to be within the inner target outline <b>1714</b>. The user interface <b>1700</b> includes a region <b>1704</b> for displaying a mosaic preview <b>1706</b> of the images of the shelf that may have been captured so far by the client device <b>115</b>. The mosaic preview <b>1706</b> includes an empty thumbnail slot <b>1708</b> serving as the placeholder for a thumbnail representation of the image <b>1702</b> to be affixed to the mosaic preview <b>1706</b>. The empty thumbnail slot <b>1708</b> is labeled ‘<b>4</b>’ since the image <b>1702</b> is the fourth image of the shelf to be captured by the client device <b>115</b>. The user interface <b>1700</b> also indicates a number of images of the shelf that may have been captured so far with a text <b>1710</b> indicating the capture of three images. An example embodiment in reference to the mosaic preview and its construction is described in more detail in <figref idrefs="DRAWINGS">FIGS. 20A-20I</figref>. In <figref idrefs="DRAWINGS">FIG. 17A</figref>, when the user selects the pause button <b>1712</b> to take a break from the capture process, the user interface <b>1700</b> goes from active capture mode into realignment mode. In <figref idrefs="DRAWINGS">FIG. 17B</figref>, the graphical representation illustrates a user interface <b>1715</b> that includes a modified inner target outline <b>1717</b> for realignment overlaid upon a ghost image <b>1719</b> (a semi-transparent image mask) of the previously captured image (i.e. image <b>1702</b> from <figref idrefs="DRAWINGS">FIG. 17A</figref>). The ghost image <b>1719</b> is displayed on the client device <b>115</b> when the user hits the realign button <b>1721</b> to continue the image capture process after the break. The user interface <b>1715</b> updates the mosaic preview <b>1706</b> to include an empty thumbnail slot <b>1723</b> labeled ‘<b>5</b>’ to indicate a location where the fifth image of the shelf may get placed once the realignment is achieved and an image of the shelf is captured. In some embodiments, the mosaic preview <b>1706</b> may also provide a visual reminder to the user of the client device <b>115</b> as to from where on the shelf to start the realignment. In <figref idrefs="DRAWINGS">FIG. 17C</figref>, the graphical representation illustrates a user interface <b>1730</b> that guides the movement of the client device <b>115</b> to realign a preview image <b>1732</b> with a ghost image <b>1719</b> of the previously captured image. The user interface <b>1730</b> indicates that the client device <b>115</b> is pitching forward and the preview image <b>1732</b> is nowhere close in appearance to the ghost image <b>1719</b> of the previously captured image. The user interface <b>1730</b> overlays the ghost image <b>1719</b> over the current preview image <b>1732</b> for visually guiding the movement of the client device <b>115</b>. There is no appearance of a visually distinct indicator such as a ball yet since the client device <b>115</b> is pitching. The ball makes an appearance when the preview image <b>1732</b> and the ghost image <b>1719</b> begin to somewhat realign with each other. In <figref idrefs="DRAWINGS">FIG. 22D</figref>, the graphical representation illustrates an updated user interface <b>1745</b> that displays the ball <b>1747</b> making an appearance outside the modified target outline <b>1717</b>. The appearance of the ball <b>1747</b> indicates that an overlap/realignment between the current preview image <b>1732</b> and the ghost image <b>1719</b> of the previously captured image has been detected since the client device <b>115</b> has moved closer to a location of the previously captured image on the shelf. In <figref idrefs="DRAWINGS">FIG. 17E</figref>, the graphical representation illustrates another user interface <b>1760</b> that displays an updated location for the ball <b>1747</b> near the target outline <b>1717</b> to show realignment is almost complete because of the development of a good overlap between the preview image <b>1732</b> and the ghost image <b>1719</b>. In <figref idrefs="DRAWINGS">FIG. 17F</figref>, the graphical representation illustrates the user interface <b>1775</b> updated to display the current preview image <b>1732</b> after realignment is achieved. There is no overlay of the ghost image <b>1719</b> from <figref idrefs="DRAWINGS">FIG. 17E</figref> in the updated user interface <b>1775</b> since the realignment is successful. This indicates to the user that the user moved the client device <b>115</b> close enough to have the ball <b>1747</b> inside the inner target outline <b>1717</b> in <figref idrefs="DRAWINGS">FIG. 17E</figref>. The user interface <b>1775</b> is now changed to capture mode with the switch back to the inner target outline <b>1714</b> from <figref idrefs="DRAWINGS">FIG. 17A</figref> and ready to capture a next image of the object of interest since realignment is complete.</div>
<div class="description-paragraph" id="p-0101" num="0100">In a retail setting, the process of capturing the state of the shelves may require snapping a lot of images with the appropriate amount of overlap. For example, a minimum of 18 to 24 images may be captured for a 16 feet×8 feet linear shelving unit. In the process of capturing the series of images for creating a linear panoramic image, the user may forget the direction (e.g., north, south, east or west) to move the client device <b>115</b> to capture a subsequent image. In some cases, the user may end up moving the client device <b>115</b> in the wrong direction altogether or in the direction where images have already been captured. For example, the user may move the client device <b>115</b> to the east along the object of interest when the user originally may have had to move the client device <b>115</b> to the south along the object of interest. Such mistakes may not to be conducive to creating a high resolution linear panoramic image of the object of interest and may unduly increase the time spent capturing images of the object of interest. In some embodiments, the user guidance module <b>207</b> instructs the user interface module <b>211</b> to generate user interface elements that can guide the user in the appropriate direction for capturing the series of images.</div>
<div class="description-paragraph" id="p-0102" num="0101">In some embodiments, the user guidance module <b>207</b> instructs the user interface module <b>211</b> to generate a user interface for providing a visually distinct indicator for direction to indicate to the user to move the client device <b>115</b> in the specified direction for capturing the subsequent image in the series of images. In some embodiments, the user guidance module <b>207</b> receives a determination from the alignment module <b>205</b> whether there is an overlap occurring between the previously captured image of the object of interest and the current preview image displayed by the client device <b>115</b> based on dynamic feature comparison. The user guidance module <b>207</b> determines the direction of movement of the client device <b>115</b> based on the overlap occurrence. The user guidance module <b>207</b> instructs the user interface module <b>211</b> to generate the visually distinct indicator for direction on the user interface in the direction of movement. The visually distinct indicator for direction can be visually distinct by one or more from the group of a shape, a size, a color, a position, an orientation, and shading.</div>
<div class="description-paragraph" id="p-0103" num="0102">In some embodiments, the user guidance module <b>207</b> receives a user selection of a pattern of image capture for capturing the series of images. For example, the selected patterns of image capture may be one from a group of a serpentine scan pattern, a raster scan pattern, and an over-and-back scan pattern. As shown in the example of <figref idrefs="DRAWINGS">FIG. 18</figref>, the graphical representation illustrates an embodiment of the serpentine scan pattern of image capture. The graphical representation <b>1800</b> includes a left-to-right serpentine pattern <b>1802</b> and a right-to-left serpentine pattern <b>1804</b> for capturing images linearly across an object of interest. The left-to-right serpentine pattern <b>1802</b> and the right-to-left serpentine pattern <b>1804</b> are shown as starting from the top leftmost position and the top rightmost position, respectively. In other embodiments, the left-to-right serpentine pattern <b>1802</b> and the right-to-left serpentine pattern <b>1804</b> may start from the bottom leftmost position and the bottom rightmost position respectively. The serpentine pattern of image capture may take into account the height and width of the object of interest such that the movement of the client device <b>115</b> in the serpentine pattern can capture the object of interest completely in the series of images. The client device <b>115</b> can be parallel to and facing the object of interest when following the serpentine pattern of image capture. The numerals inside the circles <b>1806</b> in the right-to-left serpentine pattern <b>1804</b> for example, indicate the sequence to follow for capturing the series of images and the arrows <b>1808</b> indicate the direction of movement of the client device <b>115</b> in the right-to-left serpentine pattern <b>1804</b> for capturing the series of images. In some embodiments, the user guidance module <b>207</b> determines a direction of movement for the client device <b>115</b> for capturing the series of images based on the user selected pattern of image capture. The user guidance module <b>207</b> instructs the user interface module <b>211</b> to generate the visually distinct indicator for direction on the user interface based on the capture flow as specified by the user selected pattern of image capture. For example, the user interface module <b>211</b> may generate the visually distinct indicator for direction at the center of the user interface to indicate the zigzag movement of capturing images linearly across the object of interest. The visually distinct indicator for direction may freely point to any direction in 360 degrees at the center of the user interface. The visually distinct indicator for direction may be overlaid upon the current preview image of the object of interest on the user interface. An example embodiment is described below in more detail with reference to <figref idrefs="DRAWINGS">FIGS. 20A-20I</figref>.</div>
<div class="description-paragraph" id="p-0104" num="0103">In some embodiments, the user guidance module <b>207</b> instructs the user interface module <b>211</b> to generate a mosaic preview of the images captured thus far on the user interface for indicating the image capture progress information to the user. For example, the mosaic preview may display an overview of progress of what has been captured so far relating to the object of interest. In some embodiments, the user guidance module <b>207</b> instructs the user interface module <b>211</b> to highlight a position or location with an outline on the mosaic preview. The outline indicates the location where the next image to be captured of the object of interest may be placed. The outline may be replaced with a thumbnail image representation of the object of interest after the image gets captured by the client device <b>115</b>. The mosaic preview may be a progressively growing mosaic preview based on the number of captured images. For example, the mosaic preview may include a numbered thumbnail image of each captured image and a numbered outline of an empty thumbnail slot at the location where the next captured image may get placed in the mosaic preview. Each thumbnail image appears on the mosaic preview after the image of the object of interest corresponding to the location of the thumbnail on the mosaic preview is captured. Users can preview the images captured thus far on the mosaic preview and identify whether the images captured are appropriate for a given retail category.</div>
<div class="description-paragraph" id="p-0105" num="0104">As shown in the example of <figref idrefs="DRAWINGS">FIG. 19</figref>, the graphical representation <b>1900</b> illustrates an embodiment of constructing a mosaic preview using images of a shelving unit. The graphical representation <b>1900</b> includes an outline representation <b>1902</b> of six individual images (numbered <b>1</b>-<b>6</b>) captured of a shelving unit <b>1904</b>. The graphical representation <b>1900</b> illustrates that the six images (numbered <b>1</b>-<b>6</b>) are captured with appropriate overlap (e.g., approximately 60%). The graphical representation <b>1900</b> also includes a reconstruction of the captured images in the format of a mosaic preview <b>1906</b>.</div>
<div class="description-paragraph" id="p-0106" num="0105">In some embodiments, the user guidance module <b>207</b> may determine a direction of movement of the client device <b>115</b> along the object of interest for capturing images under the selected pattern of image capture. For example, the user may initiate the capture session for capturing images of the shelving unit in an aisle from the upper leftmost location (or lower leftmost location) and move the client device <b>115</b> to the right linearly along the shelving unit for capturing the rest of the images in the series. In another example, the user may initiate the capture session for capturing images of the shelving unit in an aisle from the upper rightmost location (or lower rightmost location) and then move the client device <b>115</b> to the left linearly along the shelving unit for capturing the rest of the images in the series. In the above examples, the selected pattern of image capture by the user may be the serpentine pattern of image capture as described in <figref idrefs="DRAWINGS">FIG. 18</figref>. In some embodiments, the user guidance module <b>207</b> may determine the lateral direction for the serpentine pattern of image capture which the user has selected as the pattern for moving the client device <b>115</b> along the object of interest. In some embodiments, the user guidance module <b>207</b> may identify whether a subsequent image is captured lateral to a previous image in the sequence of the serpentine pattern of image capture and determine the direction of the serpentine pattern of the image capture. For example, the user may capture a first image of the shelving unit from the top and move the client device <b>115</b> to the bottom of the shelving unit to capture a second image of the shelf. At this moment, the user may move the client device <b>115</b> laterally either to the left or to the right for capturing a third image in the series. The user guidance module <b>207</b> identifies whether the third image is captured laterally to the left or the right of the second image of the shelf and determines the direction of movement of the client device <b>115</b> along the shelf in the aisle. For example, if the third image captured was to the left of the second captured image, the user guidance module <b>207</b> determines that the direction is a right-to-left serpentine pattern for capturing images linearly across the shelving unit. In another example, if the third image captured was to the right of the second captured image, the user guidance module <b>207</b> determines that the direction is a left-to-right serpentine pattern for capturing images. Accordingly, in some embodiments, the user guidance module <b>207</b> instructs the user interface module <b>211</b> to generate or update the visually distinct indicator for direction on the user interface for capturing subsequent images of the object of interest based on the lateral direction identified for the serpentine pattern of image capture.</div>
<div class="description-paragraph" id="p-0107" num="0106">In some embodiments, the user guidance module <b>207</b> instructs the user interface module <b>211</b> to update the mosaic preview of the captured images to indicate the direction of movement of the client device <b>115</b> along the object of interest. For example, the mosaic preview may be pushed to the left of the user interface to indicate the client device <b>115</b> is following a left-to-right serpentine pattern of image capture. In another example, the mosaic preview may be pushed to the right of the user interface to indicate that the client device <b>115</b> is following a right-to-left serpentine pattern of image capture.</div>
<div class="description-paragraph" id="p-0108" num="0107">As shown in the example of <figref idrefs="DRAWINGS">FIGS. 20A-20I</figref>, the graphical representations illustrate embodiments of the user interface displaying visually distinct indicator for direction of movement of the client device <b>115</b>.</div>
<div class="description-paragraph" id="p-0109" num="0108">In <figref idrefs="DRAWINGS">FIG. 20A</figref>, the graphical representation illustrates a user interface <b>2000</b> that includes a pair of target outlines <b>2002</b> and <b>2004</b> of concentric circles overlaid upon a current preview image <b>2006</b> of the shelving unit as displayed on the client device <b>115</b>. The user interface <b>2000</b> also includes a region <b>2008</b> for displaying a mosaic preview <b>2010</b> of captured images below the current preview image <b>2006</b>. The mosaic preview <b>2010</b> may progressively grow based on the captured images of the shelving unit being added to it. The mosaic preview <b>2010</b> included within the region <b>2008</b> can be pushed either to the right of the region <b>2008</b> or to the left of the region <b>2008</b> depending on whether a movement of client device <b>115</b> along the shelving unit is from right to left or from left to right. The mosaic preview <b>2010</b> (shown empty) in the region <b>2008</b> includes an outline labeled ‘<b>1</b>’ of an empty thumbnail image slot which can get replaced with a first image of the shelving unit when the client device <b>115</b> captures the first image of the shelving unit. In <figref idrefs="DRAWINGS">FIG. 20B</figref>, the graphical representation illustrates an updated user interface <b>2015</b> that includes an arrow <b>2017</b> hanging just outside the inner target outline <b>2002</b> to serve as the visually distinct indicator for direction. The arrow <b>2017</b> can swivel around the inner target outline <b>2002</b> and point in any direction in 360 degrees. The arrow <b>2017</b> can be customized to be of any color, shape, shading, size, symbol, etc. The user interface <b>2015</b> also includes a ball <b>2019</b> that serves as the visually distinct indicator for overlap. The arrow <b>2017</b> is pointing down on the user interface <b>2015</b> to indicate to the user to move the client device <b>115</b> down for capturing a next image of the shelving unit. The mosaic preview <b>2010</b> included within the region <b>2008</b> now includes an outline <b>2021</b> labeled ‘<b>2</b>’ for a second image to fit into the mosaic preview <b>2010</b> at a location as shown by the outline <b>2021</b>. The outline labeled ‘<b>1</b>’ of the mosaic preview <b>2010</b> from <figref idrefs="DRAWINGS">FIG. 20A</figref> is no longer visible in <figref idrefs="DRAWINGS">FIG. 20B</figref> because a thumbnail representation of the first image of the shelving unit has replaced the outline labeled ‘<b>1</b>’ in the mosaic preview <b>2010</b>. In association with the arrow <b>2017</b>, the location of the outline <b>2021</b> on the mosaic preview <b>2010</b> also serves to visually indicate where along the shelf to move the client device to capture the second image. The second image can be captured by moving the client device <b>115</b> in the downward direction to produce a decent overlap with the first image. The arrow <b>2017</b> disappears when the ball <b>2019</b> passes through the outer target outline <b>2004</b> as it is no longer needed to indicate the direction. When the ball <b>2019</b> is aligned and positioned within the inner target outline <b>2002</b>, the second image may be captured. A thumbnail of the captured second image may replace the outline <b>2021</b> labeled ‘<b>2</b>’ in the mosaic preview <b>2010</b> included within the region <b>2008</b>. The undo button <b>2023</b> when pressed by the user may allow the user to back up the shelf and retake the second image if needed. In <figref idrefs="DRAWINGS">FIG. 20C</figref>, the graphical representation illustrates an updated user interface <b>2030</b> that includes two arrows as an example: a right arrow <b>2032</b> and a left arrow <b>2034</b> to demonstrate two possible paths the user can take to capture a next image in the series. In some embodiments, the user interface <b>2030</b> may display either the right arrow <b>2032</b> or the left arrow <b>2034</b> at a time. The mosaic preview <b>2010</b> included within the region <b>2008</b> in the user interface <b>2030</b> now includes two outlines: a left outline <b>2036</b> labeled ‘<b>3</b> <i>a</i>’ and a right outline <b>2038</b> labeled ‘<b>3</b> <i>b</i>’ to indicate to the user to capture a next image that may be either to the left or right of the previous image. Assuming the user is going to move to the right, the user interface <b>2030</b> may be updated to display the right arrow <b>2032</b> and the ball <b>2019</b> on the right of the user interface as the user begins to move the client device <b>115</b> to the right. When the ball <b>2019</b> is aligned and positioned within the inner target outline <b>2002</b>, the third image may be captured. A thumbnail of the captured third image may replace the outline <b>2038</b> labeled ‘<b>3</b> <i>b</i>’ in the mosaic preview <b>2010</b> included within the region <b>2008</b>. In some embodiments, the user guidance module <b>207</b> determines the direction of movement of the client device <b>115</b> along the shelf in the aisle responsive to the third image being captured laterally to the second image. For example, the third image captured was to the right of the second captured image, the user guidance module <b>207</b> determines that the direction of movement of the client device <b>115</b> is a left-to-right serpentine pattern for capturing images. In some embodiments, the user guidance module <b>207</b> may instruct the user interface module <b>211</b> to present the user with a user interface and request the user to indicate from which side of the object of interest (e.g., aisle) to start the image capture process. The user guidance module <b>207</b> determines the direction of movement of the client device <b>115</b> based on the user input.</div>
<div class="description-paragraph" id="p-0110" num="0109">In <figref idrefs="DRAWINGS">FIG. 20D</figref>, the graphical representation illustrates an updated user interface <b>2045</b> that includes up arrow <b>2047</b> to indicate to the user to move the client device <b>115</b> upward to capture the next image of the shelving unit. The user interface <b>2045</b> includes a ball <b>2019</b> appearing at the top. The mosaic preview <b>2010</b> included within the region <b>2008</b> now includes an outline <b>2049</b> labeled ‘<b>4</b>’ for a fourth image to fit into the mosaic preview <b>2010</b> as shown by the location of the outline <b>2049</b>. The fourth image can be captured by moving the client device <b>115</b> in the upward direction and when the ball <b>2019</b> gets aligned and positioned within the inner target outline <b>2002</b>. The mosaic preview <b>2010</b> in the region <b>2008</b> is pushed all the way to the left of region <b>2008</b> to visually indicate that the direction of movement of the client device <b>115</b> is from the left to the right of the shelving unit. In <figref idrefs="DRAWINGS">FIG. 20E</figref>, the graphical representation illustrates an updated user interface <b>2060</b> that includes right arrow <b>2062</b> to indicate to the user to move the client device <b>115</b> to the right to capture the fifth image of the shelving unit. Similarly, in <figref idrefs="DRAWINGS">FIG. 20F</figref>, the graphical representation illustrates an updated user interface <b>2075</b> that includes down arrow <b>2077</b> to indicate to the user to move the client device <b>115</b> down to capture the sixth image of the shelving unit. In <figref idrefs="DRAWINGS">FIGS. 20G-20I</figref>, the graphical representations illustrate embodiments of alternate user interfaces based on the user choosing to move the client device <b>115</b> to the left in <figref idrefs="DRAWINGS">FIG. 20C</figref>. In <figref idrefs="DRAWINGS">FIGS. 20G-20I</figref>, the mosaic preview <b>2010</b> in the region <b>2008</b> is pushed all the way to the right of region <b>2008</b> to visually indicate that the direction of movement of the client device <b>115</b> is from the right to the left of the shelving unit.</div>
<div class="description-paragraph" id="p-0111" num="0110">As shown in the example of <figref idrefs="DRAWINGS">FIG. 21</figref>, the graphical representation illustrates another embodiment of the user interface <b>2100</b> displaying visually distinct indicator for direction of movement of the capture device. In <figref idrefs="DRAWINGS">FIG. 21</figref>, the graphical representation illustrates a user interface <b>2100</b> that includes an arrow <b>2104</b> outside the perimeter of an outer target outline <b>2102</b> to serve as the visually distinct indicator for direction. The arrow <b>2104</b> can swivel around the outer target outline <b>2102</b> and point in any direction in 360 degrees.</div>
<div class="description-paragraph" id="p-0112" num="0111">In some embodiments, the stitching module <b>209</b> receives the images from the feature extraction module <b>203</b> and sends the set of captured images along with the overlap information from the client device <b>115</b> to the recognition server <b>101</b> for stitching a single linear panoramic image. In some embodiments, the stitching module <b>209</b> compares the extracted features of each individual image in the set of captured image to those features stored in the data storage <b>243</b> for recognition. The stitching module <b>209</b> identifies for example, the products in the individual images and uses such information in combination with the overlap information for stitching the set of captured images together into a single linear panoramic image. As shown in the example of <figref idrefs="DRAWINGS">FIGS. 22A-22B</figref>, the graphical representations illustrate embodiments of the user interface for previewing the set of captured images in a mosaic. In <figref idrefs="DRAWINGS">FIG. 22A</figref>, the graphical representation illustrates a user interface <b>2200</b> displaying a mosaic <b>2201</b> previewing the set of all images of the shelf that have been captured so far and stitched together in a single panoramic image using the overlap information and image features obtained when the images were captured. For example, the overlap of the images shown in the user interface <b>2200</b> may be approximately the same as the overlap threshold parameter of 60 percent. The user interface <b>2200</b> also includes a tab <b>2203</b> which the user can slide to view a highlighting of thumbnail images of each one of the individually captured images. In <figref idrefs="DRAWINGS">FIG. 22B</figref>, the graphical representation illustrates a user interface <b>2250</b> highlighting thumbnail images of each one of the individually captured images in response to the user sliding the tab <b>2203</b>. For example, the user may tap the highlighted image <b>2205</b> to view the image in a larger preview user interface. In some embodiments, the stitching module <b>209</b> determines relevant analytical data including information about the state of the shelf from the linear panoramic image. For example, the stitching module <b>209</b> may identify out of stock products, unknown products, etc. from the linear panoramic image. In another example, the stitching module <b>209</b> may determine planogram compliance using the linear panoramic image. The stitching module <b>209</b> may store the panoramic image and associated metadata in the data storage <b>243</b>. The stitching module <b>209</b> may also instruct the user interface module <b>211</b> to provide instructions on the display of the client device <b>115</b> requesting the user to take corrective actions in-store. For example, the corrective action may be to arrange the products on the shelf in compliance with the planogram.</div>
<div class="description-paragraph" id="p-0113" num="0112">A system and method for capturing a series of images to create a linear panorama has been described. In the above description, for purposes of explanation, numerous specific details are set forth in order to provide a thorough understanding of the techniques introduced above. It will be apparent, however, to one skilled in the art that the techniques can be practiced without these specific details. In other instances, structures and devices are shown in block diagram form in order to avoid obscuring the description and for ease of understanding. For example, the techniques are described in one embodiment above primarily with reference to software and particular hardware. However, the present invention applies to any type of computing system that can receive data and commands, and present information as part of any peripheral devices providing services.</div>
<div class="description-paragraph" id="p-0114" num="0113">Reference in the specification to “one embodiment” or “an embodiment” means that a particular feature, structure, or characteristic described in connection with the embodiment is included in at least one embodiment. The appearances of the phrase “in one embodiment” in various places in the specification are not necessarily all referring to the same embodiment.</div>
<div class="description-paragraph" id="p-0115" num="0114">Some portions of the detailed descriptions described above are presented in terms of algorithms and symbolic representations of operations on data bits within a computer memory. These algorithmic descriptions and representations are, in some circumstances, used by those skilled in the data processing arts to convey the substance of their work to others skilled in the art. An algorithm is here, and generally, conceived to be a self-consistent sequence of steps leading to a desired result. The steps are those requiring physical manipulations of physical quantities. Usually, though not necessarily, these quantities take the form of electrical or magnetic signals capable of being stored, transferred, combined, compared, and otherwise manipulated. It has proven convenient at times, principally for reasons of common usage, to refer to these signals as bits, values, elements, symbols, characters, terms, numbers or the like.</div>
<div class="description-paragraph" id="p-0116" num="0115">It should be borne in mind, however, that all of these and similar terms are to be associated with the appropriate physical quantities and are merely convenient labels applied to these quantities. Unless specifically stated otherwise as apparent from the following discussion, it is appreciated that throughout the description, discussions utilizing terms such as “processing”, “computing”, “calculating”, “determining”, “displaying”, or the like, refer to the action and processes of a computer system, or similar electronic computing device, that manipulates and transforms data represented as physical (electronic) quantities within the computer system's registers and memories into other data similarly represented as physical quantities within the computer system memories or registers or other such information storage, transmission or display devices.</div>
<div class="description-paragraph" id="p-0117" num="0116">The techniques also relate to an apparatus for performing the operations herein. This apparatus may be specially constructed for the required purposes, or it may comprise a general-purpose computer selectively activated or reconfigured by a computer program stored in the computer. Such a computer program may be stored in a non-transitory computer readable storage medium, such as, but is not limited to, any type of disk including floppy disks, optical disks, CD-ROMs, and magnetic disks, read-only memories (ROMs), random access memories (RAMs), EPROMs, EEPROMs, magnetic or optical cards, flash memories including USB keys with non-volatile memory or any type of media suitable for storing electronic instructions, each coupled to a computer system bus.</div>
<div class="description-paragraph" id="p-0118" num="0117">Some embodiments can take the form of an entirely hardware embodiment, an entirely software embodiment or an embodiment containing both hardware and software elements. One embodiment is implemented in software, which includes but is not limited to firmware, resident software, microcode, etc.</div>
<div class="description-paragraph" id="p-0119" num="0118">Furthermore, some embodiments can take the form of a computer program product accessible from a computer-usable or computer-readable medium providing program code for use by or in connection with a computer or any instruction execution system. For the purposes of this description, a computer-usable or computer readable medium can be any apparatus that can contain, store, communicate, propagate, or transport the program for use by or in connection with the instruction execution system, apparatus, or device.</div>
<div class="description-paragraph" id="p-0120" num="0119">A data processing system suitable for storing and/or executing program code can include at least one processor coupled directly or indirectly to memory elements through a system bus. The memory elements can include local memory employed during actual execution of the program code, bulk storage, and cache memories which provide temporary storage of at least some program code in order to reduce the number of times code must be retrieved from bulk storage during execution.</div>
<div class="description-paragraph" id="p-0121" num="0120">Input/output or I/O devices (including but not limited to keyboards, displays, pointing devices, etc.) can be coupled to the system either directly or through intervening I/O controllers.</div>
<div class="description-paragraph" id="p-0122" num="0121">Network adapters may also be coupled to the system to enable the data processing system to become coupled to other data processing systems or remote printers or storage devices through intervening private or public networks. Modems, cable modem and Ethernet cards are just a few of the currently available types of network adapters.</div>
<div class="description-paragraph" id="p-0123" num="0122">Finally, the algorithms and displays presented herein are not inherently related to any particular computer or other apparatus. Various general-purpose systems may be used with programs in accordance with the teachings herein, or it may prove convenient to construct more specialized apparatus to perform the required method steps. The required structure for a variety of these systems will appear from the description above. In addition, the techniques are not described with reference to any particular programming language. It will be appreciated that a variety of programming languages may be used to implement the teachings of the various embodiments as described herein.</div>
<div class="description-paragraph" id="p-0124" num="0123">The foregoing description of the embodiments has been presented for the purposes of illustration and description. It is not intended to be exhaustive or to limit the specification to the precise form disclosed. Many modifications and variations are possible in light of the above teaching. It is intended that the scope of the embodiments be limited not by this detailed description, but rather by the claims of this application. As will be understood by those familiar with the art, the examples may be embodied in other specific forms without departing from the spirit or essential characteristics thereof. Likewise, the particular naming and division of the modules, routines, features, attributes, methodologies and other aspects are not mandatory or significant, and the mechanisms that implement the description or its features may have different names, divisions and/or formats. Furthermore, as will be apparent to one of ordinary skill in the relevant art, the modules, routines, features, attributes, methodologies and other aspects of the specification can be implemented as software, hardware, firmware or any combination of the three. Also, wherever a component, an example of which is a module, of the specification is implemented as software, the component can be implemented as a standalone program, as part of a larger program, as a plurality of separate programs, as a statically or dynamically linked library, as a kernel loadable module, as a device driver, and/or in every and any other way known now or in the future to those of ordinary skill in the art of computer programming. Additionally, the specification is in no way limited to embodiment in any specific programming language, or for any specific operating system or environment. Accordingly, the disclosure is intended to be illustrative, but not limiting, of the scope of the specification, which is set forth in the following claims.</div>
</div>
</div>
</section><section itemprop="claims" itemscope="">
<h2>Claims (<span itemprop="count">20</span>)</h2>
<div html="" itemprop="content"><div class="claims" lang="EN" load-source="patent-office" mxw-id="PCLM223925714">
<claim-statement>What is claimed is:</claim-statement>
<div class="claim"> <div class="claim" id="CLM-00001" num="00001">
<div class="claim-text">1. A computer-implemented method comprising:
<div class="claim-text">receiving a first image of a first portion of an object of interest from a client device for serving as a reference image;</div>
<div class="claim-text">for each of a plurality of preview images of other portions of the object of interest:
<div class="claim-text">comparing dynamically the reference image with a preview image to determine whether an overlap between the reference image and the preview image satisfies a predetermined overlap threshold;</div>
<div class="claim-text">responsive to the overlap between the reference image and the preview image satisfying the predetermined overlap threshold, setting the preview image as the reference image; and</div>
</div>
<div class="claim-text">stitching the plurality of reference images together into a single linear panoramic image.</div>
</div>
</div>
</div> <div class="claim-dependent"> <div class="claim" id="CLM-00002" num="00002">
<div class="claim-text">2. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, further comprising generating a first preview user interface including the single linear panoramic image.</div>
</div>
</div> <div class="claim-dependent"> <div class="claim" id="CLM-00003" num="00003">
<div class="claim-text">3. The method of <claim-ref idref="CLM-00002">claim 2</claim-ref>, further comprising highlighting individual reference images in the single linear panoramic image in response to user input.</div>
</div>
</div> <div class="claim-dependent"> <div class="claim" id="CLM-00004" num="00004">
<div class="claim-text">4. The method of <claim-ref idref="CLM-00003">claim 3</claim-ref>, further comprising:
<div class="claim-text">receiving a selection of a highlighted individual reference image from the user; and</div>
<div class="claim-text">generating a second preview user interface including the selected highlighted individual reference image.</div>
</div>
</div>
</div> <div class="claim-dependent"> <div class="claim" id="CLM-00005" num="00005">
<div class="claim-text">5. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, further comprising:
<div class="claim-text">identifying features in the plurality of reference images, wherein stitching the plurality of reference images together into a single linear panoramic image comprises using the features in the plurality of references images and overlap information.</div>
</div>
</div>
</div> <div class="claim-dependent"> <div class="claim" id="CLM-00006" num="00006">
<div class="claim-text">6. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, further comprising:
<div class="claim-text">generating a user interface including the preview image;</div>
<div class="claim-text">adding to the user interface a progressively growing mosaic preview, the mosaic preview including a thumbnail representation of the reference image;</div>
<div class="claim-text">receiving a selection of a pattern of image capture from the client device;</div>
<div class="claim-text">determining whether the reference image of the object of interest is identified lateral to a previous reference image;</div>
<div class="claim-text">identifying a lateral direction of the pattern of image capture responsive to the reference image being identified lateral to the previous reference image; and</div>
<div class="claim-text">adding to the user interface a visually distinct indicator overlaid upon the preview image, the visually distinct indicator identifying a direction for guiding a movement of the client device based on the identified lateral direction of the pattern of image capture.</div>
</div>
</div>
</div> <div class="claim-dependent"> <div class="claim" id="CLM-00007" num="00007">
<div class="claim-text">7. The method of <claim-ref idref="CLM-00006">claim 6</claim-ref>, further comprising receiving a next preview image of a different portion of the object of interest in the direction identified by the visually distinct indicator responsive to the overlap between the reference image and the preview image failing to satisfy the predetermined overlap threshold.</div>
</div>
</div> <div class="claim-dependent"> <div class="claim" id="CLM-00008" num="00008">
<div class="claim-text">8. The method of <claim-ref idref="CLM-00006">claim 6</claim-ref>, wherein the mosaic preview represents a thumbnail reconstruction of one or more reference images received for the object of interest and identifies an outline of a location for a subsequent reference image of the object of interest for placement in the mosaic preview.</div>
</div>
</div> <div class="claim-dependent"> <div class="claim" id="CLM-00009" num="00009">
<div class="claim-text">9. The method of <claim-ref idref="CLM-00006">claim 6</claim-ref>, wherein the pattern of image capture is one from a group of a serpentine scan pattern of image capture, a raster scan pattern of image capture and an over-and-back scan pattern of image capture.</div>
</div>
</div> <div class="claim-dependent"> <div class="claim" id="CLM-00010" num="00010">
<div class="claim-text">10. The method of <claim-ref idref="CLM-00006">claim 6</claim-ref>, wherein the direction for guiding the movement of the client device is one from a group of north, south, east and west.</div>
</div>
</div> <div class="claim"> <div class="claim" id="CLM-00011" num="00011">
<div class="claim-text">11. A system comprising:
<div class="claim-text">one or more processors; and</div>
<div class="claim-text">a memory, the memory storing instructions, which when executed cause the one or more processors to:
<div class="claim-text">receive a first image of a first portion of an object of interest from a client device for serving as a reference image;</div>
<div class="claim-text">for each of a plurality of preview images of other portions of the object of interest:
<div class="claim-text">compare dynamically the reference image with a preview image to determine whether an overlap between the reference image and the preview image satisfies a predetermined overlap threshold;</div>
<div class="claim-text">responsive to the overlap between the reference image and the preview image satisfying the predetermined overlap threshold, set the preview image as the reference image; and</div>
</div>
<div class="claim-text">stitch the plurality of reference images together into a single linear panoramic image.</div>
</div>
</div>
</div>
</div> <div class="claim-dependent"> <div class="claim" id="CLM-00012" num="00012">
<div class="claim-text">12. The system of <claim-ref idref="CLM-00011">claim 11</claim-ref>, wherein the instructions further cause the one or more processors to generate a first preview user interface including the single linear panoramic image.</div>
</div>
</div> <div class="claim-dependent"> <div class="claim" id="CLM-00013" num="00013">
<div class="claim-text">13. The system of <claim-ref idref="CLM-00012">claim 12</claim-ref>, wherein the instructions further cause the one or more processors to highlight individual reference images in the single linear panoramic image in response to user input.</div>
</div>
</div> <div class="claim-dependent"> <div class="claim" id="CLM-00014" num="00014">
<div class="claim-text">14. The system of <claim-ref idref="CLM-00013">claim 13</claim-ref>, wherein the instructions further cause the one or more processors to:
<div class="claim-text">receive a selection of a highlighted individual reference image from the user; and</div>
<div class="claim-text">generate a second preview user interface including the selected highlighted individual reference image.</div>
</div>
</div>
</div> <div class="claim-dependent"> <div class="claim" id="CLM-00015" num="00015">
<div class="claim-text">15. The system of <claim-ref idref="CLM-00011">claim 11</claim-ref>, wherein the instructions further cause the one or more processors to:
<div class="claim-text">identify features in the plurality of reference images, wherein to stich the plurality of reference images together into a single linear panoramic image, the instructions further cause the one or more processors to use the features in the plurality of references images and overlap information.</div>
</div>
</div>
</div> <div class="claim"> <div class="claim" id="CLM-00016" num="00016">
<div class="claim-text">16. A computer program product comprising a non-transitory computer readable medium storing a computer readable program, wherein the computer readable program when executed on a computer causes the computer to:
<div class="claim-text">receive a first image of a first portion of an object of interest from a client device for serving as a reference image;</div>
<div class="claim-text">for each of a plurality of preview images of other portions of the object of interest:
<div class="claim-text">compare dynamically the reference image with a preview image to determine whether an overlap between the reference image and the preview image satisfies a predetermined overlap threshold;</div>
<div class="claim-text">responsive to the overlap between the reference image and the preview image satisfying the predetermined overlap threshold, set the preview image as the reference image; and</div>
<div class="claim-text">stitch the plurality of reference images together into a single linear panoramic image.</div>
</div>
</div>
</div>
</div> <div class="claim-dependent"> <div class="claim" id="CLM-00017" num="00017">
<div class="claim-text">17. The computer program product of <claim-ref idref="CLM-00016">claim 16</claim-ref>, wherein the computer readable program further causes the computer to generate a first preview user interface including the single linear panoramic image.</div>
</div>
</div> <div class="claim-dependent"> <div class="claim" id="CLM-00018" num="00018">
<div class="claim-text">18. The computer program product of <claim-ref idref="CLM-00017">claim 17</claim-ref>, wherein the computer readable program further causes the computer to highlight individual reference images in the single linear panoramic image in response to user input.</div>
</div>
</div> <div class="claim-dependent"> <div class="claim" id="CLM-00019" num="00019">
<div class="claim-text">19. The computer program product of <claim-ref idref="CLM-00018">claim 18</claim-ref>, wherein the computer readable program further causes the computer to:
<div class="claim-text">receive a selection of a highlighted individual reference image from the user; and</div>
<div class="claim-text">generate a second preview user interface including the selected highlighted individual reference image.</div>
</div>
</div>
</div> <div class="claim-dependent"> <div class="claim" id="CLM-00020" num="00020">
<div class="claim-text">20. The computer program product of <claim-ref idref="CLM-00016">claim 16</claim-ref>, wherein the computer readable program further causes the computer to:
<div class="claim-text">identify features in the plurality of reference images, wherein to stich the plurality of reference images together into a single linear panoramic image, the computer readable program further causes the computer to use the features in the plurality of references images and overlap information.</div>
</div>
</div>
</div> </div>
</div>
</section>
                </article>
            </search-app>
        </body>
    </html>
    