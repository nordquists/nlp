
    <html>
        <body>
            <search-app>
                <article class="result" itemscope="" itemtype="http://schema.org/ScholarlyArticle">
    <h1 itemprop="pageTitle">US10659254B2 - Access node integrated circuit for data centers which includes a networking unit, a plurality of host units, processing clusters, a data network fabric, and a control network fabric 
        - Google Patents</h1><section itemprop="abstract" itemscope="">
<h2>Abstract</h2>
<div html="" itemprop="content"><abstract lang="EN" load-source="docdb" mxw-id="PA396701925" source="national office">
<div class="abstract">A highly-programmable access node is described that can be configured and optimized to perform input and output (I/O) tasks, such as storage and retrieval of data to and from storage devices (such as solid state drives), networking, data processing, and the like. For example, the access node may be configured to execute a large number of data I/O processing tasks relative to a number of instructions that are processed. The access node may be highly programmable such that the access node may expose hardware primitives for selecting and programmatically configuring data processing operations. As one example, the access node may be used to provide high-speed connectivity and I/O operations between and on behalf of computing devices and storage components of a network, such as for providing interconnectivity between those devices and a switch fabric of a data center.</div>
</abstract>
</div>
</section><section itemprop="description" itemscope="">
<h2>Description</h2>
<div html="" itemprop="content"><div class="description" lang="EN" load-source="patent-office" mxw-id="PDES263919450">
<div class="description-paragraph" id="p-0002" num="0001">This application claims the benefit of U.S. Provisional Appl. No. 62/530,691, filed Jul. 10, 2017, and U.S. Provisional Appl. No. 62/559,021, filed Sep. 15, 2017, the entire content of each of which is incorporated herein by reference.</div>
<heading id="h-0001">TECHNICAL FIELD</heading>
<div class="description-paragraph" id="p-0003" num="0002">This disclosure relates to computing devices for processing packets of information, for example, in the fields of networking and storage.</div>
<heading id="h-0002">BACKGROUND</heading>
<div class="description-paragraph" id="p-0004" num="0003">In a typical cloud-based data center, a large collection of interconnected servers provides computing and/or storage capacity for execution of various applications. For example, a data center may comprise a facility that hosts applications and services for subscribers, i.e., customers of the data center. The data center may, for example, host all of the infrastructure equipment, such as compute nodes, networking and storage systems, power systems, and environmental control systems. In most data centers, clusters of storage systems and application servers are interconnected via a high-speed switch fabric provided by one or more tiers of physical network switches and routers. Data centers vary greatly in size, with some public data centers containing hundreds of thousands of servers, and are usually distributed across multiple geographies for redundancy. A typical data center switch fabric includes multiple tiers of interconnected switches and routers. In current implementations, packets for a given packet flow between a source server and a destination server or storage system are always forwarded from the source to the destination along a single path through the routers and switches comprising the switching fabric.</div>
<div class="description-paragraph" id="p-0005" num="0004">Conventional compute nodes hosted by data centers typically include components such as a central processing unit (CPU), a graphics processing unit (GPU), random access memory, storage, and a network interface card (MC), such as an Ethernet interface, to connect the compute node to a network, e.g., a data center switch fabric. Typical compute nodes are processor centric such that overall computing responsibility and control is centralized with the CPU. As such, the CPU performs processing tasks, memory management tasks such as shifting data between local caches within the CPU, the random access memory, and the storage, and networking tasks such as constructing and maintaining networking stacks, and sending and receiving data from external devices or networks. Furthermore, the CPU is also tasked with handling interrupts, e.g., from user interface devices. Demands placed on the CPU have continued to increase over time, although performance improvements in development of new CPUs have decreased over time. General purpose CPUs are normally not designed for high-capacity network and storage workloads, which are typically packetized. In general, CPUs are relatively poor at performing packet stream processing, because such traffic is fragmented in time and does not cache well. Nevertheless, server devices typically use CPUs to process packet streams.</div>
<heading id="h-0003">SUMMARY</heading>
<div class="description-paragraph" id="p-0006" num="0005">In general, this disclosure describes various example implementations of a highly-programmable access node that can be configured and optimized to perform input and output (I/O) tasks, such as storage and retrieval of data to and from storage devices (such as solid state drives), networking, data processing, and the like. For example, the access node may be configured to execute a large number of data I/O processing tasks relative to a number of instructions that are processed. As one example, the access node may be used to provide high-speed connectivity and I/O operations between and on behalf of application processors and storage components of a network, such as for providing interconnectivity between those devices and a switch fabric of a data center. As various examples, the access node may be provided as an integrated circuit mounted on a motherboard of a computing device, or installed on a card connected to the motherboard, such as via a Peripheral Component Interconnect-Express (PCI-e) bus, cable or the like.</div>
<div class="description-paragraph" id="p-0007" num="0006">The access node may be highly programmable such that the access node may expose hardware primitives for selecting and programmatically configuring data processing operations. For example, the access node may include hardware implementations of high-performance data processing tasks, such as cryptography, compression (including decompression), regular expression processing, lookup engines, or the like.</div>
<div class="description-paragraph" id="p-0008" num="0007">The access node may include a plurality of processing clusters that each include at least two processing cores for performing processing tasks (e.g., to process work units), a central cluster that schedules work among the various processing clusters, a networking unit, and/or one or more host units. Each of the processing cores in the processing clusters may be programmable using a high-level programming language, e.g., C, C++, or the like. The one or more host units of the access node may provide PCI-e bus lines, which can be coupled to the server devices and/or to storage devices, such as solid state drives (SSDs). The networking unit of the access node may communicatively couple the server devices to a network, such as a data center fabric, without the need for a separate network interface card (NIC). In addition, the networking unit may perform other tasks, such as Internet protocol security (IPsec), intrusion detection/prevention, firewall, encryption for secure sockets layer (SSL), or the like.</div>
<div class="description-paragraph" id="p-0009" num="0008">In one example, this disclosure is directed to an access node integrated circuit comprising a networking unit configured to control input and output of data between a network and the access node integrated circuit; one or more host units configured to at least one of control input and output of the data between the access node integrated circuit and one or more application processors or control storage of the data with one or more storage devices; a plurality of processing clusters, each of the processing clusters including two or more programmable processing cores configured to perform processing tasks on the data; a data network fabric interconnecting the plurality of processing clusters, the one or more host units, and the networking unit, wherein the data network fabric is configured to carry the data between the networking unit, the one or more host units, and the plurality of processing clusters; and at least one control network fabric interconnecting the plurality of processing clusters, the one or more host units, and the networking unit, wherein the at least one control network fabric is configured to carry control messages identifying the processing tasks to be performed on the data by the programmable processing cores of the plurality of processing clusters.</div>
<div class="description-paragraph" id="p-0010" num="0009">In another example, this disclosure is directed to a system comprising a plurality of server devices; a plurality of storage devices; a network; and a computing device including an access node integrated circuit. The access node integrated circuit comprising a networking unit configured to control input and output of data between the network and the access node integrated circuit; one or more host units configured to at least one of control input and output of the data between the access node integrated circuit and the server devices or control storage of the data with the storage devices; a plurality of processing clusters, each of the processing clusters including two or more programmable processing cores configured to perform processing tasks on the data; a data network fabric interconnecting the plurality of processing clusters, the one or more host units, and the networking unit, wherein the data network fabric is configured to carry the data between the networking unit, the one or more host units, and the plurality of processing clusters; and at least one control network fabric interconnecting the plurality of processing clusters, the one or more host units, and the networking unit, wherein the at least one control network fabric is configured to carry control messages identifying the processing tasks to be performed on the data by the programmable processing cores of the plurality of processing clusters.</div>
<div class="description-paragraph" id="p-0011" num="0010">In a further example, this disclosure is directed to a method comprising receiving, by an access node integrated circuit of a computing device, data to be processed, wherein the access node integrated circuit includes a networking unit configured to control input and output of the data with a network and at least one host unit configured to at least one of control input and output of the data with one or more application processors or control storage of the data with one or more storage devices; receiving, by a processing cluster of a plurality of processing clusters included in the access node integrated circuit and from one of the networking unit, the host unit, or another one of the processing clusters via a control network fabric of the access node integrated circuit, a work unit indicating a processing task to be performed on the data; processing, by a programmable processing core of two or more programmable processing cores included in the processing cluster, the work unit, wherein processing the work unit includes retrieving the data on which the processing task is to be performed from one of the networking unit, the host unit, or one of the processing clusters via a data network fabric of the access node integrated circuit; and receiving, by the processing cluster and from the programmable processing core, output results of the processing task performed on the data.</div>
<div class="description-paragraph" id="p-0012" num="0011">The details of one or more examples are set forth in the accompanying drawings and the description below. Other features, objects, and advantages will be apparent from the description and drawings, and from the claims.</div>
<description-of-drawings>
<heading id="h-0004">BRIEF DESCRIPTION OF DRAWINGS</heading>
<div class="description-paragraph" id="p-0013" num="0012"> <figref idrefs="DRAWINGS">FIG. 1</figref> is a block diagram illustrating an example network having a data center in which examples of the techniques described herein may be implemented.</div>
<div class="description-paragraph" id="p-0014" num="0013"> <figref idrefs="DRAWINGS">FIG. 2</figref> is a block diagram illustrating in further detail the logical interconnectivity provided by access nodes and switch fabric within a data center.</div>
<div class="description-paragraph" id="p-0015" num="0014"> <figref idrefs="DRAWINGS">FIG. 3</figref> is a block diagram illustrating one example of network storage compute unit (NSCU) <b>40</b> including an access node group and its supported servers.</div>
<div class="description-paragraph" id="p-0016" num="0015"> <figref idrefs="DRAWINGS">FIG. 4</figref> is a block diagram illustrating an example logical rack arrangement including two NSCUs from <figref idrefs="DRAWINGS">FIG. 3</figref>.</div>
<div class="description-paragraph" id="p-0017" num="0016"> <figref idrefs="DRAWINGS">FIG. 5</figref> is a block diagram illustrating an example of full mesh connectivity between two access node groups within a logical rack.</div>
<div class="description-paragraph" id="p-0018" num="0017"> <figref idrefs="DRAWINGS">FIG. 6</figref> is a block diagram illustrating an example arrangement of a full physical rack including two logical racks from <figref idrefs="DRAWINGS">FIG. 4</figref>.</div>
<div class="description-paragraph" id="p-0019" num="0018"> <figref idrefs="DRAWINGS">FIG. 7A</figref> is a block diagram showing a logical view of the networking data paths and operations within an access node.</div>
<div class="description-paragraph" id="p-0020" num="0019"> <figref idrefs="DRAWINGS">FIG. 7B</figref> is a block diagram illustrating an example first-level network fanout achieved between a set of access nodes within a logical rack.</div>
<div class="description-paragraph" id="p-0021" num="0020"> <figref idrefs="DRAWINGS">FIG. 8</figref> is a block diagram illustrating an example multi-level network fanout across a data center switch fabric between access nodes.</div>
<div class="description-paragraph" id="p-0022" num="0021"> <figref idrefs="DRAWINGS">FIG. 9</figref> is a block diagram illustrating an example access node including two or more processing clusters, in accordance with the techniques of this disclosure.</div>
<div class="description-paragraph" id="p-0023" num="0022"> <figref idrefs="DRAWINGS">FIG. 10A</figref> is a block diagram illustrating an example processing cluster including a plurality of programmable processing cores.</div>
<div class="description-paragraph" id="p-0024" num="0023"> <figref idrefs="DRAWINGS">FIG. 10B</figref> is a block diagram illustrating an example programmable processing core of a processing cluster.</div>
<div class="description-paragraph" id="p-0025" num="0024"> <figref idrefs="DRAWINGS">FIG. 11</figref> is a flow diagram illustrating an example process by which a processing cluster processes a work unit.</div>
<div class="description-paragraph" id="p-0026" num="0025"> <figref idrefs="DRAWINGS">FIG. 12</figref> is a flow diagram illustrating an example process by which a host unit processes a data request.</div>
<div class="description-paragraph" id="p-0027" num="0026"> <figref idrefs="DRAWINGS">FIG. 13</figref> is a flow diagram illustrating an example transmission pipeline processing flow for processing stream data, such as packets.</div>
<div class="description-paragraph" id="p-0028" num="0027"> <figref idrefs="DRAWINGS">FIG. 14</figref> is a series of flow diagrams illustrating various example processing flows from a networking unit to a host unit or back to the networking unit.</div>
<div class="description-paragraph" id="p-0029" num="0028"> <figref idrefs="DRAWINGS">FIG. 15</figref> is a series of flow diagrams illustrating various example processing flows from a host unit to a networking unit.</div>
<div class="description-paragraph" id="p-0030" num="0029"> <figref idrefs="DRAWINGS">FIG. 16</figref> is a flowchart illustrating an example operation of an access node performing data processing, in accordance with the techniques described herein.</div>
</description-of-drawings>
<heading id="h-0005">DETAILED DESCRIPTION</heading>
<div class="description-paragraph" id="p-0031" num="0030"> <figref idrefs="DRAWINGS">FIG. 1</figref> is a block diagram illustrating an example system <b>8</b> having a data center <b>10</b> in which examples of the techniques described herein may be implemented. In general, data center <b>10</b> provides an operating environment for applications and services for customers <b>11</b> coupled to data center <b>10</b> by content/service provider network <b>7</b> and gateway device <b>20</b>. In other examples, content/service provider network <b>7</b> may be a data center wide-area network (DC WAN), private network or other type of network. Data center <b>10</b> may, for example, host infrastructure equipment, such as compute nodes, networking and storage systems, redundant power supplies, and environmental controls. Content/service provider network <b>7</b> may be coupled to one or more networks administered by other providers, and may thus form part of a large-scale public network infrastructure, e.g., the Internet.</div>
<div class="description-paragraph" id="p-0032" num="0031">In some examples, data center <b>10</b> may represent one of many geographically distributed network data centers. In the example of <figref idrefs="DRAWINGS">FIG. 1</figref>, data center <b>10</b> is a facility that provides information services for customers <b>11</b>. Customers <b>11</b> may be collective entities such as enterprises and governments or individuals. For example, a network data center may host web services for several enterprises and end users. Other exemplary services may include data storage, virtual private networks, file storage services, data mining services, scientific- or super-computing services, and so on.</div>
<div class="description-paragraph" id="p-0033" num="0032">In this example, data center <b>10</b> includes a set of storage systems and application servers <b>12</b> interconnected via a high-speed switch fabric <b>14</b>. In some examples, servers <b>12</b> are arranged into multiple different server groups, each including any number of servers up to, for example, n servers <b>12</b> <sub>1</sub>-<b>12</b> <sub>n</sub>. Servers <b>12</b> provide computation and storage facilities for applications and data associated with customers <b>11</b> and may be physical (bare-metal) servers, virtual machines running on physical servers, virtualized containers running on physical servers, or combinations thereof.</div>
<div class="description-paragraph" id="p-0034" num="0033">In the example of <figref idrefs="DRAWINGS">FIG. 1</figref>, software-defined networking (SDN) controller <b>21</b> provides a high-level controller for configuring and managing the routing and switching infrastructure of data center <b>10</b>. SDN controller <b>21</b> provides a logically and in some cases physically centralized controller for facilitating operation of one or more virtual networks within data center <b>10</b> in accordance with one or more embodiments of this disclosure. In some examples, SDN controller <b>21</b> may operate in response to configuration input received from a network administrator.</div>
<div class="description-paragraph" id="p-0035" num="0034">In some examples, SDN controller <b>21</b> operates to configure access nodes <b>17</b> to logically establish one or more virtual fabrics as overlay networks dynamically configured on top of the physical underlay network provided by switch fabric <b>14</b>, in accordance with the techniques described herein. For example, SDN controller <b>21</b> may learn and maintain knowledge of access nodes <b>21</b> and establish a communication control channel with each of the access nodes. SDN controller <b>21</b> uses its knowledge of access nodes <b>17</b> to define multiple sets (groups) of two of more access nodes <b>17</b> to establish different virtual fabrics over switch fabric <b>14</b>. More specifically, SDN controller <b>21</b> may use the communication control channels to notify each of access nodes <b>17</b> for a given set which other access nodes are included in the same set. In response, access nodes <b>17</b> dynamically setup tunnels with the other access nodes included in the same set as a virtual fabric over switch fabric <b>14</b>. In this way, SDN controller <b>21</b> defines the sets of access nodes <b>17</b> for each of the virtual fabrics, and the access nodes are responsible for establishing the virtual fabrics. As such, underlay components of switch fabric <b>14</b> may be unaware of virtual fabrics. In these examples, access nodes <b>17</b> interface with and utilize switch fabric <b>14</b> so as to provide full mesh (any-to-any) interconnectivity between access nodes of any given virtual fabric. In this way, the servers connected to any of the access nodes forming a given one of virtual fabrics may communicate packet data for a given packet flow to any other of the servers coupled to the access nodes for that virtual fabric using any of a number of parallel data paths within switch fabric <b>14</b> that interconnect the access nodes of that virtual fabric. More details of access nodes operating to spray packets within and across virtual overlay networks are available in U.S. Provisional Patent Application No. 62/638,788, filed Mar. 5, 2018, entitled “Network Access Node Virtual Fabrics Configured Dynamically over an Underlay Network,” the entire content of which is incorporated herein by reference.</div>
<div class="description-paragraph" id="p-0036" num="0035">Although not shown, data center <b>10</b> may also include, for example, one or more non-edge switches, routers, hubs, gateways, security devices such as firewalls, intrusion detection, and/or intrusion prevention devices, servers, computer terminals, laptops, printers, databases, wireless mobile devices such as cellular phones or personal digital assistants, wireless access points, bridges, cable modems, application accelerators, or other network devices.</div>
<div class="description-paragraph" id="p-0037" num="0036">In the example of <figref idrefs="DRAWINGS">FIG. 1</figref>, each of servers <b>12</b> is coupled to switch fabric <b>14</b> by an access node <b>17</b>. As further described herein, in one example, each access node <b>17</b> is a highly programmable I/O processor specially designed for offloading certain functions from servers <b>12</b>. In one example, each of access nodes <b>17</b> includes one or more processing cores consisting of a number of internal processor clusters, e.g., MIPS cores, equipped with hardware engines that offload cryptographic functions, compression and regular expression (RegEx) processing, data storage functions and networking operations. In this way, each access node <b>17</b> includes components for fully implementing and processing network and storage stacks on behalf of one or more servers <b>12</b>. In addition, access nodes <b>17</b> may be programmatically configured to serve as a security gateway for its respective servers <b>12</b>, freeing up the processors of the servers to dedicate resources to application workloads. In some example implementations, each access node <b>17</b> may be viewed as a network interface subsystem that implements full offload of the handling of data packets (with zero copy in server memory) and storage acceleration for the attached server systems. In one example, each access node <b>17</b> may be implemented as one or more application-specific integrated circuit (ASIC) or other hardware and software components, each supporting a subset of the servers.</div>
<div class="description-paragraph" id="p-0038" num="0037">Access nodes <b>17</b> may also be referred to as data processing units (DPUs), or devices including DPUs. In other words, the term access node may be used herein interchangeably with the term DPU. Additional example details of various example DPUs are described in U.S. Provisional Patent Application No. 62/530,691, filed Jul. 10, 2017, entitled “Data Processing Unit for Computing Devices,” the entire content of which is incorporated herein by reference.</div>
<div class="description-paragraph" id="p-0039" num="0038">In example implementations, access nodes <b>17</b> are configurable to operate in a standalone network appliance having one or more access nodes. For example, access nodes <b>17</b> may be arranged into multiple different access node groups <b>19</b>, each including any number of access nodes up to, for example, x access nodes <b>17</b> <sub>1</sub>-<b>17</b> <sub>x</sub>. As such, multiple access nodes <b>17</b> may be grouped (e.g., within a single electronic device or network appliance), referred to herein as an access node group <b>19</b>, for providing services to a group of servers supported by the set of access nodes internal to the device. In one example, an access node group <b>19</b> may comprise four access nodes <b>17</b>, each supporting four servers so as to support a group of sixteen servers.</div>
<div class="description-paragraph" id="p-0040" num="0039">In the example of <figref idrefs="DRAWINGS">FIG. 1</figref>, each access node <b>17</b> provides connectivity to switch fabric <b>14</b> for a different group of servers <b>12</b> and may be assigned respective IP addresses and provide routing operations for the servers <b>12</b> coupled thereto. As described herein, access nodes <b>17</b> provide routing and/or switching functions for communications from/directed to the individual servers <b>12</b>. For example, as shown in <figref idrefs="DRAWINGS">FIG. 1</figref>, each access node <b>17</b> includes a set of edge-facing electrical or optical local bus interfaces for communicating with a respective group of servers <b>12</b> and one or more core-facing electrical or optical interfaces for communicating with core switches within switch fabric <b>14</b>. In addition, access nodes <b>17</b> described herein may provide additional services, such as storage (e.g., integration of solid-state storage devices), security (e.g., encryption), acceleration (e.g., compression), I/O offloading, and the like. In some examples, one or more of access nodes <b>17</b> may include storage devices, such as high-speed solid-state drives or rotating hard drives, configured to provide network accessible storage for use by applications executing on the servers. Although not shown in <figref idrefs="DRAWINGS">FIG. 1</figref>, access nodes <b>17</b> may be directly coupled to each other, such as direct coupling between access nodes in a common access node group <b>19</b>, to provide direct interconnectivity between the access nodes of the same group. For example, multiple access nodes <b>17</b> (e.g., 4 access nodes) may be positioned within a common access node group <b>19</b> for servicing a group of servers (e.g., 16 servers).</div>
<div class="description-paragraph" id="p-0041" num="0040">As one example, each access node group <b>19</b> of multiple access nodes <b>17</b> may be configured as standalone network device, and may be implemented as a two rack unit (2RU) device that occupies two rack units (e.g., slots) of an equipment rack. In another example, access node <b>17</b> may be integrated within a server, such as a single 1RU server in which four CPUs are coupled to the forwarding ASICs described herein on a mother board deployed within a common computing device. In yet another example, one or more of access nodes <b>17</b> and servers <b>12</b> may be integrated in a suitable size (e.g., 10RU) frame that may, in such an example, become a network storage compute unit (NSCU) for data center <b>10</b>. For example, an access node <b>17</b> may be integrated within a mother board of a server <b>12</b> or otherwise co-located with a server in a single chassis.</div>
<div class="description-paragraph" id="p-0042" num="0041">According to the techniques herein, example implementations are described in which access nodes <b>17</b> interface and utilize switch fabric <b>14</b> so as to provide full mesh (any-to-any) interconnectivity such that any of servers <b>12</b> may communicate packet data for a given packet flow to any other of the servers using any of a number of parallel data paths within the data center <b>10</b>. Example network architectures and techniques are described in which access nodes, in example implementations, spray individual packets for packet flows between the access nodes and across some or all of the multiple parallel data paths in the data center switch fabric <b>14</b> and reorder the packets for delivery to the destinations so as to provide full mesh connectivity.</div>
<div class="description-paragraph" id="p-0043" num="0042">As described herein, the techniques of this disclosure introduce a new data transmission protocol referred to as a Fabric Control Protocol (FCP) that may be used by the different operational networking components of any of access nodes <b>17</b> to facilitate communication of data across switch fabric <b>14</b>. As further described, FCP is an end-to-end admission control protocol in which, in one example, a sender explicitly requests a receiver with the intention to transfer a certain number of bytes of payload data. In response, the receiver issues a grant based on its buffer resources, QoS, and/or a measure of fabric congestion. In general, FCP enables spray of packets of a flow to all paths between a source and a destination node, and may provide any of the advantages and techniques described herein, including resilience against request/grant packet loss, adaptive and low latency fabric implementations, fault recovery, reduced or minimal protocol overhead cost, support for unsolicited packet transfer, support for FCP capable/incapable nodes to coexist, flow-aware fair bandwidth distribution, transmit buffer management through adaptive request window scaling, receive buffer occupancy based grant management, improved end to end QoS, security through encryption and end to end authentication and/or improved ECN marking support. More details on the FCP are available in U.S. Provisional Patent Application No. 62/566,060, filed Sep. 29, 2017, entitled “Fabric Control Protocol for Data Center Networks with Packet Spraying Over Multiple Alternate Data Paths,” the entire content of which is incorporated herein by reference.</div>
<div class="description-paragraph" id="p-0044" num="0043">The techniques may provide certain advantages. For example, the techniques may increase significantly the bandwidth utilization of the underlying switch fabric <b>14</b>. Moreover, in example implementations, the techniques may provide full mesh interconnectivity between the servers of the data center and may nevertheless be non-blocking and drop-free.</div>
<div class="description-paragraph" id="p-0045" num="0044">Although access nodes <b>17</b> are described in <figref idrefs="DRAWINGS">FIG. 1</figref> with respect to switch fabric <b>14</b> of data center <b>10</b>, in other examples, access nodes may provide full mesh interconnectivity over any packet switched network. For example, the packet switched network may include a local area network (LAN), a wide area network (WAN), or a collection of one or more networks. The packet switched network may have any topology, e.g., flat or multi-tiered, as long as there is full connectivity between the access nodes. The packet switched network may use any technology, including IP over Ethernet as well as other technologies. Irrespective of the type of packet switched network, in accordance with the techniques described in this disclosure, access nodes may spray individual packets for packet flows between the access nodes and across multiple parallel data paths in the packet switched network and reorder the packets for delivery to the destinations so as to provide full mesh connectivity.</div>
<div class="description-paragraph" id="p-0046" num="0045"> <figref idrefs="DRAWINGS">FIG. 2</figref> is a block diagram illustrating in further detail the logical interconnectivity provided by access nodes <b>17</b> and switch fabric <b>14</b> within the data center. As shown in this example, access nodes <b>17</b> and switch fabric <b>14</b> may be configured to provide full mesh interconnectivity such that access nodes <b>17</b> may communicate packet data for any of servers <b>12</b> to any other of the servers <b>12</b> using any of a number of M parallel data paths to any of core switches <b>22</b>A-<b>22</b>M (collectively “core switches <b>22</b>”). Moreover, according to the techniques described herein, access nodes <b>17</b> and switch fabric <b>14</b> may be configured and arranged in a way such that the M parallel data paths in switch fabric <b>14</b> provide reduced L2/L3 hops and full mesh interconnections (e.g., bipartite graph) between servers <b>12</b>, even in massive data centers having tens of thousands of servers. Note that in this example, switches <b>22</b> are not connected to each other, which makes it much more likely that any failure of one or more of the switches will be independent of each other. In other examples, the switch fabric itself may be implemented using multiple layers of interconnected switches as in a CLOS network.</div>
<div class="description-paragraph" id="p-0047" num="0046">In some example implementations, each access node <b>17</b> may, therefore, have multiple parallel data paths for reaching any given other access node <b>17</b> and the servers <b>12</b> reachable through those access nodes. In some examples, rather than being limited to sending all of the packets of a given flow along a single path in the switch fabric, switch fabric <b>14</b> may be configured such that access nodes <b>17</b> may, for any given packet flow between servers <b>12</b>, spray the packets of the packet flow across all or a subset of the M parallel data paths of switch fabric <b>14</b> by which a given destination access node <b>17</b> for a destination server <b>12</b> can be reached.</div>
<div class="description-paragraph" id="p-0048" num="0047">According to the disclosed techniques, access nodes <b>17</b> may spray the packets of individual packet flows across the M paths end-to-end forming a virtual tunnel between a source access node and a destination access node. In this way, the number of layers included in switch fabric <b>14</b> or the number of hops along the M parallel data paths, may not matter for implementation of the packet spraying techniques described in this disclosure.</div>
<div class="description-paragraph" id="p-0049" num="0048">The technique of spraying packets of individual packet flows across all or a subset of the M parallel data paths of switch fabric <b>14</b>, however, enables the number of layers of network devices within switch fabric <b>14</b> to be reduced, e.g., to a bare minimum of one. Further, it enables fabric architectures in which the switches are not connected to each other, reducing the likelihood of failure dependence between two switches and thereby increasing the reliability of the switch fabric. Flattening switch fabric <b>14</b> may reduce cost by eliminating layers of network devices that require power and reduce latency by eliminating layers of network devices that perform packet switching. In one example, the flattened topology of switch fabric <b>14</b> may result in a core layer that includes only one level of spine switches, e.g., core switches <b>22</b>, that may not communicate directly with one another but form a single hop along the M parallel data paths. In this example, any access node <b>17</b> sourcing traffic into switch fabric <b>14</b> may reach any other access node <b>17</b> by a single, one-hop L3 lookup by one of core switches <b>22</b>.</div>
<div class="description-paragraph" id="p-0050" num="0049">An access node <b>17</b> sourcing a packet flow for a source server <b>12</b> may use any technique for spraying the packets across the available parallel data paths, such as available bandwidth, random, round-robin, hash-based or other mechanism that may be designed to maximize, for example, utilization of bandwidth or otherwise avoid congestion. In some example implementations, flow-based load balancing need not necessarily be utilized and more effective bandwidth utilization may be used by allowing packets of a given packet flow (five tuple) sourced by a server <b>12</b> to traverse different paths of switch fabric <b>14</b> between access nodes <b>17</b> coupled to the source and destinations servers. The respective destination access node <b>17</b> associated with the destination server <b>12</b> may be configured to reorder the variable length IP packets of the packet flows and deliver the packets to the destination server in the sequence in which they were sent.</div>
<div class="description-paragraph" id="p-0051" num="0050">In some example implementations, each access node <b>17</b> implements at least four different operational networking components or functions: (1) a source component operable to receive traffic from server <b>12</b>, (2) a source switching component operable to switch source traffic to other source switching components of different access nodes <b>17</b> (possibly of different access node groups) or to core switches <b>22</b>, (3) a destination switching component operable to switch inbound traffic received from other source switching components or from cores switches <b>22</b> and (4) a destination component operable to reorder packet flows and provide the packet flows to destination servers <b>12</b>.</div>
<div class="description-paragraph" id="p-0052" num="0051">In this example, servers <b>12</b> are connected to source components of the access nodes <b>17</b> to inject traffic into the switch fabric <b>14</b>, and servers <b>12</b> are similarly coupled to the destination components within the access nodes <b>17</b> to receive traffic therefrom. Because of the full-mesh, parallel data paths provided by switch fabric <b>14</b>, each source switching component and destination switching component within a given access node <b>17</b> need not perform L2/L3 switching. Instead, access nodes <b>17</b> may apply spraying algorithms to spray packets of a packet flow, e.g., available bandwidth, randomly, round-robin, based on QoS/scheduling or otherwise to efficiently forward packets without, in some examples, requiring packet analysis and lookup operations.</div>
<div class="description-paragraph" id="p-0053" num="0052">Destination switching components of access nodes <b>17</b> may provide a limited lookup necessary only to select the proper output port for forwarding packets to local servers <b>12</b>. As such, with respect to full routing tables for the data center, only core switches <b>22</b> may need to perform full lookup operations. Thus, switch fabric <b>14</b> provides a highly-scalable, flat, high-speed interconnect in which servers <b>12</b> are, in some embodiments, effectively one L2/L3 hop from any other server <b>12</b> within the data center.</div>
<div class="description-paragraph" id="p-0054" num="0053">Access nodes <b>17</b> may need to connect to a fair number of core switches <b>22</b> in order to communicate packet data to any other of access nodes <b>17</b> and the servers <b>12</b> accessible through those access nodes. In some cases, to provide a link multiplier effect, access nodes <b>17</b> may connect to core switches <b>22</b> via top of rack (TOR) Ethernet switches, electrical permutation devices, or optical permutation (OP) devices (not shown in <figref idrefs="DRAWINGS">FIG. 2</figref>). To provide an additional link multiplier effect, source components of the access nodes <b>17</b> may be configured to spray packets of individual packet flows of the traffic received from server <b>12</b> across a set of the other access nodes <b>17</b> included in one or more access node groups <b>19</b>. In one example, access node <b>17</b> may achieve an 8× multiplier effect from inter-access node spraying, and an additional 8× multiplier effect from OP devices to connect to up to sixty-four core switches <b>22</b>.</div>
<div class="description-paragraph" id="p-0055" num="0054">Flow-based routing and switching over Equal Cost Multi-Path (ECMP) paths through a network may be susceptible to highly variable load-dependent latency. For example, the network may include many small bandwidth flows and a few large bandwidth flows. In the case of routing and switching over ECMP paths, the source access node may select the same path for two of the large bandwidth flows leading to large latencies over that path. In order to avoid this issue and keep latency low across the network, an administrator may be forced to keep the utilization of the network below 25-30%, for example. The techniques described in this disclosure of configuring access nodes <b>17</b> to spray packets of individual packet flows across all available paths enables higher network utilization, e.g., 85-90%, while maintaining bounded or limited latencies. The packet spraying techniques enable a source access node <b>17</b> to fairly distribute packets of a given flow across all the available paths while taking link failures into account. In this way, regardless of the bandwidth size of the given flow, the load can be fairly spread across the available paths through the network to avoid over utilization of a particular path. The disclosed techniques enable the same amount of networking devices to pass three times the amount of data traffic through the network while maintaining low latency characteristics and reducing a number of layers of network devices that consume energy.</div>
<div class="description-paragraph" id="p-0056" num="0055">As shown in the example of <figref idrefs="DRAWINGS">FIG. 2</figref>, in some example implementations, access nodes <b>17</b> may be arranged into multiple different access node groups <b>19</b> <sub>1</sub>-<b>19</b> <sub>Y </sub>(ANGs in <figref idrefs="DRAWINGS">FIG. 2</figref>), each including any number of access nodes <b>17</b> up to, for example, x access nodes <b>17</b> <sub>1</sub>-<b>17</b> <sub>x</sub>. As such, multiple access nodes <b>17</b> may be grouped and arranged (e.g., within a single electronic device or network appliance), referred to herein as an access node group (ANG) <b>19</b>, for providing services to a group of servers supported by the set of access nodes internal to the device.</div>
<div class="description-paragraph" id="p-0057" num="0056">As described, each access node group <b>19</b> may be configured as standalone network device, and may be implemented as a device configured for installation within a compute rack, a storage rack or a converged rack. In general, each access node group <b>19</b> may be configured to operate as a high-performance I/O hub designed to aggregate and process network and/or storage I/O for multiple servers <b>12</b>. As described above, the set of access nodes <b>17</b> within each of the access node groups <b>19</b> provide highly-programmable, specialized I/O processing circuits for handling networking and communications operations on behalf of servers <b>12</b>. In addition, in some examples, each of access node groups <b>19</b> may include storage devices <b>27</b>, such as high-speed solid-state hard drives, configured to provide network accessible storage for use by applications executing on the servers. Each access node group <b>19</b> including its set of access nodes <b>17</b>, storage devices <b>27</b>, and the set of servers <b>12</b> supported by the access nodes <b>17</b> of that access node group may be referred to herein as a network storage compute unit (NSCU) <b>40</b>.</div>
<div class="description-paragraph" id="p-0058" num="0057"> <figref idrefs="DRAWINGS">FIG. 3</figref> is a block diagram illustrating one example of network storage compute unit (NSCU) <b>40</b> including an access node group <b>19</b> and its supported servers <b>52</b>. Access node group <b>19</b> may be configured to operate as a high-performance I/O hub designed to aggregate and process network and storage I/O to multiple servers <b>52</b>. In the particular example of <figref idrefs="DRAWINGS">FIG. 3</figref>, access node group <b>19</b> includes four access nodes <b>17</b> <sub>1</sub>-<b>17</b> <sub>4 </sub>(collectively, “access nodes <b>17</b>”) connected to a pool of local solid state storage <b>41</b>. In the illustrated example, access node group <b>19</b> supports a total of sixteen server nodes <b>12</b> <sub>1</sub>-<b>12</b> <sub>16 </sub>(collectively, “server nodes <b>12</b>”) with each of the four access nodes <b>17</b> within access node group <b>19</b> supporting four of server nodes <b>12</b>. In some examples, each of the four server nodes <b>12</b> supported by each of the access nodes <b>17</b> may be arranged as a server <b>52</b>. In some examples, the “servers <b>12</b>” described throughout this application may be dual-socket or dual-processor “server nodes” that are arranged in groups of two or more within a standalone server device, e.g., servers <b>52</b>.</div>
<div class="description-paragraph" id="p-0059" num="0058">Although access node group <b>19</b> is illustrated in <figref idrefs="DRAWINGS">FIG. 3</figref> as including four access nodes <b>17</b> that are all connected to a single pool of solid state storage <b>41</b>, an access node group may be arranged in other ways. In one example, each of the four access nodes <b>17</b> may be included on an individual access node sled that also includes solid state storage and/or other types of storage for the access node. In this example, an access node group may include four access node sleds each having an access node and a set of local storage devices.</div>
<div class="description-paragraph" id="p-0060" num="0059">In one example implementation, access nodes <b>17</b> within access node group <b>19</b> connect to servers <b>52</b> and solid state storage <b>41</b> using Peripheral Component Interconnect express (PCIe) links <b>48</b>, <b>50</b>, and connect to other access nodes and the datacenter switch fabric <b>14</b> using Ethernet links <b>42</b>, <b>44</b>, <b>46</b>. For example, each of access nodes <b>17</b> may support six high-speed Ethernet connections, including two externally-available Ethernet connections <b>42</b> for communicating with the switch fabric, one externally-available Ethernet connection <b>44</b> for communicating with other access nodes in other access node groups, and three internal Ethernet connections <b>46</b> for communicating with other access nodes <b>17</b> in the same access node group <b>19</b>. In one example, each of externally-available connections <b>42</b> may be a 100 Gigabit Ethernet (GE) connection. In this example, access node group <b>19</b> has 8×100 GE externally-available ports to connect to the switch fabric <b>14</b>.</div>
<div class="description-paragraph" id="p-0061" num="0060">Within access node group <b>19</b>, connections <b>42</b> may be copper, i.e., electrical, links arranged as 8×25 GE links between each of access nodes <b>17</b> and optical ports of access node group <b>19</b>. Between access node group <b>19</b> and the switch fabric, connections <b>42</b> may be optical Ethernet connections coupled to the optical ports of access node group <b>19</b>. The optical Ethernet connections may connect to one or more optical devices within the switch fabric, e.g., optical permutation devices. The optical Ethernet connections may support more bandwidth than electrical connections without increasing the number of cables in the switch fabric. For example, each optical cable coupled to access node group <b>19</b> may carry 4×100 GE optical fibers with each fiber carrying optical signals at four different wavelengths or lambdas. In other examples, the externally-available connections <b>42</b> may remain as electrical Ethernet connections to the switch fabric.</div>
<div class="description-paragraph" id="p-0062" num="0061">The four remaining Ethernet connections supported by each of access nodes <b>17</b> include one Ethernet connection <b>44</b> for communication with other access nodes within other access node groups, and three Ethernet connections <b>46</b> for communication with the other three access nodes within the same access node group <b>19</b>. In some examples, connections <b>44</b> may be referred to as “inter-access node group links” and connections <b>46</b> may be referred to as “intra-access node group links.”</div>
<div class="description-paragraph" id="p-0063" num="0062">Ethernet connections <b>44</b>, <b>46</b> provide full-mesh connectivity between access nodes within a given structural unit. In one example, such a structural unit may be referred to herein as a logical rack (e.g., a half-rack or a half physical rack) that includes two NSCUs <b>40</b> having two AGNs <b>19</b> and supports an 8-way mesh of eight access nodes <b>17</b> for those AGNs. In this particular example, connections <b>46</b> would provide full-mesh connectivity between the four access nodes <b>17</b> within the same access node group <b>19</b>, and connections <b>44</b> would provide full-mesh connectivity between each of access nodes <b>17</b> and four other access nodes within one other access node group of the logical rack (i.e., structural unit). In addition, access node group <b>19</b> may have enough, e.g., sixteen, externally-available Ethernet ports to connect to the four access nodes in the other access node group.</div>
<div class="description-paragraph" id="p-0064" num="0063">In the case of an 8-way mesh of access nodes, i.e., a logical rack of two NSCUs <b>40</b>, each of access nodes <b>17</b> may be connected to each of the other seven access nodes by a 50 GE connection. For example, each of connections <b>46</b> between the four access nodes <b>17</b> within the same access node group <b>19</b> may be a 50 GE connection arranged as 2×25 GE links. Each of connections <b>44</b> between the four access nodes <b>17</b> and the four access nodes in the other access node group may include four 50 GE links. In some examples, each of the four 50 GE links may be arranged as 2×25 GE links such that each of connections <b>44</b> includes 8×25 GE links to the other access nodes in the other access node group. This example is described in more detail below with respect to <figref idrefs="DRAWINGS">FIG. 5</figref>.</div>
<div class="description-paragraph" id="p-0065" num="0064">In another example, Ethernet connections <b>44</b>, <b>46</b> provide full-mesh connectivity between access nodes within a given structural unit that is a full-rack or a full physical rack that includes four NSCUs <b>40</b> having four AGNs <b>19</b> and supports a 16-way mesh of access nodes <b>17</b> for those AGNs. In this example, connections <b>46</b> provide full-mesh connectivity between the four access nodes <b>17</b> within the same access node group <b>19</b>, and connections <b>44</b> provide full-mesh connectivity between each of access nodes <b>17</b> and twelve other access nodes within three other access node group. In addition, access node group <b>19</b> may have enough, e.g., forty-eight, externally-available Ethernet ports to connect to the four access nodes in the other access node group.</div>
<div class="description-paragraph" id="p-0066" num="0065">In the case of a 16-way mesh of access nodes, each of access nodes <b>17</b> may be connected to each of the other fifteen access nodes by a 25 GE connection, for example. In other words, in this example, each of connections <b>46</b> between the four access nodes <b>17</b> within the same access node group <b>19</b> may be a single 25 GE link. Each of connections <b>44</b> between the four access nodes <b>17</b> and the twelve other access nodes in the three other access node groups may include 12×25 GE links.</div>
<div class="description-paragraph" id="p-0067" num="0066">As shown in <figref idrefs="DRAWINGS">FIG. 3</figref>, each of access nodes <b>17</b> within an access node group <b>19</b> may also support a set of high-speed PCIe connections <b>48</b>, <b>50</b>, e.g., PCIe Gen 3.0 or PCIe Gen 4.0 connections, for communication with solid state storage <b>41</b> within access node group <b>19</b> and communication with servers <b>52</b> within NSCU <b>40</b>. Each of servers <b>52</b> includes four server nodes <b>12</b> supported by one of access nodes <b>17</b> within access node group <b>19</b>. Solid state storage <b>41</b> may be a pool of Non-Volatile Memory express (NVMe)-based solid state drive (SSD) storage devices accessible by each of access nodes <b>17</b> via connections <b>48</b>.</div>
<div class="description-paragraph" id="p-0068" num="0067">In one example, solid state storage <b>41</b> may include twenty-four SSD devices with six SSD devices for each of access nodes <b>17</b>. The twenty-four SSD devices may be arranged in four rows of six SSD devices with each row of SSD devices being connected to one of access nodes <b>17</b>. Each of the SSD devices may provide up to 16 Terabytes (TB) of storage for a total of 384 TB per access node group <b>19</b>. As described in more detail below, in some cases, a physical rack may include four access node groups <b>19</b> and their supported servers <b>52</b>. In that case, a typical physical rack may support approximately 1.5 Petabytes (PB) of local solid state storage. In another example, solid state storage <b>41</b> may include up to 32 U.2×4 SSD devices. In other examples, NSCU <b>40</b> may support other SSD devices, e.g., 2.5″ Serial ATA (SATA) SSDs, mini-SATA (mSATA) SSDs, M.2 SSDs, and the like.</div>
<div class="description-paragraph" id="p-0069" num="0068">In the above described example in which each of the access nodes <b>17</b> is included on an individual access node sled with local storage for the access node, each of the access node sleds may include four SSD devices and some additional storage that may be hard drive or solid state drive devices. In this example, the four SSD devices and the additional storage may provide approximately the same amount of storage per access node as the six SSD devices described in the previous example.</div>
<div class="description-paragraph" id="p-0070" num="0069">In one example, each of access nodes <b>17</b> supports a total of 96 PCIe lanes. In this example, each of connections <b>48</b> may be an 8×4-lane PCI Gen 3.0 connection via which each of access nodes <b>17</b> may communicate with up to eight SSD devices within solid state storage <b>41</b>. In addition, each of connections <b>50</b> between a given access node <b>17</b> and the four server nodes <b>12</b> within the server <b>52</b> supported by the access node <b>17</b> may be a 4×16-lane PCIe Gen 3.0 connection. In this example, access node group <b>19</b> has a total of 256 external facing PCIe links that interface with servers <b>52</b>. In some scenarios, access nodes <b>17</b> may support redundant server connectivity such that each of access nodes <b>17</b> connects to eight server nodes <b>12</b> within two different servers <b>52</b> using an 8×8-lane PCIe Gen 3.0 connection.</div>
<div class="description-paragraph" id="p-0071" num="0070">In another example, each of access nodes <b>17</b> supports a total of 64 PCIe lanes. In this example, each of connections <b>48</b> may be an 8×4-lane PCI Gen 3.0 connection via which each of access nodes <b>17</b> may communicate with up to eight SSD devices within solid state storage <b>41</b>. In addition, each of connections <b>50</b> between a given access node <b>17</b> and the four server nodes <b>12</b> within the server <b>52</b> supported by the access node <b>17</b> may be a 4×8-lane PCIe Gen 4.0 connection. In this example, access node group <b>19</b> has a total of 128 external facing PCIe links that interface with servers <b>52</b>.</div>
<div class="description-paragraph" id="p-0072" num="0071"> <figref idrefs="DRAWINGS">FIG. 4</figref> is a block diagram illustrating an example logical rack arrangement <b>60</b> including two NSCUs <b>40</b> <sub>1 </sub>and <b>40</b> <sub>2 </sub>from <figref idrefs="DRAWINGS">FIG. 3</figref>. In some examples, each of NSCUs <b>40</b> may be referred to as a “compute sandwich” based on the structural arrangement of access node group <b>19</b> “sandwiched” between two servers <b>52</b> on the top and two servers <b>52</b> on the bottom. For example, server <b>52</b>A may be referred to as a top second server, server <b>52</b>B may be referred to as a top server, server <b>52</b>C may be referred to as a bottom server, and server <b>52</b>D may be referred to as a bottom second server. Each of servers <b>52</b> may include four server nodes, and each server node may be a dual-socket or dual-processor server sled.</div>
<div class="description-paragraph" id="p-0073" num="0072">Each of access node groups <b>19</b> connects to servers <b>52</b> using PCIe links <b>50</b>, and to switch fabric <b>14</b> using Ethernet links <b>42</b>. Access node groups <b>19</b> <sub>1 </sub>and <b>19</b> <sub>2 </sub>may each include four access nodes connected to each other using Ethernet links and local solid state storage connected to the access nodes using PCIe links as described above with respect to <figref idrefs="DRAWINGS">FIG. 3</figref>. The access nodes within access node groups <b>19</b> <sub>1 </sub>and <b>19</b> <sub>2 </sub>are connected to each other in a full mesh <b>64</b>, which is described in more detail with respect to <figref idrefs="DRAWINGS">FIG. 5</figref>.</div>
<div class="description-paragraph" id="p-0074" num="0073">In addition, each of access node groups <b>19</b> supports PCIe connections <b>50</b> to servers <b>52</b>. In one example, each of connections <b>50</b> may be a 4×16-lane PCIe Gen 3.0 connection such that access node group <b>19</b> has a total of 256 externally-available PCIe links that interface with servers <b>52</b>. In another example, each of connections <b>50</b> may be a 4×8-lane PCIe Gen 4.0 connection for communication between access nodes within access node group <b>19</b> and server nodes within servers <b>52</b>. In either example, connections <b>50</b> may provide a raw throughput of 512 Gigabits per access node <b>19</b> or approximately 128 Gigabits of bandwidth per server node without accounting for any overhead bandwidth costs.</div>
<div class="description-paragraph" id="p-0075" num="0074">As discussed above with respect to <figref idrefs="DRAWINGS">FIG. 3</figref>, each of NSCUs <b>40</b> supports 8×100 GE links <b>42</b> from access node group <b>19</b> to switch fabric <b>14</b>. Each of NSCUs <b>40</b> thus provides support for up to sixteen server nodes in four servers <b>52</b>, local solid state storage, and 800 Gbps of full duplex (i.e., bidirectional) network bandwidth. Each of access node groups <b>19</b> may, therefore, provide true hyper-convergence of compute, storage, networking and security of servers <b>52</b>. Logical rack <b>60</b>, including two NSCUs <b>40</b>, therefore, provides support for up to thirty-two server nodes in eight servers <b>52</b>, local solid state storage at access node groups <b>19</b>, and 16×100 GE links <b>42</b> to switch fabric <b>14</b>, which results in 1.6 Terabits per second (Tbps) of full duplex network bandwidth.</div>
<div class="description-paragraph" id="p-0076" num="0075"> <figref idrefs="DRAWINGS">FIG. 5</figref> is a block diagram illustrating an example of full mesh connectivity between two access node groups <b>19</b> <sub>1</sub>, <b>19</b> <sub>2 </sub>within a logical rack <b>60</b>. As illustrated in <figref idrefs="DRAWINGS">FIG. 5</figref>, access node group <b>19</b> <sub>1 </sub>includes four access nodes <b>17</b> <sub>1</sub>-<b>17</b> <sub>4 </sub>and access node group <b>19</b> <sub>2 </sub>also include four access nodes <b>17</b> <sub>5</sub>-<b>17</b> <sub>8</sub>. Each of access nodes <b>17</b> connects to the other access nodes within the logical rack in a mesh fabric topology. The eight access nodes <b>17</b> included in the mesh topology may be referred to as an access node “cluster.” In this way, each of access nodes <b>17</b> is able to spray incoming packets to each of the other access nodes in the cluster.</div>
<div class="description-paragraph" id="p-0077" num="0076">In the illustrated configuration of an 8-way mesh interconnecting two access node groups <b>19</b>, each access node <b>17</b> connects via full mesh connectivity to each of the other seven access nodes in the cluster. The mesh topology between access nodes <b>17</b> includes intra-access node group links <b>46</b> between the four access nodes included in the same access node group <b>19</b>, and inter-access node group links <b>44</b> between access nodes <b>17</b> <sub>1</sub>-<b>17</b> <sub>4 </sub>in access node group <b>19</b> <sub>1 </sub>and access nodes <b>17</b> <sub>5</sub>-<b>17</b> <sub>8 </sub>in access node group <b>19</b> <sub>2</sub>. Although illustrated as a single connection between each of access nodes <b>17</b>, each of connections <b>44</b>, <b>46</b> are bidirectional such that each access node connects to each other access node in the cluster via a separate link.</div>
<div class="description-paragraph" id="p-0078" num="0077">Each of access nodes <b>17</b> <sub>1</sub>-<b>17</b> <sub>4 </sub>within first access node group <b>19</b> <sub>1 </sub>has three intra-access node group connections <b>46</b> to the other access nodes in first access node group <b>19</b> <sub>1</sub>. As illustrated in first access node group <b>19</b> <sub>1</sub>, access node <b>17</b> <sub>1 </sub>supports connection <b>46</b>A to access node <b>17</b> <sub>4</sub>, connection <b>46</b>B to access node <b>17</b> <sub>3</sub>, and connection <b>46</b>C to access node <b>17</b> <sub>2</sub>. Access node <b>17</b> <sub>2 </sub>supports connection <b>46</b>A to access node <b>17</b> <sub>1</sub>, connection <b>46</b>D to access node <b>17</b> <sub>4</sub>, and connection <b>46</b>E to access node <b>17</b> <sub>3</sub>. Access node <b>17</b> <sub>3 </sub>supports connection <b>46</b>B to access node <b>17</b> <sub>1</sub>, connection <b>46</b>E to access node <b>17</b> <sub>2</sub>, and connection <b>46</b>F to access node <b>17</b> <sub>4</sub>. Access node <b>17</b> <sub>4 </sub>supports connection <b>46</b>A to access node <b>17</b> <sub>1</sub>, connection <b>46</b>D to access node <b>17</b> <sub>2</sub>, and connection <b>46</b>F to access node <b>17</b> <sub>3</sub>. The access nodes <b>17</b> <sub>5</sub>-<b>17</b> <sub>8 </sub>are similarly connected within second access node group <b>19</b> <sub>2</sub>.</div>
<div class="description-paragraph" id="p-0079" num="0078">Each of access nodes <b>17</b> <sub>1</sub>-<b>17</b> <sub>4 </sub>within first access node group <b>19</b> <sub>1 </sub>also has four inter-access node group connections <b>44</b> to the access nodes <b>17</b> <sub>5</sub>-<b>17</b> <sub>8 </sub>in second access node group <b>19</b> <sub>2</sub>. As illustrated in <figref idrefs="DRAWINGS">FIG. 5</figref>, first access node group <b>19</b> <sub>1 </sub>and second access node group <b>19</b> <sub>2 </sub>each has sixteen externally-available ports <b>66</b> to connect to each other. For example, access node <b>17</b> <sub>1 </sub>supports connections <b>44</b>A, <b>44</b>B, <b>44</b>C, and <b>44</b>D through four external facing ports <b>66</b> of first access node group <b>19</b> <sub>1 </sub>to four externally-available ports <b>66</b> of second access node group <b>19</b> <sub>2 </sub>to reach access nodes <b>17</b> <sub>5</sub>-<b>17</b> <sub>8</sub>. Specifically, access node <b>17</b> <sub>1 </sub>supports connection <b>44</b>A to access node <b>17</b> <sub>5 </sub>within second access node group <b>19</b> <sub>2</sub>, connection <b>44</b>B to access node <b>17</b> <sub>6 </sub>within second access node group <b>19</b> <sub>2</sub>, connection <b>44</b>C to access node <b>17</b> <sub>7 </sub>within second access node group <b>19</b> <sub>2</sub>, and connection <b>44</b>D to access node <b>17</b> <sub>8 </sub>within second access node group <b>19</b> <sub>2</sub>. The remaining access nodes <b>17</b> <sub>2</sub>-<b>17</b> <sub>4 </sub>within first access node group <b>19</b> <sub>1 </sub>are similarly connected to access nodes <b>17</b> <sub>5</sub>-<b>17</b> <sub>8 </sub>within second access node group <b>19</b> <sub>2</sub>. In addition, in the reverse direction, the access nodes <b>17</b> <sub>5</sub>-<b>17</b> <sub>8 </sub>are similarly connected to access nodes <b>17</b> <sub>1</sub>-<b>17</b> <sub>4 </sub>within first access node group <b>19</b> <sub>1</sub>.</div>
<div class="description-paragraph" id="p-0080" num="0079">Each of access nodes <b>17</b> may be configured to support up to 400 Gigabits of bandwidth to connect to other access nodes in the cluster. In the illustrated example, each of access nodes <b>17</b> may support up to eight 50 GE links to the other access nodes. In this example, since each of access nodes <b>17</b> only connects to seven other access nodes, 50 Gigabits of bandwidth may be leftover and used for managing the access node. In some examples, each of connections <b>44</b>, <b>46</b> may be single 50 GE connections. In other examples, each of connections <b>44</b>, <b>46</b> may be 2×25 GE connections. In still other examples, each of intra-access node group connections <b>46</b> may be 2×25 GE connections, and each of inter-access node group connections <b>44</b> may be single 50 GE connections to reduce a number of inter-box cables. For example, from each access node <b>17</b> <sub>1</sub>-<b>17</b> <sub>4 </sub>within first access node group <b>19</b> <sub>1</sub>, 4×50 GE links go off box to connect to access nodes <b>17</b> <sub>5</sub>-<b>17</b> <sub>8 </sub>in second access node group <b>19</b> <sub>2</sub>. In some examples, the 4×50 GE links may be taken out from each of the access nodes <b>17</b> using DAC cables.</div>
<div class="description-paragraph" id="p-0081" num="0080"> <figref idrefs="DRAWINGS">FIG. 6</figref> is a block diagram illustrating an example arrangement of a full physical rack <b>70</b> including two logical racks <b>60</b> from <figref idrefs="DRAWINGS">FIG. 4</figref>. In the illustrated example of <figref idrefs="DRAWINGS">FIG. 6</figref>, rack <b>70</b> has 42 rack units or slots in vertical height including a 2 rack unit (2RU) top of rack (TOR) device <b>72</b> for providing connectivity to devices within switch fabric <b>14</b>. In one example, TOR device <b>72</b> comprises a top of rack Ethernet switch. In other examples, TOR device <b>72</b> comprises an optical permutor. In some examples, rack <b>70</b> may not include an additional TOR device <b>72</b> and instead have the typical 40 rack units.</div>
<div class="description-paragraph" id="p-0082" num="0081">In the illustrated example, rack <b>70</b> includes four access node groups <b>19</b> <sub>1</sub>-<b>19</b> <sub>4 </sub>that are each separate network appliances 2RU in height. Each of the access node groups <b>19</b> includes four access nodes and may be configured as shown in the example of <figref idrefs="DRAWINGS">FIG. 3</figref>. For example, access node group <b>19</b> <sub>1 </sub>includes access nodes AN<b>1</b>-AN<b>4</b>, access node group <b>19</b> <sub>2 </sub>includes access nodes AN<b>5</b>-AN<b>8</b>, access node group <b>19</b> <sub>3 </sub>includes access nodes AN<b>9</b>-AN<b>12</b>, and access node group <b>19</b> <sub>4 </sub>includes access nodes AN<b>13</b>-AN<b>16</b>. Access nodes AN<b>1</b>-AN<b>16</b> may be substantially similar to access nodes <b>17</b> described above.</div>
<div class="description-paragraph" id="p-0083" num="0082">In this example, each of the access node groups <b>19</b> supports sixteen server nodes. For example, access node group <b>19</b> <sub>1 </sub>supports server nodes A<b>1</b>-A<b>16</b>, access node group <b>19</b> <sub>2 </sub>supports server nodes B<b>1</b>-B<b>16</b>, access node group <b>19</b> <sub>3 </sub>supports server nodes C<b>1</b>-C<b>16</b>, and access node group <b>19</b> <sub>4 </sub>supports server nodes D<b>1</b>-D<b>16</b>. A server node may be a dual-socket or dual-processor server sled that is ½ Rack in width and 1RU in height. As described with respect to <figref idrefs="DRAWINGS">FIG. 3</figref>, four of the server nodes may be arranged into a server <b>52</b> that is 2RU in height. For example, server <b>52</b>A includes server nodes A<b>1</b>-A<b>4</b>, server <b>52</b>B includes server nodes A<b>5</b>-A<b>8</b>, server <b>52</b>C includes server nodes A<b>9</b>-A<b>12</b>, and server <b>52</b>D includes server nodes A<b>13</b>-A<b>16</b>. Server nodes B<b>1</b>-B<b>16</b>, C<b>1</b>-C<b>16</b>, and D<b>1</b>-D<b>16</b> may be similarly arranged into servers <b>52</b>.</div>
<div class="description-paragraph" id="p-0084" num="0083">Access node groups <b>19</b> and servers <b>52</b> are arranged into NSCUs <b>40</b> from <figref idrefs="DRAWINGS">FIGS. 3-4</figref>. NSCUs <b>40</b> are 10RU in height and each include one 2RU access node group <b>19</b> and four 2RU servers <b>52</b>. As illustrated in <figref idrefs="DRAWINGS">FIG. 6</figref>, access node groups <b>19</b> and servers <b>52</b> may be structured as a compute sandwich, in which each access node group <b>19</b> is “sandwiched” between two servers <b>52</b> on the top and two servers <b>52</b> on the bottom. For example, with respect to access node group <b>19</b> <sub>1</sub>, server <b>52</b>A may be referred to as a top second server, server <b>52</b>B may be referred to as a top server, server <b>52</b>C may be referred to as a bottom server, and server <b>52</b>D may be referred to as a bottom second server. In the illustrated structural arrangement, access node groups <b>19</b> are separated by eight rack units to accommodate the bottom two 2RU servers <b>52</b> supported by one access node group and the top two 2RU servers <b>52</b> supported by another access node group.</div>
<div class="description-paragraph" id="p-0085" num="0084">NSCUs <b>40</b> may be arranged into logical racks <b>60</b>, i.e., half physical racks, from <figref idrefs="DRAWINGS">FIG. 5</figref>. Logical racks <b>60</b> are 20RU in height and each include two NSCUs <b>40</b> having full mesh connectivity. In the illustrated example of <figref idrefs="DRAWINGS">FIG. 6</figref>, access node group <b>19</b> <sub>1 </sub>and access node group <b>19</b> <sub>2 </sub>are included in the same logical rack <b>60</b> along with their respective supported server nodes A<b>1</b>-A<b>16</b> and B<b>1</b>-B<b>16</b>. As described in more detail above with respect to <figref idrefs="DRAWINGS">FIG. 5</figref>, access nodes AN<b>1</b>-AN<b>8</b> included the same logical rack <b>60</b> are connected to each other in an 8-way mesh. Access nodes AN<b>9</b>-AN<b>16</b> may be similarly connected in an 8-way mesh within another logical rack <b>60</b> includes access nodes groups <b>19</b> <sub>3 </sub>and <b>19</b> <sub>4 </sub>along with their respective server nodes C<b>1</b>-C<b>16</b> and D<b>1</b>-D<b>16</b>.</div>
<div class="description-paragraph" id="p-0086" num="0085">Logical racks <b>60</b> within rack <b>70</b> may be connected to the switch fabric directly or through an intermediate top of rack device <b>72</b>. As noted above, in one example, TOR device <b>72</b> comprises a top of rack Ethernet switch. In other examples, TOR device <b>72</b> comprises an optical permutor that transports optical signals between access nodes <b>17</b> and core switches <b>22</b> and that is configured such that optical communications are “permuted” based on wavelength so as to provide full-mesh connectivity between the upstream and downstream ports without any optical interference.</div>
<div class="description-paragraph" id="p-0087" num="0086">In the illustrated example, each of the access node groups <b>19</b> may connect to TOR device <b>72</b> via one or more of the 8×100 GE links supported by the access node group to reach the switch fabric. In one case, the two logical racks <b>60</b> within rack <b>70</b> may each connect to one or more ports of TOR device <b>72</b>, and TOR device <b>72</b> may also receive signals from one or more logical racks within neighboring physical racks. In other examples, rack <b>70</b> may not itself include TOR device <b>72</b>, but instead logical racks <b>60</b> may connect to one or more TOR devices included in one or more neighboring physical racks.</div>
<div class="description-paragraph" id="p-0088" num="0087">For a standard rack size of 40RU it may be desirable to stay within a typical power limit, such as a 15 kilowatt (kW) power limit. In the example of rack <b>70</b>, not taking the additional 2RU TOR device <b>72</b> into consideration, it may be possible to readily stay within or near the 15 kW power limit even with the sixty-four server nodes and the four access node groups. For example, each of the access node groups <b>19</b> may use approximately 1 kW of power resulting in approximately 4 kW of power for access node groups. In addition, each of the server nodes may use approximately 200 W of power resulting in around 12.8 kW of power for servers <b>52</b>. In this example, the 40RU arrangement of access node groups <b>19</b> and servers <b>52</b>, therefore, uses around 16.8 kW of power.</div>
<div class="description-paragraph" id="p-0089" num="0088"> <figref idrefs="DRAWINGS">FIG. 7A</figref> is a block diagram showing a logical view of the networking data paths and operations within an access node <b>17</b>. As shown in the example of <figref idrefs="DRAWINGS">FIG. 7A</figref>, in some example implementations, each access node <b>17</b> implements at least four different operational networking components or functions: (1) a source (SF) component <b>30</b> operable to receive traffic from a set of servers <b>12</b> supported by the access node, (2) a source switching (SX) component <b>32</b> operable to switch source traffic to other source switching components of different access nodes <b>17</b> (possibly of different access node groups) or to core switches <b>22</b>, (3) a destination switching (DX) component <b>34</b> operable to switch inbound traffic received from other source switching components or from cores switches <b>22</b> and (4) a destination (DF) component <b>36</b> operable to reorder packet flows and provide the packet flows to destination servers <b>12</b>.</div>
<div class="description-paragraph" id="p-0090" num="0089">In some examples, the different operational networking components of access node <b>17</b> may perform flow-based switching and ECMP based load balancing for Transmission Control Protocol (TCP) packet flows. Typically, however, ECMP load balances poorly as it randomly hashes the flows to paths such that a few large flows may be assigned to the same path and severely imbalance the fabric. In addition, ECMP relies on local path decisions and does not use any feedback about possible congestion or link failure downstream for any of the chosen paths.</div>
<div class="description-paragraph" id="p-0091" num="0090">The techniques described in this disclosure introduce a new data transmission protocol referred to as a Fabric Control Protocol (FCP) that may be used by the different operational networking components of access node <b>17</b>. FCP is an end-to-end admission control protocol in which a sender explicitly requests a receiver with the intention to transfer a certain number of bytes of payload data. In response, the receiver issues a grant based on its buffer resources, QoS, and/or a measure of fabric congestion.</div>
<div class="description-paragraph" id="p-0092" num="0091">For example, the FCP includes admission control mechanisms through which a source node requests permission before transmitting a packet on the fabric to a destination node. For example, the source node sends a request message to the destination node requesting a certain number of bytes to be transferred, and the destination node sends a grant message to the source node after reserving the egress bandwidth. In addition, instead of the flow-based switching and ECMP forwarding used to send all packets of a TCP flow on the same path to avoid packet reordering, the FCP enables packets of an individual packet flow to be sprayed to all available links between a source node and a destination node. The source node assigns a packet sequence number to each packet of the flow, and the destination node uses the packet sequence numbers to put the incoming packets of the same flow in order.</div>
<div class="description-paragraph" id="p-0093" num="0092">SF component <b>30</b> of access node <b>17</b> is considered a source node of the fabric. According to the disclosed techniques, for FCP traffic, SF component <b>30</b> is configured to spray its input bandwidth (e.g., 200 Gbps) over links to multiple SX components of access nodes within a logical rack. For example, as described in more detail with respect to <figref idrefs="DRAWINGS">FIG. 7B</figref>, SF component <b>30</b> may spray packets of the same flow across eight links to SX component <b>32</b> and seven other SX components of other access nodes within a logical rack. For non-FCP traffic, SF component <b>30</b> is configured to select one of the connected SX components to which to send packets of the same flow.</div>
<div class="description-paragraph" id="p-0094" num="0093">SX component <b>32</b> of access node <b>17</b> may receive incoming packets from multiple SF components of access nodes within the logical rack, e.g., SF component <b>30</b> and seven other SF components of other access nodes within the logical rack. For FCP traffic, SX component <b>32</b> is also configured to spray its incoming bandwidth over links to multiple core switches in the fabric. For example, as described in more detail with respect to <figref idrefs="DRAWINGS">FIG. 8</figref>, SX component <b>32</b> may spray its bandwidth across eight links to eight core switches. In some cases, SX component <b>32</b> may spray its bandwidth across eight links to four or eight intermediate devices, e.g., TOR Ethernet switches, electrical permutation devices, or optical permutation devices, which in turn forward traffic to the core switches. For non-FCP traffic, SX component <b>32</b> is configured to select one of the core switches to which to send packets of the same packet flow. Since the incoming bandwidth to SX component <b>32</b> and the outgoing bandwidth from SX component <b>32</b> is same (e.g., 200 Gbps), congestion should not occur at the SX stage even for a large number of packet flows.</div>
<div class="description-paragraph" id="p-0095" num="0094">DX component <b>34</b> of access node <b>17</b> may receive incoming packets from multiple core switches either directly or via one or more intermediate devices, e.g., TOR Ethernet switches, electrical permutation devices, or optical permutation devices. For example, DX component <b>34</b> may receive incoming packets from eight core switches, or four or eight intermediate devices. DX component <b>34</b> is configured to select a DF component to which to send the received packets. For example, DX component <b>34</b> may be connected to DF component <b>36</b> and seven other DF components of other access nodes within the logical rack. In some case, DX component <b>34</b> may become a congestion point because DX component <b>34</b> may receive a large amount of bandwidth (e.g., 200 Gbps) that is all to be sent to the same DF component. In the case of FCP traffic, DX component <b>34</b> may avoid long term congestion using the admission control mechanisms of FCP.</div>
<div class="description-paragraph" id="p-0096" num="0095">DF component <b>36</b> of access node <b>17</b> may receive incoming packets from multiple DX components of access nodes within the logical rack, e.g., DX component <b>34</b> and seven other DX components of other access nodes within the logical rack. DF component <b>36</b> is considered a destination node of the fabric. For FCP traffic, DF component <b>36</b> is configured to recorder packets of the same flow prior to transmitting the flow to a destination server <b>12</b>.</div>
<div class="description-paragraph" id="p-0097" num="0096">In some examples, SX component <b>32</b> and DX component <b>34</b> of access node <b>17</b> may use the same forwarding table to perform packet switching. In this example, the personality of access node <b>17</b> and the nexthop identified by the forwarding table for the same destination IP address may depend on a source port type of the received data packet. For example, if a source packet is received from a SF component, access node <b>17</b> operates as SX component <b>32</b> and determines a nexthop to forward the source packet over the fabric toward a destination node. If a packet is received from a fabric-facing port, access node <b>17</b> operates as DX component <b>34</b> and determines a final nexthop to forward the incoming packet directly to a destination node. In some examples, the received packet may include an input tag that specifies its source port type.</div>
<div class="description-paragraph" id="p-0098" num="0097"> <figref idrefs="DRAWINGS">FIG. 7B</figref> is a block diagram illustrating an example first-level network fanout achieved between a set of access nodes <b>17</b> <sub>1</sub>-<b>17</b> <sub>8 </sub>within a logical rack <b>60</b>. In the illustrated example of <figref idrefs="DRAWINGS">FIG. 7B</figref>, logical rack <b>60</b> includes two access node groups <b>19</b> <sub>1 </sub>and <b>19</b> <sub>2 </sub>containing eight access nodes <b>17</b> <sub>1</sub>-<b>17</b> <sub>8 </sub>and server nodes <b>12</b> supported by each of the access nodes.</div>
<div class="description-paragraph" id="p-0099" num="0098">As shown in <figref idrefs="DRAWINGS">FIG. 7B</figref>, SF components <b>30</b>A-<b>30</b>H and SX components <b>32</b>A-<b>32</b>H of access nodes <b>17</b> within logical rack <b>60</b> have full mesh connectivity in that each SF component <b>30</b> is connected to all of the SX components <b>32</b> of the eight access nodes <b>17</b> within logical rack <b>60</b>. As described above, the eight access nodes <b>17</b> within logical rack <b>60</b> may be connected to each other by an 8-way mesh of electrical Ethernet connections. In the case of FCP traffic, SF components <b>30</b> of access nodes <b>17</b> within logical rack <b>60</b> apply spraying algorithms to spray packets for any given packet flow across all available links to SX components <b>32</b>. In this way, SF components <b>30</b> need not necessarily perform a full lookup operation for L2/L3 switching of outbound packets of packet flows originating from servers <b>12</b>. In other words, packets for a given packet flow may be received by an SF component <b>30</b>, such as SF component <b>30</b>A, and sprayed across some or all of the links to SX components <b>32</b> for the logical rack <b>60</b>. In this way, access nodes <b>17</b> for a logical rack achieve a first-level fan out of, in this example, 1:8 and may do so, in some examples, without incurring any L2/L3 forwarding lookup relative to keying information in the packet headers. As such, packets for a single packet flow need not follow the same path when sprayed by a given SF component <b>30</b>.</div>
<div class="description-paragraph" id="p-0100" num="0099">Thus, according to the disclosed techniques, upon receiving source traffic from one of servers <b>12</b>, SF component <b>30</b>A implemented by access node <b>17</b> <sub>1</sub>, for example, performs an 8-way spray of packets of the same flow across all available links to SX components <b>32</b> implemented by access nodes <b>17</b> included in logical rack <b>60</b>. More specifically, SF component <b>30</b>A sprays across one internal SX component <b>32</b>A of the same access node <b>17</b> <sub>1 </sub>and seven external SX components <b>32</b>B-<b>32</b>H of the other access nodes <b>17</b> <sub>2</sub>-<b>17</b> <sub>8 </sub>within logical rack <b>60</b>. In some implementations, this 8-way spray between SFs <b>30</b> and SXs <b>32</b> within logical rack <b>60</b> may be referred to as a first-stage spray. As described in other portions of this disclosure, a second-stage spray may be performed over a second-level network fanout within the switch fabric between access nodes <b>17</b> and core switches <b>22</b>. For example, the second-stage spray may be performed through an intermediate device, such as a TOR Ethernet switch, an electric permutation device, or an optical permutation device.</div>
<div class="description-paragraph" id="p-0101" num="0100">In some examples, as described in more detail above, the first four access nodes <b>17</b> <sub>1</sub>-<b>17</b> <sub>4 </sub>may be included in a first access node group <b>19</b> <sub>1 </sub>and the second four access nodes <b>17</b> <sub>4</sub>-<b>17</b> <sub>8 </sub>may be included in a second access node group <b>19</b> <sub>2</sub>. The access nodes <b>17</b> within the first and second access node groups <b>19</b> may be connected to each other via a full-mesh in order to allow the 8-way spray between SFs <b>30</b> and SXs <b>32</b> within logical rack <b>60</b>. In some examples, logical rack <b>60</b> including the two access nodes groups together with their supported servers <b>12</b> may be referred to as a half-rack or a half physical rack. In other examples, more or fewer access nodes may be connected together using full-mesh connectivity. In one example, sixteen access nodes <b>17</b> may be connected together in a full-mesh to enable a first-stage 16-way spray within a full physical rack.</div>
<div class="description-paragraph" id="p-0102" num="0101"> <figref idrefs="DRAWINGS">FIG. 8</figref> is a block diagram illustrating an example multi-level network fanout across a data center switch fabric between access nodes <b>17</b>. In the illustrated example of <figref idrefs="DRAWINGS">FIG. 8</figref>, each of the logical racks <b>60</b> includes eight access nodes <b>17</b> <sub>1</sub>-<b>17</b> <sub>8 </sub>and server nodes <b>12</b> supported by each of the access nodes. The first logical rack <b>60</b> <sub>1 </sub>is connected to the second logical rack <b>60</b> <sub>2 </sub>through core switches <b>22</b> within the switch fabric. In some examples, the first logical rack <b>60</b> <sub>1 </sub>and the second logical rack <b>60</b> <sub>2 </sub>may be the same logical rack.</div>
<div class="description-paragraph" id="p-0103" num="0102">According to the disclosed techniques, the switch fabric comprises a FCP fabric. The FCP fabric may be visualized as including multiple channels, e.g., a request channel, a grant channel, a FCP data channel and a non-FCP data channel. As illustrated in <figref idrefs="DRAWINGS">FIG. 8</figref>, the FCP data channel carries data packets via a logical tunnel <b>100</b> that includes all paths between a source node, e.g., SF component <b>30</b>A of access node <b>17</b> <sub>1</sub>, in a first logical rack <b>60</b> <sub>1 </sub>and a destination node, e.g., DF component <b>36</b>B of access node <b>17</b> <sub>2</sub>, in a second logical rack <b>60</b> <sub>2</sub>. The FCP data channel carries the data packets using the FCP protocol. The FCP packets are sprayed over the fabric from the source node to the destination node through a suitable load balancing scheme. The FCP packets are not expected to be delivered in order, but the destination node is expected to perform packet reordering. For example, packets of a traffic flow received from a source server <b>12</b> by SF component <b>30</b>A of access node <b>17</b> <sub>1 </sub>may be sprayed over some or all possible links within logical tunnel <b>100</b> toward DF component <b>36</b>B of access node <b>17</b> <sub>2</sub>. DF component <b>36</b>B is configured to reorder the received packets to recreate the packet flow prior to transmitting the packet flow to the destination server <b>12</b>.</div>
<div class="description-paragraph" id="p-0104" num="0103">The request channel within the FCP fabric may be used to carry FCP request messages from the source node to the destination node. Similar to the FCP data packets, the FCP request messages may be sprayed over all available paths toward the destination node, but the request messages do not need to be reordered. In response, the grant channel within the FCP fabric may be used to carry FCP grant messages from the destination node to source node. The FCP grant messages may also be sprayed over all available paths toward the source node, and the grant messages do not need to be reordered. The non-FCP data channel within the FCP fabric carries data packets that do not use the FCP protocol. The non-FCP data packets may be forwarded or routed using ECMP based load balancing, and, for a given flow identified by a five tuple, the packets are expected to be delivered in order to the destination node.</div>
<div class="description-paragraph" id="p-0105" num="0104">The example of <figref idrefs="DRAWINGS">FIG. 8</figref> illustrates both the first-level network fanout between the access nodes <b>17</b> within first logical rack <b>60</b> <sub>1</sub>, as described above with respect to <figref idrefs="DRAWINGS">FIG. 7B</figref>, and a second-level network fanout between the access nodes <b>17</b> and the core switches <b>22</b>. As described above with respect to <figref idrefs="DRAWINGS">FIGS. 3-4</figref>, the eight access nodes <b>17</b> within first logical rack <b>60</b> <sub>1 </sub>are connected to core switches <b>22</b> using either electrical or optical Ethernet connections. The eight access nodes <b>17</b> within second logical rack <b>60</b> <sub>2 </sub>are similarly connected to the core switches <b>22</b>. In some examples, each of access nodes <b>17</b> may connect to eight of core switches <b>22</b>. In the case of FCP traffic, SX components <b>32</b> of access nodes <b>17</b> within first logical rack <b>60</b> <sub>1 </sub>apply spraying algorithms to spray packets for any given packet flow across all available paths to the core switches <b>22</b>. In this way, the SX components <b>32</b> may not perform a full lookup operation for L2/L3 switching of received packets.</div>
<div class="description-paragraph" id="p-0106" num="0105">Upon receiving source FCP traffic from one of the servers <b>12</b>, an SF component <b>30</b>A of access node <b>17</b> <sub>1 </sub>in the first logical rack <b>60</b> <sub>1 </sub>performs an 8-way spray of packets of the FCP traffic flow across all available paths to SX components <b>32</b> implemented by the access nodes <b>17</b> in the first logical rack <b>60</b> <sub>1</sub>. As further illustrated in <figref idrefs="DRAWINGS">FIG. 8</figref>, each of the SX components <b>32</b> then sprays the packets of the FCP traffic flow across all available paths to the core switches <b>22</b>. In the illustrated example, the multi-level fanout is 8-by-8 and, therefore, supports up to sixty-four core switches <b>22</b> <sub>1</sub>-<b>22</b> <sub>64</sub>. In other examples, in which the first-level fanout is 1:16 within a full physical rack, the multi-level fanout may be 16-by 16 and support up to 256 core switches.</div>
<div class="description-paragraph" id="p-0107" num="0106">Although illustrated in <figref idrefs="DRAWINGS">FIG. 8</figref> as occurring directly between the access nodes <b>17</b> and the core switches <b>22</b>, the second-level fanout may be performed through one or more TOR devices, such as top of rack Ethernet switches, optical permutation devices, or electrical permutation devices. The multi-level network fanout enables packets of a traffic flow received at any of the access nodes <b>17</b> within the first logical rack <b>60</b> <sub>1 </sub>to reach core switches <b>22</b> for further forwarding to any of the access nodes <b>17</b> within the second logical rack <b>60</b> <sub>2</sub>.</div>
<div class="description-paragraph" id="p-0108" num="0107">According to the disclosed techniques, in one example implementation, each of SF components <b>30</b> and SX components <b>32</b> uses an FCP spray engine configured to apply a suitable load balancing scheme to spray the packets of a given FCP packet flow across all available links to a destination node. For example, the FCP spray engine may track a number of bytes transmitted on each link in order to select a least loaded link on which to forward a packet. In addition, the FCP spray engine may track link failures downstream to provide flow fairness by spraying packets in proportion to bandwidth weight on each active link. In this way, the spray of packets may not be uniform across the available links toward the destination node, but bandwidth will be balanced across the active links even over relatively short periods.</div>
<div class="description-paragraph" id="p-0109" num="0108">In this example, the source node, e.g., SF component <b>30</b>A of access node <b>17</b> <sub>1</sub>, within first logical rack <b>60</b> <sub>1 </sub>sends a request message to the destination node, e.g., DF component <b>36</b>B of access node <b>17</b> <sub>2</sub>, within second logical rack <b>60</b> <sub>2 </sub>requesting a certain weight or bandwidth and the destination node sends a grant message to the source node after reserving the egress bandwidth. The source node also determines whether any link failures have occurred between core switches <b>22</b> and logical rack <b>60</b> <sub>2 </sub>that includes the destination node. The source node may then use all active links in proportion to the source and destination bandwidths. As an example, assume there are N links between the source node and the destination node each with source bandwidth Sb<sub>i </sub>and destination bandwidth Db<sub>i</sub>, where i=1 . . . N. The actual bandwidth from the source nodes to the destination node is equal to min(Sb, Db) determined on a link-by-link basis in order to take failures into account. More specifically, the source bandwidth (Sb) is equal to Σ<sub>i=1</sub> <sup>N</sup>Sb<sub>i</sub>, and destination bandwidth (Db) is equal to Σ<sub>i=1</sub> <sup>N</sup>Db<sub>i</sub>, and the bandwidth (b<sub>i</sub>) of each link is equal to min(Sb<sub>i</sub>, Db<sub>i</sub>). The weight of the bandwidth used on each link is equal to b<sub>i</sub>/Σ<sub>i=1</sub> <sup>N</sup>b<sub>i</sub>.</div>
<div class="description-paragraph" id="p-0110" num="0109">In the case of FCP traffic, SF components <b>30</b> and SX components <b>32</b> use the FCP spray engine to distribute packets of the FCP traffic flow based on the load on each link toward the destination node, proportion to its weight. The spray engine maintains credit memory to keep track of credits (i.e., available bandwidth) per nexthop member link, uses packet length included in an FCP header to deduct credits (i.e., reduce available bandwidth), and associates a given packet to the one of the active links having the most credits (i.e., the least loaded link). In this way, for FCP packets, the SF components <b>30</b> and SX components <b>32</b> spray packets across member links of a nexthop for a destination node in proportion to the member links' bandwidth weights.</div>
<div class="description-paragraph" id="p-0111" num="0110">Core switches <b>22</b> operate as the single hop along logical tunnel <b>100</b> between the source node, e.g., SF component <b>30</b>A of access node <b>17</b> <sub>1</sub>, in first logical rack <b>60</b> <sub>1 </sub>and the destination node, e.g., DF component <b>36</b>B of access node <b>17</b> <sub>2</sub>, in the second logical rack <b>60</b> <sub>2</sub>. Core switches <b>22</b> perform a full lookup operation for L2/L3 switching of the received packets. In this way, core switches <b>22</b> may forward all the packets for the same traffic flow toward the destination node, e.g., DF component <b>36</b>B of access node <b>17</b> <sub>2</sub>, in the second logical rack <b>60</b> <sub>2 </sub>that supports the destination server <b>12</b>. Although illustrated in <figref idrefs="DRAWINGS">FIG. 8</figref> as occurring directly between the core switches <b>22</b> and destination access node <b>17</b> <sub>2 </sub>of second logical rack <b>60</b> <sub>2</sub>, the core switches <b>22</b> may forward all the packets for the same traffic flow to an intermediate TOR device that has connectivity to the destination node. In some examples, the intermediate TOR device may forward all the packet for the traffic flow directly to DX component <b>34</b>B implemented by access node <b>17</b> <sub>2 </sub>of second logical rack <b>60</b> <sub>2</sub>. In other examples, the intermediate TOR device may be an optical or electrical permutation device configured to provide another fanout over which the packets can be sprayed between input and output ports of the permutation device. In this example, all or some portion of the DX components <b>34</b> of access nodes <b>17</b> of second logical rack <b>60</b> <sub>2 </sub>may receive sprayed packets of the same traffic flow.</div>
<div class="description-paragraph" id="p-0112" num="0111">DX components <b>34</b> and DF components <b>36</b> of access nodes <b>17</b> within second logical rack <b>60</b> <sub>2 </sub>also have full mesh connectivity in that each DX component <b>34</b> is connected to all of the DF components <b>36</b> within second logical rack <b>60</b> <sub>2</sub>. When any of DX components <b>34</b> receive the packets of the traffic flow from core switches <b>22</b>, the DX components <b>34</b> forward the packets on a direct path to DF component <b>36</b>B of access node <b>17</b> <sub>2</sub>. DF component <b>36</b>B may perform a limited lookup necessary only to select the proper output port for forwarding the packets to the destination server <b>12</b>. In response to receiving the packets of the traffic flow, DF component <b>36</b>B of access node <b>17</b> <sub>2 </sub>within second logical rack <b>60</b> <sub>2 </sub>reorders the packets of the traffic flow based on sequence numbers of the packets. As such, with respect to full routing tables for the data center, only the core switches <b>22</b> may need to perform full lookup operations. Thus, the switch fabric provides a highly-scalable, flat, high-speed interconnect in which servers are effectively one L2/L3 hop from any other server <b>12</b> within the data center.</div>
<div class="description-paragraph" id="p-0113" num="0112">More details on data center network architecture and interconnected access nodes are available in U.S. patent application Ser. No. 15/939,227, filed Mar. 28, 2018, entitled “Non-Blocking Any-to-Any Data Center Network with Packet Spraying Over Multiple Alternate Data Paths,” the entire content of which is incorporated herein by reference.</div>
<div class="description-paragraph" id="p-0114" num="0113">A brief description of FCP and one example of its operation with respect to <figref idrefs="DRAWINGS">FIG. 8</figref> is included here. In the example of <figref idrefs="DRAWINGS">FIG. 8</figref>, access nodes <b>17</b> are fabric end points (FEPs) to the FCP fabric, which is made up of switching elements, e.g., core switches <b>22</b>, arranged in a leaf-spine topology. The FPC fabric allows one access node <b>17</b> to communicate with another one through multiple paths. Core switches <b>22</b> inside the FCP fabric have shallow packet buffers. The cross-sectional bandwidth of the FCP fabric is equal to or greater than the sum of all end point bandwidths. In this way, if each access node <b>17</b> limits the incoming data rate to the FCP fabric, none of the paths inside the FCP fabric should be congested long term with very high probability.</div>
<div class="description-paragraph" id="p-0115" num="0114">As described above, FCP data packets are sent from a source node, e.g., SF component <b>30</b>A of access node <b>17</b> <sub>1 </sub>within first logical rack <b>60</b> <sub>1</sub>, to a destination node, e.g., DF component <b>36</b>B of access node <b>17</b> <sub>2 </sub>within second logical rack <b>60</b> <sub>2</sub>, via logical tunnel <b>100</b>. Before any traffic is sent over tunnel <b>100</b> using FCP, the connection must be established between the end points. A control plane protocol executed by access nodes <b>17</b> may be used to set up a pair of tunnels, one in each direction, between the two FCP end points. The FCP tunnels are optionally secured (e.g., encrypted and authenticated). Tunnel <b>100</b> is considered to be unidirectional from the source node to the destination node, and a FCP partner tunnel may be established in the other direction from the destination node to the source node. The control plane protocol negotiates the capabilities (e.g., block size, MTU size, etc.) of both end points, and establishes the FCP connection between the end points by setting up tunnel <b>100</b> and its partner tunnel and an initializing queue state context for each tunnel.</div>
<div class="description-paragraph" id="p-0116" num="0115">Each of the end points is assigned a source tunnel ID and a corresponding destination tunnel ID. At each end point, a queue ID for a given tunnel queue is derived based on the assigned tunnel ID and priority. For example, each FCP end point may allocate a local tunnel handle from a pool of handles and communicate the handle to its FCP connection partner end point. The FCP partner tunnel handle is stored in a lookup table and referenced from the local tunnel handle. For the source end point, e.g., access node <b>17</b> <sub>1 </sub>within first logical rack <b>60</b> <sub>1</sub>, a source queue is identified by the local tunnel ID and priority, and a destination tunnel ID is identified from the lookup table based on the local tunnel ID. Similarly, for the destination end point, e.g., access node <b>17</b> <sub>2 </sub>within second logical rack <b>60</b> <sub>2</sub>, a destination queue is identified by the local tunnel ID and priority, and a source tunnel ID is identified from the lookup table based on the local tunnel ID.</div>
<div class="description-paragraph" id="p-0117" num="0116">FCP tunnel queues are defined as buckets of independent traffic streams that use FCP to transport payload across the FCP fabric. An FCP queue for a given tunnel is identified by the tunnel ID and priority, and the tunnel ID is identified by the source/destination end point pair for the given tunnel. Alternatively, the end points may use a mapping table to derive the tunnel ID and priority based on an internal FCP queue ID for the given tunnel. In some examples, an FCP fabric tunnel, e.g., logical tunnel <b>100</b>, may support 1, 2, 4, or 8 queues per tunnel. The number of queues per tunnel is a FCP fabric property and may be configured at the time of deployment. All tunnels within the FCP fabric may support the same number of queues per tunnel. Each end point may support a maximum of 16,000 queues.</div>
<div class="description-paragraph" id="p-0118" num="0117">When the source node is communicating with the destination node, the source node encapsulates the packets using an FCP over UDP encapsulation. The FCP header carries fields identifying tunnel IDs, queue IDs, packet sequence numbers (PSNs) for packets, and request, grant, and data block sequence numbers between the two end points. At the destination node, the incoming tunnel ID is unique for all packets from the specific source node. The tunnel encapsulation carries the packet forwarding as well as the reordering information used by the destination node. A single tunnel carries packets for one or multiple queues between the source and destination nodes. Only the packets within the single tunnel are reordered based on sequence number tags that span across the queues of the same tunnel. The source node tags the packets with tunnel PSNs when they are sent over the tunnel toward the destination node. The destination node reorders the packets based on the tunnel ID and the PSNs. At the end of the reorder, the destination node strips the tunnel encapsulation and forwards the packets to the respective destination queues.</div>
<div class="description-paragraph" id="p-0119" num="0118">An example of how an IP packet entering FCP tunnel <b>100</b> at a source end point is transmitted to a destination end point is described here. A source server <b>12</b> having an IP address of A<b>0</b> sends an IP packet for a destination server <b>12</b> having an IP address of B<b>0</b>. The source FCP endpoint, e.g., access node <b>17</b> <sub>1 </sub>within first logical rack <b>60</b> <sub>1</sub>, transmits an FCP request packet with source IP address A and destination IP address B. The FCP request packet has an FCP header to carry the Request Block Number (RBN) and other fields. The FCP request packet is transmitted over UDP over IP. The destination FCP end point, e.g., access node <b>17</b> <sub>2 </sub>within first logical rack <b>60</b> <sub>2</sub>, sends a FCP grant packet back to the source FCP end point. The FCP grant packet has an FCP header to carry the Grant Block Number (GBN) and other fields. The FCP grant packet is transmitted over UDP over IP. The source end point transmits the FCP data packet after receiving the FCP grant packet. The source end point appends a new (IP+UDP+FCP) data header on the input data packet. The destination end point removes the append (IP+UDP+FCP) data header before delivering the packet to the destination host server.</div>
<div class="description-paragraph" id="p-0120" num="0119"> <figref idrefs="DRAWINGS">FIG. 9</figref> is a block diagram illustrating an example access node <b>150</b> including two or more processing clusters, in accordance with the techniques of this disclosure. Access node <b>150</b> generally represents a hardware chip implemented in digital logic circuitry. As various examples, access node <b>150</b> may be provided as an integrated circuit mounted on a motherboard of a computing device or installed on a card connected to the motherboard of the computing device via PCI-e, or the like. In some examples, access node <b>150</b> may be an integrated circuit within an access node group (e.g., one of access node groups <b>19</b>) configured as standalone network device for installation within a compute rack, a storage rack, or a converged rack.</div>
<div class="description-paragraph" id="p-0121" num="0120">Access node <b>150</b> may operate substantially similar to any of the access nodes <b>17</b> of <figref idrefs="DRAWINGS">FIGS. 1-8</figref>. Thus, access node <b>150</b> may be communicatively coupled to a data center fabric (e.g., switch fabric <b>14</b>), one or more server devices (e.g., server nodes <b>12</b> or servers <b>52</b>), storage media (e.g., solid state storage <b>41</b> of <figref idrefs="DRAWINGS">FIG. 3</figref>), one or more network devices, random access memory, or the like, e.g., via PCI-e, Ethernet (wired or wireless), or other such communication media in order to interconnect each of these various elements. In this example, access node <b>150</b> includes networking unit <b>152</b>, processing clusters <b>156</b>A-<b>1</b>-<b>156</b>N-M (processing clusters <b>156</b>), host units <b>154</b>A-<b>1</b>-<b>154</b>B-M (host units <b>154</b>), and central cluster <b>158</b>, and is coupled to external memory <b>170</b>.</div>
<div class="description-paragraph" id="p-0122" num="0121">In general, access node <b>150</b> represents a high performance, hyper-converged network, storage, and data processor and input/output hub. As illustrated in <figref idrefs="DRAWINGS">FIG. 9</figref>, access node <b>150</b> includes host units <b>154</b> each having PCI-e interfaces <b>166</b>, networking unit <b>152</b> having Ethernet interfaces <b>164</b>, and processing clusters <b>156</b>A-M-<b>156</b>N-M and host units <b>154</b>A-M-<b>154</b>N-M each having interfaces to off-chip external memory <b>170</b>. Access node <b>150</b> may include multiple lanes of PCI-e Generation 3/4 <b>166</b> that are organized into groups (e.g., x2, x4, x8, or x16 groups) where each of host units <b>154</b> provides one group of PCI-e lanes <b>166</b>. In addition, access node <b>150</b> may include multiple HSS Ethernet lanes <b>164</b> that may each be 25G and configurable as 25G, 50G, or 40/100G ports. Access node <b>150</b> may also act as a PCI-e endpoint to multiple PCI-e root complexes, e.g., different sockets in multi-socket servers or multi-server enclosures. In such examples, each server may have two x86 processor sockets, each connected to access node <b>150</b> using a dedicated PCI-e port.</div>
<div class="description-paragraph" id="p-0123" num="0122">In this example, access node <b>150</b> represents a high performance, programmable multi-processor architecture that may provide solutions to various problems with existing processors (e.g., x86 architecture processors). As shown in <figref idrefs="DRAWINGS">FIG. 9</figref>, access node <b>150</b> includes specialized network-on-chip (NoC) fabrics for inter-processor communication. access node <b>150</b> also provides optimizations for stream processing (packet/protocol processing). Work queues are directly attached to cores of access node <b>150</b>. Access node <b>150</b> also provides run-to-completion processing, which may eliminate interrupts, thread scheduling, cache thrashing, and associated costs. Access node <b>150</b> operates on “work units” that associate a buffer with an instruction stream to eliminate checking overhead and allow processing by reference to minimize data movement and copy. Access node <b>150</b> also operates according to a stream model, which provides streamlined buffer handling with natural synchronization, reduces contention, and eliminates locking. Access node <b>150</b> includes non-coherent buffer memory that is separate from coherent cache memory hierarchy and eliminates cache maintenance overhead and penalty, with improved memory access. Access node <b>150</b> provides a high performance, low latency messaging infrastructure that may improve inter-process and inter-core communication. Specialized direct memory access (DMA) engines of access node <b>150</b> handle bulk data movement and payload manipulation at exit points. Hardware offload modules of access node <b>150</b> reduce the work needed per packet, implement ACL, and flow lookup. Hardware allocators of access node <b>150</b> may handle memory allocation and freeing.</div>
<div class="description-paragraph" id="p-0124" num="0123">As described herein, the new processing architecture utilizing an access node may be especially efficient for stream processing applications and environments. For example, stream processing is a type of data processing architecture well suited for high performance and high efficiency processing. A stream is defined as an ordered, unidirectional sequence of computational objects that can be of unbounded or undetermined length. In a simple embodiment, a stream originates in a producer and terminates at a consumer, and is operated on sequentially. In some embodiments, a stream can be defined as a sequence of stream fragments; each stream fragment including a memory block contiguously addressable in physical address space, an offset into that block, and a valid length. Streams can be discrete, such as a sequence of packets received from the network, or continuous, such as a stream of bytes read from a storage device. A stream of one type may be transformed into another type as a result of processing. For example, TCP receive (Rx) processing consumes segments (fragments) to produce an ordered byte stream. The reverse processing is performed in the transmit (Tx) direction. Independently of the stream type, stream manipulation requires efficient fragment manipulation, where a fragment is as defined above.</div>
<div class="description-paragraph" id="p-0125" num="0124">As one example, a Work Unit (WU) is a container that is associated with a stream state and used to describe (i.e. point to) data within a stream (stored). For example, work units may dynamically originate within a peripheral unit coupled to the multi-processor system (e.g. injected by a networking unit, a host unit, or a solid state drive interface), or within a processor itself, in association with one or more streams of data, and terminate at another peripheral unit or another processor of the system. The work unit is associated with an amount of work that is relevant to the entity executing the work unit for processing a respective portion of a stream. In some examples, one or more processing cores of a DPU may be configured to execute program instructions using a work unit (WU) stack.</div>
<div class="description-paragraph" id="p-0126" num="0125">In general, work units are sets of data exchanged between processing clusters <b>156</b>, networking unit <b>152</b>, host units <b>154</b>, central cluster <b>158</b>, and external memory <b>170</b>. Each work unit may represent a fixed length (e.g., 32-bytes) data structure including an action value and one or more arguments. In one example, a 32-byte work unit includes four sixty-four (64) bit words, a first word having a value representing the action value and three additional words each representing an argument. The action value may include a work unit handler identifier that acts as an index into a table of work unit functions to dispatch the work unit, a source identifier representing a source virtual processor or other unit (e.g., one of host units <b>154</b>, networking unit <b>152</b>, external memory <b>170</b>, or the like) for the work unit, a destination identifier representing the virtual processor or other unit that is to receive the work unit, an opcode representing fields that are pointers for which data is to be accessed, and signaling network routing information.</div>
<div class="description-paragraph" id="p-0127" num="0126">The arguments of a work unit may be typed or untyped, and in some examples, one of the typed arguments acts as a pointer used in various work unit handlers. Typed arguments may include, for example, frames (having values acting as pointers to a work unit stack frame), flows (having values acting as pointers to state, which is relevant to the work unit handler function), and packets (having values acting as pointers to packets for packet and/or block processing handlers).</div>
<div class="description-paragraph" id="p-0128" num="0127">A flow argument may be used as a prefetch location for data specific to a work unit handler. A work unit stack is a data structure to help manage event driven, run-to-completion programming model of an operating system executed by access node <b>150</b>. An event driven model typically means that state which might otherwise be stored as function local variables must be stored as state outside the programming language stack. Run-to-completion also implies functions may be dissected to insert yield points. The work unit stack may provide the convenience of familiar programming constructs (call/return, call/continue, long-lived stack-based variables) to the execution model of access node <b>150</b>.</div>
<div class="description-paragraph" id="p-0129" num="0128">A frame pointer of a work unit may have a value that references a continuation work unit to invoke a subsequent work unit handler. Frame pointers may simplify implementation of higher level semantics, such as pipelining and call/return constructs. More details on work units, work unit stacks, and stream processing by data processing units are available in U.S. Provisional Patent Application No. 62/589,427, filed Nov. 21, 2017, entitled “Work Unit Stack Data Structures in Multiple Core Processor System,” and U.S. patent application Ser. No. 15/949,692, entitled “Efficient Work Unit Processing in a Multicore System,” filed Apr. 10, 2018, the entire content of each of which is incorporated herein by reference.</div>
<div class="description-paragraph" id="p-0130" num="0129">Access node <b>150</b> may deliver significantly improved efficiency over x86 for targeted use cases, such as storage and networking input/output, security and network function virtualization (NFV), accelerated protocols, and as a software platform for certain applications (e.g., storage, security, and data ingestion). Access node <b>150</b> may provide storage aggregation (e.g., providing direct network access to flash memory, such as SSDs) and protocol acceleration. Access node <b>150</b> provides a programmable platform for storage virtualization and abstraction. Access node <b>150</b> may also perform firewall and address translation (NAT) processing, stateful deep packet inspection, and cryptography. The accelerated protocols may include TCP, UDP, TLS, IPSec (e.g., accelerates AES variants, SHA, and PKC), RDMA, and iSCSI. Access node <b>150</b> may also provide quality of service (QoS) and isolation containers for data, and provide LLVM binaries.</div>
<div class="description-paragraph" id="p-0131" num="0130">Access node <b>150</b> may support software including network protocol offload (TCP/IP acceleration, RDMA and RPC); initiator and target side storage (block and file protocols); high level (stream) application APIs (compute, network and storage (regions)); fine grain load balancing, traffic management, and QoS; network virtualization and network function virtualization (NFV); and firewall, security, deep packet inspection (DPI), and encryption (IPsec, SSL/TLS).</div>
<div class="description-paragraph" id="p-0132" num="0131">In one particular example, access node <b>150</b> may expose Ethernet ports of 100 Gbps, of which a subset may be used for local consumption (termination) and the remainder may be switched back to a network fabric via Ethernet interface <b>164</b>. For each of host units <b>154</b>, access node <b>150</b> may expose a x16 PCI-e interface <b>166</b>. Access node <b>150</b> may also offer a low network latency to flash memory (e.g., SSDs) that bypasses local host processor and bus.</div>
<div class="description-paragraph" id="p-0133" num="0132">In the example of <figref idrefs="DRAWINGS">FIG. 9</figref>, processing clusters <b>156</b> and central cluster <b>158</b> are arranged in a grid. For example, access node <b>150</b> may include “M” rows of “N” processing clusters. In some examples, access node <b>150</b> may include 2 rows of 2 processing clusters for a total of 4 processing clusters <b>156</b>. In other examples, access node <b>150</b> may include 3 rows or 3 processing clusters including central cluster <b>158</b> for a total of 8 processing clusters <b>156</b> arranged with central cluster <b>158</b> in a 3×3 grid. In still other examples, access node <b>150</b> may include more processing clusters arranged around central cluster <b>158</b>. Although identified in <figref idrefs="DRAWINGS">FIG. 9</figref> as being different than processing clusters <b>156</b>, it should be understood that central cluster <b>158</b> is one of processing clusters <b>156</b> and, in some examples, may operate in the same or a similar fashion as any of processing clusters <b>156</b>.</div>
<div class="description-paragraph" id="p-0134" num="0133">In some examples, central cluster <b>158</b> may include three conceptual processing units (not shown in <figref idrefs="DRAWINGS">FIG. 9</figref>): a central dispatch unit, a coherence directory unit configured to determine locations of data within coherent memory of access node <b>150</b>, and a central synchronization unit configured to maintain proper sequencing and ordering of operations within access node <b>150</b>. Alternatively, in other examples, any of processing clusters <b>156</b> may include these conceptual processing units.</div>
<div class="description-paragraph" id="p-0135" num="0134">Central cluster <b>158</b> may also include a plurality of processing cores, e.g., MIPS (microprocessor without interlocked pipeline stages) cores, ARM (advanced RISC (reduced instruction set computing) machine) cores, PowerPC (performance optimization with enhanced RISC-performance computing) cores, RISC-V (RISC five) cores, or CISC (complex instruction set computing or x86) cores. Central cluster <b>158</b> may be configured with two or more processing cores that each include at least one virtual processor. In one specific example, central cluster <b>158</b> is configured with four processing cores, each including two virtual processors, and executes a control operating system (such as a Linux kernel). The virtual processors are referred to as “virtual processors,” in the sense that these processors are independent threads of execution of a single core. However, it should be understood that the virtual processors are implemented in digital logic circuitry, i.e., in requisite hardware processing circuitry.</div>
<div class="description-paragraph" id="p-0136" num="0135">Access node <b>150</b> may be configured according to architectural principles of using a most energy efficient way of transporting data, managing metadata, and performing computations. Access node <b>150</b> may act as an input/output (I/O) hub that is optimized for executing short instruction runs (e.g., 100 to 400 instruction runs) or micro-tasks efficiently.</div>
<div class="description-paragraph" id="p-0137" num="0136">Access node <b>150</b> may provide high performance micro-task parallelism using the components thereof through work management. For example, access node <b>150</b> may couple a low latency dispatch network with a work queue interface at processing clusters <b>156</b> to reduce delay from work dispatching to start of execution of the work by processing clusters <b>156</b>. The components of access node <b>150</b> may also operate according to a run-to-completion work flow, which may eliminate software interrupts and context switches. Hardware primitives may further accelerate work unit generation and delivery. Access node <b>150</b> may also provide low synchronization, in that the components thereof may operate according to a stream-processing model that encourages flow-through operation with low synchronization and inter-processor communication. The stream-processing model may further structure access by multiple processors (e.g., processors of processing clusters <b>156</b>) to the same data and resources, avoid simultaneous sharing, and therefore, reduce contention. A processor may relinquish control of data referenced by a work unit as the work unit is passed to the next processor in line. Furthermore, access node <b>150</b> may provide a dedicated signaling/dispatch network, as well as a high capacity data network, and implement a compact work unit representation, which may reduce communication cost and overhead.</div>
<div class="description-paragraph" id="p-0138" num="0137">Access node <b>150</b> may also provide memory-related enhancements over conventional architectures. For example, access node <b>150</b> may encourage a processing model that minimizes data movement, relying as much as possible on passing work by reference. Access node <b>150</b> may also provide hardware primitives for allocating and freeing buffer memory, as well as for virtualizing the memory space, thereby providing hardware-based memory management. By providing a non-coherent memory system for stream data, access node <b>150</b> may eliminate detrimental effects of coherency that would otherwise result in surreptitious flushes or invalidates of memory, or artifactual communication and overhead. Access node <b>150</b> also provides a high bandwidth data network that allows unfettered access to memory and peripherals such that any stream data update can be done through main memory, and stream cache-to-stream cache transfers are not required. Access node <b>150</b> may be connected through a high bandwidth interface to external memory <b>170</b>.</div>
<div class="description-paragraph" id="p-0139" num="0138">Access node <b>150</b> may also provide features that reduce processing inefficiencies and cost. For example, access node <b>150</b> may provide a stream processing library (i.e., a library of functions available to programmers for interfacing with access node <b>150</b>) to be used when implementing software to be executed by access node <b>150</b>. That is, the stream processing library may provide one or more application programming interfaces (APIs) for directing processing tasks to access node <b>150</b>. In this manner, the programmer can write software that accesses hardware-based processing units of access node <b>150</b>, such that a CPU can offload certain processing tasks to hardware-based processing units of access node <b>150</b>. The stream processing library may handle message passing on behalf of programs, such that meta-data and state are pushed to the cache and stream memory associated with the core where processing occurs. In this manner, access node <b>150</b> may reduce cache misses, that is, stalls due to memory accesses. Access node <b>150</b> may also provide lock-free operation. That is, access node <b>150</b> may be implemented according to a message-passing model that enables state updates to occur without the need for locks, or for maintaining the stream cache through coherency mechanisms. Access node <b>150</b> may also be implemented according to a stream operating model, which encourages data unit driven work partitioning and provides an intuitive framework for determining and exploiting parallelism. Access node <b>150</b> also includes well-defined hardware models that process intensive operations such as cyclical redundancy checks (CRC), cryptography, compression, and the like.</div>
<div class="description-paragraph" id="p-0140" num="0139">In general, access node <b>150</b> may satisfy a goal of minimizing data copy and data movement within the chip, with most of the work done by reference (i.e., passing pointers to the data between processors, e.g., processors within or between processing clusters <b>156</b>). Access node <b>150</b> may support two distinct memory systems: a traditional, coherent memory system with a two-level cache hierarchy, and a non-coherent buffer memory system optimized for stream processing. The buffer memory may be shared and cached at the L1 level, but coherency is not maintained by hardware of access node <b>150</b>. Instead, coherency may be achieved through machinery associated with the stream processing model, in particular, synchronization of memory updates vs. memory ownership transfer. Access node <b>150</b> uses the non-coherent memory for storing packets and other data that would not cache well within the coherent memory system. More details on the bifurcated memory system included in the access node are available in U.S. patent application Ser. No. 15/949,892, filed Apr. 10, 2018, and titled “Relay Consistent Memory Management in a Multiple Processor System,” the entire content of which is incorporated herein by reference.</div>
<div class="description-paragraph" id="p-0141" num="0140">In the example of <figref idrefs="DRAWINGS">FIG. 9</figref>, access node <b>150</b> includes at least four processing clusters <b>156</b>, although other numbers of processing clusters <b>156</b> may be used in other examples. Each of processing clusters <b>156</b> may include two or more general purpose processing cores (e.g., MIPS cores, ARM cores, PowerPC cores, RISC-V cores, or CISC or x86 cores) and one or more accelerators. In one particular example, access node <b>150</b> includes four processing clusters <b>156</b>, each including two processing cores, for a total of eight cores, and one accelerator per processing cluster. In another example, access node <b>150</b> includes eight processing clusters <b>156</b>, each including six processing cores, for a total of forty-eight cores, and two accelerators per processing cluster. In a further example, access node <b>150</b> includes fifteen processing clusters <b>156</b>, each including four processing cores, for a total of sixty cores, and two accelerators per processing cluster.</div>
<div class="description-paragraph" id="p-0142" num="0141">A general-purpose operating system, such as Linux or Unix, can run on one or more of processing clusters <b>156</b>. Central cluster <b>158</b> may be configured differently from processing clusters <b>156</b> (which may be referred to as stream processing clusters). For example, central cluster <b>158</b> executes the operating system kernel (e.g., Linux kernel) as a control plane. Processing clusters <b>156</b> may function in run-to-completion thread mode. That is, processing clusters <b>156</b> may operate in a tight loop fed by work queues associated with each virtual processor in a cooperative multi-tasking fashion. Processing cluster <b>156</b> may further include one or more hardware accelerator units to accelerate networking, matrix multiplication, cryptography, compression, timer management, direct memory access (DMA), and copy, among other tasks.</div>
<div class="description-paragraph" id="p-0143" num="0142">Networking unit <b>152</b> includes a forwarding pipeline implemented using flexible engines (e.g., a parser engine, a look-up engine, and a rewrite engine) and supports features of IP transit switching. Networking unit <b>152</b> may also use processing cores (e.g., MIPS cores, ARM cores, PowerPC cores, RISC-V cores, or CISC or x86 cores) to support control packets and low-bandwidth features, such as packet-multicast (e.g., for OSI Layers 2 and 3). Access node <b>150</b> may act as a combination of a switch/router and a number of network interface cards. The processing cores of networking unit <b>152</b> (and/or of processing clusters <b>156</b>) may perform network interface card functionality, packet switching, and the like, and may use large forwarding tables and offer programmability.</div>
<div class="description-paragraph" id="p-0144" num="0143">Host units <b>154</b>, processing clusters <b>156</b>, central cluster <b>158</b>, networking unit <b>152</b>, and external memory <b>170</b> are communicatively interconnected via four types of links. A first set of direct links <b>162</b> (represented as dashed lines in <figref idrefs="DRAWINGS">FIG. 9</figref>) directly connect central cluster <b>158</b> to each of the other components of access node <b>150</b>, that is, host units <b>154</b>, processing clusters <b>156</b>, networking unit <b>152</b>, and external memory <b>170</b>, to form a signaling network associated with the non-coherent memory system. A second set of direct links <b>163</b> (represented as dot-dot-dashed lines in <figref idrefs="DRAWINGS">FIG. 9</figref>) directly connect central cluster <b>158</b> to each of processing clusters <b>156</b> and external memory <b>170</b> to form a coherency network associated with the coherent memory system. A third set of direct links <b>165</b> (represented as dotted lines in <figref idrefs="DRAWINGS">FIG. 9</figref>) directly connect central cluster <b>158</b> to each of host units <b>154</b>, processing clusters <b>156</b>, and networking unit <b>152</b> to form a broadcast network associated with a resource management system of access node <b>150</b>. Additionally, grid links <b>160</b> (represented as solid lines in <figref idrefs="DRAWINGS">FIG. 9</figref>) connect neighboring components (including host units <b>154</b>, processing clusters <b>156</b>, networking unit <b>152</b>, and external memory <b>170</b>) to each other in a two-dimensional grid to form a data network. For example, host unit <b>154</b>A-<b>1</b> is directly coupled via grid links <b>160</b> to processing cluster <b>156</b>A-<b>1</b> and host unit <b>154</b>A-M.</div>
<div class="description-paragraph" id="p-0145" num="0144">In this manner, processing clusters <b>156</b>, host units <b>154</b>, central cluster <b>158</b>, networking unit <b>152</b>, and external memory <b>170</b> are interconnected using two or three main network-on-chip (NoC) fabrics. These internal fabrics may include a data network fabric formed by grid links <b>160</b>, and one or more control network fabrics including one or more of a signaling network formed by hub-and-spoke links <b>162</b>, a coherency network formed by hub-and-spoke links <b>163</b>, and a broadcast network formed by hub-and-spoke links <b>165</b>. The signaling network, coherency network, and broadcast network are formed by direct links similarly arranged in a star-shaped network topology. Alternatively, in other examples, only the data network and one of the signaling network or the coherency network may be included. The data network is a two-dimensional mesh topology that carries data for both coherent memory and buffer memory systems. In one example, each grid link <b>160</b> provides a 512b wide data path in each direction. In one example, each direct link <b>162</b> and each direct link <b>163</b> provides a 128b wide bidirectional data path. The coherency network is a logical hub and spoke structure that carries cache coherency transactions (not including data). The signaling network is a logical hub and spoke structure that carries buffer memory requests and replies (not including data), synchronization and other commands, and work units and notifications.</div>
<div class="description-paragraph" id="p-0146" num="0145">Access node <b>150</b> includes various resources, i.e., elements in limited quantities that are consumed during performance of various functions. Example resources include work unit queue sizes, virtual processor cycles, accelerator cycles, bandwidth of external interfaces (e.g., host units <b>154</b> and networking unit <b>152</b>), memory (including buffer memory, cache memory, and external memory), transient buffers, and time. In general, each resource can be translated to either time or space (e.g., memory). Furthermore, although certain resources can be reclaimed (such as memory), other resources (such as processing cycles and bandwidth) cannot be reclaimed.</div>
<div class="description-paragraph" id="p-0147" num="0146">Access node <b>150</b> (and more particularly, networking unit <b>152</b>, host units <b>154</b>, and processing clusters <b>156</b>, and central clusters <b>158</b>) may use the broadcast network formed by direct links <b>165</b> to broadcast a utilization status of their corresponding resources to central cluster <b>158</b>. Central cluster <b>158</b> may include an event queue manager (EQM) unit that stores copies of these utilization statuses for use when assigning various work units to these elements. Alternatively, in other examples, any of processing clusters <b>156</b> may include the EQM unit.</div>
<div class="description-paragraph" id="p-0148" num="0147">The utilization statuses may be represented as normalized color values (NCVs). Virtual processors may check the NCV of a desired resource to determine if the virtual processors can accept a work unit. If the NCV is above an allowable threshold for an initial work unit, each of the virtual processors places a corresponding flow in a pending state and sends an enqueue (NQ) event to the EQM. A flow is a sequence of computations that belong to a single ordering class. Each flow may be associated with a unique flow identifier (ID) that can be used to look up an entry for the flow in a global flow table (GFT). The flow entry may be linked to all reusable resources consumed by the flow so that these resources can be found and recovered when needed.</div>
<div class="description-paragraph" id="p-0149" num="0148">In response, the EQM enqueues the event into the specified event queue and monitors the NCV of the corresponding resource. If the NCV is below a desired dequeue (DQ) threshold, the EQM dequeues a calculated number of events from the head of the event queue. The EQM then translates these dequeued events into high-priority work unit messages and sends these work unit messages to their specified virtual processor destinations. The virtual processors use these dequeued events to determine if a flow can be transitioned from the pending state to an active state. For activated flows (i.e., those placed in the active state), the virtual processors may send a work unit to the desired resource. Work units that result from a reactivation are permitted to transmit if the NCV is below a threshold that is higher than the original threshold used to make the Event NQ decision as discussed above.</div>
<div class="description-paragraph" id="p-0150" num="0149">Access node <b>150</b> (and more particularly, networking unit <b>152</b>, host units <b>154</b>, processing clusters <b>156</b>, and central clusters <b>158</b>) uses the signaling network formed by direct links <b>162</b> to transport non-coherent buffer memory requests and replies, and work requests and notifications for inter-processor and interface unit communication (e.g., communication between processors of processing clusters <b>156</b> or processors of networking unit <b>152</b> and central cluster <b>158</b>). The signaling network formed by direct links <b>162</b> is a non-blocking, switched, low latency fabric that allows access node <b>150</b> to reduce delay between event arrival (e.g., arrival of a packet on a network interface of networking unit <b>152</b> coupled to Ethernet lanes <b>164</b>, arrival of a work request on one of PCI-e lanes <b>166</b> at one of host units <b>154</b>, or arrival of remote procedure calls (RPCs) between processing cores of processing clusters <b>156</b> and/or central cluster <b>158</b>) and start of execution by one of the cores. “Synchronization” refers to the proper sequencing and correct ordering of operations within access node <b>150</b>.</div>
<div class="description-paragraph" id="p-0151" num="0150">Access node <b>150</b> (and more particularly, processing clusters <b>156</b> and central clusters <b>158</b>) also uses the coherency network formed by direct links <b>163</b> to transport cache coherence requests and responses. Cores of processing clusters <b>156</b> and central cluster <b>158</b> may operate on a number of work queues in a prioritized matter. For example, each core may include one or more virtual processors, e.g., one to four virtual processors, and each virtual processor may operate on one to four work queues. The coherency network formed by direct links <b>162</b> provide services including inter-cluster cache coherence (e.g., for request and/or reply traffic for write updates, read miss, and flush operations).</div>
<div class="description-paragraph" id="p-0152" num="0151">Central cluster <b>158</b> is a logical central reflection point on both the signaling network formed by direct links <b>162</b> and the coherency network formed by direct links <b>163</b> that provides ordering for data sent within the signaling network and the coherency network, respectively. Central cluster <b>158</b> generally performs tasks such as handling a global cache directory and processing synchronization and coherence transactions, ensuring atomicity of synchronized operations, and maintaining a wall-clock time (WCT) that is synchronized with outside sources (e.g., using precision time protocol (PTP), IEEE 1588). Central cluster <b>158</b> is configured to address several billion synchronization/coherence messages per second. Central cluster <b>158</b> may be subdivided into sub-units where necessary for capacity to handle aggregated traffic. Alternatively, in other examples, any of processing cluster <b>156</b> may perform the tasks described herein as being performed by central cluster <b>158</b>.</div>
<div class="description-paragraph" id="p-0153" num="0152">As shown in <figref idrefs="DRAWINGS">FIG. 9</figref>, the data network is formed by grid links <b>160</b> and connects processing clusters <b>156</b>, host units <b>154</b>, central cluster <b>158</b>, networking unit <b>152</b>, and external memory <b>170</b>. In particular, each of host unit <b>154</b>A-M, processing cluster <b>156</b>A-M, processing cluster <b>156</b>N-M, and host unit <b>154</b>B-M is connected to external memory <b>170</b> via a respective grid link <b>160</b>. Although not shown in <figref idrefs="DRAWINGS">FIG. 9</figref>, data network routers are provided at intersections of columns and rows of the data network fabric (e.g., within or coupled to host units <b>154</b>, processing clusters <b>156</b>, and central cluster <b>158</b>). These routers may be coupled to respective host units <b>154</b>, processing clusters <b>156</b>, and central cluster <b>158</b> via a 512b bidirectional data network links. In the example of <figref idrefs="DRAWINGS">FIG. 9</figref>, processing clusters <b>156</b>A-<b>1</b> and <b>156</b>N-<b>1</b> are shown as communicatively coupled to networking unit <b>152</b>, although it should be understood that the routers for processing clusters <b>156</b>A-<b>1</b> and <b>156</b>N-<b>1</b> may in fact be communicatively coupled to networking unit <b>152</b> via grid links <b>160</b>.</div>
<div class="description-paragraph" id="p-0154" num="0153">Access node <b>150</b> (and more particularly, networking unit <b>152</b>, host units <b>154</b>, processing clusters <b>156</b>, and central clusters <b>158</b>) use the data network formed by grid links <b>160</b> to transport buffer memory blocks to/from L1 buffer caches of cores within processing clusters <b>156</b> and central cluster <b>158</b>. Access node <b>150</b> also uses the data network to transport cluster level buffer memory data, off-chip DRAM memory data, and data for external interfaces (e.g., interfaces provided by host units <b>154</b> and networking unit <b>152</b>). Access node <b>150</b> also uses the data network to transport coherent memory lines to and from L2 caches of processing clusters <b>156</b>, interface DMA engines, and off-chip DRAM memory.</div>
<div class="description-paragraph" id="p-0155" num="0154">“Messaging” may refer to work units and notifications for inter-processor and interface unit communication (e.g., between processing cores and/or processors of processing clusters <b>156</b>, central cluster <b>158</b>, host units <b>154</b>, and networking unit <b>152</b>). Central cluster <b>158</b> may include a central dispatch unit (CDU) (not shown) that is responsible for work unit (WU) queuing and flow control, work unit and completion notification dispatch, and load balancing and processor selection (e.g., selection of processors for performing work units among processing cores of processing clusters <b>156</b> and/or central cluster <b>158</b>). The CDU may allow ordering of work units with respect to other messages of central cluster <b>158</b>.</div>
<div class="description-paragraph" id="p-0156" num="0155">The CDU of central cluster <b>158</b> may also perform credit-based flow control, to manage the delivery of work units. The CDU may maintain a per-virtual-processor output queue plus per-peripheral unit queue of work units that are scheduled by the CDU, as the destination virtual processors allow, as a flow control scheme and to provide deadlock avoidance. The CDU may allocate each virtual processor of cores of processing clusters <b>156</b> a fixed amount of storage credits, which are returned when space is made available. The work queues may be relatively shallow. The CDU may include a work scheduling system that manages work production to match the consumption rate (this does not apply to networking unit <b>152</b>, and may be performed via scheduling requests for storage). Processing clusters <b>156</b> switch work units destined for virtual processors within a common one of processing clusters <b>156</b> locally within the processing cluster's work unit queue system.</div>
<div class="description-paragraph" id="p-0157" num="0156">In general, central cluster <b>158</b> ensures that the ordering of messages of the same type (e.g., coherence, synchronization, or work units) seen on an output towards a cluster or peripheral is the same as the order in which the messages were seen at each input to central cluster <b>158</b>. Ordering is not specified between multiple messages received from different inputs by central cluster <b>158</b>. Alternatively, in other examples, any of processing cluster <b>156</b> may include the CDU and perform the tasks described herein as being performed by central cluster <b>158</b>.</div>
<div class="description-paragraph" id="p-0158" num="0157">Networking unit <b>152</b> may expose Ethernet lanes <b>164</b> for connectivity to a network, such as switch fabric <b>14</b> of <figref idrefs="DRAWINGS">FIG. 1</figref>. In one particular example, networking unit <b>152</b> may expose twenty-four high speed symmetrical (HSS) Ethernet lanes (e.g., for 25 Gbps). Each of host units <b>154</b> may expose PCI-e lanes <b>166</b> for connectivity to host devices (e.g., servers) and data storage devices, e.g., solid state drives (SSDs). In one particular example, each of host units <b>152</b> may expose a number of PCI-e lanes <b>166</b>, which may be bifurcatable into multiple independent ports. In this example, access node <b>150</b> may be connected to four servers via two processor sockets per server using at least one PCI-e lane to each socket, and to eight SSDs using at least one PCI-e lane to each SSD.</div>
<div class="description-paragraph" id="p-0159" num="0158">Networking unit <b>152</b> connects to an Ethernet network via Ethernet lanes <b>164</b> and interfaces to the data network formed by grid links <b>160</b> and the signaling network formed by direct links <b>162</b>, i.e., the data and signaling internal fabrics. Networking unit <b>152</b> provides a Layer 3 (i.e., OSI networking model Layer 3) switch forwarding path, as well as network interface card (NIC) assistance.</div>
<div class="description-paragraph" id="p-0160" num="0159">As NIC assistance, networking unit <b>152</b> may perform various stateless assistance processes, such as checksum offload for Internet protocol (IP), e.g., IPv4 or IPv6, transmission control protocol (TCP), and/or uniform datagram protocol (UDP). Networking unit <b>152</b> may also perform assistance processes for receive side-scaling (RSS), large send offload (LSO), large receive offload (LRO), virtual local area network (VLAN) manipulation, and the like. On the Ethernet media access control (MAC) side, in one example, networking unit <b>152</b> may use multiple combination units, each with four 25 Gb HSS lanes that can be configured as 1×40/100G, 2×50G, or 4×25/10/1G. Networking unit <b>152</b> may also support Internet protocol security (IPsec), with a number of security associations (SAs). Networking unit <b>152</b> may include cryptographic units for encrypting and decrypting packets as necessary, to enable processing of the IPsec payload.</div>
<div class="description-paragraph" id="p-0161" num="0160">Networking unit <b>152</b> may also include a flexible network packet parsing unit. The packet parsing unit may be configured according to a specialized, high-performance implementation for common formats, including network tunnels (e.g., virtual extensible local area network (VXLAN), network virtualization using generic routing encapsulation (NVGRE), generic network virtualization encapsulation (GENEVE), multiprotocol label switching (MPLS), or the like). Networking unit <b>152</b> may also include an OSI Layer 3 (L3) switch that allows cut-through Ethernet to Ethernet switching, using a local memory (not shown) of networking unit <b>152</b>, as well as host-to-host switching.</div>
<div class="description-paragraph" id="p-0162" num="0161">One or more hardware direct memory access (DMA) engine instances (not shown) may be attached to three data network ports of networking unit <b>152</b>, which are coupled to respective grid links <b>160</b>. The DMA engines of networking unit <b>152</b> are configured to fetch packet data for transmission. The packet data may be in on-chip or off-chip buffer memory (e.g., within buffer memory of one of processing clusters <b>156</b> or external memory <b>170</b>), or in host memory.</div>
<div class="description-paragraph" id="p-0163" num="0162">Host units <b>154</b> provide interfaces to respective PCI-e bus lanes <b>166</b>. This allows access node <b>150</b> to operate as an endpoint or as a root (in dual mode). For example, access node <b>150</b> may connect to a host system (e.g., an x86 server) as an endpoint device, and access node <b>150</b> may connect as a root to endpoint devices, such as SSD devices.</div>
<div class="description-paragraph" id="p-0164" num="0163">In the example of <figref idrefs="DRAWINGS">FIG. 9</figref>, access node <b>150</b> includes 2 columns of “M” host units <b>154</b>. In some examples, access node <b>150</b> may include 2 columns of 2 for a total of four host units <b>154</b>. In other examples, access node <b>150</b> may include 2 columns of 3 for a total of six host units. In still other examples, access node <b>150</b> may only include one host unit. Although illustrated in a grid pattern with processing clusters <b>156</b> in <figref idrefs="DRAWINGS">FIG. 9</figref>, in other examples access node <b>150</b> may include any number of host units not necessarily tied to rows of processing clusters. In one particular example, each of host units <b>154</b> exposes 16 PCI-e lanes <b>166</b>, divisible into granularity of x4 units (e.g., for SSD) or x8 units for system connectivity. Host units <b>154</b> may include respective bifurcated controllers (not shown) that are separate entities. Each of host units <b>154</b> may include one or more controllers, e.g., one controller per set of x4 PCI-e lanes. In general, each of host units <b>154</b> includes respective virtualization resources that are not shared among other host units <b>154</b>.</div>
<div class="description-paragraph" id="p-0165" num="0164">Each of host units <b>154</b> may also include a respective hardware DMA engine (not shown). Each DMA engine is configured to fetch data and buffer descriptors from host memory, and to deliver data and completions to host memory. Each DMA engine also sends messages to the PCI controller to trigger interrupt generation. Additional functionality may be provided by core processing units of host units <b>154</b> that execute software, which consume streams of buffer descriptors, such as generating DMA addresses for payload placement and/or generating completion addresses.</div>
<div class="description-paragraph" id="p-0166" num="0165">Processing clusters <b>156</b> and central cluster <b>158</b> may perform data protection mechanisms to protect data stored in on- or off-chip memory, such as in buffers or in external memory <b>170</b>. Such data protection mechanisms may reduce or eliminate silent data corruption (SDC) probability with single bit soft errors (such errors may occur due to radiation, cosmic rays, internally generated alpha particles, noise, etc. . . . ) and escaped multi-bit errors.</div>
<div class="description-paragraph" id="p-0167" num="0166">Access node <b>150</b> may execute various types of applications. Examples of such applications are classified below according to three axes: layering, consumption model, and stream multiplexing. Three example layers of software/applications within the context of access node <b>150</b> include access software, internal software, and applications. Access software represents system software, such as drivers and protocol stacks. Such access software is typically part of the kernel and runs in root/privileged mode, although in some cases, protocol stacks may be executed in user space. Internal software includes further system software and libraries, such as storage initiator/target software that execute on top of the access software. Traditionally, internal software is executed in kernel space. Applications represents user applications that execute in user space. Consumption models can be broadly classified on a spectrum with a protocol processing model (header consumption) at one end and byte processing model (data consumption) at the other end. Typically, system software is near the protocol processing model end, and user applications tend to form the majority of applications at the byte processing model end.</div>
<div class="description-paragraph" id="p-0168" num="0167">Table 1 below categorizes example software/applications according to the various layers and consumption models discussed above:</div>
<div class="description-paragraph" id="p-0169" num="0168">
<tables id="TABLE-US-00001" num="00001">
<patent-tables colsep="0" frame="none" rowsep="0">
<table align="left" class="description-table" cols="1" colsep="0" rowsep="0" width="100%">
<thead>
<tr class="description-tr">
<td class="description-td" colspan="1" nameend="1" namest="1" rowsep="1">TABLE 1</td>
</tr>
</thead>
<tbody><tr class="description-tr">
<td align="center" class="description-td" colspan="1" nameend="1" namest="1" rowsep="1"> </td>
</tr>
<tr class="description-tr">
<td class="description-td">Application Classification</td>
</tr>
</tbody></table>
<table align="left" class="description-table" cols="3" colsep="0" rowsep="0" width="100%">
<tbody><tr class="description-tr">
<td class="description-td"> </td>
<td class="description-td"> </td>
<td class="description-td">Layering</td>
</tr>
</tbody></table>
<table align="left" class="description-table" cols="5" colsep="0" rowsep="0" width="100%">
<tbody><tr class="description-tr">
<td class="description-td"> </td>
<td class="description-td">Streams</td>
<td class="description-td">Access</td>
<td class="description-td">Internal</td>
<td class="description-td">Applications</td>
</tr>
<tr class="description-tr">
<td align="center" class="description-td" colspan="5" nameend="5" namest="1" rowsep="1"> </td>
</tr>
</tbody></table>
<table align="left" class="description-table" cols="5" colsep="0" rowsep="0" width="100%">
<tbody><tr class="description-tr">
<td class="description-td">Consumption</td>
<td class="description-td">Header</td>
<td class="description-td">Drivers</td>
<td class="description-td">Storage</td>
<td class="description-td">Firewall</td>
</tr>
<tr class="description-tr">
<td class="description-td"> </td>
<td class="description-td">Payload</td>
<td class="description-td">—</td>
<td class="description-td">Compression</td>
<td class="description-td">Deep packet</td>
</tr>
<tr class="description-tr">
<td class="description-td"> </td>
<td class="description-td"> </td>
<td class="description-td"> </td>
<td class="description-td">Encryption</td>
<td class="description-td">inspection</td>
</tr>
<tr class="description-tr">
<td align="center" class="description-td" colspan="5" nameend="5" namest="1" rowsep="1"> </td>
</tr>
</tbody></table>
</patent-tables>
</tables>
</div>
<div class="description-paragraph" id="p-0170" num="0169">In this manner, access node <b>150</b> may offer improvements over conventional processing systems with respect to work management, memory management, and/or processor execution.</div>
<div class="description-paragraph" id="p-0171" num="0170"> <figref idrefs="DRAWINGS">FIG. 10A</figref> is a block diagram illustrating an example processing cluster <b>180</b> including a plurality of programmable processing cores <b>182</b>A-<b>182</b>N. Each of processing clusters <b>156</b> of access node <b>150</b> of <figref idrefs="DRAWINGS">FIG. 9</figref> may be configured in a manner substantially similar to that shown in <figref idrefs="DRAWINGS">FIG. 10A</figref>. In this example, processing cluster <b>180</b> includes cores <b>182</b>A-<b>182</b>N (“cores <b>182</b>”), coherent cache memory <b>184</b>, non-coherent buffer memory <b>186</b>, and accelerators <b>188</b>A-<b>188</b>X (“accelerators <b>188</b>”). In one example, processing cluster <b>180</b> may include two processing cores <b>182</b> and at least one accelerator <b>188</b>. In another example, processing cluster <b>180</b> may include six processing cores <b>182</b> and two accelerators <b>188</b>. As noted above, an access node (such as access node <b>150</b> of <figref idrefs="DRAWINGS">FIG. 9</figref>) may support two distinct memory systems: a coherent memory system and a non-coherent buffer memory system. In the example of <figref idrefs="DRAWINGS">FIG. 10A</figref>, coherent cache memory <b>184</b> represents part of the coherent memory system (e.g., coherent cache memory <b>184</b> may comprise a level two (L2) coherent cache memory where cores <b>182</b> may also include one or more level one (L1) data caches, e.g., as discussed with respect to <figref idrefs="DRAWINGS">FIG. 10B</figref> below), while non-coherent buffer memory <b>186</b> represents part of the non-coherent buffer memory system. Cores <b>182</b> may represent the processing cores discussed with respect to access node <b>150</b> of <figref idrefs="DRAWINGS">FIG. 9</figref>. Cores <b>182</b> may share non-coherent buffer memory <b>186</b>, which in one example may be a 2 MB buffer memory. As one example, cores <b>182</b> may use non-coherent buffer memory <b>186</b> for sharing streaming data, such as network packets.</div>
<div class="description-paragraph" id="p-0172" num="0171">In general, accelerators <b>188</b> perform acceleration for various data-processing functions, such as look-ups, matrix multiplication, cryptography, compression, regular expressions, or the like. That is, accelerators <b>188</b> may comprise hardware implementations of look-up engines, matrix multipliers, cryptographic engines, compression engines, regular expression interpreters, or the like. For example, accelerators <b>188</b> may include a lookup engine that performs hash table lookups in hardware to provide a high lookup rate. The lookup engine may be invoked through work units from external interfaces and virtual processors of cores <b>182</b>, and generates lookup notifications through work units. Accelerators <b>188</b> may also include one or more cryptographic units to support various cryptographic processes, such as any or all of Advanced Encryption Standard (AES), Galois/Counter Mode (GCM), block cipher mode (BCM), Secure Hash Algorithm (SHA), public key cryptography, elliptic curve cryptography, RSA, any of their variants, or the like. One or more of such cryptographic units may be integrated with networking unit <b>152</b> (<figref idrefs="DRAWINGS">FIG. 9</figref>), in some examples, to perform Internet protocol security (IPsec) cryptography and/or secure sockets layer (SSL) cryptography. Accelerators <b>188</b> may also include one or more compression units to perform compression and/or decompression, e.g., according to ZIP, PKZIP, GZIP, Lempel-Ziv, public format compression such as Snappy, or the like. The compression units may be configured to perform gather-list-based data consumption and/or scatter-list-based data delivery. The compression units may receive work requests and provide work notifications. The compression units may have access to hardware allocators of access node <b>150</b> that handle memory allocation and freeing, e.g., within external memory <b>170</b> (<figref idrefs="DRAWINGS">FIG. 9</figref>), since the size of the output buffer for decompression may not be known a-priori.</div>
<div class="description-paragraph" id="p-0173" num="0172"> <figref idrefs="DRAWINGS">FIG. 10B</figref> is a block diagram illustrating components of an example programmable processing core <b>190</b> of a processing cluster. Each of cores <b>182</b> of <figref idrefs="DRAWINGS">FIG. 10A</figref> may include components substantially similar to those of core <b>190</b> of <figref idrefs="DRAWINGS">FIG. 10B</figref>. In this example, cores <b>190</b> may be a dual-issue with dual integer unit, and is configured with one or more hardware threads referred to as Virtual Processors (VPs) <b>192</b>A-<b>192</b>M (“VPs <b>19</b> <sub>2</sub>”). Core <b>190</b> also includes a level 1 (L1) instruction cache <b>19</b> <sub>4 </sub>and a L1 data cache <b>196</b>. When each of cores <b>182</b> of <figref idrefs="DRAWINGS">FIG. 10A</figref> includes an L1 data cache similar to L1 data cache <b>196</b>, the L1 data caches of cores <b>182</b> may share L2 coherent cache memory <b>184</b> of <figref idrefs="DRAWINGS">FIG. 10A</figref>. In some example, the cache size for processing cluster <b>180</b> (<figref idrefs="DRAWINGS">FIG. 10A</figref>) may be 1 MB or larger.</div>
<div class="description-paragraph" id="p-0174" num="0173">Core <b>190</b> also includes a L1 buffer cache <b>198</b>, which may be smaller than L1 data cache <b>196</b>. Core <b>190</b> may use L1 buffer cache <b>198</b> for non-coherent data, such as packets or other data for software managed through stream processing mode. L1 buffer cache <b>198</b> may store data for short-term caching, such that the data is available for fast access.</div>
<div class="description-paragraph" id="p-0175" num="0174">When one of virtual processors <b>19</b> <sub>2</sub>, such as virtual processor <b>192</b>A, accesses memory, virtual processor <b>192</b>A uses L1 data cache <b>196</b> or L1 buffer cache <b>198</b>, based on the physical memory address issued by a memory management unit (not shown). Access node <b>150</b> (<figref idrefs="DRAWINGS">FIG. 9</figref>) and components thereof, such as processing clusters <b>156</b> and cores thereof (such as cores <b>182</b> of <figref idrefs="DRAWINGS">FIG. 10A</figref>), may be configured to split memory space into separate ranges for buffer memory and coherent memory, e.g., by using high order address bits, which allows the ranges to be mapped to either buffer memory or coherent memory.</div>
<div class="description-paragraph" id="p-0176" num="0175"> <figref idrefs="DRAWINGS">FIG. 11</figref> is a flow diagram illustrating an example process by which a processing cluster <b>320</b> processes a work unit. In this example, processing cluster <b>320</b> includes cores <b>322</b>A-<b>322</b>D (cores <b>322</b>), which may include components similar to core <b>190</b> of <figref idrefs="DRAWINGS">FIG. 10B</figref>. Processing cluster <b>320</b> also includes a cluster manager (CM) <b>321</b> with work unit queue manager (WQM) <b>324</b>, look-up engine (LE) <b>326</b>, local switch <b>328</b> that participates in the signaling network (SN), and local switch <b>330</b> that participates in the data network (DN). Queue manager <b>324</b> manages hardware queues (HWQs) <b>336</b> and virtual processor queues (VPQs) <b>338</b>. Cores <b>322</b> include respective virtual processors and Level 1 (L1) caches, e.g., as discussed above with respect to <figref idrefs="DRAWINGS">FIG. 10B</figref>. Processing cluster <b>320</b> further includes L2 cache <b>332</b> and buffer memory <b>334</b>, which may correspond respectively to L2 coherent cache memory <b>184</b> and non-coherent buffer memory <b>186</b> of <figref idrefs="DRAWINGS">FIG. 10A</figref>.</div>
<div class="description-paragraph" id="p-0177" num="0176">Initially, queue manager <b>324</b> of processing cluster <b>320</b> queues a work unit (WU) in a one of hardware queues (HWQs) <b>336</b> (<b>300</b>). When queue manager <b>324</b> “pops” the work unit from the one of hardware queues <b>336</b>, queue manager <b>324</b> delivers the work unit to LE <b>326</b> (<b>302</b>). LE <b>326</b> processes the work unit (<b>304</b>) and determines that the work unit is to be delivered to one of cores <b>322</b> (in particular, core <b>322</b>A, in this example) of processing cluster <b>320</b>. Thus, LE <b>326</b> forwards the work unit to local switch <b>328</b> of the signaling network (SN) (<b>306</b>), which forwards the work unit to be queued in one of virtual processor queues (VPQs) <b>338</b>.</div>
<div class="description-paragraph" id="p-0178" num="0177">After queue manager <b>324</b> pops the work unit from the one of virtual processor queues <b>338</b> (<b>308</b>), queue manager <b>324</b> delivers the work unit via core interface <b>339</b> to core <b>322</b>A (<b>310</b>), in this example. Interface unit <b>325</b> of core <b>322</b>A then delivers the work unit to one of the virtual processors (VPs) <b>323</b>A-<b>323</b>M (VP <b>323</b>A, in this example), which processes the work unit (<b>312</b>), i.e., performs the work associated with the work unit. For example, initially, VP <b>323</b>A receives the work unit and issues a prefetch request specifying relevant addresses of cached data. VP <b>323</b>A retrieves the data from either the L1 data cache or the L1 buffer cache, depending on the addresses from the prefetch request and a cache coherency attribute. The prefetched data may be delivered to accelerators associated with core <b>322</b>A to perform accelerator processing. VP <b>323</b>A then outputs corresponding results (possibly including one or more work unit messages) from execution of the work unit back through interface unit <b>325</b> of core <b>322</b>A (<b>314</b>). For example, VP <b>323</b>A may generate the one or more new work unit messages, and may utilize interlocking of work unit transmission with the execution of a cache flushing finite state machine (FSM) of core <b>322</b>A.</div>
<div class="description-paragraph" id="p-0179" num="0178"> <figref idrefs="DRAWINGS">FIG. 12</figref> is a flow diagram illustrating an example process by which a host unit <b>390</b> processes a data request (e.g., a request for data from a connected server device, storage device such as an SSD, or the like). Host unit <b>390</b> may be similar to any of host units <b>154</b> of <figref idrefs="DRAWINGS">FIG. 9</figref>. In this example, host unit <b>390</b> includes a local switch <b>392</b> for communicating via the signaling network (SN), local switch <b>394</b> for communicating via the data network (DN), work unit queue manager (WQM) <b>396</b> for queuing and managing work units, direct memory access (DMA) unit <b>398</b>, and PCI-e complex 400, which may be configured to provide a single x16 PCI-e port, two x8 PCI-e ports, or four x4 PCI-e ports, in this example.</div>
<div class="description-paragraph" id="p-0180" num="0179">Initially, host unit <b>390</b> receives a request for data via local switch <b>392</b> of the signaling network, which forwards the request to WQM <b>396</b> (<b>410</b>), which queues the request in the form of a work unit. After WQM <b>396</b> pops the work unit from the corresponding queue, WQM <b>396</b> delivers the work unit to DMA engine <b>398</b> (<b>412</b>). DMA engine <b>398</b> processes the work unit to retrieve the requested data via PCI-e complex 400 (<b>414</b>). DMA engine <b>398</b> then delivers the requested data via local switch <b>394</b> of the data network, and completes other processes such as signaling completion and freeing buffer memory space via local switch <b>392</b> of the signaling network (<b>416</b>).</div>
<div class="description-paragraph" id="p-0181" num="0180"> <figref idrefs="DRAWINGS">FIG. 13</figref> is a flow diagram illustrating an example transmission pipeline processing flow for processing stream data, such as packets. <figref idrefs="DRAWINGS">FIG. 13</figref> illustrates examples of processing cores (which may correspond to cores <b>182</b> of <figref idrefs="DRAWINGS">FIG. 10A</figref> and include components similar to core <b>190</b> of <figref idrefs="DRAWINGS">FIG. 10B</figref>), a host unit (HU) DMA (which may correspond to a DMA engine of one of host units <b>154</b> of <figref idrefs="DRAWINGS">FIG. 9</figref>), non-coherent buffer memory (which may correspond to non-coherent buffer memory <b>186</b> of <figref idrefs="DRAWINGS">FIG. 10A</figref> and/or data stored off-chip, e.g., in external memory <b>170</b> of <figref idrefs="DRAWINGS">FIG. 9</figref>), coherent memory (which may correspond to L2 coherent cache memory <b>184</b> of <figref idrefs="DRAWINGS">FIG. 10A</figref> and/or data stored off-chip, e.g., in external memory <b>170</b> of <figref idrefs="DRAWINGS">FIG. 9</figref>), and a networking unit (NU) DMA, which may correspond to a DMA of networking unit <b>152</b> (<figref idrefs="DRAWINGS">FIG. 9</figref>).</div>
<div class="description-paragraph" id="p-0182" num="0181">Initially, processing core 0 (C0) of a processing cluster receives a work unit indicating that a packet (or other streaming unit of data) is to be constructed and transmitted from data received via a host unit (e.g., from a server device, storage device, or other device connected to an access node via the host unit) (<b>430</b>). Processing core 0 also receives a doorbell from a queue context (<b>432</b>), to cause processing core 0 to send a work unit to the host unit DMA (HU DMA) to retrieve the descriptors of the data to be transmitted (<b>434</b>).</div>
<div class="description-paragraph" id="p-0183" num="0182">The host unit DMA retrieves the data and sends a work unit including descriptor data to core 1 (C1) (<b>436</b>), in this example. Core 1 then accesses the descriptor data from the buffer memory (<b>438</b>) and determines the flow context associated with the descriptor data, and a core tasked with processing the data (e.g., core 2). Core 1 sends a work unit to core 2 (C2) (<b>440</b>), which prompts core 2 to retrieve the descriptors from the buffer memory, and also retrieve a flow context for the corresponding packet flow from coherent memory (<b>442</b>). Core 2 then sends a work unit to the host unit DMA to retrieve data for the packet to be constructed (<b>444</b>). The host unit DMA stores the data for the packet to be constructed, including a payload and any header information, in the buffer memory (<b>446</b>), and sends a work unit to core 3 (C3) (<b>448</b>) indicating that the data is available. Core 3 then generates data for constructing the packet, and sends this data in the form of a work unit to the networking unit DMA (NU DMA) (<b>450</b>).</div>
<div class="description-paragraph" id="p-0184" num="0183">The networking unit DMA then retrieves the header and payload data (<b>452</b>) from the buffer memory, fully assembles the packet, and forwards the packet, e.g., via Ethernet lanes (<b>454</b>). The networking unit DMA then provides a work unit back to core 2 (<b>456</b>), which may, for example, include data in response to the packet sent via the Ethernet lanes.</div>
<div class="description-paragraph" id="p-0185" num="0184"> <figref idrefs="DRAWINGS">FIG. 14</figref> is a series of flow diagrams illustrating various example processing flows from a networking unit (NU) to a host unit (HU) or back to the networking unit, such as networking unit <b>152</b> to one of host units <b>154</b> of <figref idrefs="DRAWINGS">FIG. 9</figref>. <figref idrefs="DRAWINGS">FIG. 14</figref> illustrates that such flows may include passage through various components of an access node, such as a buffer memory and/or an external memory. An access node in accordance with the techniques of this disclosure is generally an I/O hub, and therefore, primary flows start at an interface and end at another interface, which may be one of the host units and/or the networking unit.</div>
<div class="description-paragraph" id="p-0186" num="0185">In this example, buffer memory may correspond to on-chip memory, such as non-coherent buffer memory <b>186</b> of <figref idrefs="DRAWINGS">FIG. 10A</figref>. External memory corresponds to off-chip memory, such as external memory <b>170</b> of <figref idrefs="DRAWINGS">FIG. 9</figref>. In particular, in these examples, external memory may particularly refer to an address range allocated to buffer memory (i.e., non-coherent memory) of off-chip high bandwidth memory.</div>
<div class="description-paragraph" id="p-0187" num="0186">The first example flow corresponds to a simple transit case, such as network interface card (NIC) processing. In this case, the networking unit receives a packet from the network, selects a cluster buffer memory to store the packet, and when the packet is delivered, generates a work unit (WU) to a virtual processor of a core (typically within the same cluster) that is responsible for processing the packet. The virtual processor performs processing (in the NIC case, looking up the queue where the packet is to be delivered, and determining the host buffer in which to place the packet) and generates a work unit to the appropriate host unit scatter-gather DMA engine. The latter reads (gathers) the packet contents and delivers (scatters) the packet contents as instructed in the work unit.</div>
<div class="description-paragraph" id="p-0188" num="0187">The second example flow corresponds to the case where the virtual processor does not have a host buffer available for delivering the packet, or is required to gather more payload before delivery (e.g., for a framed protocol over TCP, such as HTTP). In this case, the payload is moved to external memory until delivery is possible. At that time, the same process is followed as above with respect to the first example flow, to move the packet to the host. That is, the virtual processor performs processing (in the NIC case, looking up the queue where the packet is to be delivered, and determining the host buffer in which to place the packet) and generates a work unit to the appropriate host unit scatter-gather DMA engine. The latter reads (gathers) the packet contents and delivers (scatters) the packet contents as instructed in the work unit.</div>
<div class="description-paragraph" id="p-0189" num="0188">The third example flow corresponds to cases that require an additional processing step following storage in external memory, such as protocols that require session level processing, e.g., SSL where the payload of an assembled record is to be decrypted. In this case, the payload is moved to a cluster buffer memory where an accelerator (e.g., a cryptography unit) engine performs the work needed, before the host unit DMA engine is instructed to deliver the result to host memory.</div>
<div class="description-paragraph" id="p-0190" num="0189">The fourth example flow shows a fabric use case where packets have been reordered in the network, and the networking unit reorders the packets after the networking unit receives the packets and determines that the packets are out of order. In this case, received packets are stored in buffer memory or external memory until they can be passed to the virtual processors in the correct order. <figref idrefs="DRAWINGS">FIG. 14</figref> shows the external memory being used by the networking unit as a reorder buffer, and packets moved to buffer memory for processing.</div>
<div class="description-paragraph" id="p-0191" num="0190">The fifth example flow depicts an access node being used to switch traffic between networking unit ports, using buffer memory as a buffer. This path is not expected in normal operation (because the networking unit is expected to switch internally), but this case may be useful in some situations to provide additional buffering.</div>
<div class="description-paragraph" id="p-0192" num="0191"> <figref idrefs="DRAWINGS">FIG. 15</figref> is a series of flow diagrams illustrating various example processing flows from a host unit to a networking unit, such as one of host units <b>154</b> to networking unit <b>152</b> of <figref idrefs="DRAWINGS">FIG. 9</figref>. <figref idrefs="DRAWINGS">FIG. 15</figref> illustrates that such flows may include passage through various components of an access node, such as a buffer memory and/or an external memory.</div>
<div class="description-paragraph" id="p-0193" num="0192">The first example scenario of <figref idrefs="DRAWINGS">FIG. 15</figref> illustrates the converse of a simple NU to HU transit scenario (e.g., the first example flow, NIC transmit case, of <figref idrefs="DRAWINGS">FIG. 14</figref>). In this scenario, a packet is stored by the host unit within a cluster buffer memory, before processing and transmission on an NU interface. Like the host unit, the networking unit features a DMA engine that can gather data from the various access node memory locations. However, unlike the networking unit, which receives packets from the network unprompted, the host unit injects packets into the access node as a result of a request (pull) work unit from a virtual processor.</div>
<div class="description-paragraph" id="p-0194" num="0193">The second example scenario is likewise the opposite of the second example flow of <figref idrefs="DRAWINGS">FIG. 14</figref>. The second example scenario of <figref idrefs="DRAWINGS">FIG. 15</figref> corresponds to a flow where additional processing is needed before transmission. One example would be using external memory as an SSD cache, where data is compressed and stored in external memory and, thus, requires decompression by the cluster level accelerator before transmission. Another example would be an application where data is processed in buffer memory first with results stored in external memory. Then a final processing step collates results into a buffer memory for transmission.</div>
<div class="description-paragraph" id="p-0195" num="0194">The third example scenario of <figref idrefs="DRAWINGS">FIG. 15</figref> is more common and corresponds to reading data off of storage devices, such as SSD devices. In this case, the host unit first delivers the data to external memory, and a virtual processor is notified of the transfer completion. The virtual processor then moves the data into buffer memory for processing by a transport virtual processor (e.g., SSL encryption), followed by transmit TCP processing, which stores a copy of the data in external memory, in case retransmission is later needed.</div>
<div class="description-paragraph" id="p-0196" num="0195">The fourth example scenario of <figref idrefs="DRAWINGS">FIG. 15</figref> extends the third example scenario with a second pass through on-chip buffer memory before transmission. A possible use case leading to this flow would be an application that reads data from a storage device into external memory, moves the data to buffer memory for processing, and stores the results into external memory. Finally, the results from multiple processors are collated into buffer memory and transmitted. If transmission uses TCP, then a copy of the transmitted data would be stored in external memory as in the third example scenario of <figref idrefs="DRAWINGS">FIG. 15</figref>.</div>
<div class="description-paragraph" id="p-0197" num="0196"> <figref idrefs="DRAWINGS">FIG. 16</figref> is a flowchart illustrating an example operation of an access node performing data processing, in accordance with the techniques described herein. The example operation of <figref idrefs="DRAWINGS">FIG. 16</figref> is described herein with respect to access node <b>150</b> of <figref idrefs="DRAWINGS">FIG. 9</figref>, processing cluster <b>180</b> of <figref idrefs="DRAWINGS">FIG. 10A</figref>, and programmable processing core <b>190</b> of <figref idrefs="DRAWINGS">FIG. 10B</figref>.</div>
<div class="description-paragraph" id="p-0198" num="0197">In the example of <figref idrefs="DRAWINGS">FIG. 16</figref>, access node <b>150</b> receives data to be processed via one of networking unit <b>152</b> or one of host units <b>154</b> of access node <b>150</b> (<b>500</b>). Networking unit <b>152</b> is configured to control input and output of the data between a network and access node <b>150</b>. Each of host unit <b>154</b> is configured to control input and output of the data between one or more application processors (e.g., local processors of a computing device that includes access node <b>150</b> or processors of server devices), and control storage of the data with storage devices.</div>
<div class="description-paragraph" id="p-0199" num="0198">A processing cluster <b>180</b> of a plurality of processing clusters <b>156</b> included in access node <b>150</b> then receives a work unit indicating a processing task to be performed on the data from one of networking unit <b>152</b>, host units <b>154</b>, or another one of the processing clusters <b>156</b> via a control network fabric (e.g., the signaling network formed by direct links <b>162</b>) of access node <b>150</b> (<b>502</b>). Upon receiving the work unit, processing cluster <b>180</b> determines the programmable processing core <b>190</b> to perform the processing task, and sends the work unit to a queue associated with a virtual processor (e.g., virtual processor <b>192</b>A) of the plurality of virtual processors <b>19</b> <sub>2 </sub>included in programmable processing core <b>190</b>.</div>
<div class="description-paragraph" id="p-0200" num="0199">A programmable processing core <b>190</b> of two or more programmable processing cores <b>182</b> included in processing cluster <b>180</b> processes the work unit, including retrieving the data on which the processing task is to be performed from one of networking unit <b>152</b>, host units <b>154</b>, or one of processing clusters <b>156</b> via a data network fabric (e.g., the data network formed by grid links <b>160</b>) of access node <b>150</b> (<b>504</b>). Processing cluster <b>180</b> includes coherent cache memory <b>184</b> and non-coherent buffer memory <b>186</b>, and, as part of retrieving the data on which the processing task is to be performed, stores stream data in non-coherent buffer memory <b>186</b> and stores other data in the coherent cache memory <b>184</b>. In addition, programmable processing core <b>190</b> includes L1 data cache <b>196</b> for caching coherent data and L1 buffer cache <b>198</b> for caching non-coherent data, and, as part of retrieving the data on which the processing task is to be performed, caches the stream data in L1 buffer cache <b>198</b> and caches the other data in L1 data cache <b>196</b>.</div>
<div class="description-paragraph" id="p-0201" num="0200">To process the work unit, virtual processor <b>192</b>A receives the work unit from the associated queue indicating the processing task to be performed on the data, and fetches the data from one of L1 data cache <b>196</b> or L1 buffer cache <b>198</b> of programmable processing core <b>190</b>. Virtual processor <b>192</b>A then performs the indicated processing task on the data, and outputs the results of the processing task including one or more work unit messages back to programmable processing core <b>190</b>. The processing cluster <b>180</b> then receives the output results of the processing task performed on the data from programmable processing core <b>190</b> (<b>506</b>).</div>
<div class="description-paragraph" id="p-0202" num="0201">For processes, apparatuses, and other examples or illustrations described herein, including in any flowcharts or flow diagrams, certain operations, acts, steps, or events included in any of the techniques described herein can be performed in a different sequence, may be added, merged, or left out altogether (e.g., not all described acts or events are necessary for the practice of the techniques). Moreover, in certain examples, operations, acts, steps, or events may be performed concurrently, e.g., through multi-threaded processing, interrupt processing, or multiple processors, rather than sequentially. Further certain operations, acts, steps, or events may be performed automatically even if not specifically identified as being performed automatically. Also, certain operations, acts, steps, or events described as being performed automatically may be alternatively not performed automatically, but rather, such operations, acts, steps, or events may be, in some examples, performed in response to input or another event.</div>
<div class="description-paragraph" id="p-0203" num="0202">The detailed description set forth above is intended as a description of various configurations and is not intended to represent the only configurations in which the concepts described herein may be practiced. The detailed description includes specific details for the purpose of providing a thorough understanding of the various concepts. However, these concepts may be practiced without these specific details. In some instances, well-known structures and components are shown in block diagram form in the referenced figures in order to avoid obscuring such concepts.</div>
<div class="description-paragraph" id="p-0204" num="0203">In one or more examples, the functions described may be implemented in hardware, software, firmware, or any combination thereof. If implemented in software, the functions may be stored, as one or more instructions or code, on and/or transmitted over a computer-readable medium and executed by a hardware-based processing unit. Computer-readable media may include computer-readable storage media, which corresponds to a tangible medium such as data storage media, or communication media including any medium that facilitates transfer of a computer program from one place to another (e.g., pursuant to a communication protocol). In this manner, computer-readable media generally may correspond to (1) tangible computer-readable storage media, which is non-transitory or (2) a communication medium such as a signal or carrier wave. Data storage media may be any available media that can be accessed by one or more computers or one or more processors to retrieve instructions, code and/or data structures for implementation of the techniques described in this disclosure. A computer program product may include a computer-readable medium.</div>
<div class="description-paragraph" id="p-0205" num="0204">By way of example, and not limitation, such computer-readable storage media can include RAM, ROM, EEPROM, CD-ROM or other optical disk storage, magnetic disk storage, or other magnetic storage devices, flash memory, or any other medium that can be used to store desired program code in the form of instructions or data structures and that can be accessed by a computer. Also, any connection is properly termed a computer-readable medium. For example, if instructions are transmitted from a website, server, or other remote source using a coaxial cable, fiber optic cable, twisted pair, digital subscriber line (DSL), or wireless technologies such as infrared, radio, and microwave, then the coaxial cable, fiber optic cable, twisted pair, DSL, or wireless technologies such as infrared, radio, and microwave are included in the definition of medium. It should be understood, however, that computer-readable storage media and data storage media do not include connections, carrier waves, signals, or other transient media, but are instead directed to non-transient, tangible storage media. Disk and disc, as used, includes compact disc (CD), laser disc, optical disc, digital versatile disc (DVD), floppy disk and Blu-ray disc, where disks usually reproduce data magnetically, while discs reproduce data optically with lasers. Combinations of the above should also be included within the scope of computer-readable media.</div>
<div class="description-paragraph" id="p-0206" num="0205">Instructions may be executed by one or more processors, such as one or more digital signal processors (DSPs), general purpose microprocessors, application specific integrated circuits (ASICs), field programmable logic arrays (FPGAs), or other equivalent integrated or discrete logic circuitry. Accordingly, the terms “processor” or “processing circuitry” as used herein may each refer to any of the foregoing structure or any other structure suitable for implementation of the techniques described. In addition, in some examples, the functionality described may be provided within dedicated hardware and/or software modules. Also, the techniques could be fully implemented in one or more circuits or logic elements.</div>
<div class="description-paragraph" id="p-0207" num="0206">The techniques of this disclosure may be implemented in a wide variety of devices or apparatuses, including a wireless handset, a mobile or non-mobile computing device, a wearable or non-wearable computing device, an integrated circuit (IC) or a set of ICs (e.g., a chip set). Various components, modules, or units are described in this disclosure to emphasize functional aspects of devices configured to perform the disclosed techniques, but do not necessarily require realization by different hardware units. Rather, as described above, various units may be combined in a hardware unit or provided by a collection of interoperating hardware units, including one or more processors as described above, in conjunction with suitable software and/or firmware.</div>
<div class="description-paragraph" id="p-0208" num="0207">Various examples have been described. These and other examples are within the scope of the following claims.</div>
</div>
</div>
</section><section itemprop="claims" itemscope="">
<h2>Claims (<span itemprop="count">32</span>)</h2>
<div html="" itemprop="content"><div class="claims" lang="EN" load-source="patent-office" mxw-id="PCLM259030838">
<claim-statement>What is claimed is:</claim-statement>
<div class="claim"> <div class="claim" id="CLM-00001" num="00001">
<div class="claim-text">1. An access node integrated circuit comprising:
<div class="claim-text">a networking unit configured to control input and output of data between a network and the access node integrated circuit;</div>
<div class="claim-text">one or more host units configured to at least one of control input and output of the data between the access node integrated circuit and one or more application processors or control storage of the data with one or more storage devices;</div>
<div class="claim-text">a plurality of processing clusters, each of the processing clusters including two or more programmable processing cores configured to perform processing tasks on the data;</div>
<div class="claim-text">a data network fabric interconnecting the plurality of processing clusters, the one or more host units, and the networking unit, wherein the data network fabric is configured to carry the data between the networking unit, the one or more host units, and the plurality of processing clusters; and</div>
<div class="claim-text">at least one control network fabric interconnecting the plurality of processing clusters, the one or more host units, and the networking unit, wherein the at least one control network fabric is configured to carry control messages identifying the processing tasks to be performed on the data by the programmable processing cores of the plurality of processing clusters.</div>
</div>
</div>
</div> <div class="claim-dependent"> <div class="claim" id="CLM-00002" num="00002">
<div class="claim-text">2. The access node integrated circuit of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein each of the processing clusters comprises:
<div class="claim-text">a coherent cache memory implemented in circuitry; and</div>
<div class="claim-text">a non-coherent buffer memory implemented in circuitry,</div>
<div class="claim-text">wherein each of the programmable processing cores of the respective processing cluster are connected to the coherent cache memory and the non-coherent buffer memory.</div>
</div>
</div>
</div> <div class="claim-dependent"> <div class="claim" id="CLM-00003" num="00003">
<div class="claim-text">3. The access node integrated circuit of <claim-ref idref="CLM-00002">claim 2</claim-ref>, wherein each of the programmable processing cores is configured to store stream data in the non-coherent buffer memory and store other data in the coherent cache memory, wherein the stream data comprises packets of network transmission data.</div>
</div>
</div> <div class="claim-dependent"> <div class="claim" id="CLM-00004" num="00004">
<div class="claim-text">4. The access node integrated circuit of <claim-ref idref="CLM-00002">claim 2</claim-ref>, wherein each of the programmable processing cores of each of the processing clusters comprises:
<div class="claim-text">a plurality of virtual processors;</div>
<div class="claim-text">a level one (L1) data cache memory for caching coherent data; and</div>
<div class="claim-text">a L1 buffer cache memory for caching non-coherent data.</div>
</div>
</div>
</div> <div class="claim-dependent"> <div class="claim" id="CLM-00005" num="00005">
<div class="claim-text">5. The access node integrated circuit of <claim-ref idref="CLM-00004">claim 4</claim-ref>, wherein each of the programmable processing cores is configured to cache the stream data in the L1 buffer cache memory and cache other data in the L1 data cache memory, wherein the stream data comprises packets of network transmission data.</div>
</div>
</div> <div class="claim-dependent"> <div class="claim" id="CLM-00006" num="00006">
<div class="claim-text">6. The access node integrated circuit of <claim-ref idref="CLM-00004">claim 4</claim-ref>, wherein, to perform processing tasks on the data, at least one processing cluster of the plurality of processing clusters is configured to:
<div class="claim-text">receive a work unit indicating a processing task to be performed on the data;</div>
<div class="claim-text">determine one of the programmable processing cores to perform the processing task;</div>
<div class="claim-text">send the work unit to a queue associated with a virtual processor of the plurality of virtual processors of the one of the programmable processing cores; and</div>
<div class="claim-text">receive results of the processing task from the one of the programmable processing cores.</div>
</div>
</div>
</div> <div class="claim-dependent"> <div class="claim" id="CLM-00007" num="00007">
<div class="claim-text">7. The access node integrated circuit of <claim-ref idref="CLM-00006">claim 6</claim-ref>, wherein, to perform the processing task on the data, the virtual processor of the plurality of virtual processors is configured to:
<div class="claim-text">receive the work unit from the associated queue indicating the processing task to be performed on the data;</div>
<div class="claim-text">fetch the data from one of the L1 data cache memory or the L1 buffer cache memory of the one of the programmable processing cores;</div>
<div class="claim-text">perform the indicated processing task on the data; and</div>
<div class="claim-text">output the results of the processing task including one or more work unit messages.</div>
</div>
</div>
</div> <div class="claim-dependent"> <div class="claim" id="CLM-00008" num="00008">
<div class="claim-text">8. The access node integrated circuit of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein each of the processing clusters further comprises one or more accelerator units implemented in circuitry, and wherein the one or more accelerator units comprise hardware implementations of one or more of a lookup engine, a matrix multiplier, a cryptographic engine, a compression engine, or a regular expression interpreter.</div>
</div>
</div> <div class="claim-dependent"> <div class="claim" id="CLM-00009" num="00009">
<div class="claim-text">9. The access node integrated circuit of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein each of the programmable processing cores of each of the processing clusters comprises one of a MIPS (microprocessor without interlocked pipeline stages) core, an ARM (advanced RISC (reduced instruction set computing) machine) core, a PowerPC (performance optimization with enhanced RISC—performance computing) core, a RISC-V (RISC five) core, or a CISC (complex instruction set computing or x86) core.</div>
</div>
</div> <div class="claim-dependent"> <div class="claim" id="CLM-00010" num="00010">
<div class="claim-text">10. The access node integrated circuit of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein each of the programmable processing cores is programmable using a high-level programming language.</div>
</div>
</div> <div class="claim-dependent"> <div class="claim" id="CLM-00011" num="00011">
<div class="claim-text">11. The access node integrated circuit of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the plurality of processing clusters includes a central cluster implemented in circuitry.</div>
</div>
</div> <div class="claim-dependent"> <div class="claim" id="CLM-00012" num="00012">
<div class="claim-text">12. The access node integrated circuit of <claim-ref idref="CLM-00011">claim 11</claim-ref>, wherein the central cluster comprises:
<div class="claim-text">a central dispatch unit configured to perform flow control, select one of the processing clusters to perform work units, and dispatch work units to the selected one of the processing clusters;</div>
<div class="claim-text">a coherence directory unit configured to determine locations of data within coherent cache memory of the access node integrated circuit; and</div>
<div class="claim-text">a central synchronization unit configured to maintain proper sequencing and ordering of operations within the access node integrated circuit.</div>
</div>
</div>
</div> <div class="claim-dependent"> <div class="claim" id="CLM-00013" num="00013">
<div class="claim-text">13. The access node integrated circuit of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the networking unit is configured to support Ethernet interfaces to connect directly to the network without a separate network interface card (NIC).</div>
</div>
</div> <div class="claim-dependent"> <div class="claim" id="CLM-00014" num="00014">
<div class="claim-text">14. The access node integrated circuit of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the networking unit is configured as a network switch to send and receive network data for the access node integrated circuit.</div>
</div>
</div> <div class="claim-dependent"> <div class="claim" id="CLM-00015" num="00015">
<div class="claim-text">15. The access node integrated circuit of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the networking unit comprises a network packet parsing unit configured to parse network packets.</div>
</div>
</div> <div class="claim-dependent"> <div class="claim" id="CLM-00016" num="00016">
<div class="claim-text">16. The access node integrated circuit of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the one or more host units are each configured to support PCI-e (Peripheral Component Interconnect-Express) interfaces to connect directly to the application processors and the storage devices.</div>
</div>
</div> <div class="claim-dependent"> <div class="claim" id="CLM-00017" num="00017">
<div class="claim-text">17. The access node integrated circuit of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the data network fabric is formed by grid links between the plurality of processing clusters, the networking unit, the one or more host units, and an external memory.</div>
</div>
</div> <div class="claim-dependent"> <div class="claim" id="CLM-00018" num="00018">
<div class="claim-text">18. The access node integrated circuit of <claim-ref idref="CLM-00017">claim 17</claim-ref>, wherein the data network fabric comprises a two-dimensional mesh topology, and wherein the data network fabric is configured to carry coherent memory data and non-coherent memory data.</div>
</div>
</div> <div class="claim-dependent"> <div class="claim" id="CLM-00019" num="00019">
<div class="claim-text">19. The access node integrated circuit of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the at least one control network fabric includes a signaling network formed by a set of direct links between a central cluster of the plurality of processing clusters and each of the other processing clusters, the networking unit, the one or more host units, and an external memory, wherein the signaling network transports control messages related to non-coherent streaming data.</div>
</div>
</div> <div class="claim-dependent"> <div class="claim" id="CLM-00020" num="00020">
<div class="claim-text">20. The access node integrated circuit of <claim-ref idref="CLM-00019">claim 19</claim-ref>, wherein the signaling network comprises a non-blocking, switched, low latency fabric, and wherein the signaling network is configured to carry flow control messages, resource status messages, work unit delivery messages, and work scheduling messages to the processing clusters.</div>
</div>
</div> <div class="claim-dependent"> <div class="claim" id="CLM-00021" num="00021">
<div class="claim-text">21. The access node integrated circuit of <claim-ref idref="CLM-00019">claim 19</claim-ref>, wherein the signaling network is formed by a first set of direct links, and wherein the at least one control network fabric further includes a coherency network formed by a second, different set of direct links between the central cluster of the plurality of processing clusters and each of the other processing clusters and the external memory, wherein the coherency network transports control messages related to coherent data.</div>
</div>
</div> <div class="claim-dependent"> <div class="claim" id="CLM-00022" num="00022">
<div class="claim-text">22. The access node integrated circuit of <claim-ref idref="CLM-00019">claim 19</claim-ref>, wherein the signaling network is formed by a first set of direct links, and wherein the at least one control network fabric further includes a broadcast network formed by a second, different set of direct links between the central cluster of the plurality of processing clusters and each of the other processing clusters, the networking unit, and the one or more host units, wherein the broadcast network is configured to transport utilization statuses for resources of components of the access node integrated circuit.</div>
</div>
</div> <div class="claim-dependent"> <div class="claim" id="CLM-00023" num="00023">
<div class="claim-text">23. The access node integrated circuit of <claim-ref idref="CLM-00022">claim 22</claim-ref>, wherein at least one of the processing clusters includes an event queue manager (EQM) unit configured to store copies of utilization statuses for resources used by each of the processing clusters, and receive utilization status values from the other processing clusters via the broadcast network.</div>
</div>
</div> <div class="claim"> <div class="claim" id="CLM-00024" num="00024">
<div class="claim-text">24. A system comprising:
<div class="claim-text">a plurality of server devices;</div>
<div class="claim-text">a plurality of storage devices;</div>
<div class="claim-text">a network; and</div>
<div class="claim-text">a computing device including an access node integrated circuit comprising:</div>
<div class="claim-text">a networking unit configured to control input and output of data between the network and</div>
<div class="claim-text">the access node integrated circuit;</div>
<div class="claim-text">one or more host units configured to at least one of control input and output of the data between the access node integrated circuit and the server devices or control storage of the data with the storage devices;</div>
<div class="claim-text">a plurality of processing clusters, each of the processing clusters including two or more programmable processing cores configured to perform processing tasks on the data;</div>
<div class="claim-text">a data network fabric interconnecting the plurality of processing clusters, the one or more host units, and the networking unit, wherein the data network fabric is configured to carry the data between the networking unit, the one or more host units, and the plurality of processing clusters; and</div>
<div class="claim-text">at least one control network fabric interconnecting the plurality of processing clusters, the one or more host units, and the networking unit, wherein the at least one control network fabric is configured to carry control messages identifying the processing tasks to be performed on the data by the programmable processing cores of the plurality of processing clusters.</div>
</div>
</div>
</div> <div class="claim-dependent"> <div class="claim" id="CLM-00025" num="00025">
<div class="claim-text">25. The system of <claim-ref idref="CLM-00024">claim 24</claim-ref>, wherein the networking unit of the access node integrated circuit is configured to support Ethernet interfaces to connect directly to the network without a separate network interface card (NIC).</div>
</div>
</div> <div class="claim-dependent"> <div class="claim" id="CLM-00026" num="00026">
<div class="claim-text">26. The system of <claim-ref idref="CLM-00024">claim 24</claim-ref>, wherein the one or more host units of the access node integrated circuit are each configured to support PCI-e (Peripheral Component Interconnect-Express) interfaces to connect directly to the server devices and the storage devices.</div>
</div>
</div> <div class="claim-dependent"> <div class="claim" id="CLM-00027" num="00027">
<div class="claim-text">27. The system of <claim-ref idref="CLM-00024">claim 24</claim-ref>, wherein the access node integrated circuit acts as an input/output hub between the server devices, the storage devices, and the network.</div>
</div>
</div> <div class="claim"> <div class="claim" id="CLM-00028" num="00028">
<div class="claim-text">28. A method comprising:
<div class="claim-text">receiving, by an access node integrated circuit of a computing device, data to be processed, wherein the access node integrated circuit includes a networking unit configured to control input and output of the data with a network and at least one host unit configured to at least one of control input and output of the data with one or more application processors or control storage of the data with one or more storage devices;</div>
<div class="claim-text">receiving, by a processing cluster of a plurality of processing clusters included in the access node integrated circuit and from one of the networking unit, the host unit, or another one of the processing clusters via a control network fabric of the access node integrated circuit, a work unit indicating a processing task to be performed on the data;</div>
<div class="claim-text">processing, by a programmable processing core of two or more programmable processing cores included in the processing cluster, the work unit, wherein processing the work unit includes retrieving the data on which the processing task is to be performed from one of the networking unit, the host unit, or one of the processing clusters via a data network fabric of the access node integrated circuit; and</div>
<div class="claim-text">receiving, by the processing cluster and from the programmable processing core, results of the processing task performed on the data.</div>
</div>
</div>
</div> <div class="claim-dependent"> <div class="claim" id="CLM-00029" num="00029">
<div class="claim-text">29. The method of <claim-ref idref="CLM-00028">claim 28</claim-ref>, wherein the processing cluster comprises a coherent cache memory and a non-coherent buffer memory, and wherein retrieving the data on which the processing task is to be performed includes storing stream data in the non-coherent buffer memory and storing other data in the coherent cache memory, wherein the stream data comprises packets of network transmission data.</div>
</div>
</div> <div class="claim-dependent"> <div class="claim" id="CLM-00030" num="00030">
<div class="claim-text">30. The method of <claim-ref idref="CLM-00029">claim 29</claim-ref>, wherein the programmable processing core comprises a plurality of virtual processors, a level one (L1) data cache memory for caching coherent data, and a L1 buffer cache memory for caching non-coherent data, and wherein retrieving the data on which the processing task is to be performed includes caching the stream data in the L1 buffer cache memory and caching the other data in the L1 data cache memory.</div>
</div>
</div> <div class="claim-dependent"> <div class="claim" id="CLM-00031" num="00031">
<div class="claim-text">31. The method of <claim-ref idref="CLM-00029">claim 29</claim-ref>, further comprising, upon receiving the work unit:
<div class="claim-text">determining, by the processing cluster, the programmable processing core to perform the processing task; and</div>
<div class="claim-text">sending, by the processing cluster, the work unit to a queue associated with a virtual processor of the plurality of virtual processors included in the programmable processing core.</div>
</div>
</div>
</div> <div class="claim-dependent"> <div class="claim" id="CLM-00032" num="00032">
<div class="claim-text">32. The method of <claim-ref idref="CLM-00031">claim 31</claim-ref>, further comprising:
<div class="claim-text">receiving, by the virtual processor, the work unit from the associated queue indicating the processing task to be performed on the data;</div>
<div class="claim-text">fetching, by the virtual processor, the data from one of the L1 data cache memory or the L1 buffer cache memory of the programmable processing core;</div>
<div class="claim-text">performing, by the virtual processor, the indicated processing task on the data; and</div>
<div class="claim-text">outputting, by the virtual processor and to the programmable processing core, the results of the processing task including one or more work unit messages.</div>
</div>
</div>
</div> </div>
</div>
</section>
                </article>
            </search-app>
        </body>
    </html>
    