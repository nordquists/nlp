
    <html>
        <body>
            <search-app>
                <article class="result" itemscope="" itemtype="http://schema.org/ScholarlyArticle">
    <h1 itemprop="pageTitle">US10904505B2 - Systems and methods for generating a digital image 
        - Google Patents</h1><section itemprop="abstract" itemscope="">
<h2>Abstract</h2>
<div html="" itemprop="content"><abstract lang="EN" load-source="patent-office" mxw-id="PA431058412">
<div class="abstract" id="p-0001" num="0000">A system, method, and computer program product for generating a digital image is disclosed. In use, a first image is received from a first image sensor, where the first image sensor detects visible light color, and a second image and a third image are received from a second image sensor, where the second image sensor detects non-visible light intensity. Using an image processing subsystem, a resulting image is generated by combining the first image, the second image, and the third image, where at least one of the first image, the second image, or the third image is sampled under strobe illumination.</div>
</abstract>
</div>
</section><section itemprop="description" itemscope="">
<h2>Description</h2>
<div html="" itemprop="content"><div class="description" lang="EN" load-source="patent-office" mxw-id="PDES284583655">
<heading id="h-0001">RELATED APPLICATIONS</heading>
<div class="description-paragraph" id="p-0002" num="0001">The present application is a continuation of, and claims priority to, U.S. patent application Ser. No. 16/154,999, titled “SYSTEMS AND METHODS FOR GENERATING A DIGITAL IMAGE USING SEPARATE COLOR AND INTENSITY DATA,” filed Oct. 9, 2018, which in turn is a continuation of, and claims priority to, U.S. patent application Ser. No. 15/885,296, titled “SYSTEMS AND METHODS FOR GENERATING A DIGITAL IMAGE,” filed Jan. 31, 2018, now U.S. Pat. No. 10,129,514, which in turn is a continuation of, and claims priority to, U.S. patent application Ser. No. 15/352,510, titled “SYSTEMS AND METHODS FOR GENERATING A DIGITAL IMAGE,” filed Nov. 15, 2016, now U.S. Pat. No. 9,912,928, which in turn claims priority to U.S. patent application Ser. No. 14/702,549, now U.S. Pat. No. 9,531,691, titled “SYSTEMS AND METHODS FOR GENERATING A DIGITAL IMAGE USING SEPARATE COLOR AND INTENSITY DATA,” filed May 1, 2015. The foregoing applications and/or patents are herein incorporated by reference in their entirety for all purposes.</div>
<div class="description-paragraph" id="p-0003" num="0002">Additionally, this application is related to the following U.S. Patent Application, the entire disclosures being incorporated by reference herein: application Ser. No. 13/573,252, now U.S. Pat. No. 8,976,264, titled “COLOR BALANCE IN DIGITAL PHOTOGRAPHY,” filed Sep. 4, 2012; U.S. patent application Ser. No. 14/534,068, now U.S. Pat. No. 9,167,174, titled “SYSTEMS AND METHODS FOR HIGH-DYNAMIC RANGE IMAGES,” filed Nov. 5, 2014; U.S. patent application Ser. No. 14/534,079, now U.S. Pat. No. 9,137,455, titled “IMAGE SENSOR APPARATUS AND METHOD FOR OBTAINING MULTIPLE EXPOSURES WITH ZERO INTERFRAME TIME,” filed Nov. 5, 2014; U.S. patent application Ser. No. 14/534,089, now U.S. Pat. No. 9,167,169, titled “IMAGE SENSOR APPARATUS AND METHOD FOR SIMULTANEOUSLY CAPTURING MULTIPLE IMAGES,” filed Nov. 5, 2014; U.S. patent application Ser. No. 14/535,274, now U.S. Pat. No. 9,154,708, titled “IMAGE SENSOR APPARATUS AND METHOD FOR SIMULTANEOUSLY CAPTURING FLASH AND AMBIENT ILLUMINATED IMAGES,” filed Nov. 6, 2014; and U.S. patent application Ser. No. 14/535,279, now U.S. Pat. No. 9,179,085, titled “IMAGE SENSOR APPARATUS AND METHOD FOR OBTAINING LOW-NOISE, HIGH-SPEED CAPTURES OF A PHOTOGRAPHIC SCENE,” filed Nov. 6, 2014.</div>
<heading id="h-0002">FIELD OF THE INVENTION</heading>
<div class="description-paragraph" id="p-0004" num="0003">The present invention relates generally to digital photographic systems, and more specifically to generating a digital image from separate color and intensity data.</div>
<heading id="h-0003">BACKGROUND</heading>
<div class="description-paragraph" id="p-0005" num="0004">The human eye reacts to light in different ways based on the response of rods and cones in the retina. Specifically, the perception of the response of the eye is different for different colors (e.g., red, green, and blue) in the visible spectrum as well as between luminance and chrominance. Conventional techniques for capturing digital images rely on a CMOS image sensor or CCD image sensor positioned under a color filter array such as a Bayer color filter. Each photodiode of the image sensor samples an analog value that represents an amount of light associated with a particular color at that pixel location. The information for three or more different color channels may then be combined (or filtered) to generate a digital image.</div>
<div class="description-paragraph" id="p-0006" num="0005">The resulting images generated by these techniques have a reduced spatial resolution due to the blending of values generated at different discrete locations of the image sensor into a single pixel value in the resulting image. Fine details in the scene could be represented poorly due to this filtering of the raw data.</div>
<div class="description-paragraph" id="p-0007" num="0006">Furthermore, based on human physiology, it is known that human vision is more sensitive to luminance information than chrominance information. In other words, the human eye can recognize smaller details due to changes in luminance when compared to changes in chrominance. However, conventional image capturing techniques do not typically exploit the differences in perception between chrominance and luminance information. Thus, there is a need to address these issues and/or other issues associated with the prior art.</div>
<heading id="h-0004">SUMMARY</heading>
<div class="description-paragraph" id="p-0008" num="0007">A system, method, and computer program product for generating a digital image is disclosed. In use, a first image is received from a first image sensor, where the first image sensor detects visible light color, and a second image and a third image are received from a second image sensor, where the second image sensor detects non-visible light intensity. Using an image processing subsystem, a resulting image is generated by combining the first image, the second image, and the third image, where at least one of the first image, the second image, or the third image is sampled under strobe illumination.</div>
<description-of-drawings>
<heading id="h-0005">BRIEF DESCRIPTION OF THE DRAWINGS</heading>
<div class="description-paragraph" id="p-0009" num="0008"> <figref idrefs="DRAWINGS">FIG. 1</figref> illustrates a flow chart of a method <b>100</b> for generating a digital image, in accordance with one embodiment;</div>
<div class="description-paragraph" id="p-0010" num="0009"> <figref idrefs="DRAWINGS">FIG. 2</figref> illustrates an image processing subsystem configured to implement the method <b>100</b> of <figref idrefs="DRAWINGS">FIG. 1</figref>, in accordance with one embodiment;</div>
<div class="description-paragraph" id="p-0011" num="0010"> <figref idrefs="DRAWINGS">FIG. 3A</figref> illustrates a digital photographic system, configured to implement one or more aspects of the present invention;</div>
<div class="description-paragraph" id="p-0012" num="0011"> <figref idrefs="DRAWINGS">FIG. 3B</figref> illustrates a processor complex within digital photographic system of <figref idrefs="DRAWINGS">FIG. 3A</figref>, according to one embodiment of the present invention;</div>
<div class="description-paragraph" id="p-0013" num="0012"> <figref idrefs="DRAWINGS">FIG. 3C</figref> illustrates a digital camera, in accordance with one embodiment;</div>
<div class="description-paragraph" id="p-0014" num="0013"> <figref idrefs="DRAWINGS">FIG. 3D</figref> illustrates a wireless mobile device, in accordance with one embodiment;</div>
<div class="description-paragraph" id="p-0015" num="0014"> <figref idrefs="DRAWINGS">FIG. 3E</figref> illustrates camera module, in accordance with one embodiment;</div>
<div class="description-paragraph" id="p-0016" num="0015"> <figref idrefs="DRAWINGS">FIG. 3F</figref> illustrates a camera module, in accordance with one embodiment;</div>
<div class="description-paragraph" id="p-0017" num="0016"> <figref idrefs="DRAWINGS">FIG. 3G</figref> illustrates camera module, in accordance with one embodiment;</div>
<div class="description-paragraph" id="p-0018" num="0017"> <figref idrefs="DRAWINGS">FIG. 4</figref> illustrates a network service system, in accordance with one embodiment;</div>
<div class="description-paragraph" id="p-0019" num="0018"> <figref idrefs="DRAWINGS">FIG. 5A</figref> illustrates a system for capturing optical scene information for conversion to an electronic representation of a photographic scene, in accordance with one embodiment;</div>
<div class="description-paragraph" id="p-0020" num="0019"> <figref idrefs="DRAWINGS">FIGS. 5B-5D</figref> illustrate three optional pixel configurations, according to one or more embodiments;</div>
<div class="description-paragraph" id="p-0021" num="0020"> <figref idrefs="DRAWINGS">FIG. 5E</figref> illustrates a system is shown for capturing optical scene information focused as an optical image on an image sensor, in accordance with one embodiment;</div>
<div class="description-paragraph" id="p-0022" num="0021"> <figref idrefs="DRAWINGS">FIG. 6A</figref> illustrates a circuit diagram for a photosensitive cell, in accordance with one possible embodiment;</div>
<div class="description-paragraph" id="p-0023" num="0022"> <figref idrefs="DRAWINGS">FIG. 6B</figref> illustrates a circuit diagram for a photosensitive cell, in accordance with another possible embodiment;</div>
<div class="description-paragraph" id="p-0024" num="0023"> <figref idrefs="DRAWINGS">FIG. 6C</figref> illustrates a system for converting analog pixel data to digital pixel data, in accordance with an embodiment;</div>
<div class="description-paragraph" id="p-0025" num="0024"> <figref idrefs="DRAWINGS">FIG. 7A</figref> illustrates a configuration of the camera module, in accordance with one embodiment;</div>
<div class="description-paragraph" id="p-0026" num="0025"> <figref idrefs="DRAWINGS">FIG. 7B</figref> illustrates a configuration of the camera module, in accordance with another embodiment;</div>
<div class="description-paragraph" id="p-0027" num="0026"> <figref idrefs="DRAWINGS">FIG. 7C</figref> illustrates a configuration of the camera module, in accordance with yet another embodiment;</div>
<div class="description-paragraph" id="p-0028" num="0027"> <figref idrefs="DRAWINGS">FIG. 8</figref> illustrates a flow chart of a method for generating a digital image, in accordance with one embodiment;</div>
<div class="description-paragraph" id="p-0029" num="0028"> <figref idrefs="DRAWINGS">FIG. 9A</figref> illustrates a viewer application configured to generate a resulting image based two image sets, in accordance with one embodiment;</div>
<div class="description-paragraph" id="p-0030" num="0029"> <figref idrefs="DRAWINGS">FIG. 9B</figref> illustrates an exemplary user interface associated with the viewer application <b>910</b> of <figref idrefs="DRAWINGS">FIG. 9A</figref>, in accordance with one embodiment;</div>
<div class="description-paragraph" id="p-0031" num="0030"> <figref idrefs="DRAWINGS">FIG. 9C</figref> illustrates a resulting image with differing levels of strobe exposure, in accordance with one embodiment; and</div>
<div class="description-paragraph" id="p-0032" num="0031"> <figref idrefs="DRAWINGS">FIG. 9D</figref> illustrates a system for generating a resulting image from a high dynamic range chrominance image and a high dynamic range luminance image, in accordance with one embodiment.</div>
</description-of-drawings>
<heading id="h-0006">DETAILED DESCRIPTION</heading>
<div class="description-paragraph" id="p-0033" num="0032">Embodiments of the present invention enable a digital photographic system to generate a digital image (or simply “image”) of a photographic scene subjected to strobe illumination. Exemplary digital photographic systems include, without limitation, digital cameras and mobile devices such as smart phones that are configured to include a digital camera module and a strobe unit. A given photographic scene is a portion of an overall scene sampled by the digital photographic system.</div>
<div class="description-paragraph" id="p-0034" num="0033">The digital photographic system may capture separate image data for chrominance components (i.e., color) and luminance (i.e., intensity) components for a digital image. For example, a first image sensor may be used to capture chrominance data and a second image sensor may be used to capture luminance data. The second image sensor may be different than the first image sensor. For example, a resolution of the second image sensor may be higher than the first image sensor, thereby producing more detail related to the luminance information of the captured scene when compared to the chrominance information captured by the first image sensor. The chrominance information and the luminance information may then be combined to generate a resulting image that produces better images than captured with a single image sensor using conventional techniques.</div>
<div class="description-paragraph" id="p-0035" num="0034">In another embodiment, two or more images are sequentially sampled by the digital photographic system to generate an image set. Each image within the image set may be generated in conjunction with different strobe intensity, different exposure parameters, or a combination thereof. Exposure parameters may include sensor sensitivity (“ISO” parameter), exposure time (shutter speed), aperture size (f-stop), and focus distance. In certain embodiments, one or more exposure parameters, such as aperture size, may be constant and not subject to determination. For example, aperture size may be constant based on a given lens design associated with the digital photographic system. At least one of the images comprising the image set may be sampled in conjunction with a strobe unit, such as a light-emitting diode (LED) strobe unit, configured to contribute illumination to the photographic scene.</div>
<div class="description-paragraph" id="p-0036" num="0035">Separate image sets may be captured for chrominance information and luminance information. For example, a first image set may capture chrominance information under ambient illumination and strobe illumination at different strobe intensities and/or exposure parameters. A second image set may capture luminance information under the same settings. The chrominance information and luminance information may then be blended to produce a resulting image with greater dynamic range that could be captured using a single image sensor.</div>
<div class="description-paragraph" id="p-0037" num="0036"> <figref idrefs="DRAWINGS">FIG. 1</figref> illustrates a flow chart of a method <b>100</b> for generating a digital image, in accordance with one embodiment. Although method <b>100</b> is described in conjunction with the systems of <figref idrefs="DRAWINGS">FIGS. 2-7C</figref>, persons of ordinary skill in the art will understand that any system that performs method <b>100</b> is within the scope and spirit of embodiments of the present invention. In one embodiment, a digital photographic system, such as digital photographic system <b>300</b> of <figref idrefs="DRAWINGS">FIG. 3A</figref>, is configured to perform method <b>100</b>. The digital photographic system <b>300</b> may be implemented within a digital camera, such as digital camera <b>302</b> of <figref idrefs="DRAWINGS">FIG. 3C</figref>, or a mobile device, such as mobile device <b>376</b> of <figref idrefs="DRAWINGS">FIG. 3D</figref>.</div>
<div class="description-paragraph" id="p-0038" num="0037">Method <b>100</b> begins at step <b>102</b>, where a processor, such as processor complex <b>310</b>, receives a first image of an optical scene that includes a plurality of chrominance values (referred to herein as a chrominance image). The chrominance image may be captured using a first image sensor, such as a CMOS image sensor or a CCD image sensor. In one embodiment, the chrominance image includes a plurality of pixels, where each pixel is associated with a different color channel component (e.g., red, green, blue, cyan, magenta, yellow, etc.). In another embodiment, each pixel is associated with a tuple of values, each value in the tuple associated with a different color channel component (i.e., each pixel includes a red value, a blue value, and a green value).</div>
<div class="description-paragraph" id="p-0039" num="0038">At step <b>104</b>, the processor receives a second image of the optical scene that includes a plurality of luminance values (referred to herein as a luminance image). The luminance image may be captured using a second image sensor, which is different than the first image sensor. Alternatively, the luminance image may be captured using the first image sensor. For example, the chrominance values may be captured by a first subset of photodiodes of the first image sensor and the luminance values may be captured by a second subset of photodiodes of the first image sensor. In one embodiment, the luminance image includes a plurality of pixels, where each pixel is associated with an intensity component. The intensity component specifies a brightness of the image at that pixel. A bit depth of the intensity component may be equal to or different from a bit depth of each of the color channel components in the chrominance image. For example, each of the color channel components in the chrominance image may have a bit depth of 8 bits, but the intensity component may have a bit depth of 12 bits. The bit depths may be different where the first image sensor and the second image sensor sample analog values generated by the photodiodes in the image sensors using analog-to-digital converters (ADCs) having a different level of precision.</div>
<div class="description-paragraph" id="p-0040" num="0039">In one embodiment, each pixel in the chrominance image is associated with one or more corresponding pixels in the luminance image. For example, the chrominance image and the luminance image may have the same resolution and pixels in the chrominance image have a 1-to-1 mapping to corresponding pixels in the luminance image. Alternatively, the luminance image may have a higher resolution than the chrominance image, where each pixel in the chrominance image is mapped to two or more pixels in the luminance image. It will be appreciated that any manner of mapping the pixels in the chrominance image to the pixels in the luminance image is contemplated as being within the scope of the present invention.</div>
<div class="description-paragraph" id="p-0041" num="0040">At step <b>106</b>, the processor generates a resulting image based on the first image and second image. In one embodiment, the resulting image has the same resolution as the second image (i.e., the luminance image). For each pixel in the resulting image, the processor blends the chrominance information and the luminance information to generate a resulting pixel value in the resulting image. In one embodiment, the processor determines one or more pixels in the chrominance image associated with the pixel in the resulting image. For example, the processor may select a corresponding pixel in the chrominance image that includes a red value, a green value, and a blue value that specifies a color in an RGB color space. The processor may convert the color specified in the RGB color space to a Hue-Saturation-Value (HSV) color value. In the HSV model, Hue represents a particular color, Saturation represents a “depth” of the color (i.e., whether the color is bright and bold or dim and grayish), and the Value represents a lightness of the color (i.e., whether the color intensity is closer to black or white). The processor may also determine one or more pixels in the luminance image associated with the pixel in the resulting image. A luminance value may be determined from the one or more pixels in the luminance image. The luminance value may be combined with the Hue value and Saturation value determined from the chrominance image to produce a new color specified in the HSV model. The new color may be different from the color specified by the chrominance information alone because the luminance value may be captured more accurately with respect to spatial resolution or precision (i.e., bit depth, etc.). In one embodiment, the new color specified in the HSV model may be converted back into the RGB color space and stored in the resulting image. Alternatively, the color may be converted into any technically feasible color space representation, such as YCrCb, R′G′B′, or other types of color spaces well-known in the art.</div>
<div class="description-paragraph" id="p-0042" num="0041">In one embodiment, the processor may apply a filter to a portion of the chrominance image to select a number of color channel component values from the chrominance image. For example, a single RGB value may be determined based on a filter applied to a plurality of individual pixel values in the chrominance image, where each pixel specifies a value for a single color channel component.</div>
<div class="description-paragraph" id="p-0043" num="0042">More illustrative information will now be set forth regarding various optional architectures and features with which the foregoing framework may or may not be implemented, per the desires of the user. It should be strongly noted that the following information is set forth for illustrative purposes and should not be construed as limiting in any manner. Any of the following features may be optionally incorporated with or without the exclusion of other features described.</div>
<div class="description-paragraph" id="p-0044" num="0043">In one embodiment, the first image may comprise a chrominance image generated by combining two or more chrominance images, as described in greater detail below. Furthermore, the second image may comprise a luminance image generated by combining two or more luminance images, as described in greater detail below.</div>
<div class="description-paragraph" id="p-0045" num="0044"> <figref idrefs="DRAWINGS">FIG. 2</figref> illustrates an image processing subsystem <b>200</b> configured to implement the method <b>100</b> of <figref idrefs="DRAWINGS">FIG. 1</figref>, in accordance with one embodiment. In one embodiment, the image processing subsystem <b>200</b> includes a software module, executed by a processor, which causes the processor to generate the resulting image <b>250</b> from the chrominance image <b>202</b> and the luminance image <b>204</b>. The processor may be a highly parallel processor such as a graphics processing unit (GPU). In one embodiment, the software module may be a shader program, such as a pixel shader or fragment shader, which is executed by the GPU once per pixel in the resulting image <b>250</b>. Each of the chrominance image <b>202</b> and the luminance image <b>204</b> may be stored as texture maps in a memory and accessed by the shader program using, e.g., a texture cache of the GPU.</div>
<div class="description-paragraph" id="p-0046" num="0045">In one embodiment, each instance of the shader program is executed for a corresponding pixel of the resulting image <b>250</b>. Each pixel in the resulting image <b>250</b> is associated with a set of coordinates that specifies a location of the pixel in the resulting image <b>250</b>. The coordinates may be used to access values in the chrominance image <b>202</b> as well as values in the luminance image <b>204</b>. The values may be evaluated by one or more functions to generate a value(s) for the pixel in the resulting image <b>250</b>. In one embodiment, at least two instances of the shader program associated with different pixels in the resulting image <b>250</b> may be executed in parallel.</div>
<div class="description-paragraph" id="p-0047" num="0046">In another embodiment, the image processing subsystem <b>200</b> may be a special function unit such as a logic circuit within an application-specific integrated circuit (ASIC). The ASIC may include the logic circuit for generating the resulting image <b>250</b> from a chrominance image <b>202</b> and a luminance image <b>204</b>. In one embodiment, the chrominance image <b>202</b> is captured by a first image sensor at a first resolution and values for pixels in the chrominance image <b>202</b> are stored in a first format. Similarly, the luminance image <b>204</b> is captured by a second image sensor at a second resolution, which may be the same as or different from the first resolution, and values for pixels in the luminance image <b>204</b> are stored in a second format. The logic may be designed specifically for the chrominance image <b>202</b> at the first resolution and first format and the luminance image <b>204</b> at the second resolution and second format.</div>
<div class="description-paragraph" id="p-0048" num="0047">In yet another embodiment, the image processing subsystem <b>200</b> is a general purpose processor designed to process the chrominance image <b>202</b> and the luminance image <b>204</b> according to a specific algorithm. The chrominance image <b>202</b> and the luminance image <b>204</b> may be received from an external source. For example, the image processing subsystem <b>200</b> may be a service supplied by a server computer over a network. A source (i.e., a client device connected to the network) may send a request to the service to process a pair of images, including a chrominance image <b>202</b> and a luminance image <b>204</b>. The source may transmit the chrominance image <b>202</b> and luminance image <b>204</b> to the service via the network. The image processing subsystem <b>200</b> may be configured to receive a plurality of pairs of images from one or more sources (e.g., devices connected to the network) and process each pair of images to generate a corresponding plurality of resulting images <b>250</b>. Each resulting image <b>250</b> may be transmitted to the requesting source via the network.</div>
<div class="description-paragraph" id="p-0049" num="0048">As described above, a chrominance image and a luminance image may be combined to generate a resulting image that has better qualities than could be achieved with conventional techniques. For example, a typical image sensor may generate only chrominance data, which results in a perceived luminance from the combination of all color channel components. However, each individual color channel component may be sampled from a different discrete location and then combined to generate a digital image where each spatial location (i.e., pixel) is a combination of all color channel components. In other words, the digital image is a blurred version of the raw optical information captured by the image sensor. By utilizing luminance information that has not been filtered and then adding color component information to each pixel, a more precise digital image may be reproduced. Furthermore, splitting the capture of the chrominance information from the luminance information allows each component of the image to be captured separately, potentially with different image sensors tailored to each application. Such advantages will be discussed in more detail below.</div>
<div class="description-paragraph" id="p-0050" num="0049"> <figref idrefs="DRAWINGS">FIG. 3A</figref> illustrates a digital photographic system <b>300</b>, configured to implement one or more aspects of the present invention. Digital photographic system <b>300</b> includes a processor complex <b>310</b> coupled to a camera module <b>330</b> and a strobe unit <b>336</b>. Digital photographic system <b>300</b> may also include, without limitation, a display unit <b>312</b>, a set of input/output devices <b>314</b>, non-volatile memory <b>316</b>, volatile memory <b>318</b>, a wireless unit <b>340</b>, and sensor devices <b>342</b>, each coupled to processor complex <b>310</b>. In one embodiment, a power management subsystem <b>320</b> is configured to generate appropriate power supply voltages for each electrical load element within digital photographic system <b>300</b>. A battery <b>322</b> may be configured to supply electrical energy to power management subsystem <b>320</b>. Battery <b>322</b> may implement any technically feasible energy storage system, including primary or rechargeable battery technologies.</div>
<div class="description-paragraph" id="p-0051" num="0050">In one embodiment, strobe unit <b>336</b> is integrated into digital photographic system <b>300</b> and configured to provide strobe illumination <b>350</b> during an image sample event performed by digital photographic system <b>300</b>. In an alternative embodiment, strobe unit <b>336</b> is implemented as an independent device from digital photographic system <b>300</b> and configured to provide strobe illumination <b>350</b> during an image sample event performed by digital photographic system <b>300</b>. Strobe unit <b>336</b> may comprise one or more LED devices. In certain embodiments, two or more strobe units are configured to synchronously generate strobe illumination in conjunction with sampling an image.</div>
<div class="description-paragraph" id="p-0052" num="0051">In one embodiment, strobe unit <b>336</b> is directed through a strobe control signal <b>338</b> to either emit strobe illumination <b>350</b> or not emit strobe illumination <b>350</b>. The strobe control signal <b>338</b> may implement any technically feasible signal transmission protocol. Strobe control signal <b>338</b> may indicate a strobe parameter, such as strobe intensity or strobe color, for directing strobe unit <b>336</b> to generate a specified intensity and/or color of strobe illumination <b>350</b>. As shown, strobe control signal <b>338</b> may be generated by processor complex <b>310</b>. Alternatively, strobe control signal <b>338</b> may be generated by camera module <b>330</b> or by any other technically feasible system element.</div>
<div class="description-paragraph" id="p-0053" num="0052">In one usage scenario, strobe illumination <b>350</b> comprises at least a portion of overall illumination in a photographic scene being photographed by camera module <b>330</b>. Optical scene information <b>352</b>, which may include strobe illumination <b>350</b> reflected from objects in the photographic scene, is focused as an optical image onto an image sensor <b>332</b>, within camera module <b>330</b>. Image sensor <b>332</b> generates an electronic representation of the optical image. The electronic representation comprises spatial color intensity information, which may include different color intensity samples, such as for red, green, and blue light. The spatial color intensity information may also include samples for white light. Alternatively, the color intensity samples may include spatial color intensity information for cyan, magenta, and yellow light. Persons skilled in the art will recognize that other and further sets of spatial color intensity information may be implemented. The electronic representation is transmitted to processor complex <b>310</b> via interconnect <b>334</b>, which may implement any technically feasible signal transmission protocol.</div>
<div class="description-paragraph" id="p-0054" num="0053">Input/output devices <b>314</b> may include, without limitation, a capacitive touch input surface, a resistive tablet input surface, one or more buttons, one or more knobs, light-emitting devices, light detecting devices, sound emitting devices, sound detecting devices, or any other technically feasible device for receiving user input and converting the input to electrical signals, or converting electrical signals into a physical signal. In one embodiment, input/output devices <b>314</b> include a capacitive touch input surface coupled to display unit <b>312</b>.</div>
<div class="description-paragraph" id="p-0055" num="0054">Non-volatile (NV) memory <b>316</b> is configured to store data when power is interrupted. In one embodiment, NV memory <b>316</b> comprises one or more flash memory devices. NV memory <b>316</b> may be configured to include programming instructions for execution by one or more processing units within processor complex <b>310</b>. The programming instructions may implement, without limitation, an operating system (OS), UI modules, image processing and storage modules, one or more modules for sampling an image set through camera module <b>330</b>, one or more modules for presenting the image set through display unit <b>312</b>. The programming instructions may also implement one or more modules for merging images or portions of images within the image set, aligning at least portions of each image within the image set, or a combination thereof. One or more memory devices comprising NV memory <b>316</b> may be packaged as a module configured to be installed or removed by a user. In one embodiment, volatile memory <b>318</b> comprises dynamic random access memory (DRAM) configured to temporarily store programming instructions, image data such as data associated with an image set, and the like, accessed during the course of normal operation of digital photographic system <b>300</b>.</div>
<div class="description-paragraph" id="p-0056" num="0055">Sensor devices <b>342</b> may include, without limitation, an accelerometer to detect motion and/or orientation, an electronic gyroscope to detect motion and/or orientation, a magnetic flux detector to detect orientation, a global positioning system (GPS) module to detect geographic position, or any combination thereof.</div>
<div class="description-paragraph" id="p-0057" num="0056">Wireless unit <b>340</b> may include one or more digital radios configured to send and receive digital data. In particular, wireless unit <b>340</b> may implement wireless standards known in the art as “WiFi” based on Institute for Electrical and Electronics Engineers (IEEE) standard 802.11, and may implement digital cellular telephony standards for data communication such as the well-known “3G” and “4G” suites of standards. Wireless unit <b>340</b> may further implement standards and protocols known in the art as LTE (long term evolution). In one embodiment, digital photographic system <b>300</b> is configured to transmit one or more digital photographs, sampled according to techniques taught herein, to an online or “cloud-based” photographic media service via wireless unit <b>340</b>. The one or more digital photographs may reside within either NV memory <b>316</b> or volatile memory <b>318</b>. In such a scenario, a user may possess credentials to access the online photographic media service and to transmit the one or more digital photographs for storage and presentation by the online photographic media service. The credentials may be stored or generated within digital photographic system <b>300</b> prior to transmission of the digital photographs. The online photographic media service may comprise a social networking service, photograph sharing service, or any other network-based service that provides storage and transmission of digital photographs. In certain embodiments, one or more digital photographs are generated by the online photographic media service based on an image set sampled according to techniques taught herein. In such embodiments, a user may upload source images comprising an image set for processing by the online photographic media service.</div>
<div class="description-paragraph" id="p-0058" num="0057">In one embodiment, digital photographic system <b>300</b> comprises a plurality of camera modules <b>330</b>. Such an embodiment may also include at least one strobe unit <b>336</b> configured to illuminate a photographic scene, sampled as multiple views by the plurality of camera modules <b>330</b>. The plurality of camera modules <b>330</b> may be configured to sample a wide angle view (greater than forty-five degrees of sweep among cameras) to generate a panoramic photograph. The plurality of camera modules <b>330</b> may also be configured to sample two or more narrow angle views (less than forty-five degrees of sweep among cameras) to generate a stereoscopic photograph. The plurality of camera modules <b>330</b> may include at least one camera module configured to sample chrominance information and at least one different camera module configured to sample luminance information.</div>
<div class="description-paragraph" id="p-0059" num="0058">Display unit <b>312</b> is configured to display a two-dimensional array of pixels to form an image for display. Display unit <b>312</b> may comprise a liquid-crystal display, an organic LED display, or any other technically feasible type of display. In certain embodiments, display unit <b>312</b> is able to display a narrower dynamic range of image intensity values than a complete range of intensity values sampled over a set of two or more images comprising the image set. Here, images comprising the image set may be merged according to any technically feasible high dynamic range (HDR) blending technique to generate a synthetic image for display within dynamic range constraints of display unit <b>312</b>. In one embodiment, the limited dynamic range specifies an eight-bit per color channel binary representation of corresponding color intensities. In other embodiments, the limited dynamic range specifies a twelve-bit per color channel binary representation.</div>
<div class="description-paragraph" id="p-0060" num="0059"> <figref idrefs="DRAWINGS">FIG. 3B</figref> illustrates a processor complex <b>310</b> within digital photographic system <b>300</b> of <figref idrefs="DRAWINGS">FIG. 3A</figref>, according to one embodiment of the present invention. Processor complex <b>310</b> includes a processor subsystem <b>360</b> and may include a memory subsystem <b>362</b>. In one embodiment, processor complex <b>310</b> comprises a system on a chip (SoC) device that implements processor subsystem <b>360</b>, and memory subsystem <b>362</b> comprising one or more DRAM devices coupled to processor subsystem <b>360</b>. In one implementation of the embodiment, processor complex <b>310</b> comprises a multi-chip module (MCM) encapsulating the SoC device and the one or more DRAM devices.</div>
<div class="description-paragraph" id="p-0061" num="0060">Processor subsystem <b>360</b> may include, without limitation, one or more central processing unit (CPU) cores <b>370</b>, a memory interface <b>380</b>, input/output interfaces unit <b>384</b>, and a display interface unit <b>382</b>, each coupled to an interconnect <b>374</b>. The one or more CPU cores <b>370</b> may be configured to execute instructions residing within memory subsystem <b>362</b>, volatile memory <b>318</b>, NV memory <b>316</b>, or any combination thereof. Each of the one or more CPU cores <b>370</b> may be configured to retrieve and store data via interconnect <b>374</b> and memory interface <b>380</b>. Each of the one or more CPU cores <b>370</b> may include a data cache, and an instruction cache. Two or more CPU cores <b>370</b> may share a data cache, an instruction cache, or any combination thereof. In one embodiment, a cache hierarchy is implemented to provide each CPU core <b>370</b> with a private cache layer, and a shared cache layer.</div>
<div class="description-paragraph" id="p-0062" num="0061">Processor subsystem <b>360</b> may further include one or more graphics processing unit (GPU) cores <b>372</b>. Each GPU core <b>372</b> comprises a plurality of multi-threaded execution units that may be programmed to implement graphics acceleration functions. GPU cores <b>372</b> may be configured to execute multiple thread programs according to well-known standards such as OpenGL™, OpenCL™, CUDA™, and the like. In certain embodiments, at least one GPU core <b>372</b> implements at least a portion of a motion estimation function, such as a well-known Harris detector or a well-known Hessian-Laplace detector. Such a motion estimation function may be used for aligning images or portions of images within the image set.</div>
<div class="description-paragraph" id="p-0063" num="0062">Interconnect <b>374</b> is configured to transmit data between and among memory interface <b>380</b>, display interface unit <b>382</b>, input/output interfaces unit <b>384</b>, CPU cores <b>370</b>, and GPU cores <b>372</b>. Interconnect <b>374</b> may implement one or more buses, one or more rings, a cross-bar, a mesh, or any other technically feasible data transmission structure or technique. Memory interface <b>380</b> is configured to couple memory subsystem <b>362</b> to interconnect <b>374</b>. Memory interface <b>380</b> may also couple NV memory <b>316</b>, volatile memory <b>318</b>, or any combination thereof to interconnect <b>374</b>. Display interface unit <b>382</b> is configured to couple display unit <b>312</b> to interconnect <b>374</b>. Display interface unit <b>382</b> may implement certain frame buffer functions such as frame refresh. Alternatively, display unit <b>312</b> may implement frame refresh. Input/output interfaces unit <b>384</b> is configured to couple various input/output devices to interconnect <b>374</b>.</div>
<div class="description-paragraph" id="p-0064" num="0063">In certain embodiments, camera module <b>330</b> is configured to store exposure parameters for sampling each image in an image set. When directed to sample an image set, the camera module <b>330</b> samples the image set according to the stored exposure parameters. A software module executing within processor complex <b>310</b> may generate and store the exposure parameters prior to directing the camera module <b>330</b> to sample the image set.</div>
<div class="description-paragraph" id="p-0065" num="0064">In other embodiments, camera module <b>330</b> is configured to store exposure parameters for sampling an image in an image set, and the camera interface unit <b>386</b> within the processor complex <b>310</b> is configured to cause the camera module <b>330</b> to first store exposure parameters for a given image comprising the image set, and to subsequently sample the image. In one embodiment, exposure parameters associated with images comprising the image set are stored within a parameter data structure. The camera interface unit <b>386</b> is configured to read exposure parameters from the parameter data structure for a given image to be sampled, and to transmit the exposure parameters to the camera module <b>330</b> in preparation of sampling an image. After the camera module <b>330</b> is configured according to the exposure parameters, the camera interface unit <b>386</b> directs the camera module <b>330</b> to sample an image. Each image within an image set may be sampled in this way. The data structure may be stored within the camera interface unit <b>386</b>, within a memory circuit within processor complex <b>310</b>, within volatile memory <b>318</b>, within NV memory <b>316</b>, or within any other technically feasible memory circuit. A software module executing within processor complex <b>310</b> may generate and store the data structure.</div>
<div class="description-paragraph" id="p-0066" num="0065">In one embodiment, the camera interface unit <b>386</b> transmits exposure parameters and commands to camera module <b>330</b> through interconnect <b>334</b>. In certain embodiments, the camera interface unit <b>386</b> is configured to directly control the strobe unit <b>336</b> by transmitting control commands to the strobe unit <b>336</b> through strobe control signal <b>338</b>. By directly controlling both the camera module <b>330</b> and the strobe unit <b>336</b>, the camera interface unit <b>386</b> may cause the camera module <b>330</b> and the strobe unit <b>336</b> to perform their respective operations in precise time synchronization. That is, the camera interface unit <b>386</b> may synchronize the steps of configuring the camera module <b>330</b> prior to sampling an image, configuring the strobe unit <b>336</b> to generate appropriate strobe illumination, and directing the camera module <b>330</b> to sample a photographic scene subjected to strobe illumination.</div>
<div class="description-paragraph" id="p-0067" num="0066">Additional set-up time or execution time associated with each step may reduce overall sampling performance. Therefore, a dedicated control circuit, such as the camera interface unit <b>386</b>, may be implemented to substantially minimize set-up and execution time associated with each step and any intervening time between steps.</div>
<div class="description-paragraph" id="p-0068" num="0067">In other embodiments, a software module executing within processor complex <b>310</b> directs the operation and synchronization of camera module <b>330</b> and the strobe unit <b>336</b>, with potentially reduced performance.</div>
<div class="description-paragraph" id="p-0069" num="0068">In one embodiment, camera interface unit <b>386</b> is configured to accumulate statistics while receiving image data from the camera module <b>330</b>. In particular, the camera interface unit <b>386</b> may accumulate exposure statistics for a given image while receiving image data for the image through interconnect <b>334</b>. Exposure statistics may include an intensity histogram, a count of over-exposed pixels or under-exposed pixels, an intensity-weighted sum of pixel intensity, or any combination thereof. The camera interface unit <b>386</b> may present the exposure statistics as memory-mapped storage locations within a physical or virtual address space defined by a processor, such as a CPU core <b>370</b>, within processor complex <b>310</b>.</div>
<div class="description-paragraph" id="p-0070" num="0069">In certain embodiments, camera interface unit <b>386</b> accumulates color statistics for estimating scene white-balance. Any technically feasible color statistics may be accumulated for estimating white balance, such as a sum of intensities for different color channels comprising red, green, and blue color channels. The sum of color channel intensities may then be used to perform a white-balance color correction on an associated image, according to a white-balance model such as a gray-world white-balance model. In other embodiments, curve-fitting statistics are accumulated for a linear or a quadratic curve fit used for implementing white-balance correction on an image. In one embodiment, camera interface unit <b>386</b> accumulates spatial color statistics for performing color-matching between or among images, such as between or among one or more ambient images and one or more images sampled with strobe illumination. As with the exposure statistics, the color statistics may be presented as memory-mapped storage locations within processor complex <b>310</b>.</div>
<div class="description-paragraph" id="p-0071" num="0070">In one embodiment, camera module <b>330</b> transmits strobe control signal <b>338</b> to strobe unit <b>336</b>, enabling strobe unit <b>336</b> to generate illumination while the camera module <b>330</b> is sampling an image. In another embodiment, camera module <b>330</b> samples an image illuminated by strobe unit <b>336</b> upon receiving an indication from camera interface unit <b>386</b> that strobe unit <b>336</b> is enabled. In yet another embodiment, camera module <b>330</b> samples an image illuminated by strobe unit <b>336</b> upon detecting strobe illumination within a photographic scene via a rapid rise in scene illumination.</div>
<div class="description-paragraph" id="p-0072" num="0071"> <figref idrefs="DRAWINGS">FIG. 3C</figref> illustrates a digital camera <b>302</b>, in accordance with one embodiment. As an option, the digital camera <b>302</b> may be implemented in the context of the details of any of the Figures disclosed herein. Of course, however, the digital camera <b>302</b> may be implemented in any desired environment. Further, the aforementioned definitions may equally apply to the description below.</div>
<div class="description-paragraph" id="p-0073" num="0072">In one embodiment, the digital camera <b>302</b> may be configured to include a digital photographic system, such as digital photographic system <b>300</b> of <figref idrefs="DRAWINGS">FIG. 3A</figref>. As shown, the digital camera <b>302</b> includes a camera module <b>330</b>, which may include optical elements configured to focus optical scene information representing a photographic scene onto an image sensor, which may be configured to convert the optical scene information to an electronic representation of the photographic scene.</div>
<div class="description-paragraph" id="p-0074" num="0073">Additionally, the digital camera <b>302</b> may include a strobe unit <b>336</b>, and may include a shutter release button <b>315</b> for triggering a photographic sample event, whereby digital camera <b>302</b> samples one or more images comprising the electronic representation. In other embodiments, any other technically feasible shutter release mechanism may trigger the photographic sample event (e.g. such as a timer trigger or remote control trigger, etc.).</div>
<div class="description-paragraph" id="p-0075" num="0074"> <figref idrefs="DRAWINGS">FIG. 3D</figref> illustrates a wireless mobile device <b>376</b>, in accordance with one embodiment. As an option, the mobile device <b>376</b> may be implemented in the context of the details of any of the Figures disclosed herein. Of course, however, the mobile device <b>376</b> may be implemented in any desired environment. Further, the aforementioned definitions may equally apply to the description below.</div>
<div class="description-paragraph" id="p-0076" num="0075">In one embodiment, the mobile device <b>376</b> may be configured to include a digital photographic system (e.g. such as digital photographic system <b>300</b> of <figref idrefs="DRAWINGS">FIG. 3A</figref>), which is configured to sample a photographic scene. In various embodiments, a camera module <b>330</b> may include optical elements configured to focus optical scene information representing the photographic scene onto an image sensor, which may be configured to convert the optical scene information to an electronic representation of the photographic scene. Further, a shutter release command may be generated through any technically feasible mechanism, such as a virtual button, which may be activated by a touch gesture on a touch entry display system comprising display unit <b>312</b>, or a physical button, which may be located on any face or surface of the mobile device <b>376</b>. Of course, in other embodiments, any number of other buttons, external inputs/outputs, or digital inputs/outputs may be included on the mobile device <b>376</b>, and which may be used in conjunction with the camera module <b>330</b>.</div>
<div class="description-paragraph" id="p-0077" num="0076">As shown, in one embodiment, a touch entry display system comprising display unit <b>312</b> is disposed on the opposite side of mobile device <b>376</b> from camera module <b>330</b>. In certain embodiments, the mobile device <b>376</b> includes a user-facing camera module <b>331</b> and may include a user-facing strobe unit (not shown). Of course, in other embodiments, the mobile device <b>376</b> may include any number of user-facing camera modules or rear-facing camera modules, as well as any number of user-facing strobe units or rear-facing strobe units.</div>
<div class="description-paragraph" id="p-0078" num="0077">In some embodiments, the digital camera <b>302</b> and the mobile device <b>376</b> may each generate and store a synthetic image based on an image stack sampled by camera module <b>330</b>. The image stack may include one or more images sampled under ambient lighting conditions, one or more images sampled under strobe illumination from strobe unit <b>336</b>, or a combination thereof. In one embodiment, the image stack may include one or more different images sampled for chrominance, and one or more different images sampled for luminance.</div>
<div class="description-paragraph" id="p-0079" num="0078"> <figref idrefs="DRAWINGS">FIG. 3E</figref> illustrates camera module <b>330</b>, in accordance with one embodiment. As an option, the camera module <b>330</b> may be implemented in the context of the details of any of the Figures disclosed herein. Of course, however, the camera module <b>330</b> may be implemented in any desired environment. Further, the aforementioned definitions may equally apply to the description below.</div>
<div class="description-paragraph" id="p-0080" num="0079">In one embodiment, the camera module <b>330</b> may be configured to control strobe unit <b>336</b> through strobe control signal <b>338</b>. As shown, a lens <b>390</b> is configured to focus optical scene information <b>352</b> onto image sensor <b>332</b> to be sampled. In one embodiment, image sensor <b>332</b> advantageously controls detailed timing of the strobe unit <b>336</b> though the strobe control signal <b>338</b> to reduce inter-sample time between an image sampled with the strobe unit <b>336</b> enabled, and an image sampled with the strobe unit <b>336</b> disabled. For example, the image sensor <b>332</b> may enable the strobe unit <b>336</b> to emit strobe illumination <b>350</b> less than one microsecond (or any desired length) after image sensor <b>332</b> completes an exposure time associated with sampling an ambient image and prior to sampling a strobe image.</div>
<div class="description-paragraph" id="p-0081" num="0080">In other embodiments, the strobe illumination <b>350</b> may be configured based on a desired one or more target points. For example, in one embodiment, the strobe illumination <b>350</b> may light up an object in the foreground, and depending on the length of exposure time, may also light up an object in the background of the image. In one embodiment, once the strobe unit <b>336</b> is enabled, the image sensor <b>332</b> may then immediately begin exposing a strobe image. The image sensor <b>332</b> may thus be able to directly control sampling operations, including enabling and disabling the strobe unit <b>336</b> associated with generating an image stack, which may comprise at least one image sampled with the strobe unit <b>336</b> disabled, and at least one image sampled with the strobe unit <b>336</b> either enabled or disabled. In one embodiment, data comprising the image stack sampled by the image sensor <b>332</b> is transmitted via interconnect <b>334</b> to a camera interface unit <b>386</b> within processor complex <b>310</b>. In some embodiments, the camera module <b>330</b> may include an image sensor controller, which may be configured to generate the strobe control signal <b>338</b> in conjunction with controlling operation of the image sensor <b>332</b>.</div>
<div class="description-paragraph" id="p-0082" num="0081"> <figref idrefs="DRAWINGS">FIG. 3F</figref> illustrates a camera module <b>330</b>, in accordance with one embodiment. As an option, the camera module <b>330</b> may be implemented in the context of the details of any of the Figures disclosed herein. Of course, however, the camera module <b>330</b> may be implemented in any desired environment. Further, the aforementioned definitions may equally apply to the description below.</div>
<div class="description-paragraph" id="p-0083" num="0082">In one embodiment, the camera module <b>330</b> may be configured to sample an image based on state information for strobe unit <b>336</b>. The state information may include, without limitation, one or more strobe parameters (e.g. strobe intensity, strobe color, strobe time, etc.), for directing the strobe unit <b>336</b> to generate a specified intensity and/or color of the strobe illumination <b>350</b>. In one embodiment, commands for configuring the state information associated with the strobe unit <b>336</b> may be transmitted through a strobe control signal <b>338</b>, which may be monitored by the camera module <b>330</b> to detect when the strobe unit <b>336</b> is enabled. For example, in one embodiment, the camera module <b>330</b> may detect when the strobe unit <b>336</b> is enabled or disabled within a microsecond or less of the strobe unit <b>336</b> being enabled or disabled by the strobe control signal <b>338</b>. To sample an image requiring strobe illumination, a camera interface unit <b>386</b> may enable the strobe unit <b>336</b> by sending an enable command through the strobe control signal <b>338</b>. In one embodiment, the camera interface unit <b>386</b> may be included as an interface of input/output interfaces <b>384</b> in a processor subsystem <b>360</b> of the processor complex <b>310</b> of <figref idrefs="DRAWINGS">FIG. 3B</figref>. The enable command may comprise a signal level transition, a data packet, a register write, or any other technically feasible transmission of a command. The camera module <b>330</b> may sense that the strobe unit <b>336</b> is enabled and then cause image sensor <b>332</b> to sample one or more images requiring strobe illumination while the strobe unit <b>336</b> is enabled. In such an implementation, the image sensor <b>332</b> may be configured to wait for an enable signal destined for the strobe unit <b>336</b> as a trigger signal to begin sampling a new exposure.</div>
<div class="description-paragraph" id="p-0084" num="0083">In one embodiment, camera interface unit <b>386</b> may transmit exposure parameters and commands to camera module <b>330</b> through interconnect <b>334</b>. In certain embodiments, the camera interface unit <b>386</b> may be configured to directly control strobe unit <b>336</b> by transmitting control commands to the strobe unit <b>336</b> through strobe control signal <b>338</b>. By directly controlling both the camera module <b>330</b> and the strobe unit <b>336</b>, the camera interface unit <b>386</b> may cause the camera module <b>330</b> and the strobe unit <b>336</b> to perform their respective operations in precise time synchronization. In one embodiment, precise time synchronization may be less than five hundred microseconds of event timing error. Additionally, event timing error may be a difference in time from an intended event occurrence to the time of a corresponding actual event occurrence.</div>
<div class="description-paragraph" id="p-0085" num="0084">In another embodiment, camera interface unit <b>386</b> may be configured to accumulate statistics while receiving image data from camera module <b>330</b>. In particular, the camera interface unit <b>386</b> may accumulate exposure statistics for a given image while receiving image data for the image through interconnect <b>334</b>. Exposure statistics may include, without limitation, one or more of an intensity histogram, a count of over-exposed pixels, a count of under-exposed pixels, an intensity-weighted sum of pixel intensity, or any combination thereof. The camera interface unit <b>386</b> may present the exposure statistics as memory-mapped storage locations within a physical or virtual address space defined by a processor, such as one or more of CPU cores <b>370</b>, within processor complex <b>310</b>. In one embodiment, exposure statistics reside in storage circuits that are mapped into a memory-mapped register space, which may be accessed through the interconnect <b>334</b>. In other embodiments, the exposure statistics are transmitted in conjunction with transmitting pixel data for a captured image. For example, the exposure statistics for a given image may be transmitted as in-line data, following transmission of pixel intensity data for the captured image. Exposure statistics may be calculated, stored, or cached within the camera interface unit <b>386</b>.</div>
<div class="description-paragraph" id="p-0086" num="0085">In one embodiment, camera interface unit <b>386</b> may accumulate color statistics for estimating scene white-balance. Any technically feasible color statistics may be accumulated for estimating white balance, such as a sum of intensities for different color channels comprising red, green, and blue color channels. The sum of color channel intensities may then be used to perform a white-balance color correction on an associated image, according to a white-balance model such as a gray-world white-balance model. In other embodiments, curve-fitting statistics are accumulated for a linear or a quadratic curve fit used for implementing white-balance correction on an image.</div>
<div class="description-paragraph" id="p-0087" num="0086">In one embodiment, camera interface unit <b>386</b> may accumulate spatial color statistics for performing color-matching between or among images, such as between or among an ambient image and one or more images sampled with strobe illumination. As with the exposure statistics, the color statistics may be presented as memory-mapped storage locations within processor complex <b>310</b>. In one embodiment, the color statistics are mapped in a memory-mapped register space, which may be accessed through interconnect <b>334</b>, within processor subsystem <b>360</b>. In other embodiments, the color statistics may be transmitted in conjunction with transmitting pixel data for a captured image. For example, in one embodiment, the color statistics for a given image may be transmitted as in-line data, following transmission of pixel intensity data for the image. Color statistics may be calculated, stored, or cached within the camera interface <b>386</b>.</div>
<div class="description-paragraph" id="p-0088" num="0087">In one embodiment, camera module <b>330</b> may transmit strobe control signal <b>338</b> to strobe unit <b>336</b>, enabling the strobe unit <b>336</b> to generate illumination while the camera module <b>330</b> is sampling an image. In another embodiment, camera module <b>330</b> may sample an image illuminated by strobe unit <b>336</b> upon receiving an indication signal from camera interface unit <b>386</b> that the strobe unit <b>336</b> is enabled. In yet another embodiment, camera module <b>330</b> may sample an image illuminated by strobe unit <b>336</b> upon detecting strobe illumination within a photographic scene via a rapid rise in scene illumination. In one embodiment, a rapid rise in scene illumination may include at least a rate of increasing intensity consistent with that of enabling strobe unit <b>336</b>. In still yet another embodiment, camera module <b>330</b> may enable strobe unit <b>336</b> to generate strobe illumination while sampling one image, and disable the strobe unit <b>336</b> while sampling a different image.</div>
<div class="description-paragraph" id="p-0089" num="0088"> <figref idrefs="DRAWINGS">FIG. 3G</figref> illustrates camera module <b>330</b>, in accordance with one embodiment. As an option, the camera module <b>330</b> may be implemented in the context of the details of any of the Figures disclosed herein. Of course, however, the camera module <b>330</b> may be implemented in any desired environment. Further, the aforementioned definitions may equally apply to the description below.</div>
<div class="description-paragraph" id="p-0090" num="0089">In one embodiment, the camera module <b>330</b> may be in communication with an application processor <b>335</b>. The camera module <b>330</b> is shown to include image sensor <b>332</b> in communication with a controller <b>333</b>. Further, the controller <b>333</b> is shown to be in communication with the application processor <b>335</b>.</div>
<div class="description-paragraph" id="p-0091" num="0090">In one embodiment, the application processor <b>335</b> may reside outside of the camera module <b>330</b>. As shown, the lens <b>390</b> may be configured to focus optical scene information onto image sensor <b>332</b> to be sampled. The optical scene information sampled by the image sensor <b>332</b> may then be communicated from the image sensor <b>332</b> to the controller <b>333</b> for at least one of subsequent processing and communication to the application processor <b>335</b>. In another embodiment, the controller <b>333</b> may control storage of the optical scene information sampled by the image sensor <b>332</b>, or storage of processed optical scene information.</div>
<div class="description-paragraph" id="p-0092" num="0091">In another embodiment, the controller <b>333</b> may enable a strobe unit to emit strobe illumination for a short time duration (e.g. less than one microsecond, etc.) after image sensor <b>332</b> completes an exposure time associated with sampling an ambient image. Further, the controller <b>333</b> may be configured to generate strobe control signal <b>338</b> in conjunction with controlling operation of the image sensor <b>332</b>.</div>
<div class="description-paragraph" id="p-0093" num="0092">In one embodiment, the image sensor <b>332</b> may be a complementary metal oxide semiconductor (CMOS) sensor or a charge-coupled device (CCD) sensor. In another embodiment, the controller <b>333</b> and the image sensor <b>332</b> may be packaged together as an integrated system or integrated circuit. In yet another embodiment, the controller <b>333</b> and the image sensor <b>332</b> may comprise discrete packages. In one embodiment, the controller <b>333</b> may provide circuitry for receiving optical scene information from the image sensor <b>332</b>, processing of the optical scene information, timing of various functionalities, and signaling associated with the application processor <b>335</b>. Further, in another embodiment, the controller <b>333</b> may provide circuitry for control of one or more of exposure, shuttering, white balance, and gain adjustment. Processing of the optical scene information by the circuitry of the controller <b>333</b> may include one or more of gain application, amplification, and analog-to-digital conversion. After processing the optical scene information, the controller <b>333</b> may transmit corresponding digital pixel data, such as to the application processor <b>335</b>.</div>
<div class="description-paragraph" id="p-0094" num="0093">In one embodiment, the application processor <b>335</b> may be implemented on processor complex <b>310</b> and at least one of volatile memory <b>318</b> and NV memory <b>316</b>, or any other memory device and/or system. The application processor <b>335</b> may be previously configured for processing of received optical scene information or digital pixel data communicated from the camera module <b>330</b> to the application processor <b>335</b>.</div>
<div class="description-paragraph" id="p-0095" num="0094"> <figref idrefs="DRAWINGS">FIG. 4</figref> illustrates a network service system <b>400</b>, in accordance with one embodiment. As an option, the network service system <b>400</b> may be implemented in the context of the details of any of the Figures disclosed herein. Of course, however, the network service system <b>400</b> may be implemented in any desired environment. Further, the aforementioned definitions may equally apply to the description below.</div>
<div class="description-paragraph" id="p-0096" num="0095">In one embodiment, the network service system <b>400</b> may be configured to provide network access to a device implementing a digital photographic system. As shown, network service system <b>400</b> includes a wireless mobile device <b>376</b>, a wireless access point <b>472</b>, a data network <b>474</b>, data center <b>480</b>, and a data center <b>481</b>. The wireless mobile device <b>376</b> may communicate with the wireless access point <b>472</b> via a digital radio link <b>471</b> to send and receive digital data, including data associated with digital images. The wireless mobile device <b>376</b> and the wireless access point <b>472</b> may implement any technically feasible transmission techniques for transmitting digital data via digital a radio link <b>471</b> without departing the scope and spirit of the present invention. In certain embodiments, one or more of data centers <b>480</b>, <b>481</b> may be implemented using virtual constructs so that each system and subsystem within a given data center <b>480</b>, <b>481</b> may comprise virtual machines configured to perform specified data processing and network tasks. In other implementations, one or more of data centers <b>480</b>, <b>481</b> may be physically distributed over a plurality of physical sites.</div>
<div class="description-paragraph" id="p-0097" num="0096">The wireless mobile device <b>376</b> may comprise a smart phone configured to include a digital camera, a digital camera configured to include wireless network connectivity, a reality augmentation device, a laptop configured to include a digital camera and wireless network connectivity, or any other technically feasible computing device configured to include a digital photographic system and wireless network connectivity.</div>
<div class="description-paragraph" id="p-0098" num="0097">In various embodiments, the wireless access point <b>472</b> may be configured to communicate with wireless mobile device <b>376</b> via the digital radio link <b>471</b> and to communicate with the data network <b>474</b> via any technically feasible transmission media, such as any electrical, optical, or radio transmission media. For example, in one embodiment, wireless access point <b>472</b> may communicate with data network <b>474</b> through an optical fiber coupled to the wireless access point <b>472</b> and to a router system or a switch system within the data network <b>474</b>. A network link <b>475</b>, such as a wide area network (WAN) link, may be configured to transmit data between the data network <b>474</b> and the data center <b>480</b>.</div>
<div class="description-paragraph" id="p-0099" num="0098">In one embodiment, the data network <b>474</b> may include routers, switches, long-haul transmission systems, provisioning systems, authorization systems, and any technically feasible combination of communications and operations subsystems configured to convey data between network endpoints, such as between the wireless access point <b>472</b> and the data center <b>480</b>. In one implementation, a wireless the mobile device <b>376</b> may comprise one of a plurality of wireless mobile devices configured to communicate with the data center <b>480</b> via one or more wireless access points coupled to the data network <b>474</b>.</div>
<div class="description-paragraph" id="p-0100" num="0099">Additionally, in various embodiments, the data center <b>480</b> may include, without limitation, a switch/router <b>482</b> and at least one data service system <b>484</b>. The switch/router <b>482</b> may be configured to forward data traffic between and among a network link <b>475</b>, and each data service system <b>484</b>. The switch/router <b>482</b> may implement any technically feasible transmission techniques, such as Ethernet media layer transmission, layer <b>2</b> switching, layer <b>3</b> routing, and the like. The switch/router <b>482</b> may comprise one or more individual systems configured to transmit data between the data service systems <b>484</b> and the data network <b>474</b>.</div>
<div class="description-paragraph" id="p-0101" num="0100">In one embodiment, the switch/router <b>482</b> may implement session-level load balancing among a plurality of data service systems <b>484</b>. Each data service system <b>484</b> may include at least one computation system <b>488</b> and may also include one or more storage systems <b>486</b>. Each computation system <b>488</b> may comprise one or more processing units, such as a central processing unit, a graphics processing unit, or any combination thereof. A given data service system <b>484</b> may be implemented as a physical system comprising one or more physically distinct systems configured to operate together. Alternatively, a given data service system <b>484</b> may be implemented as a virtual system comprising one or more virtual systems executing on an arbitrary physical system. In certain scenarios, the data network <b>474</b> may be configured to transmit data between the data center <b>480</b> and another data center <b>481</b>, such as through a network link <b>476</b>.</div>
<div class="description-paragraph" id="p-0102" num="0101">In another embodiment, the network service system <b>400</b> may include any networked mobile devices configured to implement one or more embodiments of the present invention. For example, in some embodiments, a peer-to-peer network, such as an ad-hoc wireless network, may be established between two different wireless mobile devices. In such embodiments, digital image data may be transmitted between the two wireless mobile devices without having to send the digital image data to a data center <b>480</b>.</div>
<div class="description-paragraph" id="p-0103" num="0102"> <figref idrefs="DRAWINGS">FIG. 5A</figref> illustrates a system for capturing optical scene information for conversion to an electronic representation of a photographic scene, in accordance with one embodiment. As an option, the system of <figref idrefs="DRAWINGS">FIG. 5A</figref> may be implemented in the context of the details of any of the Figures. Of course, however, the system of <figref idrefs="DRAWINGS">FIG. 5A</figref> may be implemented in any desired environment. Further, the aforementioned definitions may equally apply to the description below.</div>
<div class="description-paragraph" id="p-0104" num="0103">As shown in <figref idrefs="DRAWINGS">FIG. 5A</figref>, a pixel array <b>510</b> is in communication with row logic <b>512</b> and a column read out circuit <b>520</b>. Further, the row logic <b>512</b> and the column read out circuit <b>520</b> are both in communication with a control unit <b>514</b>. Still further, the pixel array <b>510</b> is shown to include a plurality of pixels <b>540</b>, where each pixel <b>540</b> may include four cells, cells <b>542</b>-<b>545</b>. In the context of the present description, the pixel array <b>510</b> may be included in an image sensor, such as image sensor <b>132</b> or image sensor <b>332</b> of camera module <b>330</b>.</div>
<div class="description-paragraph" id="p-0105" num="0104">As shown, the pixel array <b>510</b> includes a 2-dimensional array of the pixels <b>540</b>. For example, in one embodiment, the pixel array <b>510</b> may be built to comprise 4,000 pixels <b>540</b> in a first dimension, and 3,000 pixels <b>540</b> in a second dimension, for a total of 12,000,000 pixels <b>540</b> in the pixel array <b>510</b>, which may be referred to as a 12 megapixel pixel array. Further, as noted above, each pixel <b>540</b> is shown to include four cells <b>542</b>-<b>545</b>. In one embodiment, cell <b>542</b> may be associated with (e.g. selectively sensitive to, etc.) a first color of light, cell <b>543</b> may be associated with a second color of light, cell <b>544</b> may be associated with a third color of light, and cell <b>545</b> may be associated with a fourth color of light. In one embodiment, each of the first color of light, second color of light, third color of light, and fourth color of light are different colors of light, such that each of the cells <b>542</b>-<b>545</b> may be associated with different colors of light. In another embodiment, at least two cells of the cells <b>542</b>-<b>545</b> may be associated with a same color of light. For example, the cell <b>543</b> and the cell <b>544</b> may be associated with the same color of light.</div>
<div class="description-paragraph" id="p-0106" num="0105">Further, each of the cells <b>542</b>-<b>545</b> may be capable of storing an analog value. In one embodiment, each of the cells <b>542</b>-<b>545</b> may be associated with a capacitor for storing a charge that corresponds to an accumulated exposure during an exposure time. In such an embodiment, asserting a row select signal to circuitry of a given cell may cause the cell to perform a read operation, which may include, without limitation, generating and transmitting a current that is a function of the stored charge of the capacitor associated with the cell. In one embodiment, prior to a readout operation, current received at the capacitor from an associated photodiode may cause the capacitor, which has been previously charged, to discharge at a rate that is proportional to an incident light intensity detected at the photodiode. The remaining charge of the capacitor of the cell may then be read using the row select signal, where the current transmitted from the cell is an analog value that reflects the remaining charge on the capacitor. To this end, an analog value received from a cell during a readout operation may reflect an accumulated intensity of light detected at a photodiode. The charge stored on a given capacitor, as well as any corresponding representations of the charge, such as the transmitted current, may be referred to herein as analog pixel data. Of course, analog pixel data may include a set of spatially discrete intensity samples, each represented by continuous analog values.</div>
<div class="description-paragraph" id="p-0107" num="0106">Still further, the row logic <b>512</b> and the column read out circuit <b>520</b> may work in concert under the control of the control unit <b>514</b> to read a plurality of cells <b>542</b>-<b>545</b> of a plurality of pixels <b>540</b>. For example, the control unit <b>514</b> may cause the row logic <b>512</b> to assert a row select signal comprising row control signals <b>530</b> associated with a given row of pixels <b>540</b> to enable analog pixel data associated with the row of pixels to be read. As shown in <figref idrefs="DRAWINGS">FIG. 5A</figref>, this may include the row logic <b>512</b> asserting one or more row select signals comprising row control signals <b>530</b>(<b>0</b>) associated with a row <b>534</b>(<b>0</b>) that includes pixel <b>540</b>(<b>0</b>) and pixel <b>540</b>(<i>a</i>). In response to the row select signal being asserted, each pixel <b>540</b> on row <b>534</b>(<b>0</b>) transmits at least one analog value based on charges stored within the cells <b>542</b>-<b>545</b> of the pixel <b>540</b>. In certain embodiments, cell <b>542</b> and cell <b>543</b> are configured to transmit corresponding analog values in response to a first row select signal, while cell <b>544</b> and cell <b>545</b> are configured to transmit corresponding analog values in response to a second row select signal.</div>
<div class="description-paragraph" id="p-0108" num="0107">In one embodiment, analog values for a complete row of pixels <b>540</b> comprising each row <b>534</b>(<b>0</b>) through <b>534</b>(<i>r</i>) may be transmitted in sequence to column read out circuit <b>520</b> through column signals <b>532</b>. In one embodiment, analog values for a complete row or pixels or cells within a complete row of pixels may be transmitted simultaneously. For example, in response to row select signals comprising row control signals <b>530</b>(<b>0</b>) being asserted, the pixel <b>540</b>(<b>0</b>) may respond by transmitting at least one analog value from the cells <b>542</b>-<b>545</b> of the pixel <b>540</b>(<b>0</b>) to the column read out circuit <b>520</b> through one or more signal paths comprising column signals <b>532</b>(<b>0</b>); and simultaneously, the pixel <b>540</b>(<i>a</i>) will also transmit at least one analog value from the cells <b>542</b>-<b>545</b> of the pixel <b>540</b>(<i>a</i>) to the column read out circuit <b>520</b> through one or more signal paths comprising column signals <b>532</b>(<i>c</i>). Of course, one or more analog values may be received at the column read out circuit <b>520</b> from one or more other pixels <b>540</b> concurrently with receiving the at least one analog value from the pixel <b>540</b>(<b>0</b>) and concurrently with receiving the at least one analog value from the pixel <b>540</b>(<i>a</i>). Together, a set of analog values received from the pixels <b>540</b> comprising row <b>534</b>(<b>0</b>) may be referred to as an analog signal, and this analog signal may be based on an optical image focused on the pixel array <b>510</b>.</div>
<div class="description-paragraph" id="p-0109" num="0108">Further, after reading the pixels <b>540</b> comprising row <b>534</b>(<b>0</b>), the row logic <b>512</b> may select a second row of pixels <b>540</b> to be read. For example, the row logic <b>512</b> may assert one or more row select signals comprising row control signals <b>530</b>(<i>r</i>) associated with a row of pixels <b>540</b> that includes pixel <b>540</b>(<i>b</i>) and pixel <b>540</b>(<i>z</i>). As a result, the column read out circuit <b>520</b> may receive a corresponding set of analog values associated with pixels <b>540</b> comprising row <b>534</b>(<i>r</i>).</div>
<div class="description-paragraph" id="p-0110" num="0109">In one embodiment, the column read out circuit <b>520</b> may serve as a multiplexer to select and forward one or more received analog values to an analog-to-digital converter circuit, such as analog-to-digital unit <b>622</b> of <figref idrefs="DRAWINGS">FIG. 6C</figref>. The column read out circuit <b>520</b> may forward the received analog values in a predefined order or sequence. In one embodiment, row logic <b>512</b> asserts one or more row selection signals comprising row control signals <b>530</b>, causing a corresponding row of pixels to transmit analog values through column signals <b>532</b>. The column read out circuit <b>520</b> receives the analog values and sequentially selects and forwards one or more of the analog values at a time to the analog-to-digital unit <b>622</b>. Selection of rows by row logic <b>512</b> and selection of columns by column read out circuit <b>620</b> may be directed by control unit <b>514</b>. In one embodiment, rows <b>534</b> are sequentially selected to be read, starting with row <b>534</b>(<b>0</b>) and ending with row <b>534</b>(<i>r</i>), and analog values associated with sequential columns are transmitted to the analog-to-digital unit <b>622</b>. In other embodiments, other selection patterns may be implemented to read analog values stored in pixels <b>540</b>.</div>
<div class="description-paragraph" id="p-0111" num="0110">Further, the analog values forwarded by the column read out circuit <b>520</b> may comprise analog pixel data, which may later be amplified and then converted to digital pixel data for generating one or more digital images based on an optical image focused on the pixel array <b>510</b>.</div>
<div class="description-paragraph" id="p-0112" num="0111"> <figref idrefs="DRAWINGS">FIGS. 5B-5D</figref> illustrate three optional pixel configurations, according to one or more embodiments. As an option, these pixel configurations may be implemented in the context of the details of any of the Figures disclosed herein. Of course, however, these pixel configurations may be implemented in any desired environment. By way of a specific example, any of the pixels <b>540</b> of <figref idrefs="DRAWINGS">FIGS. 5B-5D</figref> may operate as one or more of the pixels <b>540</b> of the pixel array <b>510</b>.</div>
<div class="description-paragraph" id="p-0113" num="0112">As shown in <figref idrefs="DRAWINGS">FIG. 5B</figref>, a pixel <b>540</b> is illustrated to include a first cell (R) for measuring red light intensity, second and third cells (G) for measuring green light intensity, and a fourth cell (B) for measuring blue light intensity, in accordance with one embodiment. As shown in <figref idrefs="DRAWINGS">FIG. 5C</figref>, a pixel <b>540</b> is illustrated to include a first cell (R) for measuring red light intensity, a second cell (G) for measuring green light intensity, a third cell (B) for measuring blue light intensity, and a fourth cell (W) for measuring white light intensity, in accordance with another embodiment. In one embodiment, chrominance pixel data for a pixel may be sampled by the first, second, and third cells (red, green, and blue), and luminance data for the pixel may be sampled by the fourth cell (white for unfiltered for red, green, or blue). As shown in <figref idrefs="DRAWINGS">FIG. 5D</figref>, a pixel <b>540</b> is illustrated to include a first cell (C) for measuring cyan light intensity, a second cell (M) for measuring magenta light intensity, a third cell (Y) for measuring yellow light intensity, and a fourth cell (W) for measuring white light intensity, in accordance with yet another embodiment.</div>
<div class="description-paragraph" id="p-0114" num="0113">Of course, while pixels <b>540</b> are each shown to include four cells, a pixel <b>540</b> may be configured to include fewer or more cells for measuring light intensity. Still further, in another embodiment, while certain of the cells of pixel <b>540</b> are shown to be configured to measure a single peak wavelength of light, or white light, the cells of pixel <b>540</b> may be configured to measure any wavelength, range of wavelengths of light, or plurality of wavelengths of light.</div>
<div class="description-paragraph" id="p-0115" num="0114">Referring now to <figref idrefs="DRAWINGS">FIG. 5E</figref>, a system is shown for capturing optical scene information focused as an optical image on an image sensor <b>332</b>, in accordance with one embodiment. As an option, the system of <figref idrefs="DRAWINGS">FIG. 5E</figref> may be implemented in the context of the details of any of the Figures. Of course, however, the system of <figref idrefs="DRAWINGS">FIG. 5E</figref> may be carried out in any desired environment. Further, the aforementioned definitions may equally apply to the description below.</div>
<div class="description-paragraph" id="p-0116" num="0115">As shown in <figref idrefs="DRAWINGS">FIG. 5E</figref>, an image sensor <b>332</b> is shown to include a first cell <b>544</b>, a second cell <b>545</b>, and a third cell <b>548</b>. Further, each of the cells <b>544</b>-<b>548</b> is shown to include a photodiode <b>562</b>. Still further, upon each of the photodiodes <b>562</b> is a corresponding filter <b>564</b>, and upon each of the filters <b>564</b> is a corresponding microlens <b>566</b>. For example, the cell <b>544</b> is shown to include photodiode <b>562</b>(<b>0</b>), upon which is filter <b>564</b>(<b>0</b>), and upon which is microlens <b>566</b>(<b>0</b>). Similarly, the cell <b>545</b> is shown to include photodiode <b>562</b>(<b>1</b>), upon which is filter <b>564</b>(<b>1</b>), and upon which is microlens <b>566</b>(<b>1</b>). Still yet, as shown in <figref idrefs="DRAWINGS">FIG. 5E</figref>, pixel <b>540</b> is shown to include each of cells <b>544</b> and <b>545</b>, photodiodes <b>562</b>(<b>0</b>) and <b>562</b>(<b>1</b>), filters <b>564</b>(<b>0</b>) and <b>564</b>(<b>1</b>), and microlenses <b>566</b>(<b>0</b>) and <b>566</b>(<b>1</b>).</div>
<div class="description-paragraph" id="p-0117" num="0116">In one embodiment, each of the microlenses <b>566</b> may be any lens with a diameter of less than 50 microns. However, in other embodiments each of the microlenses <b>566</b> may have a diameter greater than or equal to 50 microns. In one embodiment, each of the microlenses <b>566</b> may include a spherical convex surface for focusing and concentrating received light on a supporting substrate beneath the microlens <b>566</b>. For example, as shown in <figref idrefs="DRAWINGS">FIG. 5E</figref>, the microlens <b>566</b>(<b>0</b>) focuses and concentrates received light on the filter <b>564</b>(<b>0</b>). In one embodiment, a microlens array <b>567</b> may include microlenses <b>566</b>, each corresponding in placement to photodiodes <b>562</b> within cells <b>544</b> of image sensor <b>332</b>.</div>
<div class="description-paragraph" id="p-0118" num="0117">In the context of the present description, the photodiodes <b>562</b> may comprise any semiconductor diode that generates a potential difference, or changes its electrical resistance, in response to photon absorption. Accordingly, the photodiodes <b>562</b> may be used to detect or measure light intensity. Further, each of the filters <b>564</b> may be optical filters for selectively transmitting light of one or more predetermined wavelengths. For example, the filter <b>564</b>(<b>0</b>) may be configured to selectively transmit substantially only green light received from the corresponding microlens <b>566</b>(<b>0</b>), and the filter <b>564</b>(<b>1</b>) may be configured to selectively transmit substantially only blue light received from the microlens <b>566</b>(<b>1</b>). Together, the filters <b>564</b> and microlenses <b>566</b> may be operative to focus selected wavelengths of incident light on a plane. In one embodiment, the plane may be a 2-dimensional grid of photodiodes <b>562</b> on a surface of the image sensor <b>332</b>. Further, each photodiode <b>562</b> receives one or more predetermined wavelengths of light, depending on its associated filter. In one embodiment, each photodiode <b>562</b> receives only one of red, blue, or green wavelengths of filtered light. As shown with respect to <figref idrefs="DRAWINGS">FIGS. 5B-5D</figref>, it is contemplated that a photodiode may be configured to detect wavelengths of light other than only red, green, or blue. For example, in the context of <figref idrefs="DRAWINGS">FIGS. 5C-5D</figref> specifically, a photodiode may be configured to detect white, cyan, magenta, yellow, or non-visible light such as infrared or ultraviolet light.</div>
<div class="description-paragraph" id="p-0119" num="0118">To this end, each coupling of a cell, photodiode, filter, and microlens may be operative to receive light, focus and filter the received light to isolate one or more predetermined wavelengths of light, and then measure, detect, or otherwise quantify an intensity of light received at the one or more predetermined wavelengths. The measured or detected light may then be represented as one or more analog values stored within a cell. For example, in one embodiment, each analog value may be stored within the cell utilizing a capacitor. Further, each analog value stored within a cell may be output from the cell based on a selection signal, such as a row selection signal, which may be received from row logic <b>512</b>. Further still, each analog value transmitted from a cell may comprise one analog value in a plurality of analog values of an analog signal, where each of the analog values is output by a different cell. Accordingly, the analog signal may comprise a plurality of analog pixel data values from a plurality of cells. In one embodiment, the analog signal may comprise analog pixel data values for an entire image of a photographic scene. In another embodiment, the analog signal may comprise analog pixel data values for a subset of the entire image of the photographic scene. For example, the analog signal may comprise analog pixel data values for a row of pixels of the image of the photographic scene. In the context of <figref idrefs="DRAWINGS">FIGS. 5A-5E</figref>, the row <b>534</b>(<b>0</b>) of the pixels <b>540</b> of the pixel array <b>510</b> may be one such row of pixels of the image of the photographic scene.</div>
<div class="description-paragraph" id="p-0120" num="0119"> <figref idrefs="DRAWINGS">FIG. 6A</figref> illustrates a circuit diagram for a photosensitive cell <b>600</b>, in accordance with one possible embodiment. As an option, the cell <b>600</b> may be implemented in the context of any of the Figures disclosed herein. Of course, however, the cell <b>600</b> may be implemented in any desired environment. Further, the aforementioned definitions may equally apply to the description below.</div>
<div class="description-paragraph" id="p-0121" num="0120">As shown in <figref idrefs="DRAWINGS">FIG. 6A</figref>, a photosensitive cell <b>600</b> includes a photodiode <b>602</b> coupled to an analog sampling circuit <b>603</b>. The photodiode <b>602</b> may be implemented as any of the photodiodes <b>562</b> of <figref idrefs="DRAWINGS">FIG. 5E</figref>. In one embodiment, a unique instance of photosensitive cell <b>600</b> may implemented as each of cells <b>542</b>-<b>545</b> comprising a pixel <b>540</b> within the context of <figref idrefs="DRAWINGS">FIGS. 5A-5E</figref>. The analog sampling circuit <b>603</b> comprises transistors <b>610</b>, <b>612</b>, <b>614</b>, and a capacitor <b>604</b>. In one embodiment, each of the transistors <b>610</b>, <b>612</b>, and <b>614</b> may be a field-effect transistor.</div>
<div class="description-paragraph" id="p-0122" num="0121">The photodiode <b>602</b> may be operable to measure or detect incident light <b>601</b> of a photographic scene. In one embodiment, the incident light <b>601</b> may include ambient light of the photographic scene. In another embodiment, the incident light <b>601</b> may include light from a strobe unit utilized to illuminate the photographic scene. In yet another embodiment, the incident light <b>601</b> may include ambient light and/or light from a strobe unit, where the composition of the incident light <b>601</b> changes as a function of exposure time. For example, the incident light <b>601</b> may include ambient light during a first exposure time, and light from a strobe unit during a second exposure time. Of course, the incident light <b>601</b> may include any light received at and measured by the photodiode <b>602</b>. Further still, and as discussed above, the incident light <b>601</b> may be concentrated on the photodiode <b>602</b> by a microlens, and the photodiode <b>602</b> may be one photodiode of a photodiode array that is configured to include a plurality of photodiodes arranged on a two-dimensional plane.</div>
<div class="description-paragraph" id="p-0123" num="0122">In one embodiment, each capacitor <b>604</b> may comprise gate capacitance for a transistor <b>610</b> and diffusion capacitance for transistor <b>614</b>. The capacitor <b>604</b> may also include additional circuit elements (not shown) such as, without limitation, a distinct capacitive structure, such as a metal-oxide stack, a poly capacitor, a trench capacitor, or any other technically feasible capacitor structures.</div>
<div class="description-paragraph" id="p-0124" num="0123">With respect to the analog sampling circuit <b>603</b>, when reset <b>616</b>(<b>0</b>) is active (e.g., high), transistor <b>614</b> provides a path from voltage source V<b>2</b> to capacitor <b>604</b>, causing capacitor <b>604</b> to charge to the potential of V<b>2</b>. When reset <b>616</b>(<b>0</b>) is inactive (e.g., low), the capacitor <b>604</b> I allowed to discharge in proportion to a photodiode current (I_PD) generated by the photodiode <b>602</b> in response to the incident light <b>601</b>. In this way, photodiode current I_PD is integrated for an exposure time when the reset <b>616</b>(<b>0</b>) is inactive, resulting in a corresponding voltage on the capacitor <b>604</b>. This voltage on the capacitor <b>604</b> may also be referred to as an analog sample. In embodiments, where the incident light <b>601</b> during the exposure time comprises ambient light, the sample may be referred to as an ambient sample; and where the incident light <b>601</b> during the exposure time comprises flash or strobe illumination, the sample may be referred to as a flash sample. When row select <b>634</b>(<b>0</b>) is active, transistor <b>612</b> provides a path for an output current from V<b>1</b> to output <b>608</b>(<b>0</b>). The output current is generated by transistor <b>610</b> in response to the voltage on the capacitor <b>604</b>. When the row select <b>634</b>(<b>0</b>) is active, the output current at the output <b>608</b>(<b>0</b>) may therefore be proportional to the integrated intensity of the incident light <b>601</b> during the exposure time.</div>
<div class="description-paragraph" id="p-0125" num="0124">The sample may be stored in response to a photodiode current I_PD being generated by the photodiode <b>602</b>, where the photodiode current I_PD varies as a function of the incident light <b>601</b> measured at the photodiode <b>602</b>. In particular, a greater amount of incident light <b>601</b> may be measured by the photodiode <b>602</b> during a first exposure time including strobe or flash illumination than during a second exposure time including ambient illumination. Of course, characteristics of the photographic scene, as well as adjustment of various exposure settings, such as exposure time and aperture for example, may result in a greater amount of incident light <b>601</b> being measured by the photodiode <b>602</b> during the second exposure time including the ambient illumination than during the first exposure time including the strobe or flash illumination.</div>
<div class="description-paragraph" id="p-0126" num="0125">In one embodiment, the photosensitive cell <b>600</b> of <figref idrefs="DRAWINGS">FIG. 6A</figref> may be implemented in a pixel array associated with a rolling shutter operation. As shown in <figref idrefs="DRAWINGS">FIG. 6A</figref>, the components of the analog sampling circuit <b>603</b> do not include any mechanism for storing the analog sample for a temporary amount of time. Thus, the exposure time for a particular sample measured by the analog sampling circuit <b>603</b> may refer to the time between when reset <b>616</b>(<b>0</b>) is driven inactive and the time when the row select <b>634</b>(<b>0</b>) is driven active in order to generate the output current at output <b>608</b>(<b>0</b>).</div>
<div class="description-paragraph" id="p-0127" num="0126">It will be appreciated that because each column of pixels in the pixel array <b>510</b> may share a single column signal <b>532</b> transmitted to the column read-out circuitry <b>520</b>, and that a column signal <b>532</b> corresponds to the output <b>608</b>(<b>0</b>), that analog values from only a single row of pixels may be transmitted to the column read-out circuitry <b>520</b> at a time. Consequently, the rolling shutter operation refers to a manner of controlling the plurality of reset signals <b>616</b> and row select signals <b>634</b> transmitted to each row <b>534</b> of pixels <b>540</b> in the pixel array <b>510</b>. For example, a first reset signal <b>616</b>(<b>0</b>) may be asserted to a first row <b>534</b>(<b>0</b>) of pixels <b>540</b> in the pixel array <b>510</b> at a first time, t<sub>0</sub>. Subsequently, a second reset signal <b>616</b>(<b>1</b>) may be asserted to a second row <b>534</b>(<b>1</b>) of pixels <b>540</b> in the pixel array <b>510</b> at a second time, t<sub>1</sub>, a third reset signal <b>616</b>(<b>2</b>) may be asserted to a third row <b>534</b>(<b>2</b>) of pixels <b>540</b> in the pixel array <b>510</b> at a third time, t<sub>2</sub>, and so forth until the last reset signal <b>616</b>(<i>z</i>) is asserted to a last row <b>534</b>(<i>z</i>) of pixels <b>540</b> in the pixel array <b>510</b> at a last time, t<sub>z</sub>. Thus, each row <b>534</b> of pixels <b>540</b> is reset sequentially from a top of the pixel array <b>510</b> to the bottom of the pixel array <b>510</b>. In one embodiment, the length of time between asserting the reset signal <b>616</b> at each row may be related to the time required to read-out a row of sample data by the column read-out circuitry <b>520</b>. In one embodiment, the length of time between asserting the reset signal <b>616</b> at each row may be related to the number of rows <b>534</b> in the pixel array <b>510</b> divided by an exposure time between frames of image data.</div>
<div class="description-paragraph" id="p-0128" num="0127">In order to sample all of the pixels <b>540</b> in the pixel array <b>510</b> with a consistent exposure time, each of the corresponding row select signals <b>634</b> are asserted a delay time after the corresponding reset signal <b>616</b> is reset for that row <b>534</b> of pixels <b>540</b>, the delay time equal to the exposure time. The operation of sampling each row in succession, thereby capturing optical scene information for each row of pixels during different exposure time periods, may be referred to herein as a rolling shutter operation. While the circuitry included in an image sensor to perform a rolling shutter operation is simpler than other circuitry designed to perform a global shutter operation, discussed in more detail below, the rolling shutter operation can cause image artifacts to appear due to the motion of objects in the scene or motion of the camera. Objects may appear skewed in the image because the bottom of the object may have moved relative to the edge of a frame more than the top of the object when the analog signals for the respective rows <b>534</b> of pixels <b>540</b> were sampled.</div>
<div class="description-paragraph" id="p-0129" num="0128"> <figref idrefs="DRAWINGS">FIG. 6B</figref> illustrates a circuit diagram for a photosensitive cell <b>640</b>, in accordance with another possible embodiment. As an option, the cell <b>640</b> may be implemented in the context of any of the Figures disclosed herein. Of course, however, the cell <b>640</b> may be implemented in any desired environment. Further, the aforementioned definitions may equally apply to the description below.</div>
<div class="description-paragraph" id="p-0130" num="0129">As shown in <figref idrefs="DRAWINGS">FIG. 6B</figref>, a photosensitive cell <b>640</b> includes a photodiode <b>602</b> coupled to an analog sampling circuit <b>643</b>. The photodiode <b>602</b> may be implemented as any of the photodiodes <b>562</b> of <figref idrefs="DRAWINGS">FIG. 5E</figref>. In one embodiment, a unique instance of photosensitive cell <b>640</b> may implemented as each of cells <b>542</b>-<b>545</b> comprising a pixel <b>540</b> within the context of <figref idrefs="DRAWINGS">FIGS. 5A-5E</figref>. The analog sampling circuit <b>643</b> comprises transistors <b>646</b>, <b>610</b>, <b>612</b>, <b>614</b>, and a capacitor <b>604</b>. In one embodiment, each of the transistors <b>646</b>, <b>610</b>, <b>612</b>, and <b>614</b> may be a field-effect transistor.</div>
<div class="description-paragraph" id="p-0131" num="0130">The transistors <b>610</b>, <b>612</b>, and <b>614</b> are similar in type and operation to the transistors <b>610</b>, <b>612</b>, and <b>614</b> of <figref idrefs="DRAWINGS">FIG. 6A</figref>. The transistor <b>646</b> may be similar in type to the transistors <b>610</b>, <b>612</b>, and <b>614</b>, but the transistor <b>646</b> has the effect of turning capacitor <b>604</b> into an in-pixel-memory of an analog voltage value. In other words, the capacitor <b>604</b> is allowed to discharge in proportion to the photodiode currect (I_PD) when the transistor <b>646</b> is active, and the capacitor <b>604</b> is prevented from discharging when the transistor <b>646</b> is inactive. The capacitor <b>604</b> may comprise gate capacitance for a transistor <b>610</b> and diffusion capacitance for transistors <b>614</b> and <b>646</b>. The capacitor <b>604</b> may also include additional circuit elements (not shown) such as, without limitation, a distinct capacitive structure, such as a metal-oxide stack, a poly capacitor, a trench capacitor, or any other technically feasible capacitor structures. Unlike analog sampling circuit <b>603</b>, analog sampling circuit <b>643</b> may be used to implement a global shutter operation where all pixels <b>540</b> in the pixel array are configured to generate a sample at the same time.</div>
<div class="description-paragraph" id="p-0132" num="0131">With respect to the analog sampling circuit <b>643</b>, when reset <b>616</b> is active (e.g., high), transistor <b>614</b> provides a path from voltage source V<b>2</b> to capacitor <b>604</b>, causing capacitor <b>604</b> to charge to the potential of V<b>2</b>. When reset <b>616</b> is inactive (e.g., low), the capacitor <b>604</b> is allowed to discharge in proportion to a photodiode current (I_PD) generated by the photodiode <b>602</b> in response to the incident light <b>601</b> as long as the transistor <b>646</b> is active. Transistor <b>646</b> may be activated by asserting the sample signal <b>618</b>, which is utilized to control the exposure time of each of the pixels <b>540</b>. In this way, photodiode current I_PD is integrated for an exposure time when the reset <b>616</b> is inactive and the sample <b>618</b> is active, resulting in a corresponding voltage on the capacitor <b>604</b>. After the exposure time is complete, the sample signal <b>618</b> may be reset to deactivate transistor <b>646</b> and stop the capacitor from discharging. When row select <b>634</b>(<b>0</b>) is active, transistor <b>612</b> provides a path for an output current from V<b>1</b> to output <b>608</b>(<b>0</b>). The output current is generated by transistor <b>610</b> in response to the voltage on the capacitor <b>604</b>. When the row select <b>634</b>(<b>0</b>) is active, the output current at the output <b>608</b>(<b>0</b>) may therefore be proportional to the integrated intensity of the incident light <b>601</b> during the exposure time.</div>
<div class="description-paragraph" id="p-0133" num="0132">In a global shutter operation, all pixels <b>540</b> of the pixel array <b>510</b> may share a global reset signal <b>616</b> and a global sample signal <b>618</b>, which control charging of the capacitors <b>604</b> and discharging of the capacitors <b>604</b> through the photodiode current I_PD. This effectively measures the amount of incident light hitting each photodiode <b>602</b> substantially simultaneously for each pixel <b>540</b> in the pixel array <b>510</b>. However, the external read-out circuitry for converting the analog values to digital values for each pixel may still require each row <b>534</b> of pixels <b>540</b> to be read out sequentially. Thus, after the global sample signal <b>618</b> is reset each corresponding row select signal <b>634</b> may be asserted and reset in order to read-out the analog values for each of the pixels. This is similar to the operation of the row select signal <b>634</b> in the rolling shutter operation except that the transistor <b>646</b> is inactive during this time such that any further accumulation of the charge in capacitor <b>604</b> is halted while all of the values are read.</div>
<div class="description-paragraph" id="p-0134" num="0133">It will be appreciated that other circuits for analog sampling circuits <b>603</b> and <b>643</b> may be implemented in lieu of the circuits set forth in <figref idrefs="DRAWINGS">FIGS. 6A and 6B</figref>, and that such circuits may be utilized to implement a rolling shutter operation or a global shutter operation, respectively. For example, the analog sampling circuits <b>603</b>, <b>643</b> may include per cell amplifiers (e.g., op-amps) that provide a gain for the voltage stored in capacitor <b>604</b> when the read-out is performed. In other embodiments, an analog sampling circuit <b>643</b> may include other types of analog memory implementations decoupled from capacitor <b>604</b> such that the voltage of capacitor <b>604</b> is stored in the analog memory when the sample signal <b>618</b> is reset and capacitor <b>604</b> is allowed to continue to discharge through the photodiode <b>602</b>. In yet another embodiment, each output <b>608</b> associated with a column of pixels may be coupled to a dedicated analog-to-digital converter (ADC) that enables the voltage at capacitor <b>604</b> to be sampled and converted substantially simultaneously for all pixels <b>540</b> in a row or portion of a row comprising the pixel array <b>510</b>. In certain embodiments, odd rows and even rows may be similarly coupled to dedicated ADC circuits to provide simultaneous conversion of all color information for a given pixel. In one embodiment, a white color cell comprising a pixel is coupled to an ADC circuit configured to provide a higher dynamic range (e.g., 12 bits or 14 bits) than a dynamic range for ADC circuits coupled to a cell having color (e.g., red, green, blue) filters (e.g., 8 bits or 10 bits).</div>
<div class="description-paragraph" id="p-0135" num="0134"> <figref idrefs="DRAWINGS">FIG. 6C</figref> illustrates a system for converting analog pixel data to digital pixel data, in accordance with an embodiment. As an option, the system of <figref idrefs="DRAWINGS">FIG. 6C</figref> may be implemented in the context of the details of any of the Figures disclosed herein. Of course, however, the system of <figref idrefs="DRAWINGS">FIG. 6C</figref> may be implemented in any desired environment. Further, the aforementioned definitions may equally apply to the description below.</div>
<div class="description-paragraph" id="p-0136" num="0135">As shown in <figref idrefs="DRAWINGS">FIG. 6C</figref>, analog pixel data <b>621</b> is received from column read out circuit <b>520</b> at analog-to-digital unit <b>622</b> under the control of control unit <b>514</b>. The analog pixel data <b>621</b> may be received within an analog signal, as noted hereinabove. Further, the analog-to-digital unit <b>622</b> generates digital pixel data <b>625</b> based on the received analog pixel data <b>621</b>. In one embodiment, a unique instance of analog pixel data <b>621</b> may include, as an ordered set of individual analog values, all analog values output from all corresponding analog sampling circuits or sample storage nodes. For example, in the context of the foregoing figures, each cell of cells <b>542</b>-<b>545</b> of a plurality of pixels <b>540</b> of a pixel array <b>510</b> may include an analog sampling circuit <b>603</b> or analog sampling circuit <b>643</b>.</div>
<div class="description-paragraph" id="p-0137" num="0136">With continuing reference to <figref idrefs="DRAWINGS">FIG. 6C</figref>, the analog-to-digital unit <b>622</b> includes an amplifier <b>650</b> and an analog-to-digital converter <b>654</b>. In one embodiment, the amplifier <b>650</b> receives an instance of analog pixel data <b>621</b> and a gain <b>652</b>, and applies the gain <b>652</b> to the analog pixel data <b>621</b> to generate gain-adjusted analog pixel data <b>623</b>. The gain-adjusted analog pixel data <b>623</b> is transmitted from the amplifier <b>650</b> to the analog-to-digital converter <b>654</b>. The analog-to-digital converter <b>654</b> receives the gain-adjusted analog pixel data <b>623</b>, and converts the gain-adjusted analog pixel data <b>623</b> to the digital pixel data <b>625</b>, which is then transmitted from the analog-to-digital converter <b>654</b>. In other embodiments, the amplifier <b>650</b> may be implemented within the column read out circuit <b>520</b> or in each individual cell instead of within the analog-to-digital unit <b>622</b>. The analog-to-digital converter <b>654</b> may convert the gain-adjusted analog pixel data <b>623</b> to the digital pixel data <b>625</b> using any technically feasible analog-to-digital conversion technique.</div>
<div class="description-paragraph" id="p-0138" num="0137">In an embodiment, the gain-adjusted analog pixel data <b>623</b> results from the application of the gain <b>652</b> to the analog pixel data <b>621</b>. In one embodiment, the gain <b>652</b> may be selected by the analog-to-digital unit <b>622</b>. In another embodiment, the gain <b>652</b> may be selected by the control unit <b>514</b>, and then supplied from the control unit <b>514</b> to the analog-to-digital unit <b>622</b> for application to the analog pixel data <b>621</b>.</div>
<div class="description-paragraph" id="p-0139" num="0138">It should be noted, in one embodiment, that a consequence of applying the gain <b>652</b> to the analog pixel data <b>621</b> is that analog noise may appear in the gain-adjusted analog pixel data <b>623</b>. If the amplifier <b>650</b> imparts a significantly large gain to the analog pixel data <b>621</b> in order to obtain highly sensitive data from the pixel array <b>510</b>, then a significant amount of noise may be expected within the gain-adjusted analog pixel data <b>623</b>. In one embodiment, the detrimental effects of such noise may be reduced by capturing the optical scene information at a reduced overall exposure. In such an embodiment, the application of the gain <b>652</b> to the analog pixel data <b>621</b> may result in gain-adjusted analog pixel data with proper exposure and reduced noise.</div>
<div class="description-paragraph" id="p-0140" num="0139">In one embodiment, the amplifier <b>650</b> may be a transimpedance amplifier (TIA). Furthermore, the gain <b>652</b> may be specified by a digital value. In one embodiment, the digital value specifying the gain <b>652</b> may be set by a user of a digital photographic device, such as by operating the digital photographic device in a “manual” mode. Still yet, the digital value may be set by hardware or software of a digital photographic device. As an option, the digital value may be set by the user working in concert with the software of the digital photographic device.</div>
<div class="description-paragraph" id="p-0141" num="0140">In one embodiment, a digital value used to specify the gain <b>652</b> may be associated with an ISO. In the field of photography, the ISO system is a well-established standard for specifying light sensitivity. In one embodiment, the amplifier <b>650</b> receives a digital value specifying the gain <b>652</b> to be applied to the analog pixel data <b>621</b>. In another embodiment, there may be a mapping from conventional ISO values to digital gain values that may be provided as the gain <b>652</b> to the amplifier <b>650</b>. For example, each of ISO <b>100</b>, ISO <b>200</b>, ISO <b>400</b>, ISO <b>800</b>, ISO <b>1600</b>, etc. may be uniquely mapped to a different digital gain value, and a selection of a particular ISO results in the mapped digital gain value being provided to the amplifier <b>650</b> for application as the gain <b>652</b>. In one embodiment, one or more ISO values may be mapped to a gain of 1. Of course, in other embodiments, one or more ISO values may be mapped to any other gain value.</div>
<div class="description-paragraph" id="p-0142" num="0141">Accordingly, in one embodiment, each analog pixel value may be adjusted in brightness given a particular ISO value. Thus, in such an embodiment, the gain-adjusted analog pixel data <b>623</b> may include brightness corrected pixel data, where the brightness is corrected based on a specified ISO. In another embodiment, the gain-adjusted analog pixel data <b>623</b> for an image may include pixels having a brightness in the image as if the image had been sampled at a certain ISO.</div>
<div class="description-paragraph" id="p-0143" num="0142"> <figref idrefs="DRAWINGS">FIG. 7A</figref> illustrates a configuration of the camera module <b>330</b>, in accordance with one embodiment. As shown in <figref idrefs="DRAWINGS">FIG. 7A</figref>, the camera module <b>330</b> may include two lenses <b>734</b> positioned above two image sensors <b>732</b>. A first lens <b>734</b>(<b>0</b>) is associated with a first image sensor <b>732</b>(<b>0</b>) and focuses optical scene information <b>752</b>(<b>0</b>) from a first viewpoint onto the first image sensor <b>732</b>(<b>0</b>). A second lens <b>734</b>(<b>1</b>) is associated with a second image sensor <b>732</b>(<b>1</b>) and focuses optical scene information <b>752</b>(<b>1</b>) from a second viewpoint onto the second image sensor <b>732</b>(<b>1</b>).</div>
<div class="description-paragraph" id="p-0144" num="0143">In one embodiment, the first image sensor <b>732</b>(<b>0</b>) may be configured to capture chrominance information associated with the scene and the second image sensor <b>732</b>(<b>1</b>) may be configured to capture luminance information associated with the scene. The first image sensor <b>732</b>(<b>0</b>) may be the same or different than the second image sensor <b>732</b>(<b>1</b>). For example, the first image sensor <b>732</b>(<b>0</b>) may be an 8 megapixel CMOS image sensor <b>732</b>(<b>0</b>) having a Bayer color filter array (CFA), as shown in the arrangement of pixel <b>540</b> of <figref idrefs="DRAWINGS">FIG. 5B</figref>, that is configured to capture red, green, and blue color information; and the second image sensor <b>732</b>(<b>1</b>) may be a 12 megapixel CMOS image sensor <b>732</b>(<b>1</b>) having no color filter array (or a color filter array in which every cell is a white color filter) that is configured to capture intensity information (over substantially all wavelengths of the visible spectrum).</div>
<div class="description-paragraph" id="p-0145" num="0144">In operation, the camera module <b>330</b> may receive a shutter release command from the camera interface <b>386</b>. The camera module <b>330</b> may reset both the first image sensor <b>732</b>(<b>0</b>) and the second image sensor <b>732</b>(<b>1</b>). One or both of the first image sensor <b>732</b>(<b>0</b>) and the second image sensor <b>732</b>(<b>1</b>) may then be sampled under ambient light conditions (i.e., the strobe unit <b>336</b> is disabled). In one embodiment, both the first image sensor <b>732</b>(<b>0</b>) and the second image sensor <b>732</b>(<b>1</b>) are sampled substantially simultaneously to generate a chrominance image and a luminance image under ambient illumination. Once the pair of images (chrominance image and luminance image) has been captured, one or more additional pairs of images may be captured under ambient illumination (e.g., using different exposure parameters for each pair of images) or under strobe illumination. The additional pairs of images may be captured in quick succession (e.g., less than 200 milliseconds between sampling each simultaneously captured pair) such that relative motion between the objects in the scene and the camera, or relative motion between two distinct objects in the scene, is minimized.</div>
<div class="description-paragraph" id="p-0146" num="0145">In the camera module <b>330</b>, it may be advantageous to position the first lens <b>734</b>(<b>0</b>) and first image sensor <b>732</b>(<b>0</b>) proximate to the second lens <b>734</b>(<b>1</b>) and the second image sensor <b>732</b>(<b>0</b>) in order to capture the images of the scene from substantially the same viewpoint. Furthermore, direction of the field of view for both the first image sensor <b>732</b>(<b>0</b>) and the second image sensor <b>732</b>(<b>1</b>) should be approximately parallel. Unlike stereoscopic cameras configured to capture two images using parallax to represent depth of objects within the scene, the pair of images captured by the first image sensor <b>732</b>(<b>0</b>) and the second image sensor <b>732</b>(<b>1</b>) is not meant to capture displacement information for a given object from two disparate viewpoints.</div>
<div class="description-paragraph" id="p-0147" num="0146">One aspect of the invention is to generate a new digital image by combining the chrominance image with the luminance image to generate a more detailed image of a scene than could be captured with a single image sensor. In other words, the purpose of having two image sensors in the same camera module <b>330</b> is to capture different aspects of the same scene to create a blended image. Thus, care should be taken to minimize any differences between the images captured by the two image sensors. For example, positioning the first image sensor <b>732</b>(<b>0</b>) and the second image sensor <b>732</b>(<b>1</b>) close together may minimize image artifacts resulting from parallax of nearby objects. This may be the opposite approach taken for cameras designed to capture stereoscopic image data using two image sensors in which the distance between the two image sensors may be selected to mimic an intra-ocular distance of the human eyes.</div>
<div class="description-paragraph" id="p-0148" num="0147">In one embodiment, the images generated by the first image sensor <b>732</b>(<b>0</b>) and the second image sensor <b>732</b>(<b>1</b>) are close enough that blending the two images will not results in any image artifacts. In another embodiment, one of the images may be warped to match the other image to correct for the disparate viewpoints. There are many techniques available to warp one image to match another and any technically feasible technique may be employed to match the two images. For example, homography matrices may be calculated that describe the transformation from a portion (i.e., a plurality of pixels) of one image to a portion of another image. A homography matrix may describe a plurality of affine transformations (e.g., translation, rotation, scaling, etc.) that, when applied to a portion of an image, transform the portion of the image into another portion of a second image. By applying the homography matrices to various portions of the first image, the first image may be warped to match the second image. In this manner, any image artifacts resulting from blending the first image with the second image may be reduced.</div>
<div class="description-paragraph" id="p-0149" num="0148">In one embodiment, each of the image sensors <b>732</b> may be configured to capture an image using either a rolling shutter operation or a global shutter operation. The image sensors <b>732</b> may be configured to use the same type of shutter operation or different shutter operations. For example, the first image sensor <b>732</b>(<b>0</b>) configured to capture chrominance information may be a cheaper image sensor that only includes analog sampling circuitry capable of implementing in a rolling shutter operation. In contrast, the second image sensor <b>732</b>(<b>1</b>) configured to capture luminance information may be a more expensive image sensor that includes more advanced analog sampling circuitry capable of implementing a global shutter operation. Thus, the first image may be captured according to a rolling shutter operation while the second image may be captured according to a global shutter operation. Of course, both image sensors <b>732</b> may be configured to use the same shutter operation, either a rolling shutter operation or a global shutter operation. The type of shutter operation implemented by the image sensor <b>732</b> may be controlled by a control unit, such as control unit <b>514</b>, included in the image sensor <b>732</b> and may be triggered by a single shutter release command.</div>
<div class="description-paragraph" id="p-0150" num="0149"> <figref idrefs="DRAWINGS">FIG. 7B</figref> illustrates a configuration of the camera module <b>330</b>, in accordance with another embodiment. As shown in <figref idrefs="DRAWINGS">FIG. 7B</figref>, the camera module <b>330</b> may include a lens <b>734</b> positioned above a beam splitter <b>736</b>. The beam splitter <b>736</b> may act to split the optical information <b>752</b> received through the lens <b>734</b> into two separate transmission paths. The beam splitter <b>736</b> may be a cube made from two triangular glass prisms, a pellicle mirror like those typically utilized in single-lens reflex (SLR) cameras, or any other type of device capable of splitting a beam of light into two different directions. A first beam of light is directed onto the first image sensor <b>732</b>(<b>0</b>) and a second beam of light is directed onto the second image sensor <b>732</b>(<b>1</b>). In one embodiment, the first beam of light and the second beam of light include approximately the same optical information for the scene.</div>
<div class="description-paragraph" id="p-0151" num="0150">The two transmission paths focus the optical information <b>752</b> from the same viewpoint onto both the first image sensor <b>732</b>(<b>0</b>) and the second image sensor <b>732</b>(<b>1</b>). Because the same beam of light is split into two paths, it will be appreciated that intensity of light reaching each of the image sensors <b>732</b> is decreased. In order to compensate for the decrease in light reaching the image sensors, the exposure parameters can be adjusted (e.g., increasing the time between resetting the image sensor and sampling the image sensor to allow more light to activate the charge of each of the pixel sites). Alternatively, a gain applied to the analog signals may be increased, but this may also increase the noise in the analog signals as well.</div>
<div class="description-paragraph" id="p-0152" num="0151"> <figref idrefs="DRAWINGS">FIG. 7C</figref> illustrates a configuration of the camera module <b>330</b>, in accordance with yet another embodiment. As shown in <figref idrefs="DRAWINGS">FIG. 7C</figref>, the camera module <b>330</b> may include a lens <b>734</b> positioned above a single image sensor <b>732</b>. The optical information <b>752</b> is focused onto the image sensor <b>732</b> by the lens <b>734</b>. In such embodiments, both the chrominance information and the luminance information may be captured by the same image sensor. A color filter array (CFA) may include a plurality of different color filters, each color filter positioned over a particular photodiode of the image sensor <b>732</b> to filter the wavelengths of light that are measured by that particular photodiode. Some color filters may be associated with photodiodes configured to measure chrominance information, such as red color filters, blue color filters, green color filters, cyan color filters, magenta color filters, or yellow color filters. Other color filters may be associated with photodiodes configured to measure luminance information, such as white color filters. As used herein, white color filters are filters that allow a substantially uniform amount of light across the visible spectrum to pass through the color filter. The color filters in the CFA may be arranged such that a first portion of the photodiodes included in the image sensor <b>732</b> capture samples for a chrominance image from the optical information <b>752</b> and a second portion of the photodiodes included in the image sensor <b>732</b> capture samples for a luminance image from the optical information <b>752</b>.</div>
<div class="description-paragraph" id="p-0153" num="0152">In one embodiment, the each pixel in the image sensor <b>732</b> may be configured with a plurality of filters as shown in <figref idrefs="DRAWINGS">FIG. 5C</figref>. The photodiodes associated with the red, green, and blue color filters may capture samples included in the chrominance image as an RGB tuple. The photodiodes associated with the white color filter may capture samples included in the luminance image. It will be appreciated that each pixel <b>540</b> in the pixel array <b>510</b> of the image sensor <b>732</b> will produce one color in an RGB format stored in the chrominance image as well as an intensity value stored in a corresponding luminance image. In other words, the chrominance image and the luminance image will have the same resolution with one value per pixel.</div>
<div class="description-paragraph" id="p-0154" num="0153">In another embodiment, the each pixel in the image sensor <b>732</b> may be configured with a plurality of filters as shown in <figref idrefs="DRAWINGS">FIG. 5D</figref>. The photodiodes associated with the cyan, magenta, and yellow color filters may capture samples included in the chrominance image as a CMY tuple. The photodiodes associated with the white color filter may capture samples included in the luminance image. It will be appreciated that each pixel <b>540</b> in the pixel array <b>510</b> of the image sensor <b>732</b> will produce one color in a CMY format stored in the chrominance image as well as an intensity value stored in a corresponding luminance image.</div>
<div class="description-paragraph" id="p-0155" num="0154">In yet another embodiment, the CFA <b>460</b> may contain a majority of color filters for producing luminance information and a minority of color filters for producing chrominance information (e.g., 60% white, 10% red, 20% green, and 10% blue, etc.). Having a majority of the color filters being related to collecting luminance information will produce a higher resolution luminance image compared to the chrominance image. In one embodiment, the chrominance image has a lower resolution than the luminance image, due to the fewer number of photodiodes associated with the filters of the various colors. Furthermore, various techniques may be utilized to interpolate or “fill-in” values of either the chrominance image or the luminance image to fill in values associated with photodiodes that captured samples for the luminance image or chrominance image, respectively. For example, an interpolation of two or more values in the chrominance image or the luminance image may be performed to generate virtual samples in the chrominance image or the luminance image. It will be appreciated that a number of techniques for converting the raw digital pixel data associated with the individual photodiodes into a chrominance image and/or a luminance image may be implemented and is within the scope of the present invention.</div>
<div class="description-paragraph" id="p-0156" num="0155"> <figref idrefs="DRAWINGS">FIG. 8</figref> illustrates a flow chart of a method <b>800</b> for generating a digital image, in accordance with one embodiment. Although method <b>800</b> is described in conjunction with the systems of <figref idrefs="DRAWINGS">FIGS. 2-7C</figref>, persons of ordinary skill in the art will understand that any system that performs method <b>800</b> is within the scope and spirit of embodiments of the present invention. In one embodiment, a digital photographic system, such as digital photographic system <b>300</b> of <figref idrefs="DRAWINGS">FIG. 3A</figref>, is configured to perform method <b>800</b>. The digital photographic system <b>300</b> may be implemented within a digital camera, such as digital camera <b>302</b> of <figref idrefs="DRAWINGS">FIG. 3C</figref>, or a mobile device, such as mobile device <b>376</b> of <figref idrefs="DRAWINGS">FIG. 3D</figref>.</div>
<div class="description-paragraph" id="p-0157" num="0156">The method <b>800</b> begins at step <b>802</b>, where the digital photographic system <b>300</b> samples an image under ambient illumination to determine white balance parameters for the scene. For example, the white balance parameters may include separate linear scale factors for red, green, and blue for a gray world model of white balance. The white balance parameters may include quadratic parameters for a quadratic model of white balance, and so forth. In one embodiment, the digital photographic system <b>300</b> causes the camera module <b>330</b> to capture an image with one or more image sensors <b>332</b>. The digital photographic system <b>300</b> may then analyze the captured image to determine appropriate white balance parameters. In one embodiment, the white balance parameters indicate a color shift to apply to all pixels in images captured with ambient illumination. In such an embodiment, the white balance parameters may be used to adjust images captured under ambient illumination. A strobe unit <b>336</b> may produce a strobe illumination of a pre-set color that is sufficient to reduce the color shift caused by ambient illumination. In another embodiment, the white balance parameters may identify a color for the strobe unit <b>336</b> to generate in order to substantially match the color of ambient light during strobe illumination. In such an embodiment, the strobe unit <b>336</b> may include red, green, and blue LEDs, or, separately, a set of discrete LED illuminators having different phosphor mixes that each produce different, corresponding chromatic peaks, to create color-controlled strobe illumination. The color-controlled strobe illumination may be used to match scene illumination for images captured under only ambient illumination and images captured under both ambient illumination and color-controlled strobe illumination.</div>
<div class="description-paragraph" id="p-0158" num="0157">At step <b>804</b>, the digital photographic system <b>300</b> captures (i.e., samples) two or more images under ambient illumination. In one embodiment, the two or more images include a chrominance image <b>202</b> from a first image sensor <b>332</b>(<b>0</b>) and a luminance image <b>204</b> from a second image sensor <b>332</b>(<b>1</b>) that form an ambient image pair. The ambient image pair may be captured using a first set of exposure parameters.</div>
<div class="description-paragraph" id="p-0159" num="0158">In one embodiment, the two or more images may also include additional ambient image pairs captured successively using different exposure parameters. For example, a first image pair may be captured using a short exposure time that may produce an underexposed image. Additional image pairs may capture images with increasing exposure times, and a last image pair may be captured using a long exposure time that may produce an overexposed image. These images may form an image set captured under ambient illumination. Furthermore, these images may be combined in any technically feasible HDR blending or combining technique to generate an HDR image, including an HDR image rendered into a lower dynamic range for display. Additionally, these images may be captured using a successive capture rolling shutter technique, whereby complete images are captured at successively higher exposures by an image sensor before the image sensor is reset in preparation for capturing a new set of images.</div>
<div class="description-paragraph" id="p-0160" num="0159">At step <b>806</b>, the digital photographic system <b>300</b> may enable a strobe unit <b>336</b>. The strobe unit <b>336</b> may be enabled at a specific time prior to or concurrent with the capture of an image under strobe illumination. Enabling the strobe unit <b>336</b> should cause the strobe unit <b>336</b> to discharge or otherwise generate strobe illumination. In one embodiment, enabling the strobe unit <b>336</b> includes setting a color for the strobe illumination. The color may be set by specifying an intensity level of each of a red, green, and blue LED to be discharged substantially simultaneously; for example the color may be set in accordance with the white balance parameters.</div>
<div class="description-paragraph" id="p-0161" num="0160">At step <b>808</b>, the digital photographic system <b>300</b> captures (i.e., samples) two or more images under strobe illumination. In one embodiment, the two or more images include a chrominance image <b>202</b> from a first image sensor <b>332</b>(<b>0</b>) and a luminance image <b>204</b> from a second image sensor <b>332</b>(<b>1</b>) that form a strobe image pair. The strobe image pair may be captured using a first set of exposure parameters.</div>
<div class="description-paragraph" id="p-0162" num="0161">In one embodiment, the two or more images may also include additional pairs of chrominance and luminance images captured successively using different exposure parameters. For example, a first image pair may be captured using a short exposure time that may produce an underexposed image. Additional image pairs may capture images with increasing exposure times, and a last image pair may be captured using a long exposure time that may produce an overexposed image. The changing exposure parameters may also include changes to the configuration of the strobe illumination unit <b>336</b>, such as an intensity of the discharge or a color of the discharge. These images may form an image set captured under strobe illumination. Furthermore, these images may be combined in any technically feasible HDR blending or combining technique to generate an HDR image, including an HDR image rendered into a lower dynamic range for display. Additionally, these images may be captured using a successive capture rolling shutter technique, whereby complete images are captured at successively higher exposures by an image sensor before the image sensor is reset in preparation for capturing a new set of images.</div>
<div class="description-paragraph" id="p-0163" num="0162">At step <b>810</b>, the digital photographic system <b>300</b> generates a resulting image from the at least two images sampled under ambient illumination and the at least two images sampled under strobe illumination. In one embodiment, the digital photographic system <b>300</b> blends the chrominance image sampled under ambient illumination with the chrominance image sampled under strobe illumination. In another embodiment, the digital photographic system <b>300</b> blends the luminance image sampled under ambient illumination with the luminance image sampled under strobe illumination. In yet another embodiment, the digital photographic system <b>300</b> may blend a chrominance image sampled under ambient illumination with a chrominance image sampled under strobe illumination to generate a consensus chrominance image, such as through averaging, or weighted averaging. The consensus chrominance image may then be blended with a selected luminance image, the selected luminance image being sampled under ambient illumination or strobe illumination, or a combination of both luminance images.</div>
<div class="description-paragraph" id="p-0164" num="0163">In one embodiment, blending two images may include performing an alpha blend between corresponding pixel values in the two images. In such an embodiment, the alpha blend weight may be determined by one or more pixel attributes (e.g., intensity) of a pixel being blended, and may be further determined by pixel attributes of surrounding pixels. In another embodiment, blending the two images may include, for each pixel in the resulting image, determining whether a corresponding pixel in a first image captured under ambient illumination is underexposed. If the pixel is underexposed, then the pixel in the resulting image is selected from the second image captured under strobe illumination. Blending the two images may also include, for each pixel in the resulting image, determining whether a corresponding pixel in a second image captured under strobe illumination is overexposed. If the pixel is overexposed, then the pixel in the resulting image is selected from the first image captured under ambient illumination. If pixel in the first image is not underexposed and the pixel in the second image is not overexposed, then the pixel in the resulting image is generated based on an alpha blend between corresponding pixel values in the two images. Furthermore, any other blending technique or techniques may be implemented in this context without departing the scope and spirit of embodiments of the present invention.</div>
<div class="description-paragraph" id="p-0165" num="0164">In one embodiment, the at least two images sampled under ambient illumination may include two or more pairs of images sampled under ambient illumination utilizing different exposure parameters. Similarly, the at least two images sampled under strobe illumination may include two or more pairs of images sampled under strobe illumination utilizing different exposure parameters. In such an embodiment, blending the two images may include selecting two pairs of images captured under ambient illumination and selecting two pairs of images captured under strobe illumination. The two pairs of images sampled under ambient illumination may be blended using any technically feasible method to generate a blended pair of images sampled under ambient illumination. Similarly, the two pairs of images sampled under strobe illumination may be blended using any technically feasible method to generate a blended pair of images sampled under strobe illumination. Then, the blended pair of images sampled under ambient illumination may be blended with the blended pair of images sampled under strobe illumination.</div>
<div class="description-paragraph" id="p-0166" num="0165"> <figref idrefs="DRAWINGS">FIG. 9A</figref> illustrates a viewer application <b>910</b> configured to generate a resulting image <b>942</b> based two image sets <b>920</b>, in accordance with one embodiment. A first image set <b>920</b>(<b>0</b>) includes two or more source images <b>922</b>, which may be generated by sampling a first image sensor <b>732</b>(<b>0</b>) of the camera module <b>330</b>. The source images <b>922</b> may correspond to chrominance images. A second image set <b>920</b>(<b>1</b>) includes two or more source images <b>923</b>, which may be generated by sampling a second image sensor <b>732</b>(<b>1</b>) of the camera module <b>330</b>. The source images <b>923</b> may correspond to luminance images. Each source image <b>922</b> in the first image set <b>920</b>(<b>0</b>) has a corresponding source image <b>923</b> in the second image set <b>920</b>(<b>1</b>). In another embodiment, the source images <b>922</b> may be generated by sampling a first portion of photodiodes in an image sensor <b>732</b> and the source images <b>923</b> may be generated by sampling a second portion of photodiodes in the image sensor <b>732</b>.</div>
<div class="description-paragraph" id="p-0167" num="0166">In one embodiment, the resulting image <b>942</b> represents a pair of corresponding source images <b>922</b>(<i>i</i>), <b>923</b>(<i>i</i>) that are selected from the image set <b>920</b>(<b>0</b>) and <b>920</b>(<b>1</b>), respectively, and blended using a color space blend technique, such as the HSV technique described above in conjunction with <figref idrefs="DRAWINGS">FIGS. 1 &amp; 2</figref>. The pair of corresponding source images may be selected according to any technically feasible technique. For example, a given source image <b>922</b> from the first image set <b>920</b>(<b>0</b>) may be selected automatically based on exposure quality. Then, a corresponding source image <b>923</b> from the second image set <b>920</b>(<b>1</b>) may be selected based on the source image <b>922</b> selected in the first image set <b>920</b>(<b>0</b>).</div>
<div class="description-paragraph" id="p-0168" num="0167">Alternatively, a pair of corresponding source images may be selected manually through a UI control <b>930</b>, discussed in greater detail below in <figref idrefs="DRAWINGS">FIG. 9B</figref>. The UI control <b>930</b> generates a selection parameter <b>918</b> that indicates the manual selection. An image processing subsystem <b>912</b> is configured to generate the resulting image <b>942</b> by blending the selected source image <b>922</b> with the corresponding source image <b>923</b>. In certain embodiments, the image processing subsystem <b>912</b> automatically selects a pair of corresponding source images and transmits a corresponding recommendation <b>919</b> to the UI control <b>930</b>. The recommendation <b>919</b> indicates, through the UI control <b>930</b>, which pair of corresponding source images was automatically selected. A user may keep the recommendation or select a different pair of corresponding source images using the UI control <b>930</b>.</div>
<div class="description-paragraph" id="p-0169" num="0168">In an alternative embodiment, viewer application <b>910</b> is configured to combine two or more pairs of corresponding source images to generate a resulting image <b>942</b>. The two or more pairs of corresponding source images may be mutually aligned by the image processing subsystem <b>912</b> prior to being combined. Selection parameter <b>918</b> may include a weight assigned to each of two or more pairs of corresponding source images. The weight may be used to perform a transparency/opacity blend (known as an alpha blend) between two or more pairs of corresponding source images.</div>
<div class="description-paragraph" id="p-0170" num="0169">In certain embodiments, source images <b>922</b>(<b>0</b>) and <b>923</b>(<b>0</b>) are sampled under exclusively ambient illumination, with the strobe unit off. Source image <b>922</b>(<b>0</b>) is generated to be white-balanced, according to any technically feasible white balancing technique. Source images <b>922</b>(<b>1</b>) through <b>922</b>(N−1) as well as corresponding source images <b>923</b>(<b>1</b>) though <b>923</b>(N−1) are sampled under strobe illumination, which may be of a color that is discordant with respect to ambient illumination. Source images <b>922</b>(<b>1</b>) through <b>922</b>(N−1) may be white-balanced according to the strobe illumination color. Discordance in strobe illumination color may cause certain regions to appear incorrectly colored with respect to other regions in common photographic settings. For example, in a photographic scene with foreground subjects predominantly illuminated by white strobe illumination and white-balanced accordingly, background subjects that are predominantly illuminated by incandescent lights may appear excessively orange or even red.</div>
<div class="description-paragraph" id="p-0171" num="0170">In one embodiment, spatial color correction is implemented within image processing subsystem <b>912</b> to match the color of regions within a selected source image <b>922</b> to that of source image <b>922</b>(<b>0</b>). Spatial color correction implements regional color-matching to ambient-illuminated source image <b>922</b>(<b>0</b>). The regions may range in overall scene coverage from individual pixels, to blocks of pixels, to whole frames. In one embodiment, each pixel in a color-corrected image includes a weighted color correction contribution from at least a corresponding pixel and an associated block of pixels.</div>
<div class="description-paragraph" id="p-0172" num="0171">In certain implementations, viewer application <b>910</b> includes an image cache <b>916</b>, configured to include a set of cached images corresponding to the source images <b>922</b>, but rendered to a lower resolution than source images <b>922</b>. The image cache <b>916</b> provides images that may be used to readily and efficiently generate or display resulting image <b>942</b> in response to real-time changes to selection parameter <b>918</b>. In one embodiment, the cached images are rendered to a screen resolution of display unit <b>312</b>. When a user manipulates the UI control <b>930</b> to select a pair of corresponding source images, a corresponding cached image may be displayed on the display unit <b>312</b>. The cached images may represent a down-sampled version of a resulting image <b>942</b> generated based on the selected pair of corresponding source images. Caching images may advantageously reduce power consumption associated with rendering a given corresponding pair of source images for display. Caching images may also improve performance by eliminating a rendering process needed to resize a given corresponding pair of source images for display each time UI control <b>530</b> detects that a user has selected a different corresponding pair of source images.</div>
<div class="description-paragraph" id="p-0173" num="0172"> <figref idrefs="DRAWINGS">FIG. 9B</figref> illustrates an exemplary user interface associated with the viewer application <b>910</b> of <figref idrefs="DRAWINGS">FIG. 9A</figref>, in accordance with one embodiment. The user interface comprises an application window <b>940</b> configured to display the resulting image <b>942</b> based on a position of the UI control <b>930</b>. The viewer application <b>910</b> may invoke the UI control <b>930</b>, configured to generate the selection parameter <b>918</b> based on a position of a control knob <b>934</b>. The recommendation <b>919</b> may determine an initial position of the control knob <b>934</b>, corresponding to a recommended corresponding pair of source images. In one embodiment, the UI control <b>930</b> comprises a linear slider control with a control knob <b>934</b> configured to slide along a slide path <b>932</b>. A user may position the control knob <b>934</b> by performing a slide gesture. For example, the slide gesture may include touching the control knob <b>934</b> in a current position, and sliding the control knob <b>934</b> to a new position. Alternatively, the user may touch along the slide path <b>932</b> to move the control knob <b>934</b> to a new position defined by a location of the touch.</div>
<div class="description-paragraph" id="p-0174" num="0173">In one embodiment, positioning the control knob <b>934</b> into a discrete position <b>936</b> along the slide path <b>932</b> causes the selection parameter <b>918</b> to indicate selection of a source image <b>922</b>(<i>i</i>) in the first image set <b>920</b>(<b>0</b>) and a corresponding source image <b>923</b> in the second image set <b>920</b>(<b>1</b>). For example, a user may move control knob <b>934</b> into discrete position <b>936</b>(<b>3</b>), to indicate that source image <b>922</b>(<b>3</b>) and corresponding source image <b>923</b>(<b>3</b>) are selected. The UI control <b>930</b> then generates selection parameter <b>918</b> to indicate that source image <b>922</b>(<b>3</b>) and corresponding source image <b>923</b>(<b>3</b>) are selected. The image processing subsystem <b>912</b> responds to the selection parameter <b>918</b> by generating the resulting image <b>942</b> based on source image <b>922</b>(<b>3</b>) and corresponding source image <b>923</b>(<b>3</b>). The control knob <b>934</b> may be configured to snap to a closest discrete position <b>936</b> when released by a user withdrawing their finger.</div>
<div class="description-paragraph" id="p-0175" num="0174">In an alternative embodiment, the control knob <b>934</b> may be positioned between two discrete positions <b>936</b> to indicate that resulting image <b>942</b> should be generated based on two corresponding pairs of source images. For example, if the control knob <b>934</b> is positioned between discrete position <b>936</b>(<b>3</b>) and discrete position <b>936</b>(<b>4</b>), then the image processing subsystem <b>912</b> generates resulting image <b>942</b> from source images <b>922</b>(<b>3</b>) and <b>922</b>(<b>4</b>) as well as source images <b>923</b>(<b>3</b>) and <b>923</b>(<b>4</b>). In one embodiment, the image processing subsystem <b>912</b> generates resulting image <b>942</b> by aligning source images <b>922</b>(<b>3</b>) and <b>922</b>(<b>4</b>) as well as source images <b>923</b>(<b>3</b>) and <b>923</b>(<b>4</b>), and performing an alpha-blend between the aligned images according to the position of the control knob <b>934</b>. For example, if the control knob <b>934</b> is positioned to be one quarter of the distance from discrete position <b>936</b>(<b>3</b>) to discrete position <b>936</b>(<b>4</b>) along slide path <b>932</b>, then an aligned image corresponding to source image <b>922</b>(<b>4</b>) may be blended with twenty-five percent opacity (seventy-five percent transparency) over a fully opaque aligned image corresponding to source image <b>922</b>(<b>3</b>).</div>
<div class="description-paragraph" id="p-0176" num="0175">In one embodiment, UI control <b>930</b> is configured to include a discrete position <b>936</b> for each source image <b>922</b> within the first image set <b>920</b>(<b>0</b>). Each image set <b>920</b> stored within the digital photographic system <b>300</b> of <figref idrefs="DRAWINGS">FIG. 3A</figref> may include a different number of source images <b>922</b>, and UI control <b>930</b> may be configured to establish discrete positions <b>936</b> to correspond to the source images <b>922</b> for a given image set <b>920</b>.</div>
<div class="description-paragraph" id="p-0177" num="0176"> <figref idrefs="DRAWINGS">FIG. 9C</figref> illustrates a resulting image <b>942</b> with differing levels of strobe exposure, in accordance with one embodiment. In this example, control knob <b>934</b> is configured to select source images <b>922</b> of <figref idrefs="DRAWINGS">FIG. 9A</figref> sampled under increasing strobe intensity from left to right. When the control knob <b>934</b> is in the left-most position, the selected source image may correspond to source image <b>922</b>(<b>0</b>) captured under ambient illumination. When the control knob <b>934</b> is in the right-most position, the selected source image may correspond to source image <b>922</b>(N−1) captured with strobe illumination. When the control knob <b>934</b> is in an intermediate position, the selected source image may correspond to one of the other source images <b>922</b>(<b>1</b>)-<b>922</b>(<i>N−</i>2).</div>
<div class="description-paragraph" id="p-0178" num="0177">In one embodiment, the source images <b>922</b> may include more than one source image captured under ambient illumination. Source images <b>922</b> may include P images captured under ambient illumination using different exposure parameters. For example, source images <b>922</b> may include four images captured under ambient illumination with increasing exposure times. Similarly, the source images <b>922</b> may include more than one source image captured under strobe illumination.</div>
<div class="description-paragraph" id="p-0179" num="0178">As shown, resulting image <b>942</b>(<b>1</b>) includes an under-exposed subject <b>950</b> sampled under insufficient strobe intensity, resulting image <b>942</b>(<b>2</b>) includes a properly-exposed subject <b>952</b> sampled under appropriate strobe intensity, and resulting image <b>942</b>(<b>3</b>) includes an over-exposed subject <b>954</b> sampled under excessive strobe intensity. A determination of appropriate strobe intensity is sometimes subjective, and embodiments of the present invention advantageously enable a user to subjectively select an image having a desirable or appropriate strobe intensity after a picture has been taken, and without loss of image quality or dynamic range. In practice, a user is able to take what is apparently one photograph by asserting a single shutter-release. The single shutter-release causes the digital photographic system <b>300</b> of <figref idrefs="DRAWINGS">FIG. 3A</figref> to sample multiple images in rapid succession, where each of the multiple images is sampled under varying strobe intensity. In one embodiment, time intervals of less than two-hundred milliseconds are defined herein to establish rapid succession. Again, the multiple images may include both chrominance images and corresponding luminance images. A resulting image set <b>920</b> enables the user to advantageously select a resulting image <b>942</b> later, such as after a particular photographic scene of interest is no longer available. This is in contrast to prior art solutions that conventionally force a user to manually take different photographs and manually adjust strobe intensity over the different photographs. This manual prior art process typically introduces substantial inter-image delay, resulting in a loss of content consistency among sampled images.</div>
<div class="description-paragraph" id="p-0180" num="0179"> <figref idrefs="DRAWINGS">FIG. 9D</figref> illustrates a system for generating a resulting image from a high dynamic range chrominance image and a high dynamic range luminance image, in accordance with one embodiment. The image sets <b>920</b> enable a user to generate a high dynamic range (HDR) image. For example, the sensitivity of an image sensor is limited. While some portions of the scene are bright, other portions may be dim. If the brightly lit portions of the scene are captured within the dynamic range of the image sensor, then the dimly lit portions of the scene may not be captured with sufficient detail (i.e., the signal to noise ratio at low analog values may not allow for sufficient details to be seen). In such cases, the image sets may be utilized to create HDR versions of both the chrominance image and the luminance image. In certain embodiments, luminance images may be sampled at an inherently higher analog dynamic range, and in one embodiment, one luminance image provides an HDR image for luminance.</div>
<div class="description-paragraph" id="p-0181" num="0180">A chrominance HDR module <b>980</b> may access two or more of the source images <b>922</b> to create an HDR chrominance image <b>991</b> with a high dynamic range. Similarly a luminance HDR module <b>990</b> may access two or more of the source images <b>923</b> to create an HDR luminance image <b>992</b> with a high dynamic range. The chrominance HDR module <b>980</b> and the luminance HDR module <b>990</b> may generate HDR images under any feasible technique, including techniques well-known in the art. The image processing subsystem <b>912</b> may then combine the HDR chrominance image <b>991</b> with the HDR luminance image <b>992</b> to generate the resulting image <b>942</b> as described above with respect to a single source image <b>922</b> and a single corresponding source image <b>923</b>.</div>
<div class="description-paragraph" id="p-0182" num="0181">One advantage of the present invention is that a user may photograph a scene using a single shutter release command, and subsequently select an image sampled according to a strobe intensity that best satisfies user aesthetic requirements for the photographic scene. The one shutter release command causes a digital photographic system to rapidly sample a sequence of images with a range of strobe intensity and/or color. For example, twenty or more full-resolution images may be sampled within one second, allowing a user to capture a potentially fleeting photographic moment with the advantage of strobe illumination. Furthermore, the captured images may be captured using one or more image sensors for capturing separate chrominance and luminance information. The chrominance and luminance information may then be blended to produce the resulting images.</div>
<div class="description-paragraph" id="p-0183" num="0182">While various embodiments have been described above with respect to a digital camera <b>302</b> and a mobile device <b>376</b>, any device configured to perform at least one aspect described herein is within the scope and spirit of the present invention. In certain embodiments, two or more digital photographic systems implemented in respective devices are configured to sample corresponding image sets in mutual time synchronization. A single shutter release command may trigger the two or more digital photographic systems.</div>
<div class="description-paragraph" id="p-0184" num="0183">While the foregoing is directed to embodiments of the present invention, other and further embodiments of the invention may be devised without departing from the basic scope thereof, and the scope thereof is determined by the claims that follow.</div>
</div>
</div>
</section><section itemprop="claims" itemscope="">
<h2>Claims (<span itemprop="count">20</span>)</h2>
<div html="" itemprop="content"><div class="claims" lang="EN" load-source="patent-office" mxw-id="PCLM280093046">
<claim-statement>What is claimed is:</claim-statement>
<div class="claim"> <div class="claim" id="CLM-00001" num="00001">
<div class="claim-text">1. A system, comprising:
<div class="claim-text">a first image sensor configured to capture a first image, wherein the first image sensor detects visible light color;</div>
<div class="claim-text">a second image sensor configured to capture a second image and a third image, wherein the second image sensor detects non-visible light intensity; and</div>
<div class="claim-text">an image processing subsystem configured to generate a resulting image by combining the first image, the second image, and the third image;</div>
<div class="claim-text">wherein at least two of the first image, the second image, or the third image is sampled under strobe illumination using a strobe unit.</div>
</div>
</div>
</div> <div class="claim-dependent"> <div class="claim" id="CLM-00002" num="00002">
<div class="claim-text">2. The system of <claim-ref idref="CLM-00001">claim 1</claim-ref>, further comprising:
<div class="claim-text">a first lens that focuses light from a photographic scene onto the first image sensor, and</div>
<div class="claim-text">a second lens that focuses light from the photographic scene onto the second image sensor.</div>
</div>
</div>
</div> <div class="claim-dependent"> <div class="claim" id="CLM-00003" num="00003">
<div class="claim-text">3. The system of <claim-ref idref="CLM-00001">claim 1</claim-ref>, further comprising a lens that focuses light from a photographic scene onto a beam splitter that is configured to split the light from the photographic scene onto the first image sensor and the second image sensor.</div>
</div>
</div> <div class="claim-dependent"> <div class="claim" id="CLM-00004" num="00004">
<div class="claim-text">4. The system of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the first image sensor comprises a first portion of photodiodes included in a photodiode array, and the second image sensor comprises a second portion of photodiodes included in the photodiode array.</div>
</div>
</div> <div class="claim-dependent"> <div class="claim" id="CLM-00005" num="00005">
<div class="claim-text">5. The system of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the first image sensor and the second image sensor sample images utilizing a global shutter operation.</div>
</div>
</div> <div class="claim-dependent"> <div class="claim" id="CLM-00006" num="00006">
<div class="claim-text">6. The system of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the first image sensor and the second image sensor sample images utilizing a rolling shutter operation.</div>
</div>
</div> <div class="claim-dependent"> <div class="claim" id="CLM-00007" num="00007">
<div class="claim-text">7. The system of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein one of the first image sensor and the second image sensor samples images utilizing a rolling shutter operation and the other of the first image sensor and the second image sensor samples images utilizing a global shutter operation.</div>
</div>
</div> <div class="claim-dependent"> <div class="claim" id="CLM-00008" num="00008">
<div class="claim-text">8. The system of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the strobe unit used for the strobe illumination includes a red light-emitting diode (LED), a green LED, and a blue LED.</div>
</div>
</div> <div class="claim-dependent"> <div class="claim" id="CLM-00009" num="00009">
<div class="claim-text">9. The system of <claim-ref idref="CLM-00008">claim 8</claim-ref>, wherein the first image sensor is configured to capture the first image under ambient illumination, and the image processing subsystem is configured to analyze the first image to determine a white balance setting, and enable the strobe unit to provide the strobe illumination of a particular color based on the white balance setting.</div>
</div>
</div> <div class="claim-dependent"> <div class="claim" id="CLM-00010" num="00010">
<div class="claim-text">10. The system of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the resulting image is generated by:
<div class="claim-text">for each pixel in the resulting image:
<div class="claim-text">determining a corresponding pixel in the first image, wherein the corresponding pixel in the first image is associated with a color specified by a red value, a green value, and a blue value;</div>
<div class="claim-text">converting the color specified by the red value, the green value, and the blue value into a Hue-Saturation-Value model that specifies the color using a Hue value, a Saturation value, and a Value value;</div>
<div class="claim-text">determining a corresponding pixel in each of the second image and the third image, wherein the corresponding pixel in the second image and the third image is associated with an intensity value; and</div>
<div class="claim-text">combining the intensity value with the Hue value and the Saturation value to specify a color for the pixel in the resulting image.</div>
</div>
</div>
</div>
</div> <div class="claim-dependent"> <div class="claim" id="CLM-00011" num="00011">
<div class="claim-text">11. The system of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the first image sensor, the second image sensor, and the image processing subsystem are included in at least one of a wireless mobile device or a digital camera.</div>
</div>
</div> <div class="claim-dependent"> <div class="claim" id="CLM-00012" num="00012">
<div class="claim-text">12. The system of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein at least one of the second image or the third image includes infrared intensity information or ultraviolet intensity information.</div>
</div>
</div> <div class="claim"> <div class="claim" id="CLM-00013" num="00013">
<div class="claim-text">13. A method, comprising:
<div class="claim-text">receiving a first image from a first image sensor, wherein the first image sensor detects visible light color;</div>
<div class="claim-text">receiving a second image and a third image from a second image sensor, wherein the second image sensor detects non-visible light intensity; and</div>
<div class="claim-text">generating, using an image processing subsystem, a resulting image by combining the first image, the second image, and the third image;</div>
<div class="claim-text">wherein at least two of the first image, the second image, or the third image is sampled under strobe illumination using a strobe unit.</div>
</div>
</div>
</div> <div class="claim-dependent"> <div class="claim" id="CLM-00014" num="00014">
<div class="claim-text">14. The method of <claim-ref idref="CLM-00013">claim 13</claim-ref>, wherein generating the resulting image comprises:
<div class="claim-text">for each pixel in the resulting image:
<div class="claim-text">determining a corresponding pixel in the first image, wherein the corresponding pixel in the first image is associated with a color specified by a red value, a green value, and a blue value;</div>
<div class="claim-text">converting the color specified by the red value, the green value, and the blue value into a Hue-Saturation-Value model that specifies the color using a Hue value, a Saturation value, and a Value value;</div>
<div class="claim-text">determining a corresponding pixel in each of the second image and the third image, wherein the corresponding pixel in the second image and the third image is associated with an intensity value; and</div>
<div class="claim-text">combining the intensity value with the Hue value and the Saturation value to specify a color for the pixel in the resulting image.</div>
</div>
</div>
</div>
</div> <div class="claim-dependent"> <div class="claim" id="CLM-00015" num="00015">
<div class="claim-text">15. The method of <claim-ref idref="CLM-00013">claim 13</claim-ref>, wherein at least one of the second image or the third image includes infrared intensity information or ultraviolet intensity information.</div>
</div>
</div> <div class="claim-dependent"> <div class="claim" id="CLM-00016" num="00016">
<div class="claim-text">16. The method of <claim-ref idref="CLM-00013">claim 13</claim-ref>, wherein a beam splitter directs light from a photographic scene onto the first image sensor and the second image sensor simultaneously.</div>
</div>
</div> <div class="claim"> <div class="claim" id="CLM-00017" num="00017">
<div class="claim-text">17. A computer program product comprising computer executable instructions stored on a non-transitory computer readable medium that when executed by a processor instruct the processor to:
<div class="claim-text">receive a first image from a first image sensor, wherein the first image sensor detects visible light color;</div>
<div class="claim-text">receive a second image and a third image from a second image sensor, wherein the second image sensor detects non-visible light intensity; and</div>
<div class="claim-text">generate, using an image processing subsystem, a resulting image by combining the first image, the second image, and the third image;</div>
<div class="claim-text">wherein at least two of the first image, the second image, or the third image is sampled under strobe illumination using a strobe unit.</div>
</div>
</div>
</div> <div class="claim-dependent"> <div class="claim" id="CLM-00018" num="00018">
<div class="claim-text">18. The computer program product of <claim-ref idref="CLM-00017">claim 17</claim-ref>, wherein at least one of the second image or the third image includes infrared intensity information or ultraviolet intensity information.</div>
</div>
</div> <div class="claim-dependent"> <div class="claim" id="CLM-00019" num="00019">
<div class="claim-text">19. The computer program product of <claim-ref idref="CLM-00017">claim 17</claim-ref>, wherein a beam splitter directs light from a photographic scene onto the first image sensor and the second image sensor simultaneously.</div>
</div>
</div> <div class="claim"> <div class="claim" id="CLM-00020" num="00020">
<div class="claim-text">20. A system, comprising:
<div class="claim-text">a first image sensor configured to capture a first image, wherein the first image sensor detects wavelengths of a visible spectrum;</div>
<div class="claim-text">a second image sensor configured to capture a second image and a third image, wherein the second image sensor detects wavelengths of a non-visible spectrum; and</div>
<div class="claim-text">an image processing subsystem configured to generate a resulting image by combining the first image, the second image, and the third image;</div>
<div class="claim-text">wherein at least two of the first image, the second image, or the third image is sampled under strobe illumination using a strobe unit.</div>
</div>
</div>
</div> </div>
</div>
</section>
                </article>
            </search-app>
        </body>
    </html>
    