
    <html>
        <body>
            <search-app>
                <article class="result" itemscope="" itemtype="http://schema.org/ScholarlyArticle">
    <h1 itemprop="pageTitle">US10955355B2 - Systems and methods for monitoring remote installations 
        - Google Patents</h1><section itemprop="abstract" itemscope="">
<h2>Abstract</h2>
<div html="" itemprop="content"><abstract lang="EN" load-source="patent-office" mxw-id="PA439079841">
<div class="abstract" id="p-0001" num="0000">A system for monitoring a petrochemical installation is disclosed. The system can include an optical imaging system comprising an array of optical detectors. The system can comprise processing electronics configured to process image data detected by the optical imaging system. The processing electronics can be configured to detect a target species based at least in part on the processed image data. The processing electronics can further be configured to, based on a detected amount of the target species, transmit an alarm notification to an external computing device over a communications network indicating that the target species has been detected at the petrochemical installation.</div>
</abstract>
</div>
</section><section itemprop="description" itemscope="">
<h2>Description</h2>
<div html="" itemprop="content"><div class="description" lang="EN" load-source="patent-office" mxw-id="PDES288071878">
<heading id="h-0001">CROSS-REFERENCE TO RELATED APPLICATIONS</heading>
<div class="description-paragraph" id="p-0002" num="0001">This application claims priority to U.S. Provisional Patent Application No. 62/462,345, filed Feb. 22, 2017, entitled “MONITORING SYSTEM FOR REMOTE FACILITIES;” and U.S. Provisional Patent Application No. 62/462,851, filed Feb. 23, 2017, entitled “SYSTEMS AND METHODS FOR MONITORING REMOTE INSTALLATIONS;” the entire contents of each of which are hereby incorporated by reference herein in their entirety and for all purposes.</div>
<heading id="h-0002">FIELD OF THE INVENTION</heading>
<div class="description-paragraph" id="p-0003" num="0002">The present invention generally relates to systems and methods for monitoring remote installations and, in particular, to systems and methods for monitoring fluid leaks at petrochemical installations or facilities.</div>
<heading id="h-0003">DESCRIPTION OF THE RELATED TECHNOLOGY</heading>
<div class="description-paragraph" id="p-0004" num="0003">Many petroleum installations (such as drilling or processing sites) may be located in remote locations that are distant from central management offices and monitoring facilities. In these remote installations, chemicals may leak, which can present risks to human users at the installation and/or a reduction in the efficiency of petroleum collection. Accordingly, there remains a continuing need for improved monitoring efforts at petroleum installations.</div>
<heading id="h-0004">SUMMARY</heading>
<div class="description-paragraph" id="p-0005" num="0004">The systems, methods and devices of this disclosure each have several innovative aspects, no single one of which is solely responsible for the desirable attributes disclosed herein.</div>
<div class="description-paragraph" id="p-0006" num="0005">Various examples of optical devices comprising grating structures and their methods of manufacturing are described herein such as the examples enumerated below:</div>
<heading id="h-0005">Example 1</heading>
<div class="description-paragraph" id="p-0007" num="0006">A system for monitoring a petrochemical installation, the system comprising:
</div> <ul> <li id="ul0001-0001" num="0000"> <ul> <li id="ul0002-0001" num="0007">an optical imaging system comprising an array of optical detectors; and</li> <li id="ul0002-0002" num="0008">processing electronics configured to process image data detected by the optical imaging system, the processing electronics configured to:</li> <li id="ul0002-0003" num="0009">detect a target species based at least in part on the processed image data; and</li> <li id="ul0002-0004" num="0010">based on a detected amount of the target species, transmit an alarm notification to an external computing device over a communications network indicating that the target species has been detected at the petrochemical installation.</li> </ul> </li> </ul>
<heading id="h-0006">Example 2</heading>
<div class="description-paragraph" id="p-0008" num="0011">The system of Example 1, wherein the processing electronics are configured to detect the target species over multiple frames of the image data and to combine the multiple frames of image data into a summary alarm image that presents the detection of the target species over a period of time.</div>
<heading id="h-0007">Example 3</heading>
<div class="description-paragraph" id="p-0009" num="0012">The system of Example 2, wherein the summary alarm image comprises a single image.</div>
<heading id="h-0008">Example 4</heading>
<div class="description-paragraph" id="p-0010" num="0013">The system of Example 2, wherein the summary alarm image comprises a plurality of images.</div>
<heading id="h-0009">Example 5</heading>
<div class="description-paragraph" id="p-0011" num="0014">The system of any one of Examples 2 to 4, wherein the processing electronics are configured to create the summary alarm image by calculating an average concentration of the target species and/or an average of the image data representative of the concentration over the period of time.</div>
<heading id="h-0010">Example 6</heading>
<div class="description-paragraph" id="p-0012" num="0015">The system of any one of Examples 2 to 5, wherein the processing electronics are configured to generate a progressive mode to sequentially present summary alarm images for successive stored fields of view (FOVs).</div>
<heading id="h-0011">Example 7</heading>
<div class="description-paragraph" id="p-0013" num="0016">The system of any one of Examples 1 to 6, wherein the processing electronics are configured to create an events log comprising a plurality of events comprising one or more target species detected by the processing electronics.</div>
<heading id="h-0012">Example 8</heading>
<div class="description-paragraph" id="p-0014" num="0017">The system of Example 7, wherein the processing electronics are configured to analyze the events log, and based on the analysis, to transmit a priority ranking of events to the external computing device.</div>
<heading id="h-0013">Example 9</heading>
<div class="description-paragraph" id="p-0015" num="0018">The system of any one of Examples 7 to 8, wherein the processing electronics are configured to associate multiple events with one another and to form a group of the associated multiple events.</div>
<heading id="h-0014">Example 10</heading>
<div class="description-paragraph" id="p-0016" num="0019">The system of Example 9, wherein the processing electronics are configured to form the group of the associated multiple events based at least in part on at least one of event type, type of the detected one or more target species, event time, and a field of view (FOV) in which the one or more target species has been detected.</div>
<heading id="h-0015">Example 11</heading>
<div class="description-paragraph" id="p-0017" num="0020">The system of any one of Examples 1 to 10, wherein the processing electronics are configured to compare the detected amount of the target species to a threshold amount and, based on that comparison, transmit the alarm notification to the external computing device over the communications network indicating that the target species has been detected at the petrochemical installation.</div>
<heading id="h-0016">Example 12</heading>
<div class="description-paragraph" id="p-0018" num="0021">The system of Example 11, wherein the threshold amount is in a range of 1 ppm-m to 1000 ppm-m of the target species.</div>
<heading id="h-0017">Example 13</heading>
<div class="description-paragraph" id="p-0019" num="0022">The system of Example 12, wherein the threshold amount is in a range of 25 ppm-m to 1000 ppm-m of the target species.</div>
<heading id="h-0018">Example 14</heading>
<div class="description-paragraph" id="p-0020" num="0023">The system of Example 13, wherein the threshold amount is in a range of 25 ppm-m to 750 ppm-m of the target species.</div>
<heading id="h-0019">Example 15</heading>
<div class="description-paragraph" id="p-0021" num="0024">The system of any one of Examples 1 to 14, wherein the target species comprises methane.</div>
<heading id="h-0020">Example 16</heading>
<div class="description-paragraph" id="p-0022" num="0025">The system of any one of Examples 1 to 15, wherein the target species comprises hydrogen sulfide.</div>
<heading id="h-0021">Example 17</heading>
<div class="description-paragraph" id="p-0023" num="0026">The system of any one of Examples 1 to 16, wherein the target species comprises a gas.</div>
<heading id="h-0022">Example 18</heading>
<div class="description-paragraph" id="p-0024" num="0027">The system of any one of Examples 1 to 16, wherein the target species comprises a liquid.</div>
<heading id="h-0023">Example 19</heading>
<div class="description-paragraph" id="p-0025" num="0028">The system of any one of Examples 1 to 18, wherein the processing electronics are configured to detect an unauthorized intrusion of an animal (including a human) into the petroleum installation and, based on the detection, to transmit a second alarm notification to the external computing device over the communications network indicating the unauthorized intrusion.</div>
<heading id="h-0024">Example 20</heading>
<div class="description-paragraph" id="p-0026" num="0029">The system of any one of Examples 1 to 19, wherein the communications network comprises a wireless communications network.</div>
<heading id="h-0025">Example 21</heading>
<div class="description-paragraph" id="p-0027" num="0030">The system of Example 20, wherein the wireless communications network comprises a cellular communications network.</div>
<heading id="h-0026">Example 22</heading>
<div class="description-paragraph" id="p-0028" num="0031">The system of Example 21 wherein the wireless communications network is configured to transmit processed image data to the external computing device at speeds in a range of 0.1 Mbps to 10 Mbps.</div>
<heading id="h-0027">Example 23</heading>
<div class="description-paragraph" id="p-0029" num="0032">The system of Example 22, wherein the wireless communications network is configured to transmit processed image data to the external computing device at speeds in a range of 0.5 Mbps to 2 Mbps.</div>
<heading id="h-0028">Example 24</heading>
<div class="description-paragraph" id="p-0030" num="0033">The system of any one of Examples 1 to 23, wherein the communications network comprises an Ethernet communications network.</div>
<heading id="h-0029">Example 25</heading>
<div class="description-paragraph" id="p-0031" num="0034">The system of any one of Examples 1 to 24, wherein the processing electronics are configured to transmit information about one or more events detected at the petrochemical installation to the external computing device, the external computing device configured to generate a user interface presentable to a user on a display device.</div>
<heading id="h-0030">Example 26</heading>
<div class="description-paragraph" id="p-0032" num="0035">The system of Example 25, wherein the user interface comprises a visible image window and an infrared image window.</div>
<heading id="h-0031">Example 27</heading>
<div class="description-paragraph" id="p-0033" num="0036">The system of any one of Examples 1 to 26, wherein the optical imaging system comprises an infrared (IR) detector array.</div>
<heading id="h-0032">Example 28</heading>
<div class="description-paragraph" id="p-0034" num="0037">The system of any one of Examples 1 to 27, wherein the optical imaging system comprises a visible light detector array.</div>
<heading id="h-0033">Example 29</heading>
<div class="description-paragraph" id="p-0035" num="0038">The system of any one of Examples 27 to 28, wherein the optical imaging system defines a plurality of optical channels being spatially and spectrally different from one another, each of the plurality of optical channels positioned to transfer radiation incident on the optical imaging system towards the array of optical detectors.</div>
<heading id="h-0034">Example 30</heading>
<div class="description-paragraph" id="p-0036" num="0039">The system of Example 29, wherein the optical imaging system and the processing electronics are contained together in a data acquisition and processing module configured to be worn or carried by a person.</div>
<heading id="h-0035">Example 31</heading>
<div class="description-paragraph" id="p-0037" num="0040">The system of Example 29, wherein the optical imaging system and the processing electronics are configured to be mounted to a support structure at the petroleum installation.</div>
<heading id="h-0036">Example 32</heading>
<div class="description-paragraph" id="p-0038" num="0041">The system of any one of Examples 1 to 31, wherein the optical system comprises a plurality of spectrally distinct infrared optical filters.</div>
<heading id="h-0037">Example 33</heading>
<div class="description-paragraph" id="p-0039" num="0042">The system of any one of Examples 1 to 32, wherein the processing electronics are configured to monitor a progression of a liquid leak over a period of time.</div>
<heading id="h-0038">Example 34</heading>
<div class="description-paragraph" id="p-0040" num="0043">The system of Example 33, wherein the processing electronics are configured to generate a color map of the progression of the liquid leak based on a length of residence time of the liquid leak at locations of the petrochemical installation.</div>
<heading id="h-0039">Example 35</heading>
<div class="description-paragraph" id="p-0041" num="0044">The system of any one of Examples 1 to 34, wherein at least a portion of the processing electronics are located remote from the optical imaging system.</div>
<heading id="h-0040">Example 36</heading>
<div class="description-paragraph" id="p-0042" num="0045">The system of any one of Examples 1 to 35, wherein at least a portion of the processing electronics are located on a monitoring computer system.</div>
<heading id="h-0041">Example 37</heading>
<div class="description-paragraph" id="p-0043" num="0046">The system of any one of Examples 1 to 36, wherein the processing electronics are configured to generate a system overview image that illustrates locations of a plurality of optical imaging systems at the petrochemical installation, each optical imaging system of the plurality of optical imaging systems associated with an identifier.</div>
<heading id="h-0042">Example 38</heading>
<div class="description-paragraph" id="p-0044" num="0047">The system of any one of Examples 1 to 37, wherein the processing electronics are configured to generate a multi-view image that illustrates image data captured by multiple optical imaging systems at multiple sites of one or a plurality of petrochemical installations.</div>
<heading id="h-0043">Example 39</heading>
<div class="description-paragraph" id="p-0045" num="0048">The system of any one of Examples 1 to 38, wherein the processing electronics are configured to generate a mosaic image comprising a plurality of fields of view (FOVs) of the optical imaging system at the petrochemical installation.</div>
<heading id="h-0044">Example 40</heading>
<div class="description-paragraph" id="p-0046" num="0049">A system for monitoring one or more installations, the system comprising:
</div> <ul> <li id="ul0003-0001" num="0000"> <ul> <li id="ul0004-0001" num="0050">a communications module configured to receive data from one or more optical imaging systems at the one or more installations, the one or more optical imaging systems configured to capture infrared image data at the one or more installations; and</li> <li id="ul0004-0002" num="0051">processing electronics configured to, based on a detected amount of a target species, transmit an alarm notification to an external computing device over a communications network indicating that the target species has been detected at the one or more installations.</li> </ul> </li> </ul>
<heading id="h-0045">Example 41</heading>
<div class="description-paragraph" id="p-0047" num="0052">The system of Example 40, wherein the processing electronics are configured to detect a target species based at least in part on the infrared image data.</div>
<heading id="h-0046">Example 42</heading>
<div class="description-paragraph" id="p-0048" num="0053">The system of any one of Examples 40 to 41, wherein the processing electronics are configured to transmit data to the one or more optical imaging systems.</div>
<heading id="h-0047">Example 43</heading>
<div class="description-paragraph" id="p-0049" num="0054">The system of any one of Examples 40 to 42, wherein the processing electronics are configured to combine multiple frames of infrared image data into a summary alarm image that presents the detection of the target species over a period of time.</div>
<heading id="h-0048">Example 44</heading>
<div class="description-paragraph" id="p-0050" num="0055">The system of Example 43, wherein the summary alarm image comprises a single image.</div>
<heading id="h-0049">Example 45</heading>
<div class="description-paragraph" id="p-0051" num="0056">The system of Example 43, wherein the summary alarm image comprises a plurality of images.</div>
<heading id="h-0050">Example 46</heading>
<div class="description-paragraph" id="p-0052" num="0057">The system of any one of Examples 43 to 45, wherein the processing electronics are configured to create the summary alarm image by calculating an average concentration of the target species and/or an average of the image data representative of the concentration over the period of time.</div>
<heading id="h-0051">Example 47</heading>
<div class="description-paragraph" id="p-0053" num="0058">The system of any one of Examples 43 to 46, wherein the processing electronics are configured to generate a progressive mode to sequentially present summary alarm images for successive fields of view (FOVs) of an optical imaging system of the one or more optical imaging systems.</div>
<heading id="h-0052">Example 48</heading>
<div class="description-paragraph" id="p-0054" num="0059">The system of any one of Examples 40 to 47, wherein the processing electronics are configured to create an events log comprising a plurality of events comprising one or more target species detected by the processing electronics.</div>
<heading id="h-0053">Example 49</heading>
<div class="description-paragraph" id="p-0055" num="0060">The system of Example 48, wherein the processing electronics are configured to analyze the events log, and based on the analysis, to transmit a priority ranking of events to the external computing device.</div>
<heading id="h-0054">Example 50</heading>
<div class="description-paragraph" id="p-0056" num="0061">The system of any one of Examples 40 to 49, wherein the processing electronics are configured to compare the detected amount of the target species to a threshold amount and, based on that comparison, transmit the alarm notification to the external computing device over the communications network indicating that the target species has been detected at the one or more installations.</div>
<heading id="h-0055">Example 51</heading>
<div class="description-paragraph" id="p-0057" num="0062">The system of Example 50, wherein the threshold amount is in a range of 1 ppm-m to 1000 ppm-m of the target species.</div>
<heading id="h-0056">Example 52</heading>
<div class="description-paragraph" id="p-0058" num="0063">The system of Example 51, wherein the threshold amount is in a range of 25 ppm-m to 1000 ppm-m of the target species.</div>
<heading id="h-0057">EXAMPLE</heading>
<div class="description-paragraph" id="p-0059" num="0064">The system of Example 52, wherein the threshold amount is in a range of 25 ppm-m to 750 ppm-m of the target species.</div>
<heading id="h-0058">Example 54</heading>
<div class="description-paragraph" id="p-0060" num="0065">The system of any one of Examples 40 to 53, wherein the target species comprises methane.</div>
<heading id="h-0059">Example 55</heading>
<div class="description-paragraph" id="p-0061" num="0066">The system of any one of Examples 40 to 54, wherein the target species comprises hydrogen sulfide.</div>
<heading id="h-0060">Example 56</heading>
<div class="description-paragraph" id="p-0062" num="0067">The system of any one of Examples 40 to 55, wherein the target species comprises a gas.</div>
<heading id="h-0061">Example 57</heading>
<div class="description-paragraph" id="p-0063" num="0068">The system of any one of Examples 40 to 56, wherein the target species comprises a liquid.</div>
<heading id="h-0062">Example 58</heading>
<div class="description-paragraph" id="p-0064" num="0069">The system of any one of Examples 40 to 57, wherein the processing electronics are configured to detect an unauthorized intrusion of an animal (including a human) into the one or more petroleum installations and, based on the detection, to transmit a second alarm notification to the external computing device over the communications network indicating the unauthorized intrusion.</div>
<heading id="h-0063">Example 59</heading>
<div class="description-paragraph" id="p-0065" num="0070">The system of any one of Examples 40 to 58, wherein the communications network comprises a wireless communications network.</div>
<heading id="h-0064">Example 60</heading>
<div class="description-paragraph" id="p-0066" num="0071">The system of Example 59, wherein the wireless communications network comprises a cellular communications network.</div>
<heading id="h-0065">Example 61</heading>
<div class="description-paragraph" id="p-0067" num="0072">The system of Example 60, wherein the wireless communications network is configured to transmit processed image data to the external computing device at speeds in a range of 0.1 Mbps to 10 Mbps.</div>
<heading id="h-0066">Example 62</heading>
<div class="description-paragraph" id="p-0068" num="0073">The system of Example 61, wherein the wireless communications network is configured to transmit processed image data to the external computing device at speeds in a range of 0.5 Mbps to 2 Mbps.</div>
<heading id="h-0067">Example 63</heading>
<div class="description-paragraph" id="p-0069" num="0074">The system of any one of Examples 40 to 58, wherein the communications network comprises an Ethernet communications network</div>
<heading id="h-0068">Example 64</heading>
<div class="description-paragraph" id="p-0070" num="0075">The system of any one of Examples 40 to 63, wherein the processing electronics are configured to transmit information about one or more events detected at the one or more installations to the external computing device, the external computing device configured to generate a user interface presentable to a user on a display device.</div>
<heading id="h-0069">Example 65</heading>
<div class="description-paragraph" id="p-0071" num="0076">The system of Example 64, wherein the user interface comprises a visible image window and an infrared image window.</div>
<heading id="h-0070">Example 66</heading>
<div class="description-paragraph" id="p-0072" num="0077">The system of any one of Examples 40 to 65, further comprising the one or more optical imaging systems.</div>
<heading id="h-0071">Example 67</heading>
<div class="description-paragraph" id="p-0073" num="0078">The system of any one of Examples 40 to 66, wherein the one or more optical imaging systems comprise an infrared (IR) detector array.</div>
<heading id="h-0072">Example 68</heading>
<div class="description-paragraph" id="p-0074" num="0079">The system of any one of Examples 40 to 67, wherein the one or more optical imaging systems comprise a visible light detector array.</div>
<heading id="h-0073">Example 69</heading>
<div class="description-paragraph" id="p-0075" num="0080">The system of any one of Examples 66 to 68, wherein the one or more optical imaging systems define a plurality of optical channels being spatially and spectrally different from one another, each of the plurality of optical channels positioned to transfer radiation incident on the optical imaging system towards an array of optical detectors.</div>
<heading id="h-0074">Example 70</heading>
<div class="description-paragraph" id="p-0076" num="0081">The system of Example 69, wherein the one or more optical imaging systems and the processing electronics are contained together in respective data acquisition and processing modules configured to be worn or carried by a person.</div>
<heading id="h-0075">Example 71</heading>
<div class="description-paragraph" id="p-0077" num="0082">The system of Example 69, wherein the one or more optical imaging systems and the processing electronics are configured to be mounted to respective support structures at the one or more petroleum installations.</div>
<heading id="h-0076">Example 72</heading>
<div class="description-paragraph" id="p-0078" num="0083">The system of any one of Examples 40 to 71, wherein the one or more optical imaging systems comprise a plurality of spectrally distinct infrared optical filters.</div>
<heading id="h-0077">Example 73</heading>
<div class="description-paragraph" id="p-0079" num="0084">The system of any one of Examples 40 to 72, wherein the processing electronics are configured to monitor a progression of a liquid leak over a period of time.</div>
<heading id="h-0078">Example 74</heading>
<div class="description-paragraph" id="p-0080" num="0085">The system of Example 73, wherein the processing electronics are configured to generate a color map of the progression of the liquid leak based on a length of residence time of the liquid leak at locations of the one or more installations.</div>
<heading id="h-0079">Example 75</heading>
<div class="description-paragraph" id="p-0081" num="0086">The system of any one of Examples 40 to 74, wherein at least a portion of the processing electronics are located remote from the one or more optical imaging systems.</div>
<heading id="h-0080">Example 76</heading>
<div class="description-paragraph" id="p-0082" num="0087">The system of any one of Examples 40 to 75, wherein the processing electronics \ are configured to generate a system overview image that illustrates locations of a plurality of optical imaging systems at the one or more installations, each optical imaging system of the plurality of optical imaging systems associated with an identifier.</div>
<heading id="h-0081">Example 77</heading>
<div class="description-paragraph" id="p-0083" num="0088">The system of any one of Examples 40 to 76, wherein the processing electronics are configured to generate a multi-view image that illustrates image data captured by multiple optical imaging systems at multiple sites of the one or a plurality of installations.</div>
<heading id="h-0082">Example 78</heading>
<div class="description-paragraph" id="p-0084" num="0089">The system of any one of Examples 40 to 77, wherein the processing electronics are configured to generate a mosaic image comprising a plurality of fields of view (FOVs) of an optical imaging system of the one or more optical imaging systems at the installation.</div>
<heading id="h-0083">Example 79</heading>
<div class="description-paragraph" id="p-0085" num="0090">A system for monitoring an installation, the system comprising:
</div> <ul> <li id="ul0005-0001" num="0000"> <ul> <li id="ul0006-0001" num="0091">an optical imaging system comprising an array of optical detectors; and processing electronics configured to process image data detected by the optical imaging system, the processing electronics configured to:</li> <li id="ul0006-0002" num="0092">detect a target species based at least in part on the processed image data over multiple frames of the processed image data; and</li> <li id="ul0006-0003" num="0093">combine the multiple frames of processed image data into a summary alarm image that presents the detection of the target species over a period of time.</li> </ul> </li> </ul>
<heading id="h-0084">Example 80</heading>
<div class="description-paragraph" id="p-0086" num="0094">The system of Example 79, wherein the summary alarm image comprises a single image.</div>
<heading id="h-0085">Example 81</heading>
<div class="description-paragraph" id="p-0087" num="0095">The system of any one of Examples 79 to 80, wherein the processing electronics are configured to create the summary alarm image by calculating an average concentration of the target species and/or an average of the image data representative of the concentration over the period of time.</div>
<heading id="h-0086">Example 82</heading>
<div class="description-paragraph" id="p-0088" num="0096">The system of any one of Examples 79 to 81, wherein the processing electronics are configured to generate a progressive mode to sequentially present summary alarm images for successive fields of view (FOVs) of the optical imaging system.</div>
<heading id="h-0087">Example 83</heading>
<div class="description-paragraph" id="p-0089" num="0097">The system of any one of Examples 79 to 82, wherein the processing electronics are configured to create an events log comprising a plurality of events comprising one or more target species detected by the processing electronics.</div>
<heading id="h-0088">Example 84</heading>
<div class="description-paragraph" id="p-0090" num="0098">The system of Example 83, wherein the processing electronics are configured to analyze the events log, and based on the analysis, to generate a priority ranking of events.</div>
<heading id="h-0089">Example 85</heading>
<div class="description-paragraph" id="p-0091" num="0099">The system of any one of Examples 79 to 84, wherein the processing electronics are configured to generate a mosaic image comprising a plurality of fields of view (FOVs) of the optical imaging system at the installation.</div>
<heading id="h-0090">Example 86</heading>
<div class="description-paragraph" id="p-0092" num="0100">The system of any one of Examples 79 to 85, wherein the processing electronics are configured to monitor a progression of a liquid leak over a period of time.</div>
<heading id="h-0091">Example 87</heading>
<div class="description-paragraph" id="p-0093" num="0101">The system of Example 86, wherein the processing electronics are configured to generate a color map of the progression of the liquid leak based on a length of residence time of the liquid leak at locations of the installation.</div>
<heading id="h-0092">Example 88</heading>
<div class="description-paragraph" id="p-0094" num="0102">The system of any one of Examples 79 to 87, wherein at least a portion of the processing electronics are located remote from the optical imaging system.</div>
<heading id="h-0093">Example 89</heading>
<div class="description-paragraph" id="p-0095" num="0103">The system of any one of Examples 79 to 88, wherein the processing electronics are configured to generate a system overview image that illustrates locations of a plurality of optical imaging systems at the installation, each optical imaging system of the plurality of optical imaging systems associated with an identifier.</div>
<heading id="h-0094">Example 90</heading>
<div class="description-paragraph" id="p-0096" num="0104">The system of any one of Examples 79 to 89, wherein the processing electronics are configured to generate a multi-view image that illustrates image data captured by multiple optical imaging systems at multiple sites of one or a plurality of installations.</div>
<heading id="h-0095">Example 91</heading>
<div class="description-paragraph" id="p-0097" num="0105">A system for monitoring an installation, the system comprising:
</div> <ul> <li id="ul0007-0001" num="0000"> <ul> <li id="ul0008-0001" num="0106">an optical imaging system comprising an array of optical detectors; and</li> <li id="ul0008-0002" num="0107">processing electronics configured to process infrared image data detected by the optical imaging system, the processing electronics configured to:</li> <li id="ul0008-0003" num="0108">detect a target species based at least in part on the processed image data, the target species comprising a liquid leak at the installation; and</li> <li id="ul0008-0004" num="0109">monitor a progression of the liquid leak over a period of time.</li> </ul> </li> </ul>
<heading id="h-0096">Example 92</heading>
<div class="description-paragraph" id="p-0098" num="0110">The system of Example 91, wherein the processing electronics are configured to generate a color map of the progression of the liquid leak based on a length of residence time of the liquid leak at locations of the one or more installations.</div>
<heading id="h-0097">Example 93</heading>
<div class="description-paragraph" id="p-0099" num="0111">A system for monitoring one or more installations, the system comprising:
</div> <ul> <li id="ul0009-0001" num="0000"> <ul> <li id="ul0010-0001" num="0112">a plurality of optical imaging systems, each optical imaging system of the plurality of optical imaging systems comprising an array of optical detectors; and</li> <li id="ul0010-0002" num="0113">processing electronics configured to process image data detected by the plurality of optical imaging systems, the processing electronics configured to:</li> <li id="ul0010-0003" num="0114">detect one or more target species at the one or more installations based at least in part on the processed image data;</li> <li id="ul0010-0004" num="0115">generate a system overview image that illustrates locations of the plurality of optical imaging systems at the one or more installations, each optical imaging system of the plurality of optical imaging systems associated with an identifier; and</li> <li id="ul0010-0005" num="0116">associate the location of the optical imaging system at which the target species has been detected with the identifier.</li> </ul> </li> </ul>
<heading id="h-0098">Example 94</heading>
<div class="description-paragraph" id="p-0100" num="0117">The system of Example 93, wherein, based on a detected amount of the one or more target species, the processing electronics are configured to transmit an alarm notification to an external computing device over a communications network indicating that the one or more target species has been detected at the one or more installations.</div>
<heading id="h-0099">Example 95</heading>
<div class="description-paragraph" id="p-0101" num="0118">The system of Example 94, wherein the processing electronics are configured to notify the external computing device of the location of the optical imaging system at which the one or more target species has been detected.</div>
<heading id="h-0100">Example 96</heading>
<div class="description-paragraph" id="p-0102" num="0119">A system for monitoring one or more installations, the system comprising:
</div> <ul> <li id="ul0011-0001" num="0000"> <ul> <li id="ul0012-0001" num="0120">a plurality of optical imaging systems, each optical imaging system of the plurality of optical imaging systems comprising an array of optical detectors; and</li> <li id="ul0012-0002" num="0121">processing electronics configured to process image data detected by the plurality of optical imaging systems, the processing electronics configured to:</li> <li id="ul0012-0003" num="0122">detect one or more target species based at least in part on the processed image data;</li> <li id="ul0012-0004" num="0123">generate a multi-view image that illustrates image data captured by the plurality of optical imaging systems at a plurality of locations of the one or more installations; and</li> <li id="ul0012-0005" num="0124">associate the location of the optical imaging system at which the target species has been detected with an identifier.</li> </ul> </li> </ul>
<heading id="h-0101">Example 97</heading>
<div class="description-paragraph" id="p-0103" num="0125">The system of Example 96, wherein, based on a detected amount of the one or more target species, the processing electronics are configured to transmit an alarm notification to an external computing device over a communications network indicating that the one or more target species has been detected at the one or more installations.</div>
<heading id="h-0102">Example 98</heading>
<div class="description-paragraph" id="p-0104" num="0126">The system of Example 97, wherein the processing electronics are configured to notify the external computing device of the location of the optical imaging system at which the one or more target species has been detected.</div>
<heading id="h-0103">Example 99</heading>
<div class="description-paragraph" id="p-0105" num="0127">A system for monitoring one or more installations, the system comprising:
</div> <ul> <li id="ul0013-0001" num="0000"> <ul> <li id="ul0014-0001" num="0128">an optical imaging system comprising an array of optical detectors; and</li> <li id="ul0014-0002" num="0129">processing electronics configured to process image data detected by the optical imaging system, the processing electronics configured to:</li> <li id="ul0014-0003" num="0130">detect a target species based at least in part on the processed image data;</li> <li id="ul0014-0004" num="0131">generate a mosaic image comprising a plurality of fields of view (FOVs) of the optical imaging system at the petrochemical installation; and</li> <li id="ul0014-0005" num="0132">identify a field of view of the plurality of FOVs at which the target species has been detected.</li> </ul> </li> </ul>
<heading id="h-0104">Example 100</heading>
<div class="description-paragraph" id="p-0106" num="0133">The system of Example 99, wherein, based on a detected amount of the target species, the processing electronics are configured to transmit an alarm notification to an external computing device over a communications network indicating that the target species has been detected at the one or more installations.</div>
<heading id="h-0105">Example 101</heading>
<div class="description-paragraph" id="p-0107" num="0134">The system of Example 100, wherein the processing electronics are configured to notify the external computing device of the field of view at which the one or more target species has been detected.</div>
<heading id="h-0106">Example 102</heading>
<div class="description-paragraph" id="p-0108" num="0135">A system for monitoring one or more installations, the system comprising:
</div> <ul> <li id="ul0015-0001" num="0000"> <ul> <li id="ul0016-0001" num="0136">an optical imaging system comprising an array of optical detectors, the array of optical detectors comprising one or more visible image sensors and one or more infrared image sensors; and</li> <li id="ul0016-0002" num="0137">processing electronics configured to process image data detected by the optical imaging system, the processing electronics configured to:</li> <li id="ul0016-0003" num="0138">detect a target species based at least in part on the processed image data;</li> <li id="ul0016-0004" num="0139">generate a visible image and an infrared image from the processed image data, the detected target species rendered on at least one of the visible image and the infrared image; and</li> <li id="ul0016-0005" num="0140">generate a user interface that simultaneously illustrates the visible image and the infrared image.</li> </ul> </li> </ul>
<heading id="h-0107">Example 103</heading>
<div class="description-paragraph" id="p-0109" num="0141">The system of Example 102, wherein the processing electronics are configured to transmit an alarm notification to an external computing device over a communications network indicating that the target species has been detected at the one or more installations.</div>
<heading id="h-0108">Example 104</heading>
<div class="description-paragraph" id="p-0110" num="0142">The system of Example 103, wherein the processing electronics are configured to transmit the user interface to the external computing device over the communications network, such that the external computing device can render the visible image and the infrared image on a display.</div>
<heading id="h-0109">Example 105</heading>
<div class="description-paragraph" id="p-0111" num="0143">A system for monitoring one or more installations, the system comprising:
</div> <ul> <li id="ul0017-0001" num="0000"> <ul> <li id="ul0018-0001" num="0144">an optical imaging system comprising an array of optical detectors; and</li> <li id="ul0018-0002" num="0145">processing electronics configured to process image data detected by the optical imaging system, the processing electronics configured to:</li> <li id="ul0018-0003" num="0146">detect one or more target species based at least in part on the processed image data; and</li> <li id="ul0018-0004" num="0147">create an events log comprising a plurality of events, the plurality of events comprising one or more leaks associated with the one or more target species detected by the processing electronics.</li> </ul> </li> </ul>
<heading id="h-0110">Example 106</heading>
<div class="description-paragraph" id="p-0112" num="0148">The system of Example 105, wherein the processing electronics are configured to analyze the events log, and based on the analysis, to create a priority ranking of the events.</div>
<heading id="h-0111">Example 107</heading>
<div class="description-paragraph" id="p-0113" num="0149">The system of any one of Examples 105 to 106, wherein the processing electronics are configured to transmit the event log to an external computing device over a communications network.</div>
<heading id="h-0112">Example 108</heading>
<div class="description-paragraph" id="p-0114" num="0150">The system of any one of Examples 105 to 107, wherein the processing electronics are configured to associate multiple events with one another and to form a group of the associated multiple events.</div>
<heading id="h-0113">Example 109</heading>
<div class="description-paragraph" id="p-0115" num="0151">The system of Example 108, wherein the processing electronics are configured to form the group of the associated multiple events based at least in part on at least one of event type, type of the detected one or more target species, event time, and a field of view (FOY) in which the one or more target species has been detected.</div>
<heading id="h-0114">Example 110</heading>
<div class="description-paragraph" id="p-0116" num="0152">A system for monitoring one or more installations, the system comprising:
</div> <ul> <li id="ul0019-0001" num="0000"> <ul> <li id="ul0020-0001" num="0153">an optical imaging system comprising an array of optical detectors; and</li> <li id="ul0020-0002" num="0154">processing electronics configured to process image data detected by the optical imaging system, the processing electronics configured to:</li> <li id="ul0020-0003" num="0155">detect one or more target species based at least in part on the processed image data; and</li> <li id="ul0020-0004" num="0156">detect an unauthorized intrusion of an animal (including a human) into the one or more installations based at least in part on the processed image data.</li> </ul> </li> </ul>
<heading id="h-0115">Example 111</heading>
<div class="description-paragraph" id="p-0117" num="0157">The system of Example 110, wherein, based on the detection of the one or more target species, the processing electronics are configured to transmit an alarm notification to an external computing device over a communications network.</div>
<heading id="h-0116">Example 112</heading>
<div class="description-paragraph" id="p-0118" num="0158">The system of any one of Examples 110 to 111, wherein, based on the detection of the unauthorized intrusion, the processing electronics are configured to transmit a second alarm notification to an external computing device over a communications network indicating the unauthorized intrusion.</div>
<heading id="h-0117">Example 113</heading>
<div class="description-paragraph" id="p-0119" num="0159">The system of any one of Examples 1 to 112, wherein the optical system comprises an infrared imaging system.</div>
<heading id="h-0118">Example 114</heading>
<div class="description-paragraph" id="p-0120" num="0160">The system of any one of Examples 1 to 113, wherein the optical imaging system defines a plurality of optical channels being spatially and spectrally different from one another, each of the plurality of optical channels positioned to transfer radiation incident on the optical imaging system towards the army of optical detectors.</div>
<heading id="h-0119">Example 115</heading>
<div class="description-paragraph" id="p-0121" num="0161">The system of Example 114, wherein the optical imaging system and the processing electronics are contained together in a data acquisition and processing module configured to be worn or carried by a person.</div>
<heading id="h-0120">Example 116</heading>
<div class="description-paragraph" id="p-0122" num="0162">The system of Example 114, wherein the optical imaging system and the processing electronics are configured to be mounted to a support structure at the petroleum installation.</div>
<heading id="h-0121">Example 117</heading>
<div class="description-paragraph" id="p-0123" num="0163">The system of any one of Examples 1 to 116, wherein the optical system comprises a plurality of spectrally distinct infrared optical filters.</div>
<heading id="h-0122">Example 118</heading>
<div class="description-paragraph" id="p-0124" num="0164">The system of any one of Examples 1 to 117, wherein the processing electronics are remote from the optical system.</div>
<heading id="h-0123">Example 119</heading>
<div class="description-paragraph" id="p-0125" num="0165">A non-transitory computer readable medium having instructions stored thereon that, when executed by processing electronics, based on a detected amount of a target species, transmits an alarm notification to an external computing device over a communications network indicating that the target species has been detected at a petrochemical installation.</div>
<heading id="h-0124">Example 120</heading>
<div class="description-paragraph" id="p-0126" num="0166">A non-transitory computer readable medium having instructions stored thereon that, when executed by processing electronics, receives data from one or more optical imaging systems at one or more installations, the one or more optical imaging systems configured to capture infrared image data at the one or more installations, and, based on a detected amount of the a target species, transmits an alarm notification to an external computing device over a communications network indicating that the target species has been detected at the one or more installations.</div>
<heading id="h-0125">Example 121</heading>
<div class="description-paragraph" id="p-0127" num="0167">A non-transitory computer readable medium having instructions stored thereon that, when executed by processing electronics, combines multiple frames of processed image data into a summary alarm image that presents a detection of a target species over a period of time.</div>
<heading id="h-0126">Example 122</heading>
<div class="description-paragraph" id="p-0128" num="0168">A non-transitory computer readable medium having instructions stored thereon that, when executed by processing electronics, monitors a progression of a liquid leak over a period of time.</div>
<heading id="h-0127">Example 123</heading>
<div class="description-paragraph" id="p-0129" num="0169">A non-transitory computer readable medium having instructions stored thereon that, when executed by processing electronics, generates a system overview image that illustrates locations of a plurality of optical imaging systems at one or more installations, each optical imaging system of the plurality of optical imaging systems associated with an identifier, and associates the location of the optical imaging system at which a target species has been detected with the identifier.</div>
<heading id="h-0128">Example 124</heading>
<div class="description-paragraph" id="p-0130" num="0170">A non-transitory computer readable medium having instructions stored thereon that, when executed by processing electronics, generates a multi-view image that illustrates image data captured by a plurality of optical imaging systems at a plurality of locations of one or more installations, and associates the location of the optical imaging system at which a target species has been detected with an identifier.</div>
<heading id="h-0129">Example 125</heading>
<div class="description-paragraph" id="p-0131" num="0171">A non-transitory computer readable medium having instructions stored thereon that, when executed by processing electronics, generates a mosaic image comprising a plurality of fields of view (FOVs) of an optical imaging system at a petrochemical installation, and identifies a field of view of the plurality of FOVs at which a target species has been detected.</div>
<heading id="h-0130">Example 126</heading>
<div class="description-paragraph" id="p-0132" num="0172">A non-transitory computer readable medium having instructions stored thereon that, when executed by processing electronics, generates a visible image and an infrared image from processed image data, wherein a detected target species is rendered on at least one of the visible image and the infrared image, and generates a user interface that simultaneously illustrates the visible image and the infrared image.</div>
<heading id="h-0131">Example 127</heading>
<div class="description-paragraph" id="p-0133" num="0173">A non-transitory computer readable medium having instructions stored thereon that, when executed by processing electronics, creates an events log comprising a plurality of events, the plurality of events comprising one or more leaks associated with one or more detected target species.</div>
<heading id="h-0132">Example 128</heading>
<div class="description-paragraph" id="p-0134" num="0174">A non-transitory computer readable medium having instructions stored thereon that, when executed by processing electronics, detects one or more target species based at least in part on processed image data, and detects an unauthorized intrusion of an animal (including a human) into one or more installations based at least in part on the processed image data.</div>
<heading id="h-0133">Example 129</heading>
<div class="description-paragraph" id="p-0135" num="0175">The non-transitory computer readable medium of any one of Examples 119 to 128, wherein the computer readable medium has instructions stored thereon that, when executed by processing electronics, detects the target species.</div>
<heading id="h-0134">Example 130</heading>
<div class="description-paragraph" id="p-0136" num="0176">The non-transitory computer readable medium of any one of Examples 119 to 129, further comprising the processing electronics of any of the preceding Examples.</div>
<heading id="h-0135">Example 131</heading>
<div class="description-paragraph" id="p-0137" num="0177">The non-transitory computer readable medium of any one of Examples 119 to 130, in combination with the system of any of the preceding Examples.</div>
<heading id="h-0136">Example 132</heading>
<div class="description-paragraph" id="p-0138" num="0178">A system comprising processing electronics configured to, based on a detected amount of a target species, transmit an alarm notification to an external computing device over a communications network indicating that the target species has been detected at a petrochemical installation.</div>
<heading id="h-0137">Example 133</heading>
<div class="description-paragraph" id="p-0139" num="0179">A system comprising processing electronics configured to combine multiple frames of processed image data into a summary alarm image that presents a detection of a target species over a period of time.</div>
<heading id="h-0138">Example 134</heading>
<div class="description-paragraph" id="p-0140" num="0180">A system comprising processing electronics configured to monitor a progression of a liquid leak over a period of time.</div>
<heading id="h-0139">Example 135</heading>
<div class="description-paragraph" id="p-0141" num="0181">A system comprising processing electronics configured to generate a system overview image that illustrates locations of a plurality of optical imaging systems at one or more installations, each optical imaging system of the plurality of optical imaging systems associated with an identifier, and to associate the location of the optical imaging system at which the target species has been detected with the identifier.</div>
<heading id="h-0140">Example 136</heading>
<div class="description-paragraph" id="p-0142" num="0182">A system comprising processing electronics configured to generate a multi-view image that illustrates image data captured by a plurality of optical imaging systems at a plurality of locations of one or more installations, and associate the location of the optical imaging system at which a target species has been detected with an identifier.</div>
<heading id="h-0141">Example 137</heading>
<div class="description-paragraph" id="p-0143" num="0183">A system comprising processing electronics configured to generate a mosaic image comprising a plurality of fields of view (FOVs) of an optical imaging system at a petrochemical installation, and to identify a field of view of the plurality of FOVs at which a target species has been detected.</div>
<heading id="h-0142">Example 138</heading>
<div class="description-paragraph" id="p-0144" num="0184">A system comprising processing electronics configured to generate a visible image and an infrared image from processed image data, wherein a detected target species is rendered on at least one of the visible image and the infrared image, and to generate a user interface that simultaneously illustrates the visible image and the infrared image.</div>
<heading id="h-0143">Example 139</heading>
<div class="description-paragraph" id="p-0145" num="0185">A system comprising processing electronics configured to create an events log comprising a plurality of events, the plurality of events comprising one or more leaks associated with one or more target species detected by the processing electronics.</div>
<heading id="h-0144">Example 140</heading>
<div class="description-paragraph" id="p-0146" num="0186">A system comprising processing electronics configured to detect one or more target species based at least in part on processed image data, and to detect an unauthorized intrusion of an animal (including a human) into one or more installations based at least in part on the processed image data.</div>
<heading id="h-0145">Example 141</heading>
<div class="description-paragraph" id="p-0147" num="0187">The system of any one of Examples 132 to 140, wherein the processing electronics are configured to detect the one or more target species.</div>
<heading id="h-0146">Example 142</heading>
<div class="description-paragraph" id="p-0148" num="0188">The system of any one of Examples 132 to 141, in combination with any of the preceding Examples.</div>
<heading id="h-0147">Example 143</heading>
<div class="description-paragraph" id="p-0149" num="0189">The system of any one of Examples 132 to 142, wherein the processing electronics are configured to receive image data based on images captured by at least one infrared optical imaging system,</div>
<heading id="h-0148">Example 144</heading>
<div class="description-paragraph" id="p-0150" num="0190">A method for monitoring one or more installations, the method comprising:
</div> <ul> <li id="ul0021-0001" num="0000"> <ul> <li id="ul0022-0001" num="0191">detecting a target species based at least in part on infrared image data captured by one or more optical imaging systems at the one or more installations; and</li> <li id="ul0022-0002" num="0192">based on a detected amount of the target species, transmitting an alarm notification to an external computing device over a communications network indicating that the target species has been detected at the one or more installations.</li> </ul> </li> </ul>
<heading id="h-0149">Example 145</heading>
<div class="description-paragraph" id="p-0151" num="0193">The method of Example 144, further comprising capturing the infrared image data with the one or more optical imaging systems.</div>
<heading id="h-0150">Example 146</heading>
<div class="description-paragraph" id="p-0152" num="0194">The method of any one of Examples 144 to 145, further comprising detecting the target species over multiple frames of the infrared image data and combining the multiple frames of infrared image data into a summary alarm image that presents the detection of the target species over a period of time.</div>
<heading id="h-0151">Example 147</heading>
<div class="description-paragraph" id="p-0153" num="0195">The method of Example 146, further comprising creating the summary alarm image by calculating an average concentration of the target species and/or an average of the image data representative of the concentration over the period of time.</div>
<heading id="h-0152">Example 148</heading>
<div class="description-paragraph" id="p-0154" num="0196">The method of any one of Examples 146 to 147, further comprising generating a progressive mode to sequentially present summary alarm images for successive fields of view (FOVs) of an optical imaging system of the one or more optical imaging systems.</div>
<heading id="h-0153">Example 149</heading>
<div class="description-paragraph" id="p-0155" num="0197">The method of any one of Examples 144 to 148, further comprising creating an events log comprising a plurality of events comprising one or more fluid leaks.</div>
<heading id="h-0154">Example 150</heading>
<div class="description-paragraph" id="p-0156" num="0198">The method of Example 149, further comprising analyzing the events log, and based on the analysis, transmitting a priority ranking of events to the external computing device.</div>
<heading id="h-0155">Example 151</heading>
<div class="description-paragraph" id="p-0157" num="0199">The method of any one of Examples 144 to 150, further comprising comparing the detected amount of the target species to a threshold amount and, based on that comparison, transmit the alarm notification to the external computing device over the communications network indicating that the target species has been detected at the one or more installations.</div>
<heading id="h-0156">Example 152</heading>
<div class="description-paragraph" id="p-0158" num="0200">The method of Example 151, wherein the threshold amount is in a range of 1 ppm-m to 1000 ppm-m of the target species.</div>
<heading id="h-0157">Example 153</heading>
<div class="description-paragraph" id="p-0159" num="0201">The method of any one of Examples 144 to 152, further comprising detecting an unauthorized intrusion of an animal (including a human) into the one or more petroleum installations and, based on the detection, transmitting a second alarm notification to the external computing device over the communications network indicating the unauthorized intrusion.</div>
<heading id="h-0158">Example 154</heading>
<div class="description-paragraph" id="p-0160" num="0202">The method of any one of Examples 144 to 153, further comprising transmitting information about one or more events detected at the one or more installations to the external computing device, the external computing device configured to render a user interface presentable to a user on a display device.</div>
<heading id="h-0159">Example 155</heading>
<div class="description-paragraph" id="p-0161" num="0203">The method of Example 154, wherein the user interface comprises a visible image window and an infrared image window.</div>
<heading id="h-0160">Example 156</heading>
<div class="description-paragraph" id="p-0162" num="0204">The method of any one of Examples 144 to 155, further generating a system overview image that illustrates locations of a plurality of optical imaging systems at the one or more installations, each optical imaging system of the plurality of optical imaging systems associated with an identifier.</div>
<heading id="h-0161">Example 157</heading>
<div class="description-paragraph" id="p-0163" num="0205">The method of any one of Examples 144 to 156, further comprising generating a multi-view image that illustrates image data captured by multiple optical imaging systems at multiple sites of the one or a plurality of installations.</div>
<div class="description-paragraph" id="p-0164" num="0206">Any of Examples 1 to 157 can include any of the features described above (for example, any of the features in Examples 1 to 157), and do not necessarily need to include the optical imaging system and does not necessarily need to detect the target species.</div>
<div class="description-paragraph" id="p-0165" num="0207">Details of one or more implementations of the subject matter described in this disclosure are set forth in the accompanying drawings and the description below. Other features, aspects, and advantages will become apparent from the description, the drawings and the claims. Note that the relative dimensions of the following figures may not be drawn to scale.</div>
<description-of-drawings>
<heading id="h-0162">BRIEF DESCRIPTION OF THE DRAWINGS</heading>
<div class="description-paragraph" id="p-0166" num="0208"> <figref idrefs="DRAWINGS">FIG. 1</figref> shows an embodiment of an imaging system including a common front objective lens that has a pupil divided spectrally and re-imaged with a plurality of lenses onto an infrared FPA.</div>
<div class="description-paragraph" id="p-0167" num="0209"> <figref idrefs="DRAWINGS">FIG. 2</figref> shows an embodiment with a divided front objective lens and an array of infrared sensing FPAs.</div>
<div class="description-paragraph" id="p-0168" num="0210"> <figref idrefs="DRAWINGS">FIG. 3A</figref> represents an embodiment employing an array of front objective lenses operably matched with there-imaging lens array.</div>
<div class="description-paragraph" id="p-0169" num="0211"> <figref idrefs="DRAWINGS">FIG. 3B</figref> illustrates a two-dimensional array of optical components corresponding to the embodiment of <figref idrefs="DRAWINGS">FIG. 3A</figref>.</div>
<div class="description-paragraph" id="p-0170" num="0212"> <figref idrefs="DRAWINGS">FIG. 4</figref> is a diagram of the embodiment employing an array of field references (e.g., field stops that can be used as references for calibration) and an array of respectively corresponding relay lenses.</div>
<div class="description-paragraph" id="p-0171" num="0213"> <figref idrefs="DRAWINGS">FIG. 5A</figref> is a diagram of a 4-by-3 pupil array comprising circular optical filters (and IR blocking material between the optical filters) used to spectrally divide an optical wavefront imaged with an embodiment of the system.</div>
<div class="description-paragraph" id="p-0172" num="0214"> <figref idrefs="DRAWINGS">FIG. 5B</figref> is a diagram of a 4-by-3 pupil array comprising rectangular optical filters (and IR blocking material between the optical filters) used to spectrally divide an optical wavefront imaged with an embodiment of the system.</div>
<div class="description-paragraph" id="p-0173" num="0215"> <figref idrefs="DRAWINGS">FIG. 6A</figref> depicts theoretical plots of transmission characteristics of a combination of band-pass filters used with an embodiment of the system.</div>
<div class="description-paragraph" id="p-0174" num="0216"> <figref idrefs="DRAWINGS">FIG. 6B</figref> depicts theoretical plots of transmission characteristics of a spectrally multiplexed notch-pass filter combination used in an embodiment of the system.</div>
<div class="description-paragraph" id="p-0175" num="0217"> <figref idrefs="DRAWINGS">FIG. 6C</figref> shows theoretical plots of transmission characteristics of spectrally multiplexed long-pass filter combination used in an embodiment of the system.</div>
<div class="description-paragraph" id="p-0176" num="0218"> <figref idrefs="DRAWINGS">FIG. 6D</figref> shows theoretical plots of transmission characteristics of spectrally multiplexed short-pass filter combination used in an embodiment of the system.</div>
<div class="description-paragraph" id="p-0177" num="0219"> <figref idrefs="DRAWINGS">FIG. 7</figref> is a set of video-frames illustrating operability of an embodiment of the system used for gas detection.</div>
<div class="description-paragraph" id="p-0178" num="0220"> <figref idrefs="DRAWINGS">FIGS. 8A and 8B</figref> are plots (on axes of wavelength in microns versus the object temperature in Celsius representing effective optical intensity of the object) illustrating results of dynamic calibration of an embodiment of the system.</div>
<div class="description-paragraph" id="p-0179" num="0221"> <figref idrefs="DRAWINGS">FIGS. 9A and 9B</figref> illustrate a cross-sectional view of different embodiments of an imaging system comprising an arrangement of reference sources and mirrors that can be used for dynamic calibration.</div>
<div class="description-paragraph" id="p-0180" num="0222"> <figref idrefs="DRAWINGS">FIGS. 10A-10C</figref> illustrate a plan view of different embodiments of an imaging system comprising an arrangement of reference sources and mirrors that can be used for dynamic calibration.</div>
<div class="description-paragraph" id="p-0181" num="0223"> <figref idrefs="DRAWINGS">FIG. 11A</figref> is a schematic diagram illustrating a mobile infrared imaging system configured to be carried or worn by a human user.</div>
<div class="description-paragraph" id="p-0182" num="0224"> <figref idrefs="DRAWINGS">FIG. 11B</figref> is a schematic diagram illustrating an installation site, that can be monitored by multiple infrared imaging systems.</div>
<div class="description-paragraph" id="p-0183" num="0225"> <figref idrefs="DRAWINGS">FIG. 12</figref> is a schematic system block diagram showing a mobile infrared imaging system, according to one embodiment.</div>
<div class="description-paragraph" id="p-0184" num="0226"> <figref idrefs="DRAWINGS">FIG. 13A</figref> is a schematic system diagram of an optical system configured to be used in the mobile infrared imaging systems disclosed herein, according to various embodiments.</div>
<div class="description-paragraph" id="p-0185" num="0227"> <figref idrefs="DRAWINGS">FIG. 13B</figref> is a schematic system diagram of an optical system configured to be used in the mobile infrared imaging systems disclosed herein, according to other embodiments.</div>
<div class="description-paragraph" id="p-0186" num="0228"> <figref idrefs="DRAWINGS">FIG. 14A</figref> is a schematic perspective view of a mobile infrared imaging system mounted to a helmet, according to various embodiments.</div>
<div class="description-paragraph" id="p-0187" num="0229"> <figref idrefs="DRAWINGS">FIG. 14B</figref> is an enlarged schematic perspective view of the mobile infrared imaging system shown in <figref idrefs="DRAWINGS">FIG. 14A</figref>.</div>
<div class="description-paragraph" id="p-0188" num="0230"> <figref idrefs="DRAWINGS">FIG. 14C</figref> is a perspective cross-sectional view of the mobile infrared imaging system shown in <figref idrefs="DRAWINGS">FIGS. 14A-14B</figref>.</div>
<div class="description-paragraph" id="p-0189" num="0231"> <figref idrefs="DRAWINGS">FIG. 15A</figref> is a schematic system diagram showing a monitoring system for detecting and/or identifying fluid leaks from a remote installation, according to various embodiments.</div>
<div class="description-paragraph" id="p-0190" num="0232"> <figref idrefs="DRAWINGS">FIG. 15B</figref> is a schematic system diagram showing an example of a monitoring computer system, according to various embodiments.</div>
<div class="description-paragraph" id="p-0191" num="0233"> <figref idrefs="DRAWINGS">FIG. 15C</figref> is a schematic diagram of a system overview window which can be rendered on a display of a central monitoring computer system.</div>
<div class="description-paragraph" id="p-0192" num="0234"> <figref idrefs="DRAWINGS">FIG. 15D</figref> is a schematic diagram of a multi-view imaging window which can be rendered on the display of a central monitoring computer system, according to various embodiments.</div>
<div class="description-paragraph" id="p-0193" num="0235"> <figref idrefs="DRAWINGS">FIG. 16A</figref> is a schematic diagram of a single imaging system window, according to various embodiments.</div>
<div class="description-paragraph" id="p-0194" num="0236"> <figref idrefs="DRAWINGS">FIG. 16B</figref> is a schematic diagram of a mosaic image window, according to various embodiments.</div>
<div class="description-paragraph" id="p-0195" num="0237"> <figref idrefs="DRAWINGS">FIG. 16C</figref> illustrates an example of a summary alarm image, according to various embodiments.</div>
<div class="description-paragraph" id="p-0196" num="0238"> <figref idrefs="DRAWINGS">FIG. 16D</figref> illustrates a time lapsed leak progression image that shows the progression of a liquid leak over a time period, according to various embodiments.</div>
<div class="description-paragraph" id="p-0197" num="0239"> <figref idrefs="DRAWINGS">FIG. 17A</figref> is a schematic diagram of an events log, according to various embodiments.</div>
<div class="description-paragraph" id="p-0198" num="0240"> <figref idrefs="DRAWINGS">FIG. 17B</figref> is a schematic diagram of an event guide, according to various embodiments.</div>
</description-of-drawings>
<div class="description-paragraph" id="p-0199" num="0241">Like reference numbers and designations in the various drawings indicate like elements.</div>
<heading id="h-0163">DETAILED DESCRIPTION</heading>
<heading id="h-0164">I. Overview of Various Embodiments</heading>
<div class="description-paragraph" id="p-0200" num="0242">The following description is directed to certain implementations for the purposes of describing the innovative aspects of this disclosure. However, a person having ordinary skill in the art will readily recognize that the teachings herein can be applied in a multitude of different ways. The described implementations may be implemented in any device, apparatus, or system that can be configured to operate as an imaging system such as in an infra-red imaging system. The methods and systems described herein can be included in or associated with a variety of devices such as, but not limited to devices used for visible and infrared spectroscopy, multispectral and hyperspectral imaging devices used in oil and gas exploration, refining, and transportation, agriculture, remote sensing, defense and homeland security, surveillance, astronomy, environmental monitoring, etc. The methods and systems described herein have applications in a variety of fields including but not limited to agriculture, biology, physics, chemistry, defense and homeland security, environment, oil and gas industry, etc. The teachings are not intended to be limited to the implementations depicted solely in the Figures, but instead have wide applicability as will be readily apparent to one having ordinary skill in the art.</div>
<div class="description-paragraph" id="p-0201" num="0243">The spectral image of the scene can be represented as a three-dimensional data cube where two axes of the cube represent two spatial dimensions of the scene and a third axes of the data cube represents spectral information of the scene in different wavelength regions. The data cube can be processed using mathematical methods to obtain information about the scene. Some of the existing spectral imaging systems generate the data cube by scanning the scene in the spatial domain (e.g., by moving a slit across the horizontal and vertical dimensions of the scene) and/or spectral domain. Such scanning approaches acquire only a portion of the full data cube at a time. These portions of the full data cube are stored and then later processed to generate a full data cube.</div>
<div class="description-paragraph" id="p-0202" num="0244">Various embodiments disclosed herein describe a divided-aperture infrared spectral imaging (DAISI) system that is structured and adapted to provide identification of target chemical contents of the imaged scene. The system is based on spectrally-resolved imaging and can provide such identification with a single-shot (also referred to as a snapshot) comprising a plurality of images having different wavelength compositions that are obtained generally simultaneously. Without any loss of generality, snapshot refers to a system in which most of the data elements that are collected are continuously viewing the light emitted from the scene. In contrast in scanning systems, at any given time only a minority of data elements are continuously viewing a scene, followed by a different set of data elements, and so on, until the full dataset is collected. Relatively fast operation can be achieved in a snapshot system because it does not need to use spectral or spatial scanning for the acquisition of infrared (IR) spectral signatures of the target chemical contents. Instead, IR detectors (such as, for example, infrared focal plane arrays or FPAs) associated with a plurality of different optical channels having different wavelength profiles can be used to form a spectral cube of imaging data. Although spectral data can be obtained from a single snapshot comprising multiple simultaneously acquired images corresponding to different wavelength ranges, in various embodiments, multiple snap shots may be obtained. In various embodiments, these multiple snapshots can be averaged. Similarly, in certain embodiments multiple snap shots may be obtained and a portion of these can be selected and possibly averaged. Also, in contrast to commonly used IR spectral imaging systems, the DAISI system does not require cooling. Accordingly, it can advantageously use uncooled infrared detectors. For example, in various implementations, the imaging systems disclosed herein do not include detectors configured to be cooled to a temperature below 300 Kelvin. As another example, in various implementations, the imaging systems disclosed herein do not include detectors configured to be cooled to a temperature below 273 Kelvin. As yet another example, in various implementations, the imaging systems disclosed herein do not include detectors configured to be cooled to a temperature below 250 Kelvin. As another example, in various implementations, the imaging systems disclosed herein do not include detectors configured to be cooled to a temperature below 200 Kelvin.</div>
<div class="description-paragraph" id="p-0203" num="0245">Implementations disclosed herein provide several advantages over existing IR spectral imaging systems, most if not all of which may require FPAs that are highly sensitive and cooled in order to compensate, during the optical detection, for the reduction of the photon flux caused by spectrum-scanning operation. The highly sensitive and cooled FPA systems are expensive and require a great deal of maintenance. Since various embodiments disclosed herein are configured to operate in single-shot acquisition mode without spatial and/or spectral scanning, the instrument can receive photons from a plurality of points (e.g., every point) of the object substantially simultaneously, during the single reading. Accordingly, the embodiments of imaging system described herein can collect a substantially greater amount of optical power from the imaged scene (for example, an order of magnitude more photons) at any given moment in time especially in comparison with spatial and/or spectral scanning systems. Consequently, various embodiments of the imaging systems disclosed herein can be operated using uncooled detectors (for example, FPA unit including an array of microbolometers) that are less sensitive to photons in the IR but are well fit for continuous monitoring applications. For example, in various implementations, the imaging systems disclosed herein do not include detectors configured to be cooled to a temperature below 300 Kelvin. As another example, in various implementations, the imaging systems disclosed herein do not include detectors configured to be cooled to a temperature below 273 Kelvin. As yet another example, in various implementations, the imaging systems disclosed herein do not include detectors configured to be cooled to a temperature below 250 Kelvin. As another example, in various implementations, the imaging systems disclosed herein do not include detectors configured to be cooled to a temperature below 200 Kelvin. Imaging systems including uncooled detectors can be capable of operating in extreme weather conditions, require less power, are capable of operation during day and night, and are less expensive. Some embodiments described herein can also be less susceptible to motion artifacts in comparison with spatially and/or spectrally scanning systems which can cause errors in either the spectral data, spatial data, or both.</div>
<div class="description-paragraph" id="p-0204" num="0246">In various embodiments disclosed herein, the DAISI system can be mobile. For example, the DAISI system can be configured to be worn or carried by a person, e.g., the DAISI system can be miniaturized to fit in a relatively small housing or compartment. For example, the components of the DAISI system can be sized and shaped to fit within small dimensions and can have a mass sufficiently small to enable the human user to carry or wear the system without undue exertion. As explained herein, in some embodiments, the DAISI system can be sized and shaped to fit within a volume of less than about 300 cubic inches, or in some embodiments, less than about 200 cubic inches. In still other embodiments, the DAISI system can be sized and shaped to fit within a volume less than about 100 cubic inches. For example, in some arrangements, the DAISI system can be sized and shaped to fit within a volume in a range of about 50 cubic inches to about 300 cubic inches. In other arrangements, the DAISI system can be sized and shaped to fit within a volume in a range of about 80 cubic inches to about 200 cubic inches.</div>
<div class="description-paragraph" id="p-0205" num="0247">Advantageously, such a portable and/or wearable DAISI system can enable the user to monitor installations in remote locations and to detect the presence of various gases (e.g., poisonous gases) in real-time. Further, the portable DAISI system can enable the user to travel to different installations to monitor the presence of gases or chemicals in multiple locations. For example, the user may travel to an oil drilling installation in which oil is pumped from the ground. The user can catty or attach the portable DAISI system to his or her clothing or body (e.g., by way of a clip, hat, etc.) and can activate the system while he or she is on-site. Optical components on board the portable DAISI system can capture one or more snapshot multispectral images of portions of the installation susceptible to gas or chemical leaks. Computing units on board the portable DAISI system can process the captured multispectral image data to detect and/or classify gases or chemicals present at the site. A communications module can notify the user of the detected gases. For example, in various embodiments, the communications module can send a notification to a user interface (such as a set of computing eyeglasses, a mobile computing device such as a mobile smartphone, a tablet computing device, a laptop computing device, or any other suitable interface), and the user interface can display information about the detected gases to the user in real-time, e.g., at the oil drilling installation.</div>
<div class="description-paragraph" id="p-0206" num="0248">In various embodiments, DAISI systems can be provided at multiple locations, for example, to monitor different installations or facilities. For example, in various arrangements, multiple DAISI systems (which may be portable in some embodiments) can be deployed at different petrochemical installations, e.g., at oil and/or gas well(s), along pipeline(s), at petrochemical processing plants, or at any other facility where it may be important to detect leaked fluids (e.g., gas leaks, liquid oil spills, etc.). Various embodiments disclosed herein enable the monitoring of one or multiple remote facilities so that the user or operator of the DAISI systems can determine the location, type, timing, and/or concentration of a fluid leak (e.g., a gas or liquid leak) at any installation being monitored.</div>
<heading id="h-0165">II. Examples of Divided Aperture Intrared Spectral Imager Systems</heading>
<div class="description-paragraph" id="p-0207" num="0249"> <figref idrefs="DRAWINGS">FIG. 1</figref> provides a diagram schematically illustrating spatial and spectral division of incoming light by an embodiment <b>100</b> of a divided aperture infrared spectral imager (DAISI) system that can image an object <b>110</b> possessing IR spectral signature(s). The system <b>100</b> includes a front objective lens <b>124</b>, an array of optical filters <b>130</b>, an array of reimaging lenses <b>128</b> and a detector array <b>136</b>. In various embodiments, the detector array <b>136</b> can include a single FPA or an array of FPAs. Each detector in the detector array <b>136</b> can be disposed at the focus of each of the lenses in the array of reimaging lenses <b>128</b>. In various embodiments, the detector array <b>136</b> can include a plurality of photo-sensitive devices. In some embodiments, the plurality of photo-sensitive devices may comprise a two-dimensional imaging sensor array that is sensitive to radiation having wavelengths between 1 μm and 20 μm (for example, in near infra-red wavelength range, mid infra-red wavelength range, or long infra-red wavelength range). In various embodiments, the plurality of photo-sensitive devices can include CCD or CMOS sensors, bolometers, microbolometers or other detectors that are sensitive to infra-red radiation.</div>
<div class="description-paragraph" id="p-0208" num="0250">An aperture of the system <b>100</b> associated with the front objective lens system <b>124</b> is spatially and spectrally divided by the combination of the array of optical filters <b>130</b> and the array of reimaging lenses <b>128</b>. In various embodiments, the combination of the array of optical filters <b>130</b> and the array of reimaging lenses <b>128</b> can be considered to form a spectrally divided pupil that is disposed forward of the optical detector array <b>136</b>. The spatial and spectral division of the aperture into distinct aperture portions forms a plurality of optical channels <b>120</b> along which light propagates. In various embodiments, the array <b>128</b> of re-imaging lenses <b>128</b> <i>a </i>and the array of spectral filters <b>130</b> which respectively correspond to the distinct optical channels <b>120</b>. The plurality of optical channels <b>120</b> can be spatially and/or spectrally distinct. The plurality of optical channels <b>120</b> can be formed in the object space and/or image space. In one implementation, the distinct channels <b>120</b> may include optical channels that are separated angularly in space. The array of spectral filters <b>130</b> may additionally include a filter-holding aperture mask (comprising, for example, IR light-blocking materials such as ceramic, metal, or plastic). Light from the object <b>110</b> (for example a cloud of gas), the optical properties of which in the IR are described by a unique absorption, reflection and/or emission spectrum, is received by the aperture of the system <b>100</b>. This light propagates through each of the plurality of optical channels <b>120</b> and is further imaged onto the optical detector array <b>136</b>. In various implementations, the detector array <b>136</b> can include at least one FPA. In various embodiments, each of the re-imaging lenses <b>128</b> <i>a </i>can be spatially aligned with a respectively-corresponding spectral region. In the illustrated implementation, each filter element from the array of spectral filters <b>130</b> corresponds to a different spectral region. Each re-imaging lens <b>128</b> <i>a </i>and the corresponding filter element of the array of spectral filter <b>130</b> can coincide with (or form) a portion of the divided aperture and therefore with respectively-corresponding spatial channel <b>120</b>. Accordingly, in various embodiment an imaging lens <b>128</b> <i>a </i>and a corresponding spectral filter can be disposed in the optical path of one of the plurality of optical channels <b>120</b>. Radiation from the object <b>110</b> propagating through each of the plurality of optical channels <b>120</b> travels along the optical path of each re-imaging lens <b>128</b> <i>a </i>and the corresponding filter element of the array of spectral filter <b>130</b> and is incident on the detector array (e.g., FPA component) <b>136</b> to form a single image (e.g., sub-image) of the object <b>110</b>. The image formed by the detector array <b>136</b> generally includes a plurality of sub-images formed by each of the optical channels <b>120</b>. Each of the plurality of sub-images can provide different spatial and spectral information of the object <b>110</b>. The different spatial information results from some parallax because of the different spatial locations of the smaller apertures of the divided aperture. In various embodiments, adjacent sub-images can be characterized by close or substantially equal spectral signatures. The detector array (e.g., FPA component) <b>136</b> is further operably connected with a processor <b>150</b> (not shown). The processor <b>150</b> can be programmed to aggregate the data acquired with the system <b>100</b> into a spectral data cube. The data cube represents, in spatial (x, y) and spectral (λ) coordinates, an overall spectral image of the object <b>110</b> within the spectral region defined by the combination of the filter elements in the array of spectral filters <b>130</b>. Additionally, in various embodiments, the processor or processing electronics <b>150</b> may be programmed to determine the unique absorption characteristic of the object <b>110</b>. Also, the processor/processing electronics <b>150</b> can, alternatively or in addition, map the overall image data cube into a cube of data representing, for example, spatial distribution of concentrations, c, of targeted chemical components within the field of view associated with the object <b>110</b>.</div>
<div class="description-paragraph" id="p-0209" num="0251">Various implementations of the embodiment <b>100</b> can include an optional moveable temperature-controlled reference source <b>160</b> including, for example, a shutter system comprising one or more reference shutters maintained at different temperatures. The reference source <b>160</b> can include a heater, a cooler or a temperature-controlled element configured to maintain the reference source <b>160</b> at a desired temperature. For example, in various implementations, the embodiment <b>100</b> can include two reference shutters maintained at different temperatures. The reference source <b>160</b> is removably and, in one implementation, periodically inserted into an optical path of light traversing the system <b>100</b> from the object <b>110</b> to the detector array (e.g., FPA component) <b>136</b> along at least one of the channels <b>120</b>. The removable reference source <b>160</b> thus can block such optical path. Moreover, this reference source <b>160</b> can provide a reference IR spectrum to recalibrate various components including the detector array <b>136</b> of the system <b>100</b> in real time. The configuration of the moveable reference source <b>160</b> is further discussed below.</div>
<div class="description-paragraph" id="p-0210" num="0252">In the embodiment <b>100</b>, the front objective lens system <b>124</b> is shown to include a single front objective lens positioned to establish a common field-of-view (FOV) for the reimaging lenses <b>128</b> <i>a </i>and to define an aperture stop for the whole system. In this specific case, the aperture stop substantially spatially coincides with and/or is about the same size as or slightly larger than the plurality of smaller limiting apertures corresponding to different optical channels <b>120</b>. As a result, the positions for spectral filters of the different optical channels <b>120</b> coincide with the position of the aperture stop of the whole system, which in this example is shown as a surface between the lens system <b>124</b> and the array <b>128</b> of the reimaging lenses <b>128</b> <i>a</i>. In various implementations, the lens system <b>124</b> can be an objective lens <b>124</b>. However, the objective lens <b>124</b> is optional and various embodiments of the system <b>100</b> need not include the objective lens <b>124</b>. In various embodiments, the objective lens <b>124</b> can slightly shift the images obtained by the different detectors in the array <b>136</b> spatially along a direction perpendicular to optical axis of the lens <b>124</b>, thus the functionality of the system <b>100</b> is not necessarily compromised when the objective lens <b>124</b> is not included. Generally, however, the field apertures corresponding to different optical channels may be located in the same or different planes. These field apertures may be defined by the aperture of the reimaging lens <b>128</b> <i>a </i>and/or filters in the divided aperture <b>130</b> in certain implementations. In one implementation, the field apertures corresponding to different optical channels can be located in different planes and the different planes can be optical conjugates of one another. Similarly, while all of the filter elements in the array of spectral filters <b>130</b> of the embodiment <b>100</b> are shown to lie in one plane, generally different filter elements of the array of spectral filter <b>130</b> can be disposed in different planes. For example, different filter elements of the array of spectral filters <b>130</b> can be disposed in different planes that are optically conjugate to one another. However, in other embodiments, the different filter elements can be disposed in non-conjugate planes.</div>
<div class="description-paragraph" id="p-0211" num="0253">In contrast to the embodiment <b>100</b>, the front objective lens <b>124</b> need not be a single optical element, but instead can include a plurality of lenses <b>224</b> as shown in an embodiment <b>200</b> of the DAISI imaging system in <figref idrefs="DRAWINGS">FIG. 2</figref>. These lenses <b>224</b> are configured to divide an incoming optical wavefront from the object <b>110</b>. For example, the array of front objective lenses <b>224</b> can be disposed so as to receive an IR wavefront emitted by the object that is directed toward the DAISI system. The plurality of front objective lenses <b>224</b> divide the wavefront spatially into non-overlapping sections. <figref idrefs="DRAWINGS">FIG. 2</figref> shows three objective lenses <b>224</b> in a front optical portion of the optical system contributing to the spatial division of the aperture of the system in this example. The plurality of objective lenses <b>224</b>, however, can be configured as a two-dimensional (2D) array of lenses. <figref idrefs="DRAWINGS">FIG. 2</figref> presents a general view of the imaging system <b>200</b> and the resultant field of view of the imaging system <b>200</b>. An exploded view <b>202</b> of the imaging system <b>200</b> is also depicted in greater detail in a figure inset of <figref idrefs="DRAWINGS">FIG. 2</figref>. As illustrated in the detailed view <b>202</b>, the embodiment of the imaging system <b>200</b> includes a field reference <b>204</b> at the front end of the system. The field reference <b>204</b> can be used to truncate the field of view. The configuration illustrated in <figref idrefs="DRAWINGS">FIG. 2</figref> has an operational advantage over embodiment <b>100</b> of <figref idrefs="DRAWINGS">FIG. 1</figref> in that the overall size and/or weight and/or cost of manufacture of the embodiment <b>200</b> can be greatly reduced because the objective lens is smaller. Each pair of the lenses in the array <b>224</b> and the array <b>128</b> is associated with a field of view (FOV). Each pair of lenses in the array <b>224</b> and the array <b>128</b> receives light from the object from a different angle. Accordingly, the FOV of the different pairs of lenses in the array <b>224</b> and the array <b>128</b> do not completely overlap as a result of parallax. As the distance between the imaging system <b>200</b> (portion <b>202</b>) and the object <b>110</b> increases, the overlapping region <b>230</b> between the FOVs of the individual lenses <b>224</b> increases while the amount of parallax <b>228</b> remains approximately the same, thereby reducing its effect on the system <b>200</b>. When the ratio of the parallax-to-object-distance is substantially equal to the pixel-size-to-system-focal-length ratio then the parallax effect may be considered to be negligible and, for practical purposes, no longer distinguishable. While the lenses <b>224</b> are shown to be disposed substantially in the same plane, optionally different objective lenses in the array of front objective lenses <b>224</b> can be disposed in more than one plane. For example, some of the individual lenses <b>224</b> can be displaced with respect to some other individual lenses <b>224</b> along the axis <b>226</b> (not shown) and/or have different focal lengths as compared to some other lenses <b>224</b>. As discussed below, the field reference <b>204</b> can be useful in calibrating the multiple detectors <b>236</b>.</div>
<div class="description-paragraph" id="p-0212" num="0254">In one implementation, the front objective lens system such as the array of lenses <b>224</b> is configured as an array of lenses integrated or molded in association with a monolithic substrate. Such an arrangement can reduce the costs and complexity otherwise accompanying the optical adjustment of individual lenses within the system. An individual lens <b>224</b> can optionally include a lens with varying magnification. As one example, a pair of thin and large diameter Alvarez plates can be used in at least a portion of the front objective lens system. Without any loss of generality, the Alvarez plates can produce a change in focal length when translated orthogonally with respect to the optical beam.</div>
<div class="description-paragraph" id="p-0213" num="0255">In further reference to <figref idrefs="DRAWINGS">FIG. 1</figref>, the detector array <b>136</b> (e.g., FPA component) configured to receive the optical data representing spectral signature(s) of the imaged object <b>110</b> can be configured as a single imaging array (e.g., FPA) <b>136</b>. This single array may be adapted to acquire more than one image (formed by more than one optical channel <b>120</b>) simultaneously. Alternatively, the detector array <b>136</b> may include a FPA unit. In various implementations, the FPA unit can include a plurality of optical FPAs. At least one of these plurality of FPAs can be configured to acquire more than one spectrally distinct image of the imaged object. For example, as shown in the embodiment <b>200</b> of <figref idrefs="DRAWINGS">FIG. 2</figref>, in various embodiments, the number of FPAs included in the FPA unit may correspond to the number of the front objective lenses <b>224</b>. In the embodiment <b>200</b> of <figref idrefs="DRAWINGS">FIG. 2</figref>, for example, three FPAs <b>236</b> are provided corresponding to the three objective lenses <b>224</b>. In one implementation of the system, the FPA unit can include an array of microbolometers. The use of multiple microbolometers advantageously allows for an inexpensive way to increase the total number of detection elements (i.e. pixels) for recording of the three-dimensional data cube in a single acquisition event (i.e. one snapshot). In various embodiments, an array of microbolometers more efficiently utilizes the detector pixels of the array of FPAs (e.g., each FPA) as the number of unused pixels is reduced, minimized and/or eliminated between the images that may exist when using a single microbolometer.</div>
<div class="description-paragraph" id="p-0214" num="0256"> <figref idrefs="DRAWINGS">FIG. 3A</figref> illustrates schematically an embodiment <b>300</b> of the imaging system in which the number of the front objective lenses <b>324</b> <i>a </i>in the lens array <b>324</b>, the number of re-imaging lenses <b>128</b> <i>a </i>in the lens array <b>128</b>, and the number of FPAs <b>336</b> are the same. So configured, each combination of respectively corresponding front objective lens <b>324</b>, re-imaging lens <b>128</b> <i>a</i>, and FPAs <b>336</b> constitutes an individual imaging channel. Such a channel is associated with acquisition of the IR light transmitted from the object <b>110</b> through an individual filter element of the array of optical filters <b>130</b>. A field reference <b>338</b> of the system <b>300</b> is configured to have a uniform temperature across its surface and be characterized by a predetermined spectral curve of radiation emanating therefrom. In various implementations, the field reference <b>338</b> can be used as a calibration target to assist in calibrating or maintaining calibration of the FPA. Accordingly, in various implementations, the field reference <b>338</b> is used for dynamically adjusting the data output from each FPA <b>336</b> after acquisition of light from the object <b>110</b>. This dynamic calibration process helps provide that output of the different (e.g., most, or each of the) FPA <b>336</b> represents correct acquired data, with respect to the other FPAs <b>336</b> for analysis, as discussed below in more detail.</div>
<div class="description-paragraph" id="p-0215" num="0257"> <figref idrefs="DRAWINGS">FIG. 3B</figref> illustrates the plan view perpendicular to the axis <b>226</b> of an embodiment <b>300</b> of the imaging system illustrated in <figref idrefs="DRAWINGS">FIG. 3A</figref>. For the embodiment shown in <figref idrefs="DRAWINGS">FIG. 3B</figref>, the optical components (e.g., objective lenses <b>324</b> <i>a</i>, filter elements of the array of spectral filters <b>130</b>, re-imaging lenses <b>128</b> <i>a </i>and FPA units <b>336</b>) are arranged as a 4×3 array. In one implementation, the 4×3 array <b>340</b> of optical components (lenses <b>324</b> <i>a</i>, <b>128</b> <i>a</i>; detector elements <b>336</b>) is used behind the temperature controlled reference target <b>160</b>. The field reference aperture <b>338</b> can be adapted to obscure and/or block a peripheral portion of the bundle of light propagating from the object <b>110</b> towards the FPA units <b>336</b>. As a result, the field reference <b>338</b> obscures and/or blocks the border or peripheral portion(s) of the images of the object <b>110</b> formed on the FPA elements located along the perimeter <b>346</b> of the detector system. Generally, two elements of the FPA unit will produce substantially equal values of digital counts when they are used to observe the same portion of the scene in the same spectral region using the same optical train. If any of these input parameters (for example, scene to be observed, spectral content of light from the scene, or optical elements delivering light from the scene to the two detector elements) differ, the counts associated with the elements of the FPA unit will differ as well. Accordingly, and as an example, in a case when the two FPAs of the FPA unit <b>336</b> (such as those denoted as #<b>6</b> and #<b>7</b> in <figref idrefs="DRAWINGS">FIG. 3B</figref>) remain substantially un-obscured by the field reference <b>338</b>, the outputs from these FPAs can be dynamically adjusted to the output from one of the FPAs located along perimeter <b>346</b> (such as, for example, the FPA element #<b>2</b> or FPA element #<b>11</b>) that processes light having similar spectral characteristics.</div>
<div class="description-paragraph" id="p-0216" num="0258"> <figref idrefs="DRAWINGS">FIG. 4</figref> illustrates schematically a portion of another embodiment of an imaging system <b>400</b> that contains an array <b>424</b> of front objective lenses <b>424</b> <i>a</i>. The array <b>424</b> of lenses <b>424</b> <i>a </i>adapted to receive light from the object <b>110</b> and relay the received light to the array <b>128</b> of re-imaging lenses <b>128</b> <i>a </i>through an array <b>438</b> of field references (or field stops) <b>438</b> <i>a</i>, and through an array <b>440</b> of the relay lenses. The spectral characteristics of the field references/field stops <b>438</b> <i>a </i>can be known. The field references <b>438</b> <i>a </i>are disposed at corresponding intermediate image planes defined, with respect to the object <b>110</b>, by respectively corresponding front objective lenses <b>424</b> <i>a</i>. When refractive characteristics of all of the front objective lenses <b>424</b> <i>a </i>are substantially the same, all of the field references <b>438</b> <i>a </i>are disposed in the same plane. A field reference <b>438</b> <i>a </i>of the array <b>438</b> obscures (or casts a shadow on) a peripheral region of a corresponding image (e.g., sub-image) formed at the detector plane <b>444</b> through a respectively corresponding spatial imaging channel <b>450</b> of the system <b>400</b> prior to such image being spectrally processed by the processor <b>150</b>. The array <b>440</b> of relay lenses then transmits light along each of the imaging channels <b>450</b> through different spectral filters <b>454</b> <i>a </i>of the filter array <b>454</b>, past the calibration apparatus that includes two temperature controlled shutters <b>460</b> <i>a</i>, <b>460</b> <i>b</i>, and then onto the detector module <b>456</b>. In various embodiments, the detector module <b>456</b> can include a microbolometer array or some other IR FPA.</div>
<div class="description-paragraph" id="p-0217" num="0259">The embodiment <b>400</b> has several operational advantages. It is configured to provide a spectrally known object within every image (e.g., sub-image) and for every snapshot acquisition which can be calibrated against. Such spectral certainty can be advantageous when using an array of IR FPAs like microbolometers, the detection characteristics of which can change from one imaging frame to the next due to, in part, changes in the scene being imaged as well as the thermal effects caused by neighboring FPAs. In various embodiments, the field reference array <b>438</b> of the embodiment <b>400</b>—can be disposed within the Rayleigh range (approximately corresponding to the depth of focus) associated with the front objective lenses <b>424</b>, thereby removing unusable blurred pixels due to having the field reference outside of this range. Additionally, the embodiment <b>400</b> of <figref idrefs="DRAWINGS">FIG. 4</figref> can be more compact than, for example, the configuration <b>300</b> of <figref idrefs="DRAWINGS">FIG. 3A</figref>. In the system shown in <figref idrefs="DRAWINGS">FIG. 3A</figref>, for example, the field reference <b>338</b> may be separated from the lens array <b>324</b> by a distance greater than several (for example, five) focal lengths to minimize/reduce blur contributed by the field reference to an image formed at a detector plane.</div>
<div class="description-paragraph" id="p-0218" num="0260">In various embodiments, the multi-optical FPA unit of the IR imaging system can additionally include an FPA configured to operate in a visible portion of the spectrum. In reference to <figref idrefs="DRAWINGS">FIG. 1</figref>, for example, an image of the scene of interest formed by such visible-light FPA may be used as a background to form a composite image by overlapping an IR image with the visible-light image. The IR image may be overlapped virtually, with the use of a processor and specifically-designed computer program product enabling such data processing, or actually, by a viewer. The IR image may be created based on the image data acquired by the individual FPAs <b>136</b>. The so-formed composite image facilitates the identification of the precise spatial location of the target species, the spectral signatures of which the system is able to detect/recognize.</div>
<div class="description-paragraph" id="h-0166" num="0000">Optical Filters.</div>
<div class="description-paragraph" id="p-0219" num="0261">The optical filters, used with an embodiment of the system, that define spectrally-distinct IR image (e.g., sub-image) of the object can employ absorption filters, interference filters, and Fabry-Perot etalon based filters, to name just a few. When interference filters are used, the image acquisition through an individual imaging channel defined by an individual re-imaging lens (such as a lens <b>128</b> <i>a </i>of <figref idrefs="DRAWINGS">FIGS. 1, 2, 3, and 4</figref>) may be carried out in a single spectral bandwidth or multiple spectral bandwidths. Referring again to the embodiments <b>100</b>, <b>200</b>, <b>300</b>, <b>400</b> of <figref idrefs="DRAWINGS">FIGS. 1 through 4</figref>, and in further reference to <figref idrefs="DRAWINGS">FIG. 3B</figref>, examples of a 4-by-3 array of spectral filters <b>130</b> is shown in <figref idrefs="DRAWINGS">FIGS. 5A and 5B</figref>. Individual filters <b>1</b> through <b>12</b> are juxtaposed with a supporting opto-mechanical element (not shown) to define a filter-array plane that is oriented, in operation, substantially perpendicularly to the general optical axis <b>226</b> of the imaging system. In various implementations, the individual filters <b>1</b> through <b>12</b> need not be discrete optical components. Instead, the individual filters <b>1</b> through <b>12</b> can comprise one or more coatings that are applied to one or more surfaces of the reimaging lenses (such as a lens <b>128</b> <i>a </i>of <figref idrefs="DRAWINGS">FIGS. 1, 2, 3, and 4</figref>) or the surfaces of one or more detectors.</div>
<div class="description-paragraph" id="p-0220" num="0262">The optical filtering configuration of various embodiments disclosed herein may advantageously use a bandpass filter defining a specified spectral band. Any of the filters <b>0</b> <i>a </i>through <b>3</b> <i>a</i>, the transmission curves of which are shown in <figref idrefs="DRAWINGS">FIG. 6A</figref> may, for example, be used. The filters may be placed in front of the optical FPA (or generally, between the optical FPA and the object). In particular, and in further reference to <figref idrefs="DRAWINGS">FIGS. 1, 2</figref> <b>3</b>, and <b>4</b>, when optical detector arrays <b>136</b>, <b>236</b>, <b>336</b>, <b>456</b> include microbolometers, the predominant contribution to noise associated with image acquisition is due to detector noise. To compensate and/or reduce the noise, various embodiments disclosed herein utilize spectrally-multiplexed filters. In various implementations, the spectral-multiplexed filters can comprise a plurality of long pass filters, a plurality long pass filters, a plurality of band pass filters and any combinations thereof. An example of the spectral transmission characteristics of spectrally-multiplexed filters <b>0</b> <i>b </i>through <b>3</b> <i>d </i>for use with various embodiments of imaging systems disclosed herein is depicted in <figref idrefs="DRAWINGS">FIG. 6B</figref>. Filters of <figref idrefs="DRAWINGS">FIG. 6C</figref> can be referred to as long-wavelength pass, LP filters. An LP filter generally attenuates shorter wavelengths and transmits (passes) longer wavelengths (e.g., over the active range of the target IR portion of the spectrum). In various embodiments, short-wavelength-pass filters, SP, may also be used. An SP filter generally attenuates longer wavelengths and transmits (passes) shorter wavelengths (e.g., over the active range of the target IR portion of the spectrum). At least in part due to the snap-shot/non-scanning mode of operation, embodiments of the imaging system described herein can use less sensitive microbolometers without compromising the SNR. The use of microbolometers, as detector-noise-limited devices, in turn not only benefits from the use of spectrally multiplexed filters, but also does not require cooling of the imaging system during normal operation.</div>
<div class="description-paragraph" id="p-0221" num="0263">Referring again to <figref idrefs="DRAWINGS">FIGS. 6A, 6B, 6C, and 6D</figref>, each of the filters (<b>0</b> <i>b </i>. . . <b>3</b> <i>d</i>) transmits light in a substantially wider region of the electromagnetic spectrum as compared to those of the filters (<b>0</b> <i>a </i>. . . <b>3</b> <i>a</i>). Accordingly, when the spectrally-multiplexed set of filters (<b>0</b> <i>b </i>. . . <b>0</b> <i>d</i>) is used with an embodiment of the imaging system, the overall amount of light received by the FPAs (for example, <b>236</b>, <b>336</b>) is larger than would be received when using the bandpass filters (<b>0</b> <i>a </i>. . . <b>4</b> <i>a</i>). This “added” transmission of light defined by the use of the spectrally-multiplexed LP (or SP) filters facilitates an increase of the signal on the FPAs above the level of the detector noise. Additionally, by using, in an embodiment of the imaging system, filters having spectral bandwidths greater than those of band-pass filters, the uncooled FPAs of the embodiment of the imaging system experience less heating from radiation incident thereon from the imaged scene and from radiation emanating from the FPA in question itself. This reduced heating is due to a reduction in the back-reflected thermal emission(s) coming from the FPA and reflecting off of the filter from the non-band-pass regions. As the transmission region of the multiplexed LP (or SP) filters is wider, such parasitic effects are reduced thereby improving the overall performance of the FPA unit.</div>
<div class="description-paragraph" id="p-0222" num="0264">In one implementation, the LP and SP filters can be combined, in a spectrally-multiplexed fashion, in order to increase or maximize the spectral extent of the transmission region of the filter system of the embodiment.</div>
<div class="description-paragraph" id="p-0223" num="0265">The advantage of using spectrally multiplexed filters is appreciated based on the following derivation, in which a system of M filters is examined (although it is understood that in practice an embodiment of the invention can employ any number of filters). As an illustrative example, the case of M=7 is considered. Analysis presented below relates to one spatial location in each of the images (e.g., sub-images) formed by the differing imaging channels (e.g., different optical channels <b>120</b>) in the system. A similar analysis can be performed for each point at an image (e.g., sub-image), and thus the analysis can be appropriately extended as required.</div>
<div class="description-paragraph" id="p-0224" num="0266">The unknown amount of light within each of the M spectral channels (corresponding to these M filters) is denoted with f<sub>⋅1⋅</sub>, f<sub>⋅2⋅</sub>, f<sub>⋅3⋅</sub>, f<sub>3 </sub>. . . f<sub>⋅M⋅</sub>, and readings from corresponding detector elements receiving light transmitted by each filter is denoted as g<sub>⋅1⋅</sub>, g<sub>⋅2⋅</sub>, g<sub>⋅3⋅ </sub>. . . g<sub>⋅M⋅</sub>, while measurement errors are represented by n<sub>⋅1⋅</sub>, n<sub>⋅2⋅</sub>, n<sub>⋅3⋅</sub>, . . . n<sub>⋅M⋅</sub>. Then, the readings at the seven FPA pixels each of which is optically filtered by a corresponding band-pass filter of <figref idrefs="DRAWINGS">FIG. 6A</figref> can be represented by:
<br/>
<i>g</i> <sub>1</sub> <i>=f</i> <sub>1</sub> <i>+n</i> <sub>1</sub>,
<br/>
<i>g</i> <sub>2</sub> <i>=f</i> <sub>2</sub> <i>+n</i> <sub>2</sub>,
<br/>
<i>g</i> <sub>3</sub> <i>=f</i> <sub>3</sub> <i>+n</i> <sub>3</sub>,
<br/>
<i>g</i> <sub>4</sub> <i>=f</i> <sub>4</sub> <i>+n</i> <sub>4</sub>,
<br/>
<i>g</i> <sub>5</sub> <i>=f</i> <sub>5</sub> <i>+n</i> <sub>5</sub>,
<br/>
<i>g</i> <sub>6</sub> <i>=f</i> <sub>6</sub> <i>+n</i> <sub>6</sub>,
<br/>
<i>g</i> <sub>7</sub> <i>=f</i> <sub>7</sub> <i>+n</i> <sub>7</sub>,
</div>
<div class="description-paragraph" id="p-0225" num="0267">These readings (pixel measurements) g<sub>⋅1⋅</sub> are estimates of the spectral intensities f<sub>⋅i⋅</sub>. The estimates g<sub>⋅i⋅ </sub>are not equal to the corresponding f<sub>⋅i⋅ </sub>values because of the measurement errors n<sub>⋅i⋅</sub>. However, if the measurement noise distribution has zero mean, then the ensemble mean of each individual measurement can be considered to be equal to the true value, i.e. <div class="patent-image small-patent-image"><a href="https://patentimages.storage.googleapis.com/8c/95/56/ff0b36e8fe85b4/US10955355-20210323-P00001.png"><img alt="Figure US10955355-20210323-P00001" class="patent-full-image" file="US10955355-20210323-P00001.TIF" he="3.22mm" height="13" id="CUSTOM-CHARACTER-00001" img-content="character" img-format="tif" inline="no" orientation="portrait" src="https://patentimages.storage.googleapis.com/8c/95/56/ff0b36e8fe85b4/US10955355-20210323-P00001.png" wi="0.68mm" width="3"/></a></div>g<sub>i</sub> <div class="patent-image small-patent-image"><a href="https://patentimages.storage.googleapis.com/fd/6c/c4/9ea8e0e07c8bcf/US10955355-20210323-P00002.png"><img alt="Figure US10955355-20210323-P00002" class="patent-full-image" file="US10955355-20210323-P00002.TIF" he="3.22mm" height="13" id="CUSTOM-CHARACTER-00002" img-content="character" img-format="tif" inline="no" orientation="portrait" src="https://patentimages.storage.googleapis.com/fd/6c/c4/9ea8e0e07c8bcf/US10955355-20210323-P00002.png" wi="1.10mm" width="4"/></a></div>=f<sub>i</sub>. Here, the angle brackets indicate the operation of calculating the ensemble mean of a stochastic variable. The variance of the measurement can, therefore, be represented as:
<br/>
<div class="patent-image small-patent-image"><a href="https://patentimages.storage.googleapis.com/8c/95/56/ff0b36e8fe85b4/US10955355-20210323-P00001.png"><img alt="Figure US10955355-20210323-P00001" class="patent-full-image" file="US10955355-20210323-P00001.TIF" he="3.22mm" height="13" id="CUSTOM-CHARACTER-00003" img-content="character" img-format="tif" inline="no" orientation="portrait" src="https://patentimages.storage.googleapis.com/8c/95/56/ff0b36e8fe85b4/US10955355-20210323-P00001.png" wi="0.68mm" width="3"/></a></div>(<i>g</i> <sub>i</sub> <i>−f</i> <sub>i</sub>)<sup>2</sup> <div class="patent-image small-patent-image"><a href="https://patentimages.storage.googleapis.com/fd/6c/c4/9ea8e0e07c8bcf/US10955355-20210323-P00002.png"><img alt="Figure US10955355-20210323-P00002" class="patent-full-image" file="US10955355-20210323-P00002.TIF" he="3.22mm" height="13" id="CUSTOM-CHARACTER-00004" img-content="character" img-format="tif" inline="no" orientation="portrait" src="https://patentimages.storage.googleapis.com/fd/6c/c4/9ea8e0e07c8bcf/US10955355-20210323-P00002.png" wi="1.10mm" width="4"/></a></div> <i>=</i> <div class="patent-image small-patent-image"><a href="https://patentimages.storage.googleapis.com/8c/95/56/ff0b36e8fe85b4/US10955355-20210323-P00001.png"><img alt="Figure US10955355-20210323-P00001" class="patent-full-image" file="US10955355-20210323-P00001.TIF" he="3.22mm" height="13" id="CUSTOM-CHARACTER-00005" img-content="character" img-format="tif" inline="no" orientation="portrait" src="https://patentimages.storage.googleapis.com/8c/95/56/ff0b36e8fe85b4/US10955355-20210323-P00001.png" wi="0.68mm" width="3"/></a></div> <i>n</i> <sub>i</sub> <sup>2</sup> <div class="patent-image small-patent-image"><a href="https://patentimages.storage.googleapis.com/fd/6c/c4/9ea8e0e07c8bcf/US10955355-20210323-P00002.png"><img alt="Figure US10955355-20210323-P00002" class="patent-full-image" file="US10955355-20210323-P00002.TIF" he="3.22mm" height="13" id="CUSTOM-CHARACTER-00006" img-content="character" img-format="tif" inline="no" orientation="portrait" src="https://patentimages.storage.googleapis.com/fd/6c/c4/9ea8e0e07c8bcf/US10955355-20210323-P00002.png" wi="1.10mm" width="4"/></a></div>=σ<sup>2 </sup>
</div>
<div class="description-paragraph" id="p-0226" num="0268">In embodiments utilizing spectrally-multiplexed filters, in comparison with the embodiments utilizing band-pass filters, the amount of radiant energy transmitted by each of the spectrally-multiplexed LP or SP filters towards a given detector element can exceed that transmitted through a spectral band of a band-pass filter. In this case, the intensities of light corresponding to the independent spectral bands can be reconstructed by computational means. Such embodiments can be referred to as a “multiplex design”.</div>
<div class="description-paragraph" id="p-0227" num="0269">One matrix of such “multiplexed filter” measurements includes a Hadamard matrix requiring “negative” filters that may not be necessarily appropriate for the optical embodiments disclosed herein. An S-matrix approach (which is restricted to having a number of filters equal to an integer that is multiple of four minus one) or a row-doubled Hadamard matrix (requiring a number of filters to be equal to an integer multiple of eight) can be used in various embodiments. Here, possible numbers of filters using an S-matrix setup are 3, 7, 11, etc and, if a row-doubled Hadamard matrix setup is used, then the possible number of filters is 8, 16, 24, etc. For example, the goal of the measurement may be to measure seven spectral band f<sub>⋅i⋅ </sub>intensities using seven measurements g<sub>⋅i⋅ </sub>as follows:
<br/>
<i>g</i> <sub>1</sub> <i>=f</i> <sub>1</sub>+0+<i>f</i> <sub>3</sub>+0+<i>f</i> <sub>5</sub>0+<i>f</i> <sub>7</sub> <i>+n</i> <sub>1</sub>,
<br/>
<i>g</i> <sub>2</sub>=0+<i>f</i> <sub>2</sub> <i>+f</i> <sub>3</sub>+0+0+<i>f</i> <sub>6</sub> <i>+f</i> <sub>7</sub> <i>+n</i> <sub>2 </sub>
<br/>
<i>g</i> <sub>3</sub> <i>=f</i> <sub>1</sub> <i>+f</i> <sub>2</sub>+0+0+<i>f</i> <sub>5</sub>+0+<i>f</i> <sub>7</sub> <i>+n</i> <sub>3 </sub>
<br/>
<i>g</i> <sub>4</sub>=0+0+0+<i>f</i> <sub>4</sub> <i>+f</i> <sub>5</sub> <i>+f</i> <sub>7</sub> <i>+f</i> <sub>8</sub> <i>+n</i> <sub>4 </sub>
<br/>
<i>g</i> <sub>5</sub> <i>=f</i> <sub>1</sub>+0+<i>f</i> <sub>3</sub> <i>+f</i> <sub>4</sub>+0+<i>f</i> <sub>6</sub>+0+<i>n</i> <sub>5 </sub>
<br/>
<i>g</i> <sub>6</sub>=0+<i>f</i> <sub>2</sub> <i>+f</i> <sub>3</sub> <i>+f</i> <sub>4</sub> <i>+f</i> <sub>5</sub>+0+0+<i>n</i> <sub>6 </sub>
<br/>
<i>g</i> <sub>7</sub> <i>=f</i> <sub>1</sub> <i>+f</i> <sub>2</sub>+0+<i>f</i> <sub>4</sub>+0+0+<i>f</i> <sub>7</sub> <i>+n</i> <sub>7 </sub>
</div>
<div class="description-paragraph" id="p-0228" num="0270">Optical transmission characteristics of the filters described above are depicted in <figref idrefs="DRAWINGS">FIG. 6B</figref>. Here, a direct estimate of the f<sub>⋅i⋅ </sub>is no longer provided through a relationship similar to <div class="patent-image small-patent-image"><a href="https://patentimages.storage.googleapis.com/8c/95/56/ff0b36e8fe85b4/US10955355-20210323-P00001.png"><img alt="Figure US10955355-20210323-P00001" class="patent-full-image" file="US10955355-20210323-P00001.TIF" he="3.22mm" height="13" id="CUSTOM-CHARACTER-00007" img-content="character" img-format="tif" inline="no" orientation="portrait" src="https://patentimages.storage.googleapis.com/8c/95/56/ff0b36e8fe85b4/US10955355-20210323-P00001.png" wi="0.68mm" width="3"/></a></div>g<sub>i</sub> <div class="patent-image small-patent-image"><a href="https://patentimages.storage.googleapis.com/fd/6c/c4/9ea8e0e07c8bcf/US10955355-20210323-P00002.png"><img alt="Figure US10955355-20210323-P00002" class="patent-full-image" file="US10955355-20210323-P00002.TIF" he="3.22mm" height="13" id="CUSTOM-CHARACTER-00008" img-content="character" img-format="tif" inline="no" orientation="portrait" src="https://patentimages.storage.googleapis.com/fd/6c/c4/9ea8e0e07c8bcf/US10955355-20210323-P00002.png" wi="1.10mm" width="4"/></a></div>=f<sub>i</sub>. Instead, if a “hat” notation is used to denote an estimate of a given value, then a linear combination of the measurements can be used such as, for example,</div>
<div class="description-paragraph" id="p-0229" num="0271">
<maths id="MATH-US-00001" num="00001">
<math overflow="scroll">
<mrow>
<mrow>
<msub>
<mover>
<mi>f</mi>
<mi>^</mi>
</mover>
<mn>1</mn>
</msub>
<mo>=</mo>
<mrow>
<mfrac>
<mn>1</mn>
<mn>4</mn>
</mfrac>
<mo>⁢</mo>
<mrow>
<mo>(</mo>
<mrow>
<mrow>
<mo>+</mo>
<msub>
<mi>g</mi>
<mn>1</mn>
</msub>
</mrow>
<mo>-</mo>
<msub>
<mi>g</mi>
<mn>2</mn>
</msub>
<mo>+</mo>
<msub>
<mi>g</mi>
<mn>3</mn>
</msub>
<mo>-</mo>
<msub>
<mi>g</mi>
<mn>4</mn>
</msub>
<mo>+</mo>
<msub>
<mi>g</mi>
<mn>5</mn>
</msub>
<mo>-</mo>
<msub>
<mi>g</mi>
<mn>6</mn>
</msub>
<mo>+</mo>
<msub>
<mi>g</mi>
<mn>7</mn>
</msub>
</mrow>
<mo>)</mo>
</mrow>
</mrow>
</mrow>
<mo>,</mo>
<mstyle>
<mtext>
</mtext>
</mstyle>
<mo>⁢</mo>
<mrow>
<msub>
<mover>
<mi>f</mi>
<mi>^</mi>
</mover>
<mn>2</mn>
</msub>
<mo>=</mo>
<mrow>
<mfrac>
<mn>1</mn>
<mn>4</mn>
</mfrac>
<mo>⁢</mo>
<mrow>
<mo>(</mo>
<mrow>
<mrow>
<mo>-</mo>
<msub>
<mi>g</mi>
<mn>1</mn>
</msub>
</mrow>
<mo>+</mo>
<msub>
<mi>g</mi>
<mn>2</mn>
</msub>
<mo>+</mo>
<msub>
<mi>g</mi>
<mn>3</mn>
</msub>
<mo>-</mo>
<msub>
<mi>g</mi>
<mn>4</mn>
</msub>
<mo>-</mo>
<msub>
<mi>g</mi>
<mn>5</mn>
</msub>
<mo>+</mo>
<msub>
<mi>g</mi>
<mn>6</mn>
</msub>
<mo>+</mo>
<msub>
<mi>g</mi>
<mn>7</mn>
</msub>
</mrow>
<mo>)</mo>
</mrow>
</mrow>
</mrow>
<mo>,</mo>
<mstyle>
<mtext>
</mtext>
</mstyle>
<mo>⁢</mo>
<mrow>
<msub>
<mover>
<mi>f</mi>
<mo>^</mo>
</mover>
<mn>3</mn>
</msub>
<mo>=</mo>
<mrow>
<mfrac>
<mn>1</mn>
<mn>4</mn>
</mfrac>
<mo>⁢</mo>
<mrow>
<mo>(</mo>
<mrow>
<mrow>
<mo>+</mo>
<msub>
<mi>g</mi>
<mn>1</mn>
</msub>
</mrow>
<mo>+</mo>
<msub>
<mi>g</mi>
<mn>2</mn>
</msub>
<mo>-</mo>
<msub>
<mi>g</mi>
<mn>3</mn>
</msub>
<mo>-</mo>
<msub>
<mi>g</mi>
<mn>4</mn>
</msub>
<mo>+</mo>
<msub>
<mi>g</mi>
<mn>5</mn>
</msub>
<mo>+</mo>
<msub>
<mi>g</mi>
<mn>6</mn>
</msub>
<mo>-</mo>
<msub>
<mi>g</mi>
<mn>7</mn>
</msub>
</mrow>
<mo>)</mo>
</mrow>
</mrow>
</mrow>
<mo>,</mo>
<mstyle>
<mtext>
</mtext>
</mstyle>
<mo>⁢</mo>
<mrow>
<msub>
<mover>
<mi>f</mi>
<mi>^</mi>
</mover>
<mn>4</mn>
</msub>
<mo>=</mo>
<mrow>
<mfrac>
<mn>1</mn>
<mn>4</mn>
</mfrac>
<mo>⁢</mo>
<mrow>
<mo>(</mo>
<mrow>
<mrow>
<mo>-</mo>
<msub>
<mi>g</mi>
<mn>1</mn>
</msub>
</mrow>
<mo>-</mo>
<msub>
<mi>g</mi>
<mn>2</mn>
</msub>
<mo>-</mo>
<msub>
<mi>g</mi>
<mn>3</mn>
</msub>
<mo>+</mo>
<msub>
<mi>g</mi>
<mn>4</mn>
</msub>
<mo>+</mo>
<msub>
<mi>g</mi>
<mn>5</mn>
</msub>
<mo>+</mo>
<msub>
<mi>g</mi>
<mn>6</mn>
</msub>
<mo>+</mo>
<msub>
<mi>g</mi>
<mn>7</mn>
</msub>
</mrow>
<mo>)</mo>
</mrow>
</mrow>
</mrow>
<mo>,</mo>
<mstyle>
<mtext>
</mtext>
</mstyle>
<mo>⁢</mo>
<mrow>
<msub>
<mover>
<mi>f</mi>
<mi>^</mi>
</mover>
<mn>5</mn>
</msub>
<mo>=</mo>
<mrow>
<mfrac>
<mn>1</mn>
<mn>4</mn>
</mfrac>
<mo>⁢</mo>
<mrow>
<mo>(</mo>
<mrow>
<mrow>
<mo>+</mo>
<msub>
<mi>g</mi>
<mn>1</mn>
</msub>
</mrow>
<mo>-</mo>
<msub>
<mi>g</mi>
<mn>2</mn>
</msub>
<mo>+</mo>
<msub>
<mi>g</mi>
<mn>3</mn>
</msub>
<mo>+</mo>
<msub>
<mi>g</mi>
<mn>4</mn>
</msub>
<mo>-</mo>
<msub>
<mi>g</mi>
<mn>5</mn>
</msub>
<mo>+</mo>
<msub>
<mi>g</mi>
<mn>6</mn>
</msub>
<mo>-</mo>
<msub>
<mi>g</mi>
<mn>7</mn>
</msub>
</mrow>
<mo>)</mo>
</mrow>
</mrow>
</mrow>
<mo>,</mo>
<mstyle>
<mtext>
</mtext>
</mstyle>
<mo>⁢</mo>
<mrow>
<msub>
<mover>
<mi>f</mi>
<mi>^</mi>
</mover>
<mn>6</mn>
</msub>
<mo>=</mo>
<mrow>
<mfrac>
<mn>1</mn>
<mn>4</mn>
</mfrac>
<mo>⁢</mo>
<mrow>
<mo>(</mo>
<mrow>
<mrow>
<mo>-</mo>
<msub>
<mi>g</mi>
<mn>1</mn>
</msub>
</mrow>
<mo>+</mo>
<msub>
<mi>g</mi>
<mn>2</mn>
</msub>
<mo>+</mo>
<msub>
<mi>g</mi>
<mn>3</mn>
</msub>
<mo>+</mo>
<msub>
<mi>g</mi>
<mn>4</mn>
</msub>
<mo>+</mo>
<msub>
<mi>g</mi>
<mn>5</mn>
</msub>
<mo>-</mo>
<msub>
<mi>g</mi>
<mn>6</mn>
</msub>
<mo>-</mo>
<msub>
<mi>g</mi>
<mn>7</mn>
</msub>
</mrow>
<mo>)</mo>
</mrow>
</mrow>
</mrow>
<mo>,</mo>
<mstyle>
<mtext>
</mtext>
</mstyle>
<mo>⁢</mo>
<mrow>
<msub>
<mover>
<mi>f</mi>
<mi>^</mi>
</mover>
<mn>7</mn>
</msub>
<mo>=</mo>
<mrow>
<mfrac>
<mn>1</mn>
<mn>4</mn>
</mfrac>
<mo>⁢</mo>
<mrow>
<mo>(</mo>
<mrow>
<mrow>
<mo>+</mo>
<msub>
<mi>g</mi>
<mn>1</mn>
</msub>
</mrow>
<mo>+</mo>
<msub>
<mi>g</mi>
<mn>2</mn>
</msub>
<mo>-</mo>
<msub>
<mi>g</mi>
<mn>3</mn>
</msub>
<mo>+</mo>
<msub>
<mi>g</mi>
<mn>4</mn>
</msub>
<mo>-</mo>
<msub>
<mi>g</mi>
<mn>5</mn>
</msub>
<mo>-</mo>
<msub>
<mi>g</mi>
<mn>6</mn>
</msub>
<mo>+</mo>
<msub>
<mi>g</mi>
<mn>7</mn>
</msub>
</mrow>
<mo>)</mo>
</mrow>
</mrow>
</mrow>
<mo>,</mo>
</mrow>
</math>
</maths>
</div>
<div class="description-paragraph" id="p-0230" num="0272">These {circumflex over (f)}<sub>i </sub>are unbiased estimates when the n<sub>⋅1⋅</sub> are zero mean stochastic variables, so that <div class="patent-image small-patent-image"><a href="https://patentimages.storage.googleapis.com/8c/95/56/ff0b36e8fe85b4/US10955355-20210323-P00001.png"><img alt="Figure US10955355-20210323-P00001" class="patent-full-image" file="US10955355-20210323-P00001.TIF" he="3.22mm" height="13" id="CUSTOM-CHARACTER-00009" img-content="character" img-format="tif" inline="no" orientation="portrait" src="https://patentimages.storage.googleapis.com/8c/95/56/ff0b36e8fe85b4/US10955355-20210323-P00001.png" wi="0.68mm" width="3"/></a></div>{circumflex over (f)}<sub>i</sub>−f<sub>i</sub> <div class="patent-image small-patent-image"><a href="https://patentimages.storage.googleapis.com/8c/95/56/ff0b36e8fe85b4/US10955355-20210323-P00001.png"><img alt="Figure US10955355-20210323-P00001" class="patent-full-image" file="US10955355-20210323-P00001.TIF" he="3.22mm" height="13" id="CUSTOM-CHARACTER-00010" img-content="character" img-format="tif" inline="no" orientation="portrait" src="https://patentimages.storage.googleapis.com/8c/95/56/ff0b36e8fe85b4/US10955355-20210323-P00001.png" wi="0.68mm" width="3"/></a></div>=0. The measurement variance corresponding to i<sup>th</sup>. measurement is given by the equation below:</div>
<div class="description-paragraph" id="p-0231" num="0273">
<maths id="MATH-US-00002" num="00002">
<math overflow="scroll">
<mrow>
<mrow>
<mo>〈</mo>
<msup>
<mrow>
<mo>(</mo>
<mrow>
<msub>
<mover>
<mi>f</mi>
<mi>^</mi>
</mover>
<mi>i</mi>
</msub>
<mo>-</mo>
<msub>
<mi>f</mi>
<mi>i</mi>
</msub>
</mrow>
<mo>)</mo>
</mrow>
<mn>2</mn>
</msup>
<mo>〉</mo>
</mrow>
<mo>=</mo>
<mrow>
<mfrac>
<mn>7</mn>
<mrow>
<mn>1</mn>
<mo>⁢</mo>
<mn>6</mn>
</mrow>
</mfrac>
<mo>⁢</mo>
<msup>
<mi>σ</mi>
<mn>2</mn>
</msup>
</mrow>
</mrow>
</math>
</maths>
</div>
<div class="description-paragraph" id="p-0232" num="0274">From the above equation, it is observed that by employing spectrally-multiplexed system the signal-to-noise ratio (SNR) of a measurement is improved by a factor of √{square root over (<b>16</b>/<b>7</b>)}=1.51 √{square root over (<b>7</b>/<b>16</b>)}=0.66.</div>
<div class="description-paragraph" id="p-0233" num="0275">For N channels, the SNR improvement achieved with a spectrally-multiplexed system can be expressed as (N+1)/(2√{square root over (N)}) For example, an embodiment employing 12 spectral channels (N=12) is characterized by a SNR improvement, over a non-spectrally-multiplexed system, comprising a factor of up to 1.88.</div>
<div class="description-paragraph" id="p-0234" num="0276">Two additional examples of related spectrally-multiplexed filter arrangements <b>0</b> <i>c </i>through <b>3</b> <i>c </i>and <b>0</b> <i>d </i>through <b>3</b> <i>d </i>that can be used in various embodiments of the imaging systems described herein are shown in <figref idrefs="DRAWINGS">FIGS. 6C and 6D</figref>, respectively. The spectrally-multiplexed filters shown in <figref idrefs="DRAWINGS">FIGS. 6C and 6D</figref> can be used in embodiments of imaging systems employing uncooled FPAs (such as microbolometers). <figref idrefs="DRAWINGS">FIG. 6C</figref> illustrates a set of spectrally-multiplexed long-wavelength pass (LP) filters used in the system. An LP filter generally attenuates shorter wavelengths and transmits (passes) longer wavelengths (e.g., over the active range of the target IR portion of the spectrum). A single spectral channel having a transmission characteristic corresponding to the difference between the spectral transmission curves of at least two of these LP filters can be used to procure imaging data for the data cube using an embodiment of the system described herein. In various implementations, the spectral filters disposed with respect to the different FPAs can have different spectral characteristics. In various implementations, the spectral filters may be disposed in front of only some of the FPAs while the remaining FPAs may be configured to receive unfiltered light. For example, in some implementations, only 9 of the 12 detectors in the 4×3 array of detectors described above may be associated with a spectral filter while the other 3 detectors may be configured to received unfiltered light. Such a system may be configured to acquire spectral data in 10 different spectral channels in a single data acquisition event.</div>
<div class="description-paragraph" id="p-0235" num="0277">The use of microbolometers, as detector-noise-limited devices, in turn not only can benefit from the use of spectrally multiplexed filters, but also does not require cooling of the imaging system during normal operation. In contrast to imaging systems that include highly sensitive FPA units with reduced noise characteristics, the embodiments of imaging systems described herein can employ less sensitive microbolometers without compromising the SNR. This result is at least in part due to the snap-shot/non-scanning mode of operation.</div>
<div class="description-paragraph" id="p-0236" num="0278">As discussed above, an embodiment may optionally, and in addition to a temperature-controlled reference unit (for example temperature controlled shutters such as shutters <b>160</b>, <b>460</b> <i>a</i>, <b>460</b> <i>b</i>), employ a field reference component (e.g., field reference aperture <b>338</b> in <figref idrefs="DRAWINGS">FIG. 3A</figref>), or an array of field reference components (e.g., filed reference apertures <b>438</b> in <figref idrefs="DRAWINGS">FIG. 4</figref>), to enable dynamic calibration. Such dynamic calibration can be used for spectral acquisition of one or more or every data cube. Such dynamic calibration can also be used for a spectrally-neutral camera-to-camera combination to enable dynamic compensation of parallax artifacts. The use of the temperature-controlled reference unit (for example, temperature-controlled shutter system <b>160</b>) and field-reference component(s) facilitates maintenance of proper calibration of each of the FPAs individually and the entire FPA unit as a whole.</div>
<div class="description-paragraph" id="p-0237" num="0279">In particular, and in further reference to <figref idrefs="DRAWINGS">FIGS. 1, 2, 3, and 4</figref>, the temperature-controlled unit generally employs a system having first and second temperature zones maintained at first and second different temperatures. For example, shutter system of each of the embodiments <b>100</b>, <b>200</b>, <b>300</b> and <b>400</b> can employ not one but at least two temperature-controlled shutters that are substantially parallel to one another and transverse to the general optical axis <b>226</b> of the embodiment(s) <b>100</b>, <b>200</b>, <b>300</b>, <b>400</b>. Two shutters at two different temperatures may be employed to provide more information for calibration; for example, the absolute value of the difference between FPAs at one temperature as well as the change in that difference with temperature change can be recorded. Referring, for example, to <figref idrefs="DRAWINGS">FIG. 4</figref>, in which such multi-shutter structure is shown, the use of multiple shutters enables the user to create a known reference temperature difference perceived by the FPAs <b>456</b>. This reference temperature difference is provided by the IR radiation emitted by the shutter(s) <b>460</b> <i>a</i>, <b>460</b> <i>b </i>when these shutters are positioned to block the radiation from the object <b>110</b>. As a result, not only the offset values corresponding to each of the individual FPAs pixels can be adjusted but also the gain values of these FPAs. In an alternative embodiment, the system having first and second temperature zones may include a single or multi-portion piece. This single or multi-portion piece may comprise for example a plate. This piece may be mechanically-movable across the optical axis with the use of appropriate guides and having a first portion at a first temperature and a second portion at a second temperature.</div>
<div class="description-paragraph" id="p-0238" num="0280">Indeed, the process of calibration of an embodiment of the imaging system starts with estimating gain and offset by performing measurements of radiation emanating, independently, from at least two temperature-controlled shutters of known and different radiances. The gain and offset can vary from detector pixel to detector pixel. Specifically, first the response of the detector unit <b>456</b> to radiation emanating from one shutter is carried out. For example, the first shutter <b>460</b> <i>a </i>blocks the FOV of the detectors <b>456</b> and the temperature T<sub>⋅1⋅ </sub>is measured directly and independently with thermistors. Following such initial measurement, the first shutter <b>460</b> <i>a </i>is removed from the optical path of light traversing the embodiment and another second shutter (for example, <b>460</b> <i>b</i>) is inserted in its place across the optical axis <b>226</b> to prevent the propagation of light through the system. The temperature of the second shutter <b>460</b> <i>b </i>can be different than the first shutter (T<sub>⋅2⋅</sub>≠T<sub>⋅1⋅</sub>). The temperature of the second shutter <b>460</b> <i>b </i>is also independently measured with thermistors placed in contact with this shutter, and the detector response to radiation emanating from the shutter <b>460</b> <i>b </i>is also recorded. Denoting operational response of FPA pixels (expressed in digital numbers, or “counts”) as g<sub>⋅i⋅ </sub>to a source of radiance L<sub>⋅i⋅</sub>, the readings corresponding to the measurements of the two shutters can be expressed as:
<br/>
<i>g</i> <sub>1</sub> <i>=γL</i> <sub>1</sub>(<i>T</i> <sub>1</sub>)+<i>g</i> <sub>offset </sub>
<br/>
<i>g</i> <sub>2</sub> <i>=γL</i> <sub>2</sub>(<i>T</i> <sub>2</sub>)+<i>g</i> <sub>offset </sub>
</div>
<div class="description-paragraph" id="p-0239" num="0281">Here, g<sub>⋅offset⋅ </sub>is the pixel offset value (in units of counts), and γ is the pixel gain value (in units of counts per radiance unit). The solutions of these two equations with respect to the two unknowns g<sub>⋅offset⋅ </sub>and γ can be obtained if the values of g<sub>⋅1⋅ </sub>and g<sub>⋅2⋅ </sub>and the radiance values L<sub>⋅1⋅ </sub>and L<sub>⋅2⋅ </sub>are available. These values can, for example, be either measured by a reference instrument or calculated from the known temperatures T<sub>⋅1⋅ </sub>and T<sub>⋅2⋅ </sub>together with the known spectral response of the optical system and FPA. For any subsequent measurement, one can then invert the equation(s) above in order to estimate the radiance value of the object from the detector measurement, and this can be done for each pixel in each FPA within the system.</div>
<div class="description-paragraph" id="p-0240" num="0282">As already discussed, and in reference to <figref idrefs="DRAWINGS">FIGS. 1 through 4</figref>, the field-reference apertures may be disposed in an object space or image space of the optical system, and dimensioned to block a particular portion of the IR radiation received from the object. In various implementations, the field-reference aperture, the opening of which can be substantially similar in shape to the boundary of the filter array (for example, and in reference to a filter array of <figref idrefs="DRAWINGS">FIGS. 3B, 5B</figref>—e.g., rectangular). The field-reference aperture can be placed in front of the objective lens (<b>124</b>, <b>224</b>, <b>324</b>, <b>424</b>) at a distance that is at least several times (in one implementation—at least five times) larger than the focal length of the lens such that the field-reference aperture is placed closer to the object. Placing the field-reference aperture closer to the object can reduce the blurriness of the image. In the embodiment <b>400</b> of <figref idrefs="DRAWINGS">FIG. 4</figref>, the field-reference aperture can be placed within the depth of focus of an image conjugate plane formed by the front objective lens <b>424</b>. The field reference, generally, can facilitate, effectuates and/or enable dynamic compensation in the system by providing a spectrally known and temporally-stable object within every scene to reference and stabilize the output from the different FPAs in the array.</div>
<div class="description-paragraph" id="p-0241" num="0283">Because each FPA's offset value is generally adjusted from each frame to the next frame by the hardware, comparing the outputs of one FPA with another can have an error that is not compensated for by the static calibration parameters g<sub>⋅offset⋅ </sub>and γ established, for example, by the movable shutters <b>160</b>. In order to ensure that FPAs operate in radiometric agreement over time, it is advantageous for a portion of each detector array to view a reference source (such as the field reference <b>338</b> in <figref idrefs="DRAWINGS">FIG. 3A</figref>, for example) over a plurality of frames obtained over time. If the reference source spectrum is known a priori (such as a blackbody source at a known temperature), one can measure the response of each FPA to the reference source in order to estimate changes to the pixel offset value. However, the temperature of the reference source need not be known. In such implementations, dynamic calibration of the different detectors can be performed by monitoring the change in the gain and the offset for the various detectors from the time the movable shutters used for static calibration are removed. An example calculation of the dynamic offset proceeds as follows.</div>
<div class="description-paragraph" id="p-0242" num="0284">Among the FPA elements in an array of FPAs in an embodiment of the imaging system, one FPA can be selected to be the “reference FPA”. The field reference temperature measured by all the other FPAs can be adjusted to agree with the field reference temperature measured by the reference as discussed below. The image obtained by each FPA includes a set of pixels obscured by the field reference <b>338</b>. Using the previously obtained calibration parameters g<sub>⋅offset⋅ </sub>and γ (the pixel offset and gain), the effective blackbody temperature T<sub>⋅i⋅ </sub>of the field reference as measured by each FPA is estimated using the equation below:
<br/>
<i>T</i> <sub>i</sub>=mean{(<i>g+Δg</i> <sub>i</sub> <i>+g</i> <sub>offset</sub>/γ}=mean{(<i>g−g</i> <sub>offset</sub>)/γ}+Δ<i>T</i> <sub>i </sub>
</div>
<div class="description-paragraph" id="p-0243" num="0285">Using the equation above, the mean value over all pixels that are obscured by the field reference is obtained. In the above equation Δg<sub>⋅i⋅</sub> is the difference in offset value of the current frame from Δg<sub>⋅offset⋅ </sub>obtained during the calibration step. For the reference FPA, Δg<sub>⋅i⋅</sub> can be simply set to zero. Then, using the temperature differences measured by each FPA, one obtains
<br/>
<i>T</i> <sub>i</sub> <i>−T</i> <sub>ref</sub>=mean{(<i>g+Δg</i> <sub>i</sub> <i>+g</i> <sub>offset</sub> <i>/γ}+ΔT</i> <sub>i</sub>−mean{(<i>g−g</i> <sub>offset</sub>)/γ}=Δ<i>T</i> <sub>i </sub>
</div>
<div class="description-paragraph" id="p-0244" num="0286">Once ΔT<sub>⋅i⋅</sub> for each FPA is measured, its value can be subtracted from each image in order to force operational agreement between such FPA and the reference FPA. While the calibration procedure has been discussed above in reference to calibration of temperature, a procedurally similar methodology of calibration with respect to radiance value can also be implemented.</div>
<div class="description-paragraph" id="h-0167" num="0000">Examples of Methodology of Measurements.</div>
<div class="description-paragraph" id="p-0245" num="0287">Prior to optical data acquisition using an embodiment of the IR imaging system as described herein, one or more, most, or potentially all the FPAs of the system can be calibrated. For example, greater than 50%, 60%, 70%, 80% or 90% of the FPAs <b>336</b> can be initially calibrated. As shown in <figref idrefs="DRAWINGS">FIG. 3A</figref>, these FPAs <b>336</b> may form separate images of the object using light delivered in a corresponding optical channel that may include the combination of the corresponding front objective and re-imaging lenses <b>324</b>, <b>128</b>. The calibration procedure can allow formation of individual images in equivalent units (so that, for example, the reading from the FPA pixels can be re-calculated in units of temperature or radiance units, etc.). Moreover, the calibration process can also allow the FPAs (e.g., each of the FPAs) to be spatially co-registered with one another so that a given pixel of a particular FPA can be optically re-mapped through the optical system to the same location at the object as the corresponding pixel of another FPA.</div>
<div class="description-paragraph" id="p-0246" num="0288">To achieve at least some of these goals, a spectral differencing method may be employed. The method involves forming a difference image from various combinations of the images from different channels. In particular, the images used to form difference images can be registered by two or more different FPAs in spectrally distinct channels having different spectral filters with different spectral characteristics. Images from different channels having different spectral characteristics will provide different spectral information. Comparing (e.g., subtracting) these images, can therefore yield valuable spectral based information. For example, if the filter element of the array of spectral filters <b>130</b> corresponding to a particular FPA <b>336</b> transmits light from the object <b>110</b> including a cloud of gas, for example, with a certain spectrum that contains the gas absorption peak or a gas emission peak while another filter element of the array of spectral filters <b>130</b> corresponding to another FPA <b>336</b> does not transmit such spectrum, then the difference between the images formed by the two FPAs at issue will highlight the presence of gas in the difference image.</div>
<div class="description-paragraph" id="p-0247" num="0289">A shortcoming of the spectral differencing method is that contributions of some auxiliary features associated with imaging (not just the target species such as gas itself) can also be highlighted in and contribute to the difference image. Such contributing effects include, to name just a few, parallax-induced imaging of edges of the object, influence of magnification differences between the two or more optical channels, and differences in rotational positioning and orientation between the FPAs. While magnification-related errors and FPA-rotation-caused errors can be compensated for by increasing the accuracy of the instrument construction as well as by post-processing of the acquired imaging, parallax is scene-induced and is not so easily correctable. In addition, the spectral differencing method is vulnerable to radiance calibration errors. Specifically, if one FPA registers radiance of light from a given feature of the object as having a temperature of 40° C., for example, while the data from another FPA represents the temperature of the same object feature as being 39° C., then such feature of the object will be enhanced or highlighted in the difference image (formed at least in part based on the images provided by these two FPAs) due to such radiance-calibration error.</div>
<div class="description-paragraph" id="p-0248" num="0290">One solution to some of such problems is to compare (e.g., subtract) images from the same FPA obtained at different instances in time. For example, images can be compared to or subtracted from a reference image obtained at another time. Such reference image, which is subtracted from other later obtained images, may be referred to as a temporal reference image. This solution can be applied to spectral difference images as well. For example, the image data resulting from spectral difference images can be normalized by the data corresponding to a temporal reference image. For instance, the temporal reference images can be subtracted from the spectral difference image to obtain the temporal difference image. This process is referred to, for the purposes of this disclosure, as a temporal differencing algorithm or method and the resultant image from subtracting the temporal reference image from another image (such as the spectral difference image) is referred to as the temporal difference image. In some embodiments where spectral differencing is employed, a temporal reference image may be formed, for example, by creating a spectral difference image from the two or more images registered by the two or more FPAs at a single instance in time. This spectral difference image is then used as a temporal reference image. The temporal reference image can then be subtracted from other later obtained images to provide normalization that can be useful in subtracting out or removing various errors or deleterious effects. For example, the result of the algorithm is not affected by a prior knowledge of whether the object or scene contains a target species (such as gas of interest), because the algorithm can highlight changes in the scene characteristics. Thus, a spectral difference image can be calculated from multiple spectral channels as discussed above based on a snap-shot image acquisition at any later time and can be subtracted from the temporal reference image to form a temporal difference image. This temporal difference image is thus a normalized difference image. The difference between the two images (the temporal difference image) can highlight the target species (gas) within the normalized difference image, since this species was not present in the temporal reference frame. In various embodiments, more than two FPAs can be used both for registering the temporal reference image and a later-acquired difference image to obtain a better SNR figure of merit. For example, if two FPAs are associated with spectral filters having the same spectral characteristic, then the images obtained by the two FPAs can be combined after they have been registered to get a better SNR figure.</div>
<div class="description-paragraph" id="p-0249" num="0291">While the temporal differencing method can be used to reduce or eliminate some of the shortcomings of the spectral differencing, it can introduce unwanted problems of its own. For example, temporal differencing of imaging data is less sensitive to calibration and parallax induced errors than the spectral differencing of imaging data. However, any change in the imaged scene that is not related to the target species of interest (such as particular gas, for example) is highlighted in a temporally-differenced image. Thus such change in the imaged scene may be erroneously perceived as a location of the target species triggering, therefore, an error in detection of target species. For example, if the temperature of the background against which the gas is being detected changes (due to natural cooling down as the day progresses, or increases due to a person or animal or another object passing through the FOV of the IR imaging system), then such temperature change produces a signal difference as compared to the measurement taken earlier in time. Accordingly, the cause of the scenic temperature change (the cooling object, the person walking, etc.) may appear as the detected target species (such as gas). It follows, therefore, that an attempt to compensate for operational differences among the individual FPAs of a multi-FPA IR imaging system with the use of methods that turn on spectral or temporal differencing can cause additional problems leading to false detection of target species. Among these problems are scene-motion-induced detection errors and parallax-caused errors that are not readily correctable and/or compensatable. Accordingly, there is a need to compensate for image data acquisition and processing errors caused by motion of elements within the scene being imaged. Various embodiments of data processing algorithms described herein address and fulfill the need to compensate for such motion-induced and parallax-induced image detection errors.</div>
<div class="description-paragraph" id="p-0250" num="0292">In particular, to reduce or minimize parallax-induced differences between the images produced with two or more predetermined FPAs, another difference image can be used that is formed from the images of at least two different FPAs to estimate parallax effects. Parallax error can be determined by comparing the images from two different FPAs where the position between the FPAs is known. The parallax can be calculated from the known relative position difference. Differences between the images from these two FPAs can be attributed to parallax, especially, if the FPA have the same spectral characteristics, for example have the same spectral filter or both have no spectral filters. Parallax error correction, however, can still be obtained from two FPAs that have different spectral characteristics or spectral filters, especially if the different spectral characteristics, e.g., the transmission spectra of the respective filters are known and/or negligible. Use of more than two FPAs or FPAs of different locations such as FPAs spaced farther apart can be useful. For example, when the spectral differencing of the image data is performed with the use of the difference between the images collected by the outermost two cameras in the array (such as, for example, the FPAs corresponding to filters <b>2</b> and <b>3</b> of the array of filters of <figref idrefs="DRAWINGS">FIG. 5A</figref>), a difference image referred to as a “difference image <b>2</b>-<b>3</b>” is formed. In this case, the alternative “difference image <b>1</b>-<b>4</b>” is additionally formed from the image data acquired by, for example, the alternative FPAs corresponding to filters <b>1</b> and <b>4</b> of <figref idrefs="DRAWINGS">FIG. 5A</figref>. Assuming or ensuring that both of these two alternative FPAs have approximately the same spectral sensitivity to the target species, the alternative “difference image <b>1</b>-<b>4</b>” will highlight pixels corresponding to parallax-induced features in the image. Accordingly, based on positive determination that the same pixels are highlighted in the spectral “difference image <b>2</b>-<b>3</b>” used for target species detection, a conclusion can be made that the image features corresponding to these pixels are likely to be induced by parallax and not the presence of target species in the imaged scene. It should be noted that compensation of parallax can also be performed using images created by individual re-imaging lenses, <b>128</b> <i>a</i>, when using a single FPA or multiple FPA's as discussed above. FPAs spaced apart from each other in different directions can also be useful. Greater than 2, for example, 3 or 4, or more FPAs can be used to establish parallax for parallax correction. In certain embodiments two central FPAs and one corner FPA are used for parallax correction. These FPA may, in certain embodiments, have substantially similar or the same spectral characteristics, for example, have filters having similar or the same transmission spectrum or have no filter at all.</div>
<div class="description-paragraph" id="p-0251" num="0293">Another capability of the embodiments described herein is the ability to perform the volumetric estimation of a gas cloud. This can be accomplished by using (instead of compensating or negating) the parallax induced effects described above. In this case, the measured parallax between two or more similar spectral response images (e.g., two or more channels or FPAs) can be used to estimate a distance between the imaging system and the gas cloud or between the imaging system and an object in the field of view of the system. The parallax induced transverse image shift, d, between two images is related to the distance, z, between the cloud or object <b>110</b> and the imaging system according to the equation z=−sz′/d. Here, s, is the separation between two similar spectral response images, and z′ is the distance to the image plane from the back lens. The value for z′ is typically approximately equal to the focal length f of the lens of the imaging system. Once the distance z between the cloud and the imaging system is calculated, the size of the gas cloud can be determined based on the magnification, m=f/z, where each image pixel on the gas cloud, Δx′, corresponds to a physical size in object space Δx=Δx′/m. To estimate the volume of the gas cloud, a particular symmetry in the thickness of the cloud based on the physical size of the cloud can be assumed. For example; the cloud image can be rotated about a central axis running through the cloud image to create a three dimensional volume estimate of the gas cloud size. It is worth noting that in the embodiments described herein only a single imaging system is required for such volume estimation. Indeed, due to the fact that the information about the angle at which the gas cloud is seen by the system is decoded in the parallax effect, the image data includes the information about the imaged scene viewed by the system in association with at least two angles.</div>
<div class="description-paragraph" id="p-0252" num="0294">When the temporal differencing algorithm is used for processing the acquired imaging data, a change in the scene that is not caused by the target species can inadvertently be highlighted in the resulting image. In various embodiments, compensation for this error makes use of the temporal differencing between two FPAs that are substantially equally spectrally sensitive to the target species. In this case, the temporal difference image will highlight those pixels the intensity of which have changed in time (and not in wavelength). Therefore, subtracting the data corresponding to these pixels on both FPAs, which are substantially equally spectrally sensitive to the target species, to form the resulting image, excludes the contribution of the target species to the resulting image. The differentiation between (i) changes in the scene due to the presence of target species and (ii) changes in the scene caused by changes in the background not associated with the target species is, therefore, possible. In some embodiments, these two channels having the same or substantially similar spectral response so as to be substantially equally spectrally sensitive to the target species may comprise FPAs that operate using visible light. It should also be noted that, the data acquired with a visible light FPA (when present as part of the otherwise IR imaging system) can also be used to facilitate such differentiation and compensation of the motion-caused imaging errors. Visible cameras generally have much lower noise figure than IR cameras (at least during daytime). Consequently, the temporal difference image obtained with the use of image data from the visible light FPA can be quite accurate. The visible FPA can be used to compensate for motion in the system as well as many potential false-alarms in the scene due to motion caused by people, vehicles, birds, and steam, for example, as long as the moving object can be observed in the visible region of the spectra. This has the added benefit of providing an additional level of false alarm suppression without reducing the sensitivity of the system since many targets such as gas clouds cannot be observed in the visible spectral region. In various implementations, an IR camera can be used to compensate for motion artifacts.</div>
<div class="description-paragraph" id="p-0253" num="0295">Another method for detection of the gases is to use a spectral unmixing approach. A spectral unmixing approach assumes that the spectrum measured at a detector pixel is composed of a sum of component spectra (e.g., methane and other gases). This approach attempts to estimate the relative weights of these components needed to derive the measurement spectrum. The component spectra are generally taken from a predetermined spectral library (for example, from data collection that has been empirically assembled), though sometimes one can use the scene to estimate these as well (often called “endmember determination”). In various embodiments, the image obtained by the detector pixel is a radiance spectrum and provides information about the brightness of the object. To identify the contents of a gas cloud in the scene and/or to estimate the concentration of the various gases in the gas cloud, an absorption/emission spectrum of the various gases of interest can be obtained by comparing the measured brightness with an estimate of the expected brightness. The spectral unmixing methodology can also benefit from temporal, parallax, and motion compensation techniques.</div>
<div class="description-paragraph" id="p-0254" num="0296">In various embodiments, a method of identifying the presence of a target species in the object includes obtaining the radiance spectrum (or the absorption spectrum) from the object in a spectral region indicative of the presence of the target species and calculating a correlation (e.g., a correlation coefficient) by correlating the obtained radiance spectrum (or the absorption spectrum) with a reference spectrum for the target species. The presence or absence of the target species can be determined based on an amount of correlation (e.g., a value of correlation coefficient). For example, the presence of the target species in the object can be confirmed if the amount of correlation or the value of correlation coefficient is greater than a threshold. In various implementations, the radiance spectrum (or the absorption spectrum) can be obtained by obtaining a spectral difference image between a filtered optical channel and/or another filtered optical channel/unfiltered optical channel or any combinations thereof.</div>
<div class="description-paragraph" id="p-0255" num="0297">For example, an embodiment of the system configured to detect the presence of methane in a gas cloud comprises optical components such that one or more of the plurality of optical channels is configured to collect IR radiation to provide spectral data corresponding to a discrete spectral band located in the wavelength range between about 7.9 μm and about 8.4 μm corresponding to an absorption peak of methane. The multispectral data obtained in the one or more optical channels can be correlated with a predetermined absorption spectrum of methane in the wavelength range between about 7.9 μm and 8.4 μm. In various implementations, the predetermined absorption spectrum of methane can be saved in a database or a reference library accessible by the system. Based on an amount of correlation (e.g., a value of correlation coefficient), the presence or absence of methane in the gas cloud can be detected.</div>
<div class="description-paragraph" id="h-0168" num="0000">Examples of Practical Embodiments and Operation</div>
<div class="description-paragraph" id="p-0256" num="0298">The embodiment <b>300</b> of <figref idrefs="DRAWINGS">FIG. 3</figref> is configured to employ 12 optical channels and 12 corresponding microbolometer FPAs <b>336</b> to capture a video sequence substantially immediately after performing calibration measurements. The video sequence corresponds to images of a standard laboratory scene and the calibration measurements are performed with the use of a reference source including two shutters, as discussed above, one at room temperature and one 5° C. above room temperature. The use of 12 FPAs allows increased chance of simultaneous detection and estimation of the concentrations of about 8 or 9 gases present at the scene. In various embodiments, the number of FPAs <b>336</b> can vary, depending on the balance between the operational requirements and consideration of cost.</div>
<div class="description-paragraph" id="p-0257" num="0299">Due to the specifics of operation in the IR range of the spectrum, the use of the so-called noise-equivalent temperature difference (or NETD) is preferred and is analogous to the SNR commonly used in visible spectrum instruments. The array of microbolometer FPAs <b>336</b> is characterized to perform at NETD≤72 mK at an f-number of 1.2. Each measurement was carried out by summing four consecutive frames, and the reduction in the NETD value expected due to such summation would be described by corresponding factor of √4=2. Under ideal measurement conditions, therefore, the FPA NETD should be about 36 mK.</div>
<div class="description-paragraph" id="p-0258" num="0300">It is worth noting that the use of optically-filtered FPAs in various embodiments of the system described herein can provide a system with higher number of pixels. For example, embodiments including a single large format microbolometer FPA array can provide a system with large number of pixels. Various embodiments of the systems described herein can also offer a high optical throughput for a substantially low number of optical channels. For example, the systems described herein can provide a high optical throughput for a number of optical channels between 4 and 50. By having a lower number of optical channels (e.g., between 4 and 50 optical channels), the systems described herein have wider spectral bins which allows the signals acquired within each spectral bin to have a greater integrated intensity.</div>
<div class="description-paragraph" id="p-0259" num="0301">An advantage of the embodiments described herein over various scanning based hyperspectral systems that are configured for target species detection (for example, gas cloud detection) is that, the entire spectrum can be resolved in a snapshot mode (for example, during one image frame acquisition by the FPA array). This feature enables the embodiments of the imaging systems described herein to take advantage of the compensation algorithms such as the parallax and motion compensation algorithms mentioned above. Indeed, as the imaging data required to implement these algorithms are collected simultaneously with the target-species related data, the compensation algorithms are carried out with respect to target-species related data and not with respect to data acquired at another time interval. This rapid data collection thus improves the accuracy of the data compensation process. In addition, the frame rate of data acquisition is much higher. For example, embodiments of the imaging system described herein can operate at video rates from about 5 Hz and higher. For example, various embodiments described herein can operate at frame rates from about 5 Hz to about 60 Hz or 200 Hz. Thus, the user is able to recognize in the images the wisps and swirls typical of gas mixing without blurring out of these dynamic image features and other artifacts caused by the change of scene (whether spatial or spectral) during the lengthy measurements. In contradistinction, scanning based imaging systems involve image data acquisition over a period of time exceeding a single-snap-shot time and can, therefore, blur the target gas features in the image and inevitably reduce the otherwise achievable sensitivity of the detection. This result is in contrast to embodiments of the imaging system described herein that are capable of detecting the localized concentrations of gas without it being smeared out with the areas of thinner gas concentrations. In addition, the higher frame rate also enables a much faster response rate to a leak of gas (when detecting such leak is the goal). For example, an alarm can trigger within fractions of a second rather than several seconds.</div>
<div class="description-paragraph" id="p-0260" num="0302">To demonstrate the operation and gas detection capability of the imaging systems described herein, a prototype was constructed in accordance with the embodiment <b>300</b> of <figref idrefs="DRAWINGS">FIG. 3A</figref> and used to detect a hydrocarbon gas cloud of propylene at a distance of approximately 10 feet. <figref idrefs="DRAWINGS">FIG. 7</figref> illustrates video frames <b>1</b> through <b>12</b> representing gas-cloud-detection output <b>710</b> (seen as a streak of light) in a sequence from t=1 to t=12. The images <b>1</b> through <b>12</b> are selected frames taken from a video-data sequence captured at a video-rate of 15 frames/sec. The detected propylene gas is shown as a streak of light <b>710</b> (highlighted in red) near the center of each image. The first image is taken just prior to the gas emerging from the nozzle of a gas-contained, while the last image represents the system-output shortly after the nozzle has been turned off.</div>
<div class="description-paragraph" id="p-0261" num="0303">The same prototype of the system can also demonstrate the dynamic calibration improvement described above by imaging the scene, surrounding the system (the laboratory) with known temperature differences. The result of implementing the dynamic correction procedure is shown in <figref idrefs="DRAWINGS">FIGS. 8A, 8B</figref>, where the curves labeled “obj” (or “A”) represent temperature estimates of an identified region in the scene. The abscissa in each of the plots of <figref idrefs="DRAWINGS">FIGS. 8A, 8B</figref> indicates the number of a FPA, while the ordinate corresponds to temperature (in degrees C.). Accordingly, it is expected that when all detector elements receive radiant data that, when interpreted as the object's temperature, indicates that the object's temperature perceived by all detector elements is the same, any given curve would be a substantially flat line. Data corresponding to each of the multiple “obj” curves are taken from a stream of video frames separated from one another by about 0.5 seconds (for a total of 50 frames). The recorded “obj” curves shown in <figref idrefs="DRAWINGS">FIG. 8A</figref> indicate that the detector elements disagree about the object's temperature, and that difference in object's temperature perceived by different detector elements is as high as about 2.5° C. In addition, all of the temperature estimates are steadily drifting in time, from frame to frame. The curves labeled “ref” (or “C”) correspond to the detectors' estimates of the temperature of the aperture <b>338</b> of the embodiment <b>300</b> of <figref idrefs="DRAWINGS">FIG. 3A</figref>. The results of detection of radiation carried out after each detector pixel has been subjected to the dynamic calibration procedure described above are expressed with the curved labeled “obj CM” (or “B”). Now, the difference in estimated temperature of the object among the detector elements is reduced to about 0.5° C. (thereby improving the original reading at least by a factor of 5).</div>
<div class="description-paragraph" id="p-0262" num="0304"> <figref idrefs="DRAWINGS">FIG. 8B</figref> represents the results of similar measurements corresponding to a different location in the scene (a location which is at a temperature about 9° C. above the estimated temperature of the aperture <b>338</b> of <figref idrefs="DRAWINGS">FIG. 3A</figref>). As shown, the correction algorithm discussed above is operable and effective and applicable to objects kept at different temperature. Accordingly, the algorithm is substantially temperature independent.</div>
<div class="description-paragraph" id="h-0169" num="0000">Dynamic Calibration Elements and References</div>
<div class="description-paragraph" id="p-0263" num="0305"> <figref idrefs="DRAWINGS">FIGS. 9A and 9B</figref> illustrates schematically different implementations <b>900</b> and <b>905</b> respectively of the imaging system that include a variety of temperature calibration elements to facilitate dynamic calibration of the FPAs. The temperature calibration elements can include mirrors <b>975</b> <i>a</i>, <b>975</b> <i>b </i>(represented as M<sub>⋅1A⋅</sub>, M<sub>⋅9A⋅</sub>, etc.) as well as reference sources <b>972</b> <i>a </i>and <b>972</b> <i>b</i>. The implementation <b>900</b> can be similarly configured as the embodiment <b>300</b> and include one or more front objective lens, a divided aperture, one or more spectral filters, an array of imaging lenses <b>928</b> <i>a </i>and an imaging element <b>936</b>. In various implementations, the imaging element <b>936</b> (e.g., camera block) can include an array of cameras. In various implementations, the array of cameras can comprise an optical FPA unit. The optical FPA unit can comprise a single FPA, an array of FPAs. In various implementations, the array of cameras can include one or more detector arrays represented as detector array <b>1</b>, detector array <b>5</b>, detector array <b>9</b> in <figref idrefs="DRAWINGS">FIGS. 9A and 9B</figref>. In various embodiments, the FOV of each of the detector arrays <b>1</b>, <b>5</b>, <b>9</b> can be divided into a central region and a peripheral region. Without any loss of generality, the central region of the FOV of each of the detector arrays <b>1</b>, <b>5</b>, <b>9</b> can include the region where the FOV of all the detector arrays <b>1</b>, <b>5</b>, <b>9</b> overlap. In the embodiment illustrated in <figref idrefs="DRAWINGS">FIG. 9A</figref>, the reference sources <b>972</b> <i>a </i>and <b>972</b> <i>b </i>are placed at a distance from the detector arrays <b>1</b>, <b>5</b>, <b>9</b>, for example, and mirrors <b>975</b> <i>a </i>and <b>975</b> <i>b </i>that can image them onto the detector arrays are then placed at the location of the scene reference aperture (e.g., <b>338</b> of <figref idrefs="DRAWINGS">FIG. 3A</figref>).</div>
<div class="description-paragraph" id="p-0264" num="0306">In <figref idrefs="DRAWINGS">FIG. 9A</figref>, the mirrors <b>975</b> <i>a </i>and <b>975</b> <i>b </i>are configured to reflect radiation from the reference sources <b>972</b> <i>a </i>and <b>972</b> <i>b </i>(represented as ref A and ref B). The mirrors <b>975</b> <i>a </i>and <b>975</b> <i>b </i>can be disposed away from the central FOV of the detector arrays <b>1</b>, <b>5</b>, <b>9</b> such that the central FOV is not blocked or obscured by the image of the reference source <b>972</b> <i>a </i>and <b>972</b> <i>b</i>. In various implementations, the FOV of the detector array <b>5</b> could be greater than the FOV of the detector arrays <b>1</b> and <b>9</b>. In such implementations, the mirrors <b>975</b> <i>a </i>and <b>975</b> <i>b </i>can be disposed away from the central FOV of the detector array <b>5</b> at a location such that the reference source <b>972</b> <i>a </i>and <b>972</b> <i>b </i>is imaged by the detector array <b>5</b>. The mirrors <b>975</b> <i>a </i>and <b>975</b> <i>b </i>may comprise imaging optical elements having optical power that image the reference sources <b>972</b> <i>a </i>and <b>972</b> <i>b </i>onto the detector arrays <b>1</b> and <b>9</b>. In this example, the reference sources <b>972</b> <i>a </i>and <b>972</b> <i>b </i>can be disposed in the same plane as the re-imaging lenses <b>928</b> <i>a</i>, however, the reference sources <b>972</b> <i>a </i>and <b>972</b> <i>b </i>can be disposed in a different plane or in different locations. For example, the reference sources <b>972</b> <i>a </i>and <b>972</b> <i>b </i>can be disposed in a plane that is conjugate to the plane in which the detector array <b>1</b>, detector array <b>5</b>, and detector array <b>9</b> are disposed such that a focused image of the reference sources <b>972</b> <i>a </i>and <b>972</b> <i>b </i>is formed by the detector arrays. In some implementations, the reference sources <b>972</b> <i>a </i>and <b>972</b> <i>b </i>can be disposed in a plane that is spaced apart from the conjugate plane such that a defocused image of the reference sources <b>972</b> <i>a </i>and <b>972</b> <i>b </i>is formed by the detector arrays. In various implementations, the reference sources <b>972</b> <i>a </i>and <b>972</b> <i>b </i>need not be disposed in the same plane.</div>
<div class="description-paragraph" id="p-0265" num="0307">As discussed above, in some embodiments, the reference sources <b>972</b> <i>a </i>and <b>972</b> <i>b </i>are imaged onto the detector array <b>1</b> and detector array <b>9</b>, without much blur such that the reference sources <b>972</b> <i>a </i>and <b>972</b> <i>b </i>are focused. In contrast, in other embodiments, the image of reference sources <b>972</b> <i>a </i>and <b>972</b> <i>b </i>formed on the detector array <b>1</b>, and detector array <b>9</b> are blurred such that the reference sources <b>972</b> <i>a </i>and <b>972</b> <i>b </i>are defocused, and thereby provide some averaging, smoothing, and/or low pass filtering. The reference sources <b>972</b> <i>a </i>and <b>972</b> <i>b </i>may comprise a surface of known temperature and may or may not include a heater or cooler attached thereto or in thermal communication therewith. For example, the reference source <b>972</b> <i>a </i>and <b>972</b> <i>b </i>may comprises heaters and coolers respectively or may comprise a surface with a temperature sensor and a heater and sensor respectively in direct thermal communication therewith to control the temperature of the reference surface. In various implementations, the reference sources <b>972</b> <i>a </i>and <b>972</b> <i>b </i>can include a temperature controller configured to maintain the reference sources <b>972</b> <i>a </i>and <b>972</b> <i>b </i>at a known temperature. In some implementations, the reference sources <b>972</b> <i>a </i>and <b>972</b> <i>b </i>can be associated with one or more sensors that measure the temperature of the reference sources <b>972</b> <i>a </i>and <b>972</b> <i>b </i>and/communicate the measured temperature to the temperature controller. In some implementations, the one or more sensors can communicate the measured temperature to the data-processing unit. In various implementations, the reference sources <b>972</b> <i>a </i>and <b>972</b> <i>b </i>may comprise a surface of unknown temperature. For example, the reference sources may comprise a wall of a housing comprising the imaging system. In some implementations, the reference sources <b>972</b> <i>a </i>and <b>972</b> <i>b </i>can comprise a surface that need not be associated with sensors, temperature controllers. However, in other implementations, the reference sources <b>972</b> <i>a </i>and <b>972</b> <i>b </i>can comprise a surface that can be associated with sensors, temperature controllers.</div>
<div class="description-paragraph" id="p-0266" num="0308">In <figref idrefs="DRAWINGS">FIG. 9B</figref>, the temperature-calibration elements comprise temperature-controlled elements <b>972</b> <i>a </i>and <b>972</b> <i>b </i>(e.g., a thermally controlled emitter; a heating strip, a heater or a cooler) disposed a distance from the detector arrays <b>1</b>, <b>5</b>, <b>9</b>. In various embodiments, the temperature-controlled elements <b>972</b> <i>a </i>and <b>972</b> <i>b </i>can be disposed away from the central FOV of the detector arrays <b>1</b>, <b>5</b>, <b>9</b> such that the central FOV is not blocked or obscured by the image of the reference source <b>972</b> <i>a </i>and <b>972</b> <i>b</i>. The radiation emitted from the reference sources <b>972</b> <i>a </i>and <b>972</b> <i>b </i>is also imaged by the detector array <b>936</b> along with the radiation incident from the object. Depending on the position of the reference sources <b>972</b> <i>a </i>and <b>972</b> <i>b </i>the image obtained by the detector array of the reference sources can be blurred (or defocused) or sharp (or focused). The images <b>980</b> <i>a</i>, <b>980</b> <i>b</i>, <b>980</b> <i>c</i>, <b>980</b> <i>d</i>, <b>980</b> <i>e </i>and <b>980</b> <i>f </i>of the temperature-controlled elements <b>972</b> <i>a </i>and <b>972</b> <i>b </i>can be used as a reference to dynamically calibrate the one or more cameras in the array of cameras.</div>
<div class="description-paragraph" id="p-0267" num="0309">In the implementations depicted in <figref idrefs="DRAWINGS">FIGS. 9A and 9B</figref>, the detector arrays <b>1</b>, <b>5</b> and <b>9</b> are configured to view (or image) both the reference sources <b>972</b> <i>a </i>and <b>972</b> <i>b</i>. Accordingly, multiple frames (e.g., every or substantially every frame) within a sequence of images contains one or more regions in the image in which the object image has known thermal and spectral properties. This allows multiple (e.g., most or each) cameras within the array of cameras to be calibrated to agree with other (e.g., most or every other) camera imaging the same reference source(s) or surface(s). For example, detector arrays <b>1</b> and <b>9</b> can be calibrated to agree with each other. As another example, detector arrays <b>1</b>, <b>5</b> and <b>9</b> can be calibrated to agree with each other. In various embodiments, the lenses <b>928</b> <i>a </i>provide blurred (or defocused) images of the reference sources <b>972</b> <i>a</i>, <b>972</b> <i>b </i>on the detector arrays <b>1</b> and <b>9</b> because the location of the reference sources are not exactly in a conjugate planes of the detector arrays <b>1</b> and <b>9</b>. Although the lenses <b>928</b> <i>a </i>are described as providing blurred or defocused images, in various embodiments, reference sources or surfaces are imaged on the detectors arrays <b>1</b>, <b>5</b>, <b>9</b> without such blur and defocus and instead are focused images. Additionally optical elements may be used, such as for example, the mirrors shown in <figref idrefs="DRAWINGS">FIG. 9A</figref> to provide such focused images.</div>
<div class="description-paragraph" id="p-0268" num="0310">The temperature of the reference sources <b>972</b> <i>b</i>, <b>972</b> <i>a </i>can be different. For example, the reference source <b>972</b> <i>a </i>can be at a temperature T<sub>⋅A⋅</sub>, and the reference source <b>972</b> <i>b </i>can be at a temperature T<sub>⋅B⋅</sub> lower than the temperature T<sub>⋅A⋅</sub>. A heater can be provided under the temperature-controlled element <b>972</b> <i>a </i>to maintain it at a temperature T<sub>⋅A⋅</sub>, and a cooler can be provided underneath the temperature-controlled element <b>972</b> <i>b </i>to maintain it at a temperature T<sub>⋅B⋅</sub>. In various implementations, the embodiments illustrated in <figref idrefs="DRAWINGS">FIGS. 9A and 9B</figref> can be configured to image a single reference source <b>972</b> instead of two references sources <b>972</b> <i>a </i>and <b>972</b> <i>b </i>maintained at different temperatures. It is understood that the single reference source need not be thermally controlled. For example, in various implementations, a plurality of detectors in the detector array can be configured to image a same surface of at least one calibration element whose thermal and spectral properties are unknown. In such implementations, one of the plurality of detectors can be configured as a reference detector and the temperature of the surface of the at least one calibration element imaged by the plurality of detectors can be estimated using the radiance spectrum obtained by the reference detector. The remaining plurality of detectors can be calibrated such that their temperature and/or spectral measurements agree with the reference detector. For example, detector arrays <b>1</b> and <b>9</b> can be calibrated to agree with each other. As another example, detector arrays <b>1</b>, <b>5</b> and <b>9</b> can be calibrated to agree with each other.</div>
<div class="description-paragraph" id="p-0269" num="0311">The reference sources <b>972</b> <i>a </i>and <b>972</b> <i>b </i>can be coated with a material to make it behave substantially as a blackbody (for which the emission spectrum is known for any given temperature). If a temperature sensor is used at the location of each reference source, then the temperature can be tracked at these locations. As a result, the regions in the image of each camera (e.g., on the detector arrays <b>1</b> and <b>9</b>) in which the object has such known temperature (and, therefore, spectrum) can be defined. A calibration procedure can thus be used so that most of the cameras (if not every camera) so operated agrees, operationally, with most or every other camera, for objects at the temperatures represented by those two sources. Calibrating infrared cameras using sources at two different temperatures is known as a “two-point” calibration; and assumes that the measured signal at a given pixel is linearly related to the incident irradiance. Since this calibration can be performed during multiple, more, or even every frame of a sequence, it is referred to as a “dynamic calibration”.</div>
<div class="description-paragraph" id="p-0270" num="0312">An example of the dynamic calibration procedure is as follows. If there is a temperature sensor on the reference sources or reference surface, then the temperature measurements obtained by these temperature sensors can be used to determine their expected emission spectra. These temperature measurements are labeled as T<sub>⋅A⋅</sub>[R], T<sub>⋅B⋅</sub>[R], and T<sub>⋅C⋅</sub>[R] for the “reference temperatures” of sources/surfaces A, B, and C. These temperature measurements can be used as scalar correction factors to apply to the entire image of a given camera, forcing it to agree with the reference temperatures. Correcting the temperature estimate of a given pixel from T to T′ can use formulae analogous to those discussed below in reference to <figref idrefs="DRAWINGS">FIGS. 10A, 10B, 10C</figref>. If no direct temperature sensor is used, then one of the cameras can be used instead. This camera can be referred to as the “reference camera”. In this case, the same formulae as those provided in paragraph below can be used, but with T<sub>⋅A⋅</sub>[R] and T<sub>⋅B⋅</sub>[R] representing the temperatures of the reference sources/surfaces A and B as estimated by the reference camera. By applying the dynamic calibration correction formulae, all of the other cameras are forced to match the temperature estimates of the reference camera.</div>
<div class="description-paragraph" id="p-0271" num="0313">In the configuration illustrated in <figref idrefs="DRAWINGS">FIG. 9B</figref>, the reference sources <b>972</b> <i>a </i>and <b>972</b> <i>b </i>are placed such that the images of the sources on the detector arrays are blurred. The configuration illustrated in <figref idrefs="DRAWINGS">FIG. 9A</figref> is similar to the system <b>400</b> illustrated in <figref idrefs="DRAWINGS">FIG. 4</figref> where the reference sources are placed at an intermediate image plane (e.g., a conjugate image plane). In this configuration, the array of reference apertures, similar to reference apertures <b>438</b> <i>a </i>in <figref idrefs="DRAWINGS">FIG. 4</figref>, will have an accompanying array of reference sources or reference surfaces such that the reference sources or surfaces (e.g., each reference source or surface) are imaged onto a camera or a detector array such as FPAs <b>1</b>, <b>5</b>, <b>9</b>. With this approach, the reference source or surface images are at a conjugate image plane and thus are not appreciably blurred, so that their images can be made to block a smaller portion of each camera's field of view.</div>
<div class="description-paragraph" id="p-0272" num="0314">A “static” calibration (a procedure in which the scene is largely blocked with a reference source such as the moving shutters <b>960</b> in <figref idrefs="DRAWINGS">FIGS. 9A and 9B</figref>, so that imaging of an unknown scene cannot be performed in parallel with calibration) allows a plurality of the cameras (for example, most or each camera) to accurately estimate the temperature of a plurality of elements (for example, most or each element in the scene) immediately after the calibration is complete. It cannot, however, prevent the cameras' estimates from drifting away from one another during the process of imaging an unknown scene. The dynamic calibration can be used to reduce or prevent this drift, so that all cameras imaging a scene can be forced to agree on the temperature estimate of the reference sources/surfaces, and adjust this correction during every frame.</div>
<div class="description-paragraph" id="p-0273" num="0315"> <figref idrefs="DRAWINGS">FIG. 10A</figref> illustrates schematically a related embodiment <b>1000</b> of the imaging system, in which one or more mirrors M<sub>⋅0A⋅</sub>, . . . M<sub>⋅11A⋅ </sub>and M<sub>⋅B⋅</sub>, . . . M<sub>⋅11B⋅ </sub>are placed within the fields of view of one or more cameras <b>0</b>, . . . , <b>11</b>, partially blocking the field of view. The cameras <b>0</b>, . . . , <b>11</b> are arranged to form an outer ring of cameras including cameras <b>0</b>, <b>1</b>, <b>2</b>, <b>3</b>, <b>7</b>, <b>11</b>, <b>10</b>, <b>9</b>, <b>8</b> and <b>4</b> surrounding the central cameras <b>5</b> and <b>6</b>. In various implementations, the FOV of the central cameras <b>5</b> and <b>6</b> can be less than or equal to the FOV of the outer ring of cameras <b>0</b>, <b>1</b>, <b>2</b>, <b>3</b>, <b>7</b>, <b>11</b>, <b>10</b>, <b>9</b>, <b>8</b> and <b>4</b>. In such implementations, the one or more minors M<sub>⋅0A⋅</sub>, . . . M<sub>⋅11A⋅</sub> and M<sub>⋅0B⋅</sub>, . . . M<sub>⋅11B⋅ </sub>can be placed outside the central FOV of the cameras <b>5</b> and <b>6</b> and is placed in a peripheral FOV of the cameras outer ring of cameras <b>0</b>, <b>1</b>, <b>2</b>, <b>3</b>, <b>7</b>, <b>11</b>, <b>10</b>, <b>9</b>, <b>8</b> and <b>4</b> which does not overlap with the central FOV of the cameras <b>5</b> and <b>6</b> such that the reference sources A and B are not imaged by the cameras <b>5</b> and <b>6</b>. In various implementations, the FOV of the central cameras <b>5</b> and <b>6</b> can be greater than the FOV of the outer ring of cameras <b>0</b>, <b>1</b>, <b>2</b>, <b>3</b>, <b>7</b>, <b>11</b>, <b>10</b>, <b>9</b>, <b>8</b> and <b>4</b>. In such implementations, the one or more mirrors M<sub>⋅0A⋅</sub>, . . . M<sub>⋅11A⋅</sub> and M<sub>⋅0B⋅</sub>, . . . M<sub>⋅11B⋅ </sub>can be placed in a peripheral FOV of the cameras <b>5</b> and <b>6</b> which does overlap with the central FOV of the outer ring of cameras <b>0</b>, <b>1</b>, <b>2</b>, <b>3</b>, <b>7</b>, <b>11</b>, <b>10</b>, <b>9</b>, <b>8</b> and <b>4</b> such that the reference sources A and B are imaged by the cameras <b>5</b> and <b>6</b>.</div>
<div class="description-paragraph" id="p-0274" num="0316">This design is an enhancement to the systems <b>300</b> and <b>400</b> shown in <figref idrefs="DRAWINGS">FIGS. 3A and 4A</figref>. In the system <b>1000</b> shown in <figref idrefs="DRAWINGS">FIG. 10A</figref>, an array of two or more imaging elements (curved mirrors, for example) is installed at a distance from the FPAs, for example, in the plane of the reference aperture <b>160</b> shown in <figref idrefs="DRAWINGS">FIG. 3A</figref>. These elements (mirror or imaging elements) are used to image one or more temperature-controlled reference sources A and B onto the detector elements of two or more of the cameras. The primary difference between embodiment <b>1000</b> and embodiment <b>300</b> or <b>400</b> is that now a plurality or most or all of the outer ring of cameras in the array can image both the reference sources A and B instead of imaging one of the two reference source A and B. Accordingly, most or all of the outer ring of cameras image an identical reference source or an identical set of reference sources (e.g., both the reference sources A and B) rather than using different reference sources for different cameras or imaging different portions of the reference sources as shown in <figref idrefs="DRAWINGS">FIGS. 3A and 4A</figref>. Thus, this approach improves the robustness of the calibration, as it eliminates potential failures and errors due to the having additional thermal sensors estimating each reference source.</div>
<div class="description-paragraph" id="p-0275" num="0317">The imaging elements in the system <b>1000</b> (shown as mirrors in <figref idrefs="DRAWINGS">FIGS. 10A and 10B</figref>) image one or more controlled-temperature reference sources or a surface of a calibration element (shown as A and B in <figref idrefs="DRAWINGS">FIGS. 10A and 10B</figref>) into the blocked region of the cameras' fields of view. <figref idrefs="DRAWINGS">FIG. 10B</figref> shows an example in which mirror M<sub>⋅0A⋅</sub> images reference source/surface A onto camera <b>0</b>, and mirror M<sub>⋅0B⋅</sub> images reference source/surface B onto camera <b>0</b>, and likewise for cameras <b>1</b>, <b>2</b>, and <b>3</b>. This way, each of the mirrors is used to image a reference source/surface onto a detector array of a camera, so that many, most, or every frame within a sequence of images contains one or more regions in the image in which the object image has known thermal and spectral properties. This approach allows most of the camera, if not each camera, within the array of cameras to be calibrated to agree with most or every other camera imaging the same reference source or sources. For example, cameras <b>0</b>, <b>1</b>, <b>2</b>, <b>3</b>, <b>7</b>, <b>11</b>, <b>10</b>, <b>9</b>, <b>8</b> and <b>4</b> can be calibrated to agree with each other. As another example, cameras <b>0</b>, <b>1</b>, <b>2</b> and <b>3</b> can be calibrated to agree with each other. As yet another example, cameras <b>0</b>, <b>1</b>, <b>2</b>, <b>3</b>, <b>7</b>, <b>11</b>, <b>10</b>, <b>9</b>, <b>8</b>, <b>4</b>, <b>5</b> and <b>6</b> can be calibrated to agree with each other. Accordingly, in various implementations, two, three, four, five, six, seven, eight, nine, ten, eleven or twelve cameras can be calibrated to agree with each other. In certain embodiments, however, not all the cameras are calibrated to agree with each other. For example, one, two, or more cameras may not be calibrated to agree with each other while others may be calibrated to agree with each other. In various embodiments, these mirrors may be configured to image the reference sources/surfaces A and B onto different respective pixels a given FPA. Without any loss of generality, <figref idrefs="DRAWINGS">FIGS. 10A and 10B</figref> represent a top view of the embodiment shown in <figref idrefs="DRAWINGS">FIG. 9A</figref>.</div>
<div class="description-paragraph" id="p-0276" num="0318"> <figref idrefs="DRAWINGS">FIG. 10C</figref> illustrates schematically a related embodiment <b>1050</b> of the imaging system, in which one or more reference sources R<sub>⋅0A⋅</sub>, . . . , R<sub>⋅11A⋅</sub> and R<sub>⋅0B⋅</sub>, . . . , R<sub>⋅11B⋅</sub> are disposed around the array of detectors <b>0</b>, . . . , <b>11</b>. In various implementations, the one or more reference sources R<sub>⋅0A⋅</sub>, . . . , R<sub>⋅11A⋅</sub> and R<sub>⋅0B⋅</sub>, . . . , R<sub>⋅11B⋅</sub> can be a single reference source that is imaged by the detectors <b>0</b>, <b>11</b>. In various implementations, central detector arrays <b>5</b> and <b>6</b> can have a FOV that is equal to or lesser than the FOV of the outer ring of the detectors <b>0</b>, <b>1</b>, <b>2</b>, <b>3</b>, <b>7</b>, <b>11</b>, <b>10</b>, <b>9</b>, <b>8</b> and <b>4</b>. In such implementations, the reference sources R<sub>⋅0A⋅</sub>, . . . R<sub>⋅11A⋅ </sub>can be disposed away from the central FOV of the detector arrays <b>5</b> and <b>6</b> such that the radiation from the reference sources R<sub>⋅0A⋅</sub>, . . . , R<sub>⋅11A⋅ </sub>is imaged only by the outer ring of detectors <b>0</b>, <b>1</b>, <b>2</b>, <b>3</b>, <b>7</b>, <b>11</b>, <b>10</b>, <b>9</b>, <b>8</b> and <b>4</b>. In various implementations, central detector arrays <b>5</b> and <b>6</b> can have a FOV that is greater than the FOV of the outer ring of the detectors <b>0</b>, <b>1</b>, <b>2</b>, <b>3</b>, <b>7</b>, <b>11</b>, <b>10</b>, <b>9</b>, <b>8</b> and <b>4</b>. In such implementations, the reference sources R<sub>⋅0A⋅</sub>, . . . , R<sub>⋅11A⋅</sub> can be disposed in the peripheral FOV of the detector arrays <b>5</b> and <b>6</b> such that the radiation from the reference sources R<sub>⋅0A⋅</sub>, . . . , R<sub>⋅11A⋅ </sub>is imaged only by the outer ring of detectors <b>0</b>, <b>1</b>, <b>2</b>, <b>3</b>, <b>7</b>, <b>11</b>, <b>10</b>, <b>9</b>, <b>8</b> and <b>4</b>. The radiation from the reference sources R<sub>⋅0A⋅</sub>, . . . , R<sub>⋅11A⋅ </sub>is therefore imaged by the outer ring of detectors <b>0</b>, <b>1</b>, <b>2</b>, <b>3</b>, <b>7</b>, <b>11</b>, <b>10</b>, <b>9</b>, <b>8</b> and <b>4</b> as well as central cameras <b>5</b> and <b>6</b>. Without any loss of generality, <figref idrefs="DRAWINGS">FIG. 10C</figref> represents a top view of the embodiment shown in <figref idrefs="DRAWINGS">FIG. 9B</figref>.</div>
<div class="description-paragraph" id="p-0277" num="0319">In various implementations, a heater can be provided underneath, adjacent to, or in thermal communication with reference source/surface A to give it a higher temperature T<sub>⋅A⋅</sub>, and a cooler can be provided underneath, adjacent to, or in thermal communication with reference source B to give it a lower temperature T<sub>⋅B⋅</sub>. In various implementations, the embodiments illustrated in <figref idrefs="DRAWINGS">FIGS. 10A, 10B and 10C</figref> can be configured to image a single reference source A instead of two references sources A and B maintained at different temperatures. As discussed above, the embodiments illustrated in <figref idrefs="DRAWINGS">FIGS. 10A, 10B and 10C</figref> can be configured to image a same surface of a calibration element. In such implementations, the temperature of the surface of the calibration element need not be known. Many, most or each reference source/surface can be coated with a material to make it behave approximately as a blackbody, for which the emission spectrum is known for any given temperature. If many, most, or each camera in the array of cameras images both of references A and B, so that there are known regions in the image of these cameras in which the object has a known temperature (and therefore spectrum), then one can perform a calibration procedure. This procedure can provide that many, most or every camera so operated agrees with various, most, or every other camera, for objects at the temperatures represented by those two sources. For example, two, three, four, five, six, seven, eight, nine, ten, eleven or twelve cameras can be calibrated to agree with each other. In certain embodiments, however, not all the cameras are calibrated to agree with each other. For example, one, two, or more cameras may not be calibrated to agree with each other while others may be calibrated to agree with each other. As discussed above, calibration of infrared cameras using sources at two different temperatures is known as a “two-point” calibration, and assumes that the measured signal at a given pixel is linearly related to the incident irradiance.</div>
<div class="description-paragraph" id="p-0278" num="0320">The dynamic calibration is used to obtain a corrected temperature T′ from the initial temperature T estimated at each pixel in a camera using the following formulae:
<br/>
<i>T</i>′[<i>x,y,c</i>]=(<i>T</i>[<i>x,y,c</i>]−<i>T</i> <sub>A</sub>[<i>R</i>])<i>G</i>[<i>c</i>]+<i>T</i> <sub>A</sub>[<i>R</i>]
</div>
<div class="description-paragraph" id="p-0279" num="0321">where is T<sub>⋅A⋅</sub>[R] is a dynamic offset correction factor, and,</div>
<div class="description-paragraph" id="p-0280" num="0322"> <maths id="MATH-US-00003" num="00003"> <math overflow="scroll"> <mrow> <mrow> <mrow> <mi>G</mi> <mo>⁡</mo> <mrow> <mo>[</mo> <mi>c</mi> <mo>]</mo> </mrow> </mrow> <mo>=</mo> <mfrac> <mrow> <mrow> <msub> <mi>T</mi> <mi>B</mi> </msub> <mo>⁡</mo> <mrow> <mo>[</mo> <mi>R</mi> <mo>]</mo> </mrow> </mrow> <mo>-</mo> <mrow> <msub> <mi>T</mi> <mi>A</mi> </msub> <mo>⁡</mo> <mrow> <mo>[</mo> <mi>R</mi> <mo>]</mo> </mrow> </mrow> </mrow> <mrow> <mrow> <msub> <mi>T</mi> <mi>B</mi> </msub> <mo>⁡</mo> <mrow> <mo>[</mo> <mi>c</mi> <mo>]</mo> </mrow> </mrow> <mo>-</mo> <mrow> <msub> <mi>T</mi> <mi>A</mi> </msub> <mo>⁡</mo> <mrow> <mo>[</mo> <mi>c</mi> <mo>]</mo> </mrow> </mrow> </mrow> </mfrac> </mrow> <mo>,</mo> </mrow> </math> </maths> <br/>
is a dynamic gain correction factor. The term c discussed above is a camera index that identifies the camera whose data is being corrected.
</div>
<heading id="h-0170">III. Examples of a Mobile DAISI System</heading>
<div class="description-paragraph" id="p-0281" num="0323">The DAISI systems disclosed herein can be configured to be installed at a suitable location on a long-term basis, according to some embodiments. For example, the DAISI systems disclosed in Section II above can be affixed to a fixture mounted to the ground at a location to continuously or periodically monitor the presence of gases or chemicals at the location. In some embodiments, for example, the DAISI systems can be attached to a pole, post, or any suitable fixture at the location to be monitored. In such arrangements, the DAISI system can continuously or periodically capture multispectral, multiplexed image data of the scene, and an on-board or remote computing unit can process the captured image data to identify or characterize gases or chemicals at the location. A communications module can communicate data relating to the identified gases or chemicals to any suitable external system, such as a central computing server, etc. For such long-term installations of the DAISI system, the installation site may include a power source (e.g., electrical transmission lines connected to a junction box at the site) and network communications equipment (e.g., network wiring, routers, etc.) to provide network communication between the DAISI system and the external systems.</div>
<div class="description-paragraph" id="p-0282" num="0324">It can be advantageous to provide a mobile DAISI system configured to be worn or carried by a user. For example, it may be unsuitable or undesirable to install a DAISI system at some locations on a long-term basis. As an example, some oil well sites may not have sufficient infrastructure, such as power sources or network communication equipment, to support the DAISI system. In addition, it can be challenging to move the DAISI system from site to site to monitor different locations. For example, installing and removing the DAISI system from a site for transport may involve substantial effort and time for the user when the system is connected to infrastructure at the site to be monitored. Accordingly, it can be desirable to provide a DAISI system that can be used independently of the facilities or infrastructure at the site to be monitored. Furthermore, it can be advantageous to implement the DAISI system in a form factor and with a weight that can be carried or worn by a user. For example, a mobile DAISI system can enable the user to easily transport the system from site-to-site, while monitoring the presence of gases or chemicals in real-time.</div>
<div class="description-paragraph" id="p-0283" num="0325">It should be appreciated that each of the systems disclosed herein can be used to monitor potential gas leaks in any suitable installation site, including, without limitation, drilling rigs, refineries, pipelines, transportations systems, ships or other vessels (such as off-shore oil rigs, trains, tanker trucks, petro-chemical plants, chemical plants, etc. In addition, each of the embodiments and aspects disclosed and illustrated herein such as above, e.g., with respect to <figref idrefs="DRAWINGS">FIGS. 1-10C</figref>, can be used in combination with each of the embodiments disclosed and illustrated herein with respect to <figref idrefs="DRAWINGS">FIGS. 11A-14C</figref>.</div>
<div class="description-paragraph" id="p-0284" num="0326"> <figref idrefs="DRAWINGS">FIG. 11A</figref> is a schematic diagram illustrating a mobile infrared imaging system <b>1000</b> (e.g., a mobile or portable DAISI system) configured to be carried or worn by a human user <b>1275</b>. The user <b>1275</b> may wear a hat or helmet <b>1200</b> when he travels to a site to be monitored, such as an oil well site, a refinery, etc. The system <b>1000</b> shown in <figref idrefs="DRAWINGS">FIG. 11A</figref> is attached to the helmet <b>1200</b> by way of a support <b>1204</b> that securely mounts the system <b>1000</b> to the helmet <b>1200</b>. For example, the support <b>1204</b> can comprise a fastener, a strap, or any other suitable structure. Advantageously, mounting the system <b>1000</b> to the helmet <b>1200</b> can enable the user <b>1275</b> to capture images within the system's field of view (FOV) by turning his head to face a particular location to be monitored. For example, the user <b>1275</b> can walk through the site and can capture video images of each portion of the site, e.g., various structures that may be susceptible to gas or chemical leaks, such as valves, fittings, etc. Thus, in the embodiment shown in <figref idrefs="DRAWINGS">FIG. 11A</figref>, the user <b>1275</b> can image each portion of the site by facing the area to be imaged and ensuring that the system <b>1000</b> is activated. In addition, by mounting the system <b>1000</b> to the user's helmet <b>1200</b>, the user <b>1275</b> may use his hands for other tasks while the system <b>1000</b> images the site. Although the system <b>1000</b> of <figref idrefs="DRAWINGS">FIG. 11A</figref> is shown as being mounted to the user's helmet <b>1200</b>, it should be appreciated that the system <b>1000</b> can instead be worn on other parts of the user's clothing or can be carried by the user, e.g., in a bag, case, or other suitable container. Furthermore, in some embodiments, a wind sensor can be provided to the user, e.g., on the user's clothing and/or on or near the system <b>1000</b>. The wind sensor can be used to estimate wind conditions at the installation site, which can be used to improve the detection of gas leaks. In other embodiments, the system <b>1000</b> can be coupled to or formed with a housing that defines a “gun”-like structure which can be aimed or pointed by the user in a particular direction.</div>
<div class="description-paragraph" id="p-0285" num="0327">As explained herein, a gas cloud <b>1202</b> emitted from a structure at the site can be imaged by pointing the system <b>1000</b> towards the gas cloud <b>1202</b> and capturing an image of the gas cloud <b>1202</b> when the cloud <b>1202</b> is within the FOV of the system <b>1000</b>. Unlike other systems, the system <b>1000</b> can capture multispectral image data of a single scene over a range of IR wavelengths with a single snapshot, as explained in further detail herein. The single snapshot can be captured in a short timeframe, e.g., less than about 3 seconds, less than about 2 seconds, or less than about 1.5 seconds (for example, in about 1 second, in some embodiments). The single snapshot can be captured in greater than about 5 milliseconds, greater than about 0.2 seconds, or greater than about 0.5 seconds. The captured image data can be processed on board the system <b>1000</b> by a processing unit, as explained in further detail herein. For example, the processing unit can process the image data from the different optical channels and can compare the captured spectral information with a database of known chemicals to identify and/or characterize the gases that are included in the gas cloud <b>1202</b>.</div>
<div class="description-paragraph" id="p-0286" num="0328">A communications module on board the system <b>1000</b> can transmit information relating to the identified gases or chemicals to any suitable external device. For example, the communications module can wirelessly communicate (e.g., by Bluetooth, WiFi, etc.) the information to a suitable mobile computing device, such as an electronic eyewear apparatus <b>1201</b>, a tablet computing device <b>1212</b>, a mobile smartphone, a laptop or notebook computer <b>1203</b>, or any other suitable mobile computing device. In some embodiments, if a gas cloud is detected, the system <b>1000</b> can warn the user by way of sending a signal to the mobile device (e.g., tablet computing device <b>1212</b> or a mobile smartphone. The mobile device can emit an audible ring and/or can vibrate to notify the user of a potential gas leak. In the embodiment of <figref idrefs="DRAWINGS">FIG. 11A</figref>, the electronic eyewear apparatus <b>1201</b> can include a user interface comprising a display that the user <b>1275</b> can view in real-time as he visits the site. In some embodiments, the electronic eyewear apparatus <b>1201</b> comprises eyewear that includes a display. The electronics eyewear apparatus <b>1201</b> can be further configured to present images from this display to the wearer. The electronics eyewear apparatus <b>1201</b> may for example include projection optics that projects the image into the eye. The electronic eyewear apparatus <b>1201</b> may comprise heads up display optics the presents the image on the lens portion(s) of the eyewear so that the wearer can view the image and also see through the eyewear and peer at objects in the distance. Other configurations are possible. In some arrangements, the eyewear apparatus <b>1201</b> can comprise a Google Glass device, sold by Google, Inc., of Mountain View, Calif.</div>
<div class="description-paragraph" id="p-0287" num="0329">The processing unit can configure the processed image data such that the types of identified gases are displayed to the user <b>1275</b> on the display of the eyewear apparatus <b>1201</b>. For example, in some embodiments, color-coded data may represent different types of gases or concentrations of a particular gas, and may be overlaid on a visible light image of the scene. For example, the color-coded data and image of the gas cloud can be seen by the user on the electronic eyewear apparatus <b>1201</b>. In various embodiments, text data and statistics about the composition of the gas cloud <b>1202</b> may also be displayed to the user <b>1275</b>. Thus, the user <b>1275</b> can walk the site and can view the different types of gases in the gas cloud <b>1202</b> substantially in real-time. Advantageously, such real-time display of the composition of the gas cloud <b>1202</b> can enable the user <b>1275</b> to quickly report urgent events, such as the leakage of a toxic gas or chemical. In some embodiments, detection of a toxic leak can trigger an alarm, which may cause emergency personnel to help evacuate the site and/or fix the leak.</div>
<div class="description-paragraph" id="p-0288" num="0330">In some embodiments, the processed image data can be transmitted from the system <b>1000</b> to the tablet computing device <b>1212</b>, laptop computer <b>1203</b>, and/or smartphone. The user <b>1275</b> can interact with the table computing device <b>1212</b> or laptop computer <b>1203</b> to conduct additional analysis of the imaged and processed gas cloud <b>1202</b>. Furthermore, information about the gas cloud (including the processed data and/or the raw image data) may also be transmitted to a central server for centralized collection, processing, and analysis. In various arrangements, a global positioning system (GPS) module can also be installed on board the system <b>1000</b> and/or on the mobile computing device (such as a tablet computing device, smartphone, etc.). The GPS module can identify the coordinates of the user <b>1275</b> when a particular image is captured. The location data for the captured image data can be stored on the central server for further analysis.</div>
<div class="description-paragraph" id="p-0289" num="0331">Thus, the system <b>1000</b> shown in <figref idrefs="DRAWINGS">FIG. 11A</figref> can enable the user <b>1275</b> to image multiple locations of a particular site to be monitored, such as an oil well site. Advantageously, the optical components, the processing components, and the communications components of the system <b>1000</b> can be integrated within a relatively small housing that can be carried or worn by the user <b>1275</b>. For example, in various embodiments, the system <b>1000</b> does not include complex mechanical components for movement, such as gimbals, actuators, motors, etc. Without such components, the size of the system <b>1000</b> can be reduced relative to other systems.</div>
<div class="description-paragraph" id="p-0290" num="0332">Unlike other systems, in which the system components are bulky or are assembled over a large form factor, the mobile system <b>1000</b> can be sized and shaped in such a manner so as to be easily moved and manipulated when the user <b>1275</b> moves about the site. Indeed, it can be very challenging to integrate the various system components in a small form-factor. Advantageously, the systems <b>1000</b> can be worn or carried by a human user. For example, the components of the system <b>1000</b> can be contained together in a data acquisition and processing module <b>1020</b>, which may include a housing to support the system components. The components of the system <b>1000</b> (including the optical or imaging components, the focal plane array, the on-board processing electronics, and the communications components) may be packaged or assembled in the data acquisition and processing module <b>1020</b> and may occupy a volume less than about 300 cubic inches, less than about 200 cubic inches, or less than about 100 cubic inches. In various embodiments, the components of the system <b>1000</b> (including the optical or imaging components, the focal plane array, the on-board processing electronics, and the communications components) may be packaged or assembled in the data acquisition and processing module <b>1020</b> and may occupy a volume greater than about 2 cubic inches, or greater than about 16 cubic inches.</div>
<div class="description-paragraph" id="p-0291" num="0333">The data acquisition and processing module <b>1020</b> (with the system components mounted therein or thereon) may be sized and shaped to fit within a box-shaped boundary having dimensions X×Y×Z. For example, the data acquisition and processing module <b>1020</b>, including the imaging optics, focal plane array, and on board processing electronics, may be included in a package that is sized and shaped to fit within the box-shaped boundary having dimensions X×Y×Z. This package may also contain a power supply, such as a battery and/or solar module. In some embodiments, the data acquisition and processing module <b>1020</b> (including the imaging optics, focal plane array, and on board processing electronics may) can be sized and shaped to fit within a box-shaped boundary smaller than 8 inches×6 inches×6 inches. In some embodiments, the data acquisition and processing module <b>1020</b> (including the imaging optics, focal plane array, and on board processing electronics may) can be sized and shaped to fit within a box-shaped boundary smaller than 7 inches×5 inches×5 inches, e.g., a box-shaped boundary small than 7 inches×3 inches×3 inches. In some embodiments, the data acquisition and processing module <b>1020</b> (including the imaging optics, focal plane array, and on board processing electronics may) can be sized and shaped to fit within a box-shaped boundary smaller than 6 inches×4 inches×4 inches. In some embodiments, the data acquisition and processing module <b>1020</b> (including the imaging optics, focal plane array, and on board processing electronics may) can be sized and shaped to fit within a box-shaped boundary smaller than 2 inches×2 inches×6 inches. In some embodiments, the data acquisition and processing module <b>1020</b> (including the imaging optics, focal plane array, and on board processing electronics may) can be sized and shaped to fit within a box-shaped boundary having dimensions larger than 4 inches×2 inches×2 inches. In some embodiments, the data acquisition and processing module <b>1020</b> (including the imaging optics, focal plane array, and on board processing electronics may) can be sized and shaped to fit within a box-shaped boundary having dimensions larger than 3 inches×3 inches×7 inches. In some embodiments, the data acquisition and processing module <b>1020</b> (including the imaging optics, focal plane array, and on board processing electronics may) can be sized and shaped to fit within a box-shaped boundary having dimensions larger than 2 inches×1 inches×1 inches. The data acquisition and processing module <b>1020</b> (including the imaging optics, focal plane array, and on board processing electronics may) can have dimensions less than 2 inches×2 inches×6 inches. The data acquisition and processing module <b>1020</b> (including the imaging optics, focal plane array, and on board processing electronics may) can have dimensions greater than 1 inches×1 inches×3 inches. The data acquisition and processing module <b>1020</b> (including the imaging optics, focal plane array, and on board processing electronics may) can have dimensions greater than 2 inches×2 inches×4 inches. said data acquisition and processing module has dimensions less than 6 inches×3 inches×3 inches. The data acquisition and processing module <b>1020</b> (including the imaging optics, focal plane array, and on board processing electronics may) can have dimensions less than 4 inches×3 inches×3 inches. The data acquisition and processing module <b>1020</b> (including the imaging optics, focal plane array, and on board processing electronics may) can have dimensions less than 3 inches×2 inches×2 inches. The data acquisition and processing module <b>1020</b> (including the imaging optics, focal plane array, and on board processing electronics may) can have dimensions greater than 2 inches×1 inches×1 inches. The data acquisition and processing module <b>1020</b> (including the imaging optics, focal plane array, and on board processing electronics may) can have dimensions greater than 1 inches×0.5 inch×0.5 inch. The data acquisition and processing module <b>1020</b> (including the imaging optics, focal plane array, and on board processing electronics may) can have a volume less than 30 cubic inches. The data acquisition and processing module <b>1020</b> (including the imaging optics, focal plane array, and on board processing electronics may) can have a volume less than 20 cubic inches. The data acquisition and processing module <b>1020</b> (including the imaging optics, focal plane array, and on board processing electronics may) can have a volume less than 15 cubic inches. The data acquisition and processing module <b>1020</b> (including the imaging optics, focal plane array, and on board processing electronics may) can have a volume less than 10 cubic inches. The data acquisition and processing module <b>1020</b> (including the imaging optics, focal plane array, and on board processing electronics may) can have a volume more than 1 cubic inches. The data acquisition and processing module <b>1020</b> (including the imaging optics, focal plane array, and on board processing electronics may) can have a volume more than 4 cubic inches. The data acquisition and processing module <b>1020</b> (including the imaging optics, focal plane array, and on board processing electronics may) can have a volume more 5 cubic inches. The data acquisition and processing module <b>1020</b> (including the imaging optics, focal plane array, and on board processing electronics may) can have a volume more 10 cubic inches. This package may also contain a power supply, including a battery and/or solar module, a communications module, or both and fit into the above-referenced dimensions. It should be appreciated that the dimensions disclosed herein may not correspond to the directions shown in <figref idrefs="DRAWINGS">FIG. 11A</figref> with respect to X, Y, and Z.</div>
<div class="description-paragraph" id="p-0292" num="0334">Moreover, the system <b>1000</b> can have a mass and weight sufficiently small so as to enable the user <b>1275</b> to easily carry or wear the data acquisition and processing module <b>1020</b> at the site. Thus, the embodiment shown in <figref idrefs="DRAWINGS">FIG. 11A</figref> can be sized and shaped and configured to have a mass that enables a human user to easily and effectively manipulate the system <b>1000</b>.</div>
<div class="description-paragraph" id="p-0293" num="0335"> <figref idrefs="DRAWINGS">FIG. 11B</figref> is a schematic diagram illustrating an installation site (e.g. an oil well site, etc.) that can be monitored by multiple infrared imaging systems <b>1000</b> (e.g., a DAISI system). For example, as shown in <figref idrefs="DRAWINGS">FIG. 11B</figref>, an imaging system <b>1000</b>A can be mounted to a pole <b>1309</b> or other stationary structure at the site. An imaging system <b>1000</b>B can be worn or carried by multiple users <b>1275</b>, an imaging system <b>1000</b>C can be mounted on a truck <b>1500</b>, and/or an imaging system <b>1000</b>D can be mounted on an aerial platform <b>1501</b>, such as an unmanned aerial vehicle (UAV) or a piloted airplane. In some arrangements, the UAV can comprise an airplane, a helicopter (such as a quad helicopter), etc. The embodiments disclosed herein can utilize the image data captured by any combination of the systems <b>1000</b>A-<b>1000</b>D at the installation site to image the entire installation site in an efficient manner. Indeed, each installation site can include any suitable number and type of system-<b>1000</b>A-<b>1000</b>D. For example, each installation site can include greater than two systems <b>1000</b>A-<b>1000</b>D, greater than five systems <b>1000</b>A-<b>1000</b>D, greater than ten systems <b>1000</b>A-<b>1000</b>D, greater than twenty systems <b>1000</b>A-<b>1000</b>D. Each installation site may include less than about 100 systems <b>1000</b>A-<b>1000</b>D.</div>
<div class="description-paragraph" id="p-0294" num="0336">For example, the central server can track the real-time locations of each imaging system <b>1000</b>A-<b>1000</b>D based on the GPS coordinates of the particular system or on pre-determined knowledge about the system's stationary location. The distributed nature of the imaging systems <b>1000</b>A-<b>1000</b>D can provide rich information to the central server about the types and locations of gas leaks or other problems throughout multiple installation sites. Although <figref idrefs="DRAWINGS">FIG. 11B</figref> illustrates a stationary system <b>1000</b>A mounted to a fixture, a portable system <b>1000</b>B to be worn or carried by a human, a truck-based system <b>1000</b>C, and an aerial-based system <b>1000</b>D, it should be appreciated that other types of systems may be suitable. For example, in some embodiments, a robotic vehicle or a walking robot can be used as a platform for the systems <b>1000</b> disclosed herein. In various embodiments, a floating platform (such as a boat) can be used as a platform for the systems <b>1000</b> disclosed herein. It should also be appreciated that the systems disclosed herein can utilize any combination of the platforms (e.g., stationary fixtures such as a pole, human user(s), truck(s) or other vehicle, aerial platform(s), floating platform(s), robotic platform(s), etc.) to support the systems <b>1000</b>.</div>
<div class="description-paragraph" id="p-0295" num="0337">The systems <b>1000</b> shown in <figref idrefs="DRAWINGS">FIG. 11B</figref> can comprise a mobile DAISI system, similar to that illustrated in <figref idrefs="DRAWINGS">FIG. 11A</figref>. In other embodiments, the systems <b>1000</b> can comprise a larger DAISI system configured for use on a relatively long-term basis. For example, the stationary imaging system <b>1000</b>A shown in <figref idrefs="DRAWINGS">FIG. 11B</figref> can be installed on a pole <b>1309</b> or other suitable structure for monitoring a storage tank <b>1301</b>. A solar panel <b>1300</b> can be provided at or near the system <b>1000</b> to help provide power to the system <b>1000</b>. An antenna <b>1303</b> can electrically couple to the system and can provide wireless communication between the system <b>1000</b> and any other external entity, such as a central server, for storing and/or processing the data captured by the system <b>1000</b>.</div>
<div class="description-paragraph" id="p-0296" num="0338">The stationary infrared imaging system <b>1000</b>A can be programmed to continuously or periodically monitor the site. If a gas cloud <b>1302</b> escapes from the storage tank <b>1301</b>, such as by leaking from a broken valve, then the system <b>1000</b>A can capture a multispectral, snapshot image or series of images (e.g., a video stream) of the gas cloud <b>1302</b>. As with the embodiment of <figref idrefs="DRAWINGS">FIG. 11A</figref>, the imaging system <b>1000</b>A can include imaging, processing, and communications components on board the system <b>1000</b>A to identify and characterize the types of gases in the cloud <b>1302</b> and to transmit the processed data to the central server, e.g., by way of the antenna <b>1303</b>.</div>
<div class="description-paragraph" id="p-0297" num="0339">The imaging systems <b>1000</b>B worn or carried by the multiple users <b>1275</b> can advantageously capture and process multispectral image data of the portions of the installation site that each user <b>1275</b> visits. It should be appreciated that the different users <b>1275</b> may work in or travel through different portions of the installation site (and also to a number of installation sites) over a period of time. When activated, the imaging systems <b>1000</b>B worn or carried by the users <b>1275</b> can continuously or periodically capture multispectral image data of the different locations at the installation site(s) to which the user <b>1275</b> travels. As explained herein, the system <b>1000</b>B can transmit the image data and the location at which the image was captured to the central server. If the system <b>1000</b>B or the central server detects a problem (such as a gas leak), then the central server can associate that leak with a particular location and time.</div>
<div class="description-paragraph" id="p-0298" num="0340">Furthermore, because the central server can receive image data and location data from multiple users at different locations and viewing from different perspectives, the central server can create an organization-wide mapping of gas leaks that include, e.g., the locations of gas leaks in any of multiple installation sites, the type and concentrations and expanse or extent of each gas leaked, the particular user <b>1275</b> that captured the image data, and the time at which the image was taken. Thus, each user <b>1275</b> that carries or wears a portable imaging system <b>1000</b>B can contribute information to the central server that, when aggregated by the central server, provides rich details on the status of any gas leaks at any installation sites across the organization.</div>
<div class="description-paragraph" id="p-0299" num="0341">The track-mounted imaging system <b>1000</b>C can be mounted to a truck or other type of vehicle (such as a car, van, all-terrain vehicle, etc.). As shown in <figref idrefs="DRAWINGS">FIG. 11B</figref>, the imaging system <b>1000</b>C can be connected to an end of an extendable pole or extension member mounted to the truck <b>1500</b>. The system <b>1000</b>C can be raised and lowered by a control system to enable the system <b>1000</b>C to image a wide area of the installation site. In some embodiments, actuators can be provided to change the angular orientation of the system <b>1000</b>C, e.g., its pitch and yaw. A vibration isolation or reduction mechanism can also be provided to reduce vibrations, which may disturb the imaging process. The system <b>1000</b>C can be battery powered and/or can be powered by the truck; in some embodiments, a generator can be used to supply power to the system <b>1000</b>C. A user can drive the truck <b>1500</b> throughout the installation site to image various portions of the site to detect leaks. In addition, the user can drive the truck <b>1500</b> to other installation sites to detect gas leaks. As explained herein, the location of the truck <b>1500</b> can be communicated to the central server and the location of the truck <b>1500</b> can be associated with each captured image. The truck <b>1500</b> may include GPS electronics to assist in tracking the location of the truck <b>1500</b> and/or system <b>1000</b>C over time as the user drives from place to place. Similarly, the aerial platform <b>1501</b> (such as an unmanned aerial vehicle, or UAV) can support the imaging system <b>1000</b>D. The aerial platform <b>1501</b> can be piloted (either remotely or non-remotely) to numerous installation sites to capture multispectral image data to detect gas clouds.</div>
<div class="description-paragraph" id="p-0300" num="0342">Thus, the systems <b>1000</b>A-<b>1000</b>D can provide extensive data regarding the existence of leaks at numerous installations across an organization. Monitoring numerous cameras simultaneously or concurrently across an organization, site, region, or the entire country can be enabled at least in part by providing wireless (or wired) communication between the systems <b>1000</b>A-<b>1000</b>D and one or more central servers. Advantageously, the collection of image data from multiple sources and multiple platforms can enable the organization to create a real-time mapping of potential gas leaks, the types and amounts of gases being leaks, the locations of the leaks, and the time the image data of the leak was captured. In some arrangements, the aggregation of data about a site can improve the safety of installation sites. For example, if a gas leak is detected at a particular installation, the embodiments disclosed herein can alert the appropriate personnel, who can begin safety and/or evacuation procedures. Moreover, the aggregation of data across an organization (such as an oil service company) can provide site-wide, region-wide, and/or company-wide metrics for performance. For example, a given facility can monitor its total emissions over time and use the resulting data to help determine the facility's overall performance. A given region (such as a metropolitan area, a state, etc.) can monitor trends in emissions over time, providing a value on which to base decisions. Likewise, a company can look at the emissions performance at all of its facilities and can make decisions about whether some facilities should make new investments to improve performance, and/or whether the entire company should make various improvements. The mobile systems <b>1000</b> disclosed herein can thus provide a ubiquitous monitoring system for decision making. In addition, the systems <b>1000</b> disclosed herein can be used in a feedback control process to improve various manufacturing procedures based on the gases detected by the system(s) <b>1000</b>. Accordingly, a control module may be provided to adjust the manufacturing procedure and/or parameters according to the gases measured by the system <b>1000</b>.</div>
<div class="description-paragraph" id="p-0301" num="0343">The embodiments of the mobile infrared imaging system <b>1000</b> disclosed herein provide various advantages over other systems. As explained above, aggregation of data about a site and its potential gas leaks can provide an organization- or system-wide mapping of potential problems. Furthermore, automatic detection of gas leaks (and identification of the gases in the gas cloud) can simplify operation of the system <b>1000</b> and can reduce the risk of user errors in attempting to detect or identify gas clouds manually. Moreover, the small size of the systems <b>1000</b> disclosed herein are more easily carried or worn by the user than other systems. In addition, the systems <b>1000</b> disclosed herein can overlay the identified gas clouds on a visible image of the scene and can color code the gas cloud according to, e.g., type of gas, concentration, etc.</div>
<div class="description-paragraph" id="p-0302" num="0344"> <figref idrefs="DRAWINGS">FIG. 12</figref> is a schematic system block diagram showing a mobile infrared imaging system <b>1000</b> (e.g., a mobile DAISI system), according to one embodiment. The imaging system <b>1000</b> can include a data acquisition and processing module <b>1020</b> configured to be worn or carried by a person. The data acquisition and processing module <b>1020</b> can include, contain, or house an optical system <b>1015</b>, a processing unit <b>1021</b>, a power supply <b>1026</b>, a communication module <b>1025</b>, and GPS module <b>1025</b>. In other embodiments, the data acquisition and processing module <b>1020</b> can be configured to be mounted to a structure at the site to be monitored, such as a post. The power unit <b>1026</b> can be provided on board the system <b>1000</b>. The power unit <b>1026</b> can be configured to provide power to the various system components, such as the optical system <b>1015</b>, the processing unit <b>1021</b>, the communication module <b>1024</b>, and/or the GPS module <b>1025</b>. In some embodiments, the power unit <b>1026</b> can comprise one or more batteries (which may be rechargeable) to power the system components. In some embodiments, the power unit <b>1026</b> can include a solar power system including one or more solar panels for powering the system by sunlight. In some embodiments, the power unit <b>1026</b> can include various power electronics circuits for converting AC power supplied by standard power transmission lines to DC power for powering the system components. Other types of power supply may be suitable for the power unit <b>1026</b>.</div>
<div class="description-paragraph" id="p-0303" num="0345">The system <b>1000</b> can include an optical system <b>1015</b> configured to capture multispectral image data in a single snapshot, as explained herein. The optical system <b>1015</b> can correspond to any suitable type of DAISI system, such as, but not limited to, the optical systems and apparatus illustrated in <figref idrefs="DRAWINGS">FIGS. 1-10C</figref> above and/or in the optical systems <b>1015</b> illustrated in <figref idrefs="DRAWINGS">FIGS. 13A-13B</figref> below. For example, the optical system <b>1015</b> can include an optical focal plane array (FPA) unit and components that define at least two optical channels that are spatially and spectrally different from one another. The two optical channels can be positioned to transfer IR radiation incident on the optical system towards the optical FPA. The multiple channels can be used to multiplex different spectral images of the same scene and to image the different spectral images on the FPA unit.</div>
<div class="description-paragraph" id="p-0304" num="0346">The processing unit <b>1021</b> can also be provided on board the data acquisition and processing module <b>1020</b>. The processing unit <b>1021</b> can include a processor <b>1023</b> and a memory <b>1022</b>. The processor <b>1023</b> can be in operable cooperation with the memory <b>1022</b>, which can contain a computer-readable code that, when loaded onto the processor <b>1023</b>, enables the processor <b>1023</b> to acquire multispectral optical data representing a target species of gas or chemical from IR radiation received at the optical FPA unit of the optical system <b>1015</b>. The memory <b>1022</b> can be any suitable type of memory (such as a non-transitory computer-readable medium) that stores data captured by the optical system <b>1015</b> and/or processed by the processing unit <b>1021</b>. The memory <b>1022</b> can also store the software that is executed on the processor <b>1023</b>. The processor <b>1023</b> can be configured to execute software instructions that process the multispectral image data captured by the optical system <b>1015</b>. For example, the processor <b>1023</b> can analyze the different images detected by the FPA and can compare the captured data with known signatures of various types of gases or chemicals. Based on the analysis of the captured image data, the processor can be programmed to determine the types and concentrations of gases in a gas cloud. Further, as explained herein, the processor <b>1023</b> can analyze calibration data provided by the optical system <b>1015</b> to improve the accuracy of the measurements.</div>
<div class="description-paragraph" id="p-0305" num="0347">Advantageously, the processor <b>1023</b> can comprise one or more field-programmable gate arrays (FPGA) configured to execute methods used in the analysis of the images captured by the optical system <b>1015</b>. For example, the FPGA can include logic gates and read access memory (RAM) blocks that are designed to quickly implement the computations used to detect the types of gases in a gas cloud. The small size/weight, and high performance characteristics of the FPGA can enable on board computation and analysis within the data acquisition and detection unit <b>1020</b> worn or carried by the user. The use of FPGA (or similar electronics) on board the system <b>1000</b> can reduce costs associated with using an off-site central server or larger computing device to conduct the image analysis computations. In addition, enabling computation with one or more FPGA devices on board the wearable system can also prevent or reduce communication bottlenecks associated with wirelessly transmitting large amounts of raw data from the system <b>1000</b> to a remote server or computer, which can be used in some embodiments.</div>
<div class="description-paragraph" id="p-0306" num="0348">The communication module <b>1024</b> can be configured to communicate with at least one device physically separate from the data acquisition and processing module <b>1020</b>. For example, the communication module <b>1024</b> can include a wireless communication module configured to wirelessly communicate with the at least one separate device. The wireless communication module can be configured to provide wireless communication over wireless networks (e.g., WiFi internet networks, Bluetooth networks, etc.) and/or over telecommunications networks (e.g., 3G networks, 4G networks, etc.).</div>
<div class="description-paragraph" id="p-0307" num="0349">In some embodiments, for example, the wireless communication module can provide data communication between the data acquisition and processing module <b>1020</b> and a mobile device such as an electronic eyewear apparatus, a tablet computing device, a mobile smartphone, a laptop or notebook computer, or any other suitable mobile computing device. As explained herein; the mobile device can include a display on which the processed image data can be displayed to the user. For example, the types (and/or concentrations) of gases in a gas cloud can be illustrated on the display, e.g., by color coding or other suitable illustration scheme. The processed data can overlie a visible image of the scene in some arrangements. In some embodiments, the wireless communication module can provide data communication between the system <b>1000</b> and an external device remote from the system <b>1000</b>, such as a central server. For example, the processed image data and/or the raw image data may be transmitted over a telecommunications network to the central server for storage and/or further analysis. In some embodiments, the processed or raw image data can be uploaded to the mobile device (e.g., notebook computer, smartphone, tablet computing device, etc.), which can in turn communicate the image data to the central server.</div>
<div class="description-paragraph" id="p-0308" num="0350">The GPS module <b>1025</b> can be configured to determine the location of the data acquisition and processing module <b>1020</b> at a particular time. The processing unit <b>1021</b> can store the location data and can associate the location data with a particular image captured by the optical system <b>1015</b> in some arrangements. The location data associated with the captured images can be transmitted by the communication module <b>1024</b> (or by an external device) to a central server in some arrangements.</div>
<div class="description-paragraph" id="p-0309" num="0351">The optical system <b>1015</b>, the processing unit <b>1021</b>, the power supply <b>1026</b>, the communication module <b>1024</b>, and/or the GPS module <b>1025</b> may be contained or housed in the data acquisition and processing module <b>1020</b>, which can be carried or worn by the user. The components of the system <b>1000</b> (including the optical components, the processing components, and the communications components) may be packaged or assembled in the data acquisition and processing module <b>1020</b> and may occupy a volume less than about 300 cubic inches, less than about 200 cubic inches, or less than about 100 cubic inches. In various embodiments, the components of the system <b>1000</b> (including the optical components, the processing components, and the communications components) may be packaged or assembled in the data acquisition and processing module <b>1020</b> and may occupy a volume greater than about 2 cubic inches, or greater than about 16 cubic inches. A power supply, including a battery and/or solar module may also be included among the components packaged or assembled in the data acquisition and processing module <b>1020</b> and fit into the above-referenced volumetric dimensions.</div>
<div class="description-paragraph" id="p-0310" num="0352">The data acquisition and processing module <b>1020</b> (with the system components mounted therein or thereon, including the imaging optics, focal plane array, and on board processing electronics may) may be sized and shaped to fit within a box-shaped boundary having dimensions X×Y×Z. For example, in some embodiments, the data acquisition and processing module <b>1020</b> (including the imaging optics, focal plane array, and on board processing electronics may) can be sized and shaped to fit within a box-shaped boundary smaller than 8 inches×6 inches×6 inches. In some embodiments, the data acquisition and processing module <b>1020</b> (including the imaging optics, focal plane array, and on board processing electronics may) can be sized and shaped to fit within a box-shaped boundary smaller than 7 inches×5 inches×5 inches. In some embodiments, the data acquisition and processing module <b>1020</b> (including the imaging optics, focal plane array, and on board processing electronics may) can be sized and shaped to fit within a box-shaped boundary smaller than 6 inches×4 inches×4 inches. In some embodiments, the data acquisition and processing module <b>1020</b> (including the imaging optics, focal plane array, and on board processing electronics may) can be sized and shaped to fit within a box-shaped boundary having dimensions larger than 4 inches by 2 inches×2 inches. In some embodiments, the data acquisition and processing module <b>1020</b> (including the imaging optics, focal plane array, and on board processing electronics may) can be sized and shaped to fit within a box-shaped boundary having dimensions larger than 2 inches by 1 inches×1 inches. A power supply, including a battery and/or solar module, a communications module, or both may be included in the data acquisition and processing module <b>1020</b> and fit into the above-referenced dimensions. It should be appreciated that the dimensions disclosed herein may not correspond to the directions shown in <figref idrefs="DRAWINGS">FIG. 11A</figref> with respect to X, Y, and Z. Moreover, the system <b>1000</b> can have a mass and weight sufficiently small so as to enable the user <b>1275</b> to easily carry or wear the data acquisition and processing module <b>1020</b> at the site.</div>
<div class="description-paragraph" id="p-0311" num="0353"> <figref idrefs="DRAWINGS">FIG. 13A</figref> is a schematic system diagram of an optical system <b>1015</b> configured to be used in the mobile infrared imaging systems <b>1000</b> disclosed herein, according to various embodiments. As explained herein, the optical system <b>1015</b> shown in <figref idrefs="DRAWINGS">FIG. 13A</figref> can be provided in the data acquisition and processing module <b>1020</b> to be worn or carried by the user. The optical system <b>1015</b> can be configured to capture multispectral image data of an object <b>1007</b>, such as a gas cloud, chemical spill, etc. The components of the optical system <b>1015</b> shown in <figref idrefs="DRAWINGS">FIG. 13A</figref> may be similar to or the same as the components of the optical systems and devices explained herein with respect to <figref idrefs="DRAWINGS">FIGS. 1-10C</figref>. The optical system <b>1015</b> can include a focal plane array (FPA) unit <b>1008</b> configured to record infrared image data captured by the system <b>1000</b>. As shown in <figref idrefs="DRAWINGS">FIG. 13A</figref>, the FPA unit <b>1008</b> may advantageously be uncooled, e.g., devoid of a cooling system.</div>
<div class="description-paragraph" id="p-0312" num="0354">The optical system <b>1015</b> can include a front window <b>1006</b> through which, light from the object <b>1007</b> passes. A first moveable blackbody source <b>1003</b> and a second moveable blackbody source <b>1004</b> can be provided to enable calibration of the optical system <b>1015</b>. The moveable sources <b>1003</b>, <b>1004</b> can be moved in front of the field of view such that the optics image these sources for calibration. For example, the first and second blackbody sources <b>1003</b>, <b>1004</b> can be maintained at different known temperatures in a stable manner. For example, a heater and a temperature sensor can be attached to each blackbody source <b>1003</b>, <b>1004</b> to provide feedback to create a stable and known temperature difference (e.g., at least 50 mK in some arrangements) between different spatial regions of the sources.</div>
<div class="description-paragraph" id="p-0313" num="0355">In addition, the optical system <b>1000</b> can include a dynamic calibration apparatus to dynamically calibrate the system <b>1000</b>. As shown in <figref idrefs="DRAWINGS">FIG. 13A</figref>, one or more calibration sources <b>1009</b>, <b>1010</b> can be provided. The calibration sources <b>1009</b>, <b>1010</b> can comprise a thermal electrically controlled (TEC) material with a temperature sensor attached thereto. The calibration sources <b>1009</b>, <b>1010</b> can be coated with a spectrally measured coating or paint. Light from the calibration sources <b>1009</b>, <b>1010</b> can be reflected from one or more mirrors <b>1005</b> and directed through the lens array <b>1002</b> (described below) to be imaged on a portion of the FPA unit <b>1008</b> to assist in dynamically calibrating the system <b>1000</b> (e.g., while imaging of the target gas cloud is simultaneously being imaged).</div>
<div class="description-paragraph" id="p-0314" num="0356">The optical system <b>1000</b> can include a lens array <b>1002</b> to focus the incoming light onto the FPA unit <b>1008</b>. As shown in <figref idrefs="DRAWINGS">FIG. 13A</figref>, each lens of the lens array <b>1002</b> can at least partially define or be included in an optical channel to be imaged by the FPA unit <b>1008</b>. To improve the mobility and portability of the mobile imaging system <b>1000</b>, the lens array <b>1002</b> can comprise an integrated unit formed from or assembled into a single unitary body. Such an integrated lens array <b>1002</b> can reduce the size of the imaging system <b>1015</b>, and therefore, the size of the system <b>1000</b>, to at least partially enable the system <b>1000</b> to be worn or carried by the user. The lens array <b>1002</b> can be monolithically formed in any suitable manner. For example, in various embodiments, the lens array <b>1002</b> can be formed by a diamond milling tool. In some embodiments, the lens array <b>1002</b> can comprise a monolithic piece of transparent material which has separate regions shaped into curved refractive surfaces for creating separate lenses. In some embodiments, the lenses can be inserted into an array of openings formed in a plate or substrate to create the lens array <b>1002</b>.</div>
<div class="description-paragraph" id="p-0315" num="0357">The optical system <b>1000</b> can also include an array of infrared (IR) filters <b>1001</b> configured to filter wavelengths of infrared light in an appropriate manner. Examples of IR filters and filtering techniques are disclosed herein, for example, with respect to <figref idrefs="DRAWINGS">FIGS. 5A-6D</figref>. As shown in <figref idrefs="DRAWINGS">FIG. 13A</figref>, the IR filters <b>1001</b> can be disposed between the lens array <b>1002</b> and the FPA unit <b>1008</b>. The IR filters <b>1001</b> can at least partially define the multiple optical channels to be imaged by the FPA unit <b>1008</b>. In some embodiments, the IR filters <b>1001</b> can be positioned between the lens array <b>1002</b> and the first and second moveable blackbody sources <b>1009</b>, <b>1010</b>.</div>
<div class="description-paragraph" id="p-0316" num="0358"> <figref idrefs="DRAWINGS">FIG. 13B</figref> is a schematic system diagram of an optical system <b>1015</b> configured to be used in the mobile infrared imaging systems <b>1000</b> disclosed herein, according to various embodiments. As explained herein, the optical system <b>1015</b> shown in <figref idrefs="DRAWINGS">FIG. 13B</figref> can be provided in the data acquisition and processing module <b>1020</b> to be worn or carried by the user. The components of the optical system <b>1015</b> shown in <figref idrefs="DRAWINGS">FIG. 13B</figref> may be similar to or the same as the components of the optical systems and devices explained herein with respect to <figref idrefs="DRAWINGS">FIGS. 1-10C and 13A</figref>.</div>
<div class="description-paragraph" id="p-0317" num="0359">The optical system <b>1015</b> of <figref idrefs="DRAWINGS">FIG. 13B</figref> can include an FPA unit <b>1408</b> configured to image an object <b>1409</b>, such as a gas cloud or chemical leak. As with the embodiment illustrated in <figref idrefs="DRAWINGS">FIG. 13A</figref>, the system <b>1015</b> of <figref idrefs="DRAWINGS">FIG. 13B</figref> can include a front window <b>1406</b> through which light from the object <b>1409</b> passes, first and second moveable blackbody sources <b>1403</b>, <b>1404</b>, an IR filter array <b>1401</b>, and a lens array <b>1402</b>. As with the embodiment of <figref idrefs="DRAWINGS">FIG. 13A</figref>, the lens array <b>1402</b> can comprise a unitary or monolithic body. In the embodiment of <figref idrefs="DRAWINGS">FIG. 13B</figref>, the lens array <b>1402</b> may be disposed between the filter array <b>1401</b> and the FPA unit <b>1408</b>. In other arrangements, the filter array <b>1401</b> may be disposed between the lens array <b>1402</b> and the FPA unit <b>1408</b>.</div>
<div class="description-paragraph" id="p-0318" num="0360">The optical system <b>1015</b> of <figref idrefs="DRAWINGS">FIG. 13B</figref> can include a cooling unit <b>1430</b> configured to cool the FPA unit <b>1408</b>. The cooling unit <b>1430</b> can comprise a cooling finger configured to cryogenically cool the FPA array <b>1408</b> in various arrangements. As shown in <figref idrefs="DRAWINGS">FIG. 13B</figref>, the filter array <b>1401</b>, the lens array <b>1402</b>, and the FPA unit <b>1408</b> can be disposed in a cooled region <b>1440</b>. The blackbody sources <b>1403</b>, <b>1404</b> and front window <b>1406</b> can be disposed in an uncooled region <b>1450</b>. Disposing the blackbody sources <b>1403</b>, <b>1404</b> at uncooled temperatures and the filter array <b>1401</b>, lens array <b>1402</b>, and FPA unit <b>1408</b> at cooled temperatures can assist in the periodic calibration of the system <b>1000</b>.</div>
<div class="description-paragraph" id="p-0319" num="0361"> <figref idrefs="DRAWINGS">FIG. 14A</figref> is a schematic perspective view of a mobile infrared imaging system <b>1000</b> (e.g., mobile DAISI system) mounted to a helmet <b>1200</b>, according to various embodiments. <figref idrefs="DRAWINGS">FIG. 14B</figref> is an enlarged schematic perspective view of the mobile infrared imaging system <b>1000</b> shown in <figref idrefs="DRAWINGS">FIG. 14A</figref>. The helmet <b>1200</b> can comprise a portion of a user's personal protective equipment and can also advantageously be used as a platform for the imaging system <b>1000</b>. As explained above, the helmet <b>1200</b> can be worn by a user as the user visits a particular installation site to be monitored, such as an oil well site, a refinery, etc. The system <b>1000</b> can be activated to continuously monitor and analyze the sites that the user visits. The system <b>1000</b> can thereby continuously and actively search for gas leaks wherever the user visits and can initiate an alarm or other notification if a leak is detected.</div>
<div class="description-paragraph" id="p-0320" num="0362">In the embodiment illustrated in <figref idrefs="DRAWINGS">FIG. 14B</figref>, the imaging system <b>1000</b> can comprise a housing <b>1590</b>, within or to which a data acquisition and processing module <b>1020</b> (see, e.g., <figref idrefs="DRAWINGS">FIG. 12</figref> and associated description) is mounted or coupled. A support <b>1592</b> can be coupled to or formed with the housing <b>1590</b> and can be configured to attach to the helmet <b>1200</b> or to any other suitable platform. For example, in some embodiments, the support <b>1592</b> can include one or more mounting holes for attaching to the helmet <b>1200</b> by way of, e.g., screws, bolts, or other fasteners. In addition, as shown in <figref idrefs="DRAWINGS">FIG. 14B</figref>, a front window <b>1506</b> can be provided at a front end of the housing <b>1590</b>. The front window <b>1506</b> can be transparent to IR radiation and can at least partially define the aperture of the system <b>1000</b>. In some embodiments, the window <b>1506</b> comprises germanium. A diamond like coating (DLC) or other coating or layer can be disposed over the window <b>1506</b> to provide a durable surface.</div>
<div class="description-paragraph" id="p-0321" num="0363">As explained herein, the system <b>1000</b> can be configured to be worn or carried by a human user. Accordingly, the data acquisition and processing module <b>1020</b> can be suitably dimensioned such that a user can easily wear or carry the system <b>1000</b>. For example, the data acquisition and processing module <b>1020</b> can be defined at least in part by dimensions X×Y×Z, as shown in <figref idrefs="DRAWINGS">FIGS. 14A and 14B</figref>.</div>
<div class="description-paragraph" id="p-0322" num="0364">Unlike other systems, in which the system components are bulky or are assembled over a large form factor, the mobile system <b>1000</b> can be sized and shaped in such a manner so as to be easily moved and manipulated when the user moves about the site. Indeed, it can be very challenging to integrate the various system components in a small form-factor. Advantageously, the systems <b>1000</b> disclosed herein can be worn or carried by a human user. For example, the components of the system <b>1000</b> can be contained together in the data acquisition and processing module <b>1020</b>, which may include the housing <b>1590</b> to support the system components. The components of the system <b>1000</b> (including the optical or imaging components, the focal plane array, the on-board processing electronics, and the communications components) may be packaged or assembled in the data acquisition and processing module <b>1020</b> and may occupy a volume less than about 300 cubic inches, less than about 200 cubic inches, or less than about 100 cubic inches. In various embodiments, the components of the system <b>1000</b> (including the optical or imaging components, the focal plane array, the on-board processing electronics, and the communications components) may be packaged or assembled in the data acquisition and processing module <b>1020</b> and may occupy a volume greater than about 2 cubic inches, or greater than about 16 cubic inches. In some embodiments, the components of the system <b>1000</b> (including the optical or imaging components, the focal plane array, the on-board processing electronics, and the communications components) may be packaged or assembled in the data acquisition and processing module <b>1020</b> and may occupy a volume in a range of about 4 cubic inches to about 15 cubic inches. In some embodiments, the components of the system <b>1000</b> (including the optical or imaging components, the focal plane array, the on-board processing electronics, and the communications components) may be packaged or assembled in the data acquisition and processing module <b>1020</b> and may occupy a volume in a range of about 5 cubic inches to about 12 cubic inches. In some embodiments, the components of the system <b>1000</b> (including the optical or imaging components, the focal plane array, the on-board processing electronics, and the communications components) may be packaged or assembled in the data acquisition and processing module <b>1020</b> and may occupy a volume in a range of about 4 cubic inches to about 6.5 cubic inches, e.g., about 5.63 cubic inches in one embodiment. In some embodiments, the components of the system <b>1000</b> (including the optical or imaging components, the focal plane array, the on-board processing electronics, and the communications components) may be packaged or assembled in the data acquisition and processing module <b>1020</b> and may occupy a volume in a range of about 9 cubic inches to about 13 cubic inches, e.g., about 11.25 cubic inches in one embodiment. In some embodiments, the components of the system <b>1000</b> (including the optical or imaging components, the focal plane array, the on-board processing electronics, and the communications components) may be packaged or assembled in the data acquisition and processing module <b>1020</b> and may occupy a volume in a range of about 6 cubic inches to about 10 cubic inches.</div>
<div class="description-paragraph" id="p-0323" num="0365">The data acquisition and processing module <b>1020</b> (with the system components mounted therein or thereon) may be sized and shaped to fit within a box-shaped boundary having dimensions X×Y×Z. For example, the data acquisition and processing module <b>1020</b>, including the imaging optics, focal plane array, and on board processing electronics may be included in a package that is sized and shaped to fit within the box-shaped boundary having dimensions X×Y×Z. This package may also contain a power supply, such as a battery and/or solar module. In some embodiments, the data acquisition and processing module <b>1020</b> (including the imaging optics, focal plane array, and on board processing electronics may) can be sized and shaped to fit within a box-shaped boundary smaller than 8 inches×6 inches×6 inches. In some embodiments, the data acquisition and processing module <b>1020</b> (including the imaging optics, focal plane array, and on board processing electronics may) can be sized and shaped to fit within a box-shaped boundary smaller than 7 inches×5 inches×5 inches. In some embodiments, the data acquisition and processing module <b>1020</b> (including the imaging optics, focal plane array, and on board processing electronics may) can be sized and shaped to fit within a box-shaped boundary smaller than 6 inches×4 inches×4 inches. In some embodiments, the data acquisition and processing module <b>1020</b> (including the imaging optics, focal plane array, and on board processing electronics may) can be sized and shaped to fit within a box-shaped boundary smaller than 6 inches×2 inches×2 inches. In some embodiments, the data acquisition and processing module <b>1020</b> (including the imaging optics, focal plane array, and on board processing electronics may) can be sized and shaped to fit within a box-shaped boundary having dimensions larger than 4 inches×2 inches×2 inches. In some embodiments, the data acquisition and processing module <b>1020</b> (including the imaging optics, focal plane array, and on board processing electronics may) can be sized and shaped to fit within a box-shaped boundary having dimensions larger than 2 inches×1 inches×1 inches. The data acquisition and processing module <b>1020</b> (including the imaging optics, focal plane array, and on board processing electronics may) can have dimensions less than 3 inches×2 inches×2 inches. The data acquisition and processing module <b>1020</b> (including the imaging optics, focal plane array, and on board processing electronics may) can have dimensions greater than 1 inches×0.5 inch×0.5 inch. The data acquisition and processing module <b>1020</b> (including the imaging optics, focal plane array, and on board processing electronics may) can have a volume less than 30 cubic inches. The data acquisition and processing module <b>1020</b> (including the imaging optics, focal plane array, and on board processing electronics may) can have a volume less than 20 cubic inches. The data acquisition and processing module <b>1020</b> (including the imaging optics, focal plane array, and on board processing electronics may) can have a volume less than 15 cubic inches. The data acquisition and processing module <b>1020</b> (including the imaging optics, focal plane array, and on board processing electronics may) can have a volume less than 10 cubic inches. The data acquisition and processing module <b>1020</b> (including the imaging optics, focal plane array, and on board processing electronics may) can have a volume more than 1 cubic inches. The data acquisition and processing module <b>1020</b> (including the imaging optics, focal plane array, and on board processing electronics may) can have a volume more than 4 cubic inches. The data acquisition and processing module <b>1020</b> (including the imaging optics, focal plane array, and on board processing electronics may) can have a volume more 5 cubic inches. The data acquisition and processing module <b>1020</b> (including the imaging optics, focal plane array, and on board processing electronics may) can have a volume more 10 cubic inches. This package may also contain a power supply, including a battery and/or solar module, a communications module, or both and fit into the above-referenced dimensions. It should be appreciated that the dimensions disclosed herein may not correspond to the directions shown in <figref idrefs="DRAWINGS">FIG. 11A</figref> with respect to X, Y, and Z. This package may also contain a power supply, including a battery and/or solar module, a communications module, or both and fit into the above-referenced dimensions. It should be appreciated that the dimensions disclosed herein may not correspond to the directions shown in <figref idrefs="DRAWINGS">FIG. 11A</figref> with respect to X, Y, and Z.</div>
<div class="description-paragraph" id="p-0324" num="0366">In some embodiments, the dimension X shown in <figref idrefs="DRAWINGS">FIG. 14B</figref> can be in a range of about 2 inches to about 7 inches, or more particularly, in a range of about 2 inches to about 4 inches, e.g., about 2.5 inches in one embodiment. In some embodiments, the dimension X shown in <figref idrefs="DRAWINGS">FIG. 14B</figref> can be in a range of about 4 inches to about 6 inches, e.g., about 5 inches in one embodiment. In some embodiments, the dimension Y shown in <figref idrefs="DRAWINGS">FIG. 14B</figref> can be in a range of about 1 inch to about 5 inches, or more particularly, in a range of about 1 inch to about 3 inches, e.g., about 1.5 inches in one embodiment. In some embodiments, the dimension Z shown in <figref idrefs="DRAWINGS">FIG. 14B</figref> can be in a range of about 1 inch to about 5 inches, or more particularly, in a range of about 1 inch to about 3 inches, e.g., about 1.5 inches in one embodiment.</div>
<div class="description-paragraph" id="p-0325" num="0367">Moreover, the system <b>1000</b> can have a mass and weight sufficiently small so as to enable the user <b>1275</b> to easily carry or wear the data acquisition and processing module <b>1020</b> at the site. For example, the system <b>1000</b> can have a weight in a range of about 0.5 pounds to 5 pounds, or more particularly, in a range of about 0.5 pounds to 2 pounds, or more particularly in a range of about 0.25 pounds to about 2 pounds, or more particularly, in a range of about 0.25 pounds to about 1.5 pounds. In one embodiment, for example, the system <b>1000</b> can weight about 1 pound. In another embodiment, for example, the system <b>1000</b> can weigh about 0.5 pounds. Thus, the embodiment shown in <figref idrefs="DRAWINGS">FIG. 11A</figref> can be sized and shaped and configured to have a mass that enables a human user to easily and effectively manipulate the system <b>1000</b>.</div>
<div class="description-paragraph" id="p-0326" num="0368"> <figref idrefs="DRAWINGS">FIG. 14C</figref> is a perspective cross-sectional view of the mobile infrared imaging system <b>1000</b> shown in <figref idrefs="DRAWINGS">FIGS. 14A-14B</figref>. The mobile infrared imaging system <b>1000</b> can include one or more movable shutters <b>1503</b> (e.g., two shutters) rear of the window <b>1506</b> and a lens assembly <b>1502</b> rear of the shutter(s) <b>1503</b>. A filter array <b>1501</b> can be disposed rear (or forward) of the second lens array <b>1502</b>B, and an optical focal plane array (FPA) unit <b>1508</b> can be disposed rear of the filter array <b>1501</b>. The optical FPA unit <b>1508</b> can be mechanically and electrically coupled with one or more substrates <b>1586</b>, which may comprise printed circuit board or PCB substrates. In various embodiments, the FPA unit <b>1508</b> comprises a single FPA or detector array. Additionally, as explained herein, the lens assembly <b>1502</b>, filter array <b>1501</b>, and optical FPA unit can at least partially define one or more optical channels that are spatially and spectrally different. A number of the optical channels can be at least 4, at least 5, at least 8, at least 9, at least 12, at least 13, or at least 20. In some embodiments, a number of the optical channels is between 4 and 50.</div>
<div class="description-paragraph" id="p-0327" num="0369">One or more batteries <b>1588</b> can supply power to the system <b>1000</b> by way of the substrate(s) <b>1586</b>. In addition, a visible light imaging sensor <b>1580</b> can be disposed in the housing <b>1590</b> and can be configured to provide a visible light image of the scene being captured by the system <b>1000</b>. The processed IR image data can be overlaid upon the visible light image. In various embodiments the visible light imaging sensor <b>1580</b> can be used for reduction of scene-motion-induced detection errors, for example, to detect a moving object that enters the field of view (such as an animal or person) and would interfere with the data being collected.</div>
<div class="description-paragraph" id="p-0328" num="0370">As explained herein, the movable shutter(s) <b>1503</b> can be configured to provide spectral-radiometric calibration for the system <b>1000</b>. The shutter(s) <b>1503</b> can be configured to move in and out of the field of view of the lens assembly <b>1502</b> periodically, e.g., in a time period in a range of about 1 minute to about 15 minutes, or more particularly, in a range of about 3 minutes to about 7 minutes, e.g., about 5 minutes. Although one shutter <b>1503</b> is illustrated in <figref idrefs="DRAWINGS">FIG. 14C</figref>, it should be appreciated that two or more shutters may be provided. The shutter(s) <b>1503</b> can be used in static calibration procedures to provide the system with absolute temperature values. In some embodiments, only static calibration is performed, e.g., no dynamic calibration is performed. In some embodiments, both static and dynamic calibration procedures are performed.</div>
<div class="description-paragraph" id="p-0329" num="0371">The lens assembly <b>1502</b>; can include a first lens array <b>1502</b>A and a second lens array <b>1502</b>B. In some embodiments, the lens assembly <b>1502</b> can comprise an array of two-part lenses denoted by the first and second arrays <b>1502</b>A, <b>1502</b>B. In some embodiments, the lens assembly <b>1502</b> can comprise an array of two separate lenses denoted by the first and second arrays <b>1502</b>A, <b>1502</b>B. Each of the lens arrays <b>1502</b>A, <b>1502</b>B can comprise a 4×3 array of lenses, each of which may correspond to a particular detector region in the FPA unit <b>1508</b> and can define an optical channel of the system <b>1000</b>. The lenses used in the first lens array <b>1502</b>A may be different from the lenses used in the second lens array <b>1502</b>B. The lenses can be any suitable type of lens, including, e.g., spherical lenses, aspheric lenses, rod lenses, etc. or any combination thereof. For example, the lenses used in the first lens array <b>1502</b>A can comprise aspheric lenses, and the lenses used in the second lens array <b>1502</b>B can comprise rod lenses. Although the lens assembly <b>1502</b> shown in <figref idrefs="DRAWINGS">FIG. 14C</figref> includes two lens arrays, it should be appreciated that additional lens arrays may be used, e.g., three lens arrays, four lens arrays, five lens arrays, etc. In addition, to assist in enabling a small system size, the diameter of each lens in the assembly <b>1502</b> can be less than about 0.5″, e.g., in a range of about 0.1″ to about 0.5″. The f-number of each lens can be less than about 2, e.g., in a range of about 0.2 to 2, or more particularly, in a range of about 0.5 to 2, or 1.0 to 2 or 1.1 to 2.</div>
<div class="description-paragraph" id="p-0330" num="0372">The first lens array <b>1502</b>A and the second lens array <b>1502</b>B can be coupled to one another by way of a mounting plate <b>1584</b> sized and shaped to support or receive each lens array <b>1502</b>A, <b>1502</b>B. For example, the first lens array <b>1502</b>A can be mounted on one side of the mounting plate <b>1584</b>, and the second lens array <b>1502</b>B can be mounted on an opposite side of the mounting plate <b>1584</b>. The mounting plate <b>1584</b> can be machined to have diameter tolerances of about +1-25 microns. The lenses of the arrays <b>1502</b>A, <b>1502</b>B can be secured to the mounting plate <b>1584</b> with a curable epoxy. For example, the lenses may fit into opposite sides of holes formed in the mounting plate <b>1584</b>.</div>
<div class="description-paragraph" id="p-0331" num="0373">The optical FPA unit <b>1508</b> can comprise any suitable type of detector array that is configured to detect infrared radiation, for example, greater than 1 micron, or greater than 2 microns, or greater than 3 microns or greater than 5 microns, or greater than 6 microns and possibly lower than 20 microns, or 15 microns, or 13 microns, or 12 microns or 10 microns, in wavelength, and may be cooled or uncooled. In some embodiments the optical FPA unit <b>1508</b> comprises one or more microbolometer arrays, which may be uncooled. For example, an array of about 1000×1000 microbolometer arrays may be used in the embodiments disclosed herein. Microbolometer arrays such as those manufactured by DRS Technologies of Arlington; Virginia, and Sofradir EC, Inc., of Fairfield, N.J., may be suitable for the embodiments disclosed herein. For example, the DRS U8000 FPA manufactured by DRS Technologies may be used in some embodiments. In some arrangements, the microbolometer array may have a resolution of 1024×768 with a pixel pitch of 12 microns. The array of lenses can form separate channels having image detection regions that form part of the may. For example, 12 channels can be included in the 1024×768 pixel array with on the detector array (microbolometer array) that are for example 250×250 pixels for each of the 12 channels. Detector arrays having more or less pixels may be employed. Similarly the number of channels be larger or smaller than 12 and the detection are on the detector array for a single channel may be larger or smaller than 250×250 pixels. For example, the detection region may comprise from between 100-200 pixels×100-200 pixels per detection region, For example, the detection region may comprise from between 100-200 pixels×100-200 pixels per detection region, from between 200-300 pixels×200-300 pixels per detection region, or from between 300-400 pixels×300-400 pixels or from between 400-500 pixels×400-500 pixels. Likewise the detection region for a channel may measure 100-200 pixels on a side, 200-300 pixels on a side, 300-400 pixels on a side, 400-500 pixels on side or larger or smaller. In some arrangements, the spectral band of the microbolometer can be about 7.5 microns to 14 microns. The microbolometer array can operate at a frame rate of about 30 Hz and can operate at operating temperatures of about −40° C. to +70° C. In various embodiments, the microbolometer array is an uncooled microbolometer that does not include a cooler. The sensitivity of the microbolometer at F/1 can be &lt;about 40 mK. The systems <b>1000</b> disclosed herein can be used to detect wavelengths in a range of about 1 micron to about 20 microns. For example, the systems <b>1000</b> disclosed herein can be used to detect wavelengths above about 6 microns, e.g., in a range of about 6 microns to about 18 microns, or more particularly, in a range of about 7 microns to about 14 microns. In various embodiments, the individual detector elements of the microbolometer array can be spaced relatively close together to at least partially enable a small, compact system. For example, adjacent detector elements of the array can be spaced apart by a distance in a range of about 7 microns to about 15 microns, or more particularly in a range of about 9 microns to about 13 microns, e.g., about 11 microns. The individual lenses can be spaced apart by a distance in a range of about 20 mm to about 35 mm, e.g. in a range of about 24 mm to about 30 mm, e.g., about 27.5 mm. Likewise the spatially and spectrally spaced channels may be physically spaced apart by 20 to 35 mm, 24 mm to 30 mm, etc. Although various embodiments of the system are described as including an FPA comprising for example a mircobolometer array, certain embodiments comprise a plurality of FPAs. In some embodiments, a single optical FPA is used. In some embodiments, detectors of the optical FPA are configured to detect radiation in the same band of IR wavelengths.</div>
<div class="description-paragraph" id="p-0332" num="0374">The on-board processing electronics of the data acquisition and processing module <b>1020</b> can process the IR optical data to detect and/or identify a target species from the IR radiation received at the optical FPA. For example, the module <b>1020</b> can be configured to acquire multispectral image data and analyze the acquired image data to identify the target species. For example, the mobile imaging systems <b>1000</b> disclosed herein can be configured to image a 10 m×10 m object area at a distance of about 17 m at a resolution of about 0.04 m. In this example, any gas leaks that generate a gas cloud of at least about 1.5 inches in size can be detected and/or identified by the system <b>1000</b>. The detection and identification methods can be performed substantially in real-time such that the user can be alerted if any leaks are identified.</div>
<div class="description-paragraph" id="p-0333" num="0375">As explained above, the infrared image data captured by the system <b>1000</b> can be processed on board the data acquisition and processing module <b>1020</b> of the imaging system <b>1000</b>. One way to provide a smaller system <b>1000</b> is to process the image data using one or more field-programmable gate arrays (FPGA) configured to execute methods used in the analysis of the images captured by the optical system <b>1015</b>. In some embodiments, one or more Application Specific Integrated Circuits (ASICs) may be used instead of, or in addition to, the FPGAs. For example, an ASICs chip may include an FPGA. The FPGA(s) (and/or ASIC(s)) can be mounted to and electrically coupled with the substrate(s) <b>1586</b> shown in <figref idrefs="DRAWINGS">FIG. 14C</figref> and can be physically located proximate the optical system. For example, the FPGA can include logic gates and read access memory (RAM) blocks that are designed to quickly implement the computations used to detect the types of gases in a gas cloud. The small size/weight, and high performance characteristics of the FPGA can enable on board computation and analysis within the data acquisition and detection unit <b>1020</b> worn or carried by the user. The use of FPGA (or similar electronics) on board the system <b>1000</b> can reduce costs associated with using an off-site central server or larger computing device to conduct the image analysis computations. Advantageously, the embodiments disclosed herein can enable on-board computation even though it can be challenging to implement complex methods on the limited computing platform that FPGAs provide.</div>
<div class="description-paragraph" id="p-0334" num="0376">In addition, enabling computation with one or more FPGA devices on board the wearable system can also prevent or reduce communication bottlenecks associated with wirelessly transmitting large amounts of raw data from the system <b>1000</b> to a remote server or computer. For example, the infrared optical system <b>1015</b> disclosed herein may generate up to about 380 Mbps of raw image data at 30 frames per second, and the visible sensor <b>1580</b> may generate about 425 Mbps of raw image data at 30 frames per second. The resulting data rate of about 800 Mbps is faster than most conventional wireless technologies. While data compression and/or pre-processing may reduce the raw data rates for the visible and IR images, in some embodiments, the IR image data may only be compressed by a ratio of about 2:1. The resulting overall data rate of about 192 Mbps may not be transmitted effectively by conventional wireless communications devices. Accordingly, performing the image processing calculations on board the system <b>1000</b> (e.g., on the data acquisition and processing module <b>1020</b>) can reduce the occurrence of or avoid bottlenecks generated by wirelessly communicating the raw image data to an off-site central server.</div>
<div class="description-paragraph" id="p-0335" num="0377">One challenge to implementing a mobile imaging system is the power requirements of each component of the system, including, e.g., the IR optical system <b>1015</b>, the visible sensor <b>1580</b>, the processing electronics, the wireless communications modules, etc. Advantageously, the mobile infrared imaging systems <b>1000</b> disclosed herein can be configured to operate by battery power for long periods of time without recharging or replacing the batteries <b>1588</b>. In some arrangements the one or more batteries <b>1588</b> can comprise lithium ion batteries, which have relatively high energy densities. In addition, to help reduce power consumption within the system <b>1000</b>, the FPGAs of the data acquisition and processing module <b>1020</b> can be advantageously programmed such that power consumption is lower than that used for other types of processing electronics.</div>
<div class="description-paragraph" id="p-0336" num="0378">The systems <b>1000</b> disclosed herein can advantageously operate for between 8 hours and 36 hours without recharging or replacing the batteries, or more particularly between about 10 hours and 24 hours without recharging or replacing the batteries. In some embodiments, the system <b>1000</b> can operate for at least about 12 hours without recharging or replacing the batteries. The components of the data acquisition and processing module <b>1020</b> (including the imaging optics, focal plane array, and on board processing electronics may) can be configured to operate at relatively low electrical power levels, e.g., at power levels in a range of about 3 W to about 10 W, or more particularly in a range of about 4 W to about 7 W, or in a range of about 4 W to about 6 W, e.g., about 5 W in some embodiments. The components of the data acquisition and processing module <b>1020</b> (including the imaging optics, focal plane array, and on board processing electronics may) can also be configured to operate at relatively low overall energy levels for a single charge of the batteries <b>1588</b>, e.g., at energy levels in a range of about 60 Watt-hours (Wh) to about 100 Wh, or more particularly in a range of about 80 Wh to about 95 Wh, or in a range of about 85 Wh to about 90 Wh.</div>
<div class="description-paragraph" id="p-0337" num="0379">In addition, for each of the embodiments disclosed herein, various motion detection and/or compensation techniques can be implemented to account for relatively large-scale motions that are induced by the user moving his or her head during use. For example, when a user is visiting a well site or other installation, the user may be continuously walking and looking in different directions (e.g., by rotating his or her head). Additionally, vibration can be introduced by the user's natural unsteadiness. Such movement can continuously change the system's field of view at a relatively rapid rate, which can affect the accuracy of the methods used to determine the identity of species in a gas cloud or other object. Accordingly, it can be desirable to provide improved motion detection and/or compensation techniques to reduce errors associated with the movements of the user.</div>
<div class="description-paragraph" id="p-0338" num="0380">Each of the embodiments disclosed herein can be used to estimate various characteristics of gases present in a gas leak imaged by the infrared imaging systems disclosed herein.</div>
<heading id="h-0171">IV. Examples of Remote Monitoring Systems and Methods</heading>
<div class="description-paragraph" id="p-0339" num="0381">Various embodiments disclosed herein enable the efficient monitoring of multiple DAISI systems at one or more sites situated at one or multiple installations or facilities, or any combinations thereof. For example, various embodiments disclosed herein relate to the remote monitoring of petrochemical installations. As explained herein, petrochemical installations, such as hydrocarbon well sites (e.g., oil, natural gas, or other drilling sites for petrochemicals), petrochemical processing sites, petrochemical transportation and/or storage sites, may be located in remote areas that are many miles from population centers, and/or that otherwise lack access to high speed communications networks (such as high speed optical networks, or other high speed wired or wireless networks). Although many embodiments disclosed herein relate to the monitoring of petrochemical installations, it should be appreciated that these embodiments could similarly be deployed at any suitable type of installation, including, e.g., chemical plants, manufacturing facilities, etc. For example, petrochemical installations may be located at remote sites without reliable high speed cellular or wireless communications with a central processing facility or server. In some arrangements, the remote petrochemical installations may communicate with the central processing facility or server by way of slower, low frequency, low bandwidth cellular networks. Transmission of large amounts of data over these slower cellular networks may result in excessively long transmission times and the overuse of network resources.</div>
<div class="description-paragraph" id="p-0340" num="0382">In various embodiments, a system for monitoring a petrochemical installation is disclosed. The system can include any suitable processing electronics, including processors, computer-readable memory readable by the processors, and any other suitable hardware, which may be interconnected by a communications networks. The system can include an optical imaging system comprising an array of optical detectors and processing electronics configured to process image data detected by the optical imaging system. The processing electronics can be configured to detect a target species based at least in part on the processed image data and, based on a detected amount of the target species, transmit an alarm notification to an external computing device over a communications network indicating that the target species has been detected at the petrochemical installation.</div>
<div class="description-paragraph" id="p-0341" num="0383">In some embodiments, the processing electronics can be configured to detect the target species over multiple frames of the image data and to combine the multiple frames of image data into a summary image that presents the detection of the target species over a period of time. In various embodiments, the summary image comprises a single image. In other embodiments, the summary image comprises a plurality of images. The processing electronics can be configured to create the summary image by calculating an average concentration of the target species over a plurality of successive frames, and/or an average of the image data representative of the concentration, over the period of time, or other calculations based on an accumulation of values (such as concentrations or data for calculating concentrations) over time. In some embodiments, the processing electronics can be configured to create an events log comprising a plurality of events comprising one or a plurality of target species detected by the processing electronics. The processing electronics can be configured to analyze the events log, and based on the analysis, to transmit a priority ranking of events to the external computing device. For example, the processing electronics can be configured to assign a higher priority to more dangerous gases (e.g., hydrogen sulfide) than to other gases. As another example, the processing electronics can be configured to assign a higher priority to a gas leak in which the concentration of leaked gas is greater than another gas leak with a lower concentration of leaked gas. Still other ways of assigning priority may be suitable.</div>
<div class="description-paragraph" id="p-0342" num="0384">In various embodiments, the processing electronics can be configured to compare the detected amount of the target species to a threshold amount and, based on that comparison, transmit the alarm notification to the external computing device over the communications network indicating that the target species has been detected at the petrochemical installation. In some implementations, the threshold amount can be in a range of 1 ppm-m to 1000 ppm-m of the target species. In some implementations, the threshold amount can be in a range of 25 ppm-m to 1000 ppm-m of the target species. In some implementations, the threshold amount can be in a range of 25 ppm-m to 750 ppm-m of the target species, or in a range of 50 ppm-m to 550 ppm-m.</div>
<div class="description-paragraph" id="p-0343" num="0385">The target species can be any suitable type of target species, including, e.g., various types of petrochemical species including liquids and/or gases. In some embodiments, the target species comprises methane. In some embodiments, the target species comprises hydrogen sulfide. In various embodiments, the target species comprises a gas. In various embodiments, the target species comprises a liquid, such as oil.</div>
<div class="description-paragraph" id="p-0344" num="0386">In various embodiments, the processing electronics can be configured to detect an unauthorized intrusion of an animal (including a human) into the petroleum installation and, based on the detection, to transmit a second alarm notification to the external computing device over the communications network indicating the unauthorized intrusion. The processing electronics can be programmed with various thresholds so as to ensure that only various unauthorized events and/or unexpected leaks are detected and transmitted to the external computing device. In various embodiments, for example, the processing electronics can comprise image recognition and/or motion detection techniques to determine if there is an unauthorized intrusion. In various systems, the external computing device may be bombarded with numerous extraneous alarms or notifications if the detection events criteria and/or thresholds are not adequately set. In the embodiments disclosed herein, the detection events can correspond to various predetermined thresholds or detection criteria such as but not limited to those disclosed herein.</div>
<div class="description-paragraph" id="p-0345" num="0387">In some embodiments, the communications network comprises a wireless communications network. In various embodiments, the wireless communications network comprises a cellular communications network, e.g., conforming to any suitable cellular communications standard, such as 3G, LTE, etc. The processing electronics can be configured to transmit processed image data (such as a summary alarm image) to the external computing device at speeds in a range of 0.1 Mbps to 10 Mbps. In some embodiments, the processing electronics can be configured to transmit processed image data (such as a summary alarm image) to the external computing device at speeds in a range of 0.5 Mbps to 2 Mbps. In various embodiments, the processing electronics can be configured to render a user interface presentable to a user on a display device. The user interface can comprise a visible image window and an infrared image window showing images obtained using primarily visible light and infrared light respectively (e.g., using visible and infrared imaging sensors, respectively).</div>
<div class="description-paragraph" id="p-0346" num="0388">The optical imaging system can include any of the optical imaging systems disclosed herein, e.g., any of the systems disclosed in <figref idrefs="DRAWINGS">FIGS. 1-14C</figref>. For example, the optical imaging system can comprise one or more infrared (IR) detector arrays. The optical imaging system can additionally or alternatively comprise a visible light detector array. The optical imaging system can define a plurality of optical channels being spatially and spectrally different from one another, each of the plurality of optical channels positioned to transfer radiation incident on the optical imaging system towards the array of optical detectors. In various embodiments, the optical imaging system and the processing electronics can be contained together in a data acquisition and processing module configured to be worn or carried by a person. The optical imaging system and the processing electronics can be configured to be mounted to a support structure at the petroleum installation. The optical system can comprise a plurality of spectrally distinct infrared optical filters. In other embodiments, the monitoring systems and methods can be used in conjunction with other types of imaging systems (e.g., IR systems), including single channel imaging systems or any other suitable type of imaging systems.</div>
<div class="description-paragraph" id="p-0347" num="0389"> <figref idrefs="DRAWINGS">FIG. 15A</figref> is a schematic system diagram showing a monitoring system <b>2010</b> for detecting and/or identifying fluid leaks (e.g., gas or liquid leaks) from one or a plurality of sites situated at one or a plurality of remote installations or facilities (e.g., a petrochemical facility). As shown in <figref idrefs="DRAWINGS">FIG. 15A</figref>, the system <b>2010</b> can comprise a plurality of imaging systems <b>2000</b> disposed at a corresponding plurality of locations, installations, or facilities. The imaging systems <b>2000</b> can be disposed at different locations within a particular installation or facility, for example, at different locations within a large oil refinery. In other arrangements, the imaging systems <b>2000</b> can be disposed at facilities at entirely different geographic locations that may be geographically remote. In <figref idrefs="DRAWINGS">FIG. 15A</figref>, four imaging systems <b>2000</b> (Imaging System <b>1</b>, Imaging System <b>2</b>, Imaging System <b>3</b>, and Imaging System <b>4</b>) are illustrated, but it should be appreciated that more or fewer imaging systems <b>2000</b> may be monitored by the system <b>2010</b> shown in <figref idrefs="DRAWINGS">FIG. 15A</figref>.</div>
<div class="description-paragraph" id="p-0348" num="0390">The imaging systems <b>2000</b> can comprise any of the imaging systems disclosed herein in <figref idrefs="DRAWINGS">FIGS. 1-14C</figref>, including, e.g., the DAISI systems disclosed herein. The imaging systems <b>2000</b> can be configured to be worn or carried by a person, can be fixedly mounted to a tower or pole (or other fixed structure or building), can be mounted to a truck or other moving vehicle, or can be attached to an aerial platform (such as a drone or other aircraft). The imaging systems <b>2000</b> can comprise multi-spectral (e.g., multi-channel) infrared imaging systems, similar to the systems disclosed herein. In other embodiments, however, the imaging systems <b>2000</b> can comprise other types of imaging systems, e.g., single channel IR imaging systems, or non-IR imaging systems.</div>
<div class="description-paragraph" id="p-0349" num="0391">As shown in <figref idrefs="DRAWINGS">FIG. 15A</figref>, each imaging system <b>2000</b> can be in network or data communication with a central monitoring computer system <b>2001</b> (e.g., one or a plurality of external computing devices) or network of multiple computing devices. The monitoring computer system <b>2001</b> can comprise any suitable type of computer system or processing electronics. In some embodiments, the monitoring computer system <b>2001</b> can comprise a centralized server with processing electronics and/or one or more communications modules programmed to receive and/or transmit information to and/or from the imaging systems <b>2000</b>. In some embodiments, the monitoring computer system can comprise a desktop or laptop computer with software stored thereon, or accessed via the Internet (such as over a web browser), that is configured to communicate with the imaging systems <b>2000</b>. The monitoring computer system <b>2001</b> can communicate with the imaging systems <b>2000</b> using any suitable communications network, including, e.g., cellular data connections (e.g., 3G, 4G network standards, or any other suitable cellular network standard), wireless network connection (e.g., WiFi connection), wired network connection (e.g., Ethernet connection), etc.</div>
<div class="description-paragraph" id="p-0350" num="0392">For example, in embodiments in which the central monitoring computer system <b>2001</b> is located near the imaging system(s) <b>2000</b>, a wired or wireless internet connection may be suitable to transfer video and/or other image data over a high bandwidth connection. However, in some embodiments, the imaging system(s) <b>2000</b> may be located at facilities that are remote from the monitoring computer system <b>2001</b>, in which case a cellular network having a relatively low bandwidth may provide wireless data communication between the imaging systems <b>2000</b> and the monitoring computer system <b>2001</b>. In various embodiments, the fluid leak detection and/or identification may be performed at the facility or installation, and the processed image data may be communicated to the monitoring computer system <b>2001</b>. In other embodiments, the raw image data (e.g., raw IR image data) may be communicated to the monitoring computer system <b>2001</b>, and the monitoring computer system <b>2001</b> may be programmed to detect and/or identify the target species (e.g., target gases or liquids). Still other combinations of detecting and/or identifying the fluid leak are possible.</div>
<div class="description-paragraph" id="p-0351" num="0393">The monitoring computer system <b>2001</b> can be configured with suitable authorization protocols such that only authorized system users can access the computer system <b>2001</b> to monitor the selected imaging systems <b>2000</b>. For example, in some embodiments, monitoring software can comprise computer-readable instructions that when executed, provide the monitoring computer system <b>2001</b> (and hence the user) with tools to monitor the imaging systems <b>2000</b>. As explained herein, the monitoring system software can provide various user interfaces to the user to enable the user to view information about the scene(s) that is/are being imaged by the imaging system <b>2000</b>, including, e.g., video feeds of the scene(s), alerts or alarms related to fluid leaks, alerts or alarms related to intruders, or any other information detected by the systems <b>2000</b>. The monitoring systems can further provide tools for the user to interact with the imaging systems <b>2000</b>, e.g., to select which imaging systems <b>2000</b> are to be monitored, to set thresholds for fluid leak detection algorithms, etc.</div>
<div class="description-paragraph" id="p-0352" num="0394">In various embodiments, the software can be executed on processing electronics associated with each imaging system <b>2000</b>, and/or with a centralized server associated with the systems <b>2000</b>, and can be remotely accessed by way of a suitable communications protocol. For example, in some embodiments, each imaging system <b>2000</b> can comprise a unique network address (e.g., an internet protocol, or IP, address) that can be accessed over the World Wide Web. When accessed, e.g., over a web browser, a login screen or other authorization mechanism can be presented to the user. The user can enter a suitable username and password combination (or other authentication mechanism) to access the imaging system(s) <b>2000</b> associated with the user. The user can view and/or control all the associated imaging systems <b>2000</b> through a secure web portal or browser. The embodiments disclosed herein can therefore enable secure, remote access to the monitoring system.</div>
<div class="description-paragraph" id="p-0353" num="0395"> <figref idrefs="DRAWINGS">FIG. 15B</figref> is a schematic system diagram showing an example of a monitoring computer system <b>2001</b>, according to various embodiments. As shown in <figref idrefs="DRAWINGS">FIG. 15B</figref>, the monitoring computer system <b>2001</b> can comprise one or a plurality of servers <b>2051</b> in network communication with one or a plurality of user computer systems <b>2050</b>. As explained above, the imaging systems <b>2000</b> can communicate with the monitoring computer system <b>2001</b> (e.g., the server(s) <b>2051</b>) over any suitable communications network, such as the Internet by way of a WiFI or Ethernet connection, by a cellular network, etc. In turn, the server(s) <b>2051</b> can communicate with the user computer system(s) <b>2050</b> by way of any suitable communications network. As explained above, for example, the user computer system(s) <b>2050</b> can access information about events (e.g., fluid leaks, etc.) over the Internet by accessing a secure web page hosted by the server(s) <b>2051</b>. Although the monitoring computer system <b>2001</b> utilizes one or more servers to transmit information about events detected at the imaging systems <b>2000</b> to the user computer systems <b>2050</b>, in other embodiments, however, the imaging systems <b>2000</b> can communicate directly with the user computer systems <b>2050</b> over a communications network, e.g., the Internet, over a cellular communications network, etc.</div>
<div class="description-paragraph" id="p-0354" num="0396">In various embodiments disclosed herein, the imaging system(s) <b>2000</b> can comprise processing electronics that are configured to process captured image data to detect and/or identify a target species, such as a target gas or liquid. The processing electronics of the imaging systems <b>2000</b> can be locally connected with the optical components of the system <b>2000</b>. For example, the processing electronics can be physically located at or near the optical components in some embodiments. In other embodiments, the processing electronics may be remote from the optical components. For example, in other embodiments, the image data can be processed by processing electronics included in and/or associated with the computer monitoring system <b>2001</b> such as the server(s) <b>2051</b> or user computer system(s) <b>2050</b>. In various embodiments, and as explained below, processing electronics of the imaging system(s) <b>2000</b> can be configured to generate an alarm based on the detection of the target species, and can transmit the alarm to the monitoring computer system <b>2001</b> (e.g., the server(s) <b>2051</b> in some embodiments). In other embodiments, the processing electronics of the imaging system(s) <b>2000</b> or monitoring computer system <b>2001</b> can be configured to transmit the alarm to the user computer system(s) <b>2050</b>. In some embodiments, the processing electronics of the imaging system <b>2000</b> can be configured to generate an event log that includes one or more events detected by the imaging system(s) <b>2000</b> at the installation being monitored. In other embodiments, the monitoring computer system <b>2001</b> can generate the event log. Furthermore, as explained herein, the processing electronics of the imaging system(s) <b>2000</b> can be configured to generate the summary alarm image(s) described below, and can transmit the summary alarm image(s) to the monitoring computer system <b>2001</b>. In various embodiments, the processing electronics of the imaging system(s) <b>2000</b> can be configured to generate the time-lapsed image(s) of fluid leaks (such as liquid leaks) described below, and can transmit the time-lapsed image(s) to the monitoring computer system <b>2001</b>. In various embodiments the server(s) <b>2051</b> can transmit the event data at the installation being monitored (including information about fluid leaks, etc.) to the user computer systems <b>2050</b>, over a communications network. In various embodiments, the user computer systems <b>2050</b> can access the information about the event(s) over the network, and can render the information in a user interface to be viewed and/or controlled by the user or operator, who may be remote from the imaging system(s) <b>2000</b> and the installations being monitored. In some embodiments, processing electronics of the imaging system(s) <b>2000</b> can be configured to generate the progressive mode described below, in which summary alarm image(s) can be generated in a cycle and sent to the user(s), for example, by way of the monitoring computer system <b>2001</b>. In other embodiments, the monitoring computer system <b>2001</b> (e.g., the server(s) <b>2051</b>) can be configured to generate the progressive mode described herein. In various embodiments disclosed herein, the processing electronics can comprise a first portion of electronics (e.g., one or more processors) at or near the imaging system <b>2000</b> and a second portion of electronics (e.g., one or more processors) remote from the imaging system <b>2000</b>. In some implementations, the processing electronics can be included in whole or in part in the monitoring computer system <b>2001</b>, e.g. the server(s) <b>2051</b> and/or user computer system(s) <b>2050</b>. Accordingly, in some implementations, the summary alarm image(s), time-lapsed images(s), mosaic images, overview image(s), multi-view or progressive modes, event log, or other images, formats, functions, and/or modes discussed below, may be potentially be produced at least in part by processing electronics in or associated with the monitoring computer system <b>2001</b>, e.g. the server(s) <b>2051</b> and/or user computer system(s). In some implementations, such images, formats, functions, and/or modes may be produced in whole or in part by processing electronics included with the imaging system(s) <b>2000</b>. In some implementations, processing electronics associated with the imaging system(s) <b>2000</b> and processing electronics associated with the monitoring computer system <b>2001</b>, e.g. the server(s) <b>2051</b> and/or user computer system(s) may be used to produce such images, formats, functions, and/or modes.</div>
<div class="description-paragraph" id="p-0355" num="0397"> <figref idrefs="DRAWINGS">FIG. 15C</figref> is a schematic diagram of a system overview window <b>2002</b>, which can be rendered on a display of the central monitoring computer system <b>2001</b>. The system overview window <b>2002</b> can provide the user or system operator with a system-level overview of the location of each imaging system <b>2000</b> (for example, overlaid on a map or image of the locations to be monitored), which may be assigned a unique identifier or name. As shown in <figref idrefs="DRAWINGS">FIG. 15C</figref>, the system overview window <b>2002</b> can comprise an event log panel <b>2003</b>, an overview image <b>2004</b>, and a tools panel <b>2005</b>. As explained in more detail below, the event log panel <b>2003</b> can provide the user with information related to events that occur or that are detected by the imaging systems <b>2000</b> (e.g., detection of a gas or liquid leak). Also, as explained in more detail herein, the tools panel <b>2005</b> can provide the user with an interface to select various modes of operation, select which imaging systems are monitored or displayed in the window, and select other imaging or monitoring parameters, such as alarm thresholds, intrusion detection, etc. The overview image <b>2004</b> of the system overview window <b>2002</b> can provide a rendering of the locations to be monitored, e.g., overlaid on a map or image of the locations that are to monitored by the imaging systems <b>2000</b>. In the overview image <b>2004</b> of <figref idrefs="DRAWINGS">FIG. 15C</figref>, for example, Imaging Systems <b>1</b> through <b>4</b> are schematically overlaid on an image of an installation to be monitored. The imaging systems <b>2000</b> can be identified in any suitable manner, for example, by way of a system name or unique identifier. Beneficially, the overview image <b>2004</b> can allow the user to determine the physical location of a particular imaging system <b>2000</b>, for example, if an alarm alerts the user to an event (e.g., fluid leak) at that particular imaging system <b>2000</b>. Associating an event with a physical location can enable the user or operator to quickly address any urgent events, such as fluid leaks.</div>
<div class="description-paragraph" id="p-0356" num="0398"> <figref idrefs="DRAWINGS">FIG. 15D</figref> is a schematic diagram of a multi-view imaging window <b>2006</b> which can be rendered on a display of the central monitoring computer system <b>2001</b>. The multi-view imaging window <b>2006</b> can comprise a plurality of images <b>2007</b>, each of which are associated with a different imaging system <b>2000</b>. For example, as shown in <figref idrefs="DRAWINGS">FIG. 15D</figref>, a first image <b>2007</b> can comprise an image (e.g., visible or IR image) of a particular field of view (FOV) at a first location that is being monitored by Imaging System <b>1</b>. Similar, a second image <b>2007</b> can comprise an image (e.g., visible or IR image) of a particular FOV at a second location that is being monitored by Imaging System <b>2</b>; a third image <b>2007</b> can comprise an image (e.g., visible or IR image) of a particular FOV at a third location that is being monitored by Imaging System <b>3</b>; and a fourth image <b>2007</b> can comprise an image (e.g., visible or IR image) of a particular FOV at a fourth location that is being monitored by Imaging System <b>4</b>.</div>
<div class="description-paragraph" id="p-0357" num="0399">The multi-view window <b>2006</b> can beneficially enable the user to view at least one FOV of each location that is being monitored by the imaging systems <b>2000</b> of the monitoring system <b>2010</b>. If an event is detected at a particular location and imaging system <b>2000</b>, the system <b>2010</b> can alert the user to the location at which the event occurs on the multi-view window <b>2006</b>. The user can select the location at which the event is detected to obtain more information about the event, and/or can view the event in the event log <b>2003</b>. The images <b>2007</b> of the multi-view window <b>2006</b> can be viewed in a progressive viewing mode, which is explained in more detail below. As explained below, the progressive viewing mode can give the user a high-level, low resolution overview of each location that is being monitored by the respective imaging systems <b>2000</b>.</div>
<div class="description-paragraph" id="p-0358" num="0400"> <figref idrefs="DRAWINGS">FIG. 16A</figref> is a schematic diagram of a imaging system window <b>2008</b>, according to various embodiments. The imaging system window <b>2008</b> can be rendered on the display of the central monitoring computer system <b>2001</b>. The imaging system window <b>2008</b> can render information associated with a particular imaging system <b>2000</b>, e.g., any one of Imaging Systems <b>1</b> to <b>4</b>. The user can access the imaging system window <b>2008</b> associated with a particular imaging system <b>2000</b> by engaging with a user interface of the computer system <b>2001</b>, e.g., by selecting a particular imaging system <b>2000</b> from a menu, by selecting an associated image <b>2007</b> from the multi-view window <b>2006</b>, or in any other suitable manner. The imaging system window <b>2008</b> can advantageously provide the user with detailed information (including information about various events, etc.) associated with the particular imaging system <b>2000</b>.</div>
<div class="description-paragraph" id="p-0359" num="0401">As shown in <figref idrefs="DRAWINGS">FIG. 16A</figref>, the imaging system window <b>2008</b> can comprise an events log panel <b>2003</b> (which can log events from one or a plurality of imaging systems <b>2000</b> at one or a plurality of locations), a menu <b>2009</b> for navigating to various other views or tools, a visible image window <b>2021</b>, an infrared image window <b>2022</b>, a mosaic image window <b>2023</b>, and a tools panel <b>2005</b>, as discussed below. In various embodiments, the visible image window and the infrared image window, obtained primarily with visible light and infrared light, respectively, can be displayed simultaneously on the user interface, e.g., side-by-side in a split image view. <figref idrefs="DRAWINGS">FIG. 16B</figref> is a schematic diagram of another example of the mosaic image window <b>2023</b>. The mosaic image window <b>2023</b> can provide the user with a mosaic image of the entire field of view (FOV) that is imaged by the particular imaging system <b>2000</b>, for example, as the imaging system <b>2000</b> is scanned or moved across the scene. As shown in <figref idrefs="DRAWINGS">FIGS. 16A-16B</figref>, for example, the mosaic image window <b>2023</b> can enable the user to view the full scene which can be monitored by the imaging system <b>2000</b> of interest.</div>
<div class="description-paragraph" id="p-0360" num="0402">Within the larger mosaic image are a plurality of detection FOVs, associated with corresponding detection regions which are monitored by the imaging system. For example, as shown in <figref idrefs="DRAWINGS">FIGS. 16A-16B</figref>, the imaging system <b>2000</b> can comprise a plurality of designated or “stored” FOVs <b>2020</b> within the mosaic image. It should be appreciated that, as used herein, the designated or “stored” FOVs <b>2020</b> can correspond to FOVs that the imaging systems <b>2000</b> monitor at various time periods. Thus, the designated or “stored” FOVs <b>2020</b> can be regularly monitored by the imaging system(s) <b>2000</b> to detect fluid leaks. The imaging system(s) <b>2000</b> can monitor one designated FOV <b>2020</b> during a first time period (or first dwell time), and after the first time period, can monitor another designated FOV <b>2020</b> for a second time period (or second dwell time), and so on until different monitored FOVs <b>2020</b> are observed for a desired time period or dwell time. The system <b>2000</b> can cycle through the FOVs <b>2020</b> repeatedly, e.g., to continually, monitor the petrochemical installation. For example, as shown in <figref idrefs="DRAWINGS">FIG. 16A</figref>, the user can engage with a path configuration interface <b>2017</b> to cause the system <b>2010</b> to regularly and repeatedly monitor the stored detection FOVs <b>2020</b> to determine if there are any fluid leaks (e.g., gas leaks or oil leaks). The user can cause the path configuration interface <b>2017</b> to cycle through the stored FOVs <b>2020</b> in any desired order. For example, the mosaic window <b>2023</b> can be used to change the FOV and the programmed paths taken by imaging systems <b>2000</b>. Thus, in the embodiments disclosed herein a path can comprise a series of different FOVs that are looped continuously in a cycle. When a particular FOV is to be monitored in the cycle, the imaging system <b>2000</b> can monitor the FOV for events such as fluid leaks, and the particular FOV can be rendered on the visible and/or IR image windows <b>2021</b>, <b>2022</b>.</div>
<div class="description-paragraph" id="p-0361" num="0403">Thus, at a particular time period, a particular active FOV <b>2019</b> can be selected (e.g., automatically or manually by the user) within the path cycle (e.g., the order that FOVs are monitored by the system <b>2000</b>). The selected active FOV <b>2019</b> can correspond to the detection FOV that is being monitored during the particular time period. As shown in <figref idrefs="DRAWINGS">FIG. 16A</figref>, for the particular active FOV <b>2019</b>, the imaging system window <b>2008</b> can render the active FOV <b>2019</b> in the visible image window <b>2021</b> and in the IR image window <b>2022</b>. The visible image window <b>2021</b> can correspond to visible image data captured from a visible camera, such as a pan-tilt-zoom (PTZ) camera <b>2033</b>. The IR image window <b>2022</b> can render IR image data taken from one or more of the multiple channels of the IR imaging camera (e.g., a DAISI or other IR system). An infrared or temperature bar <b>2031</b> can provide the user with an estimate of the temperature in the infrared image window <b>2022</b>. A status bar <b>2032</b> can be provided to indicate the status of the detection process to the user, e.g., whether or not there is an alarm within the particular FOV representative of a fluid leak. In various embodiments, if a fluid leak is detected, the fluid leak can be viewed in real-time video, as a colored schematic representation overlaid on the IR (or visible) image. In some embodiments, if a leak is detected, image data of the FOV in which the leak is observed can automatically pop up as video data and/or as a summary alarm image. In some embodiments, the image data can be manually retrieved by the user, e.g., by way of the event log panel <b>2003</b>.</div>
<div class="description-paragraph" id="p-0362" num="0404">The visible and IR image windows <b>2021</b>, <b>2022</b> can beneficially provide the user with enlarged visualizations of the active FOV <b>2019</b> while the imaging system <b>2000</b> is determining whether there are any fluid leaks within the active FOV <b>2019</b>. If the system <b>2000</b> determines that there are fluid leaks within the active FOV <b>2019</b>, then the imaging system <b>2000</b> can trigger an alert, which can update the status bar <b>2032</b> and/or the event log <b>2003</b>. If no fluid leak is detected, then the status bar <b>2032</b> can indicate that no alarm or alert is present. When the monitoring time period (e.g., 15 sec) is over, another stored FOV <b>2020</b> is cycled to become the active FOV <b>2019</b>, and so on until every stored FOV <b>2020</b> in the mosaic image window <b>2023</b> is monitored. The path cycle repeats so that the sites in the installation can be continually monitored over time. For example, at the end of the monitoring time period, the imaging system <b>2000</b> can be moved (e.g., rotated, tilted, translated, etc.) so that another FOV <b>2020</b> is monitored by the imaging system <b>2000</b>. If a fluid leak (e.g., gas or liquid leak) is detected within a particular active FOV, then the FOV in which the leak is detected can be denoted within an alarm FOV <b>2018</b>. Advantageously, the alarm FOV <b>2018</b> can be color coded to easily alert the user to the presence of an event such as a fluid leak.</div>
<div class="description-paragraph" id="p-0363" num="0405">The tools panel <b>2005</b> of the imaging system window <b>2008</b> can include various interactive tools that the user can engage to select the FOVs of which imaging systems <b>2000</b> are to be rendered in the imaging system window <b>2008</b>. The tools panel <b>2005</b> can also include a fluid threshold tab <b>2029</b> in which the user can select threshold values for the fluid detection algorithm, e.g., to make the algorithm more or less sensitive based on the user preferences. For example, the user can modify threshold values for one or more gases or liquids that are being monitored. The system <b>2000</b> will alarm when the threshold values are exceeded for that particular gas or liquid The tools panel <b>2005</b> can comprise a camera selection <b>2028</b> tab in which the user selects which imaging system's FOV is rendered in the imaging system window <b>2008</b>. Further, the tools panel <b>2005</b> can include a timer indicator <b>2027</b>, which indicates the amount of time that a particular designated or “stored” FOV <b>2020</b> is being monitored at a particular period and is rendered on the imaging system window <b>2008</b>. As explained above, the system <b>2010</b> can cycle through the designated or “stored” FOVs <b>2020</b> to monitor the particular FOV for fluid leaks during a predetermined period of time, or dwell time (e.g., 15 seconds, 30 seconds, 1 minute, etc.). For example, as explained above, the system <b>2010</b> can render image data (e.g., video stream and/or summary alarm image) of a first FOV during a first time period or dwell time. After the first time period or dwell time expires, the system <b>2010</b> can render image data (e.g., video stream and/or summary alarm image) of a second FOV during a second time period or dwell time. The system <b>2010</b> can cycle through the FOVs to repeatedly (e.g., continually) observe successive FOVs for respective dwell times. The tools panel <b>2005</b> can also include a gas list <b>2026</b>, indicating which gases (or other fluids) are being actively monitored by the systems <b>2000</b>. The user can select any suitable number of fluids to be monitored. In addition, in some embodiments, the user can elect to also monitor for intrusions by unwanted third parties into the FOV. A view options bar <b>2025</b> can be provided to allow the user to view, for example, only the visible image window <b>2021</b>, only the IR image window <b>2022</b>, or a split screen (<figref idrefs="DRAWINGS">FIG. 16A</figref>) that shows the visible image window <b>2021</b>, the IR image window <b>2022</b>, and the mosaic window <b>2023</b>, or any combination thereof. A viewing mode bar <b>2024</b> can be provided to allow the user to select a desired viewing mode, e.g., a live-stream video mode, a progressive mode (discussed below), etc. A user bar <b>2030</b> can indicate which user is logged onto the system, and/or additional information about the user (e.g., user's authorization status, user's name and location, etc.).</div>
<div class="description-paragraph" id="p-0364" num="0406">The events log <b>2003</b> can provide an indication of the events that have been detected by the system <b>2010</b>, e.g., events at a particular imaging system <b>2000</b> and/or system-wide events for all (or a plurality of) imaging systems <b>2000</b>. The events log <b>2003</b> can comprise a download interface <b>2011</b> to enable the user to download one or a plurality of the logged events. The downloaded events can be presented in a spreadsheet-style document. In some embodiments, the user can download video streams and/or summary images (see <figref idrefs="DRAWINGS">FIG. 16C</figref>) by way of the events log <b>2003</b> and download interface <b>2011</b>. An acknowledge-all interface <b>2012</b> can be provided so that the user can acknowledge all listed events. Further, individual acknowledgement interfaces <b>2015</b> can be provided so that the user can acknowledge a particular event (or events) to indicate that the user has addressed the event in an appropriate manner. In addition, a sort interface <b>2013</b> can be provided such that the user can sort the events in any suitable manner, e.g., sort by type of alarm, date and time of alarm, etc. The user can engage a highlight event interface <b>2014</b> to indicate events that should be followed up with at a later date. As explained herein, the user can select a video or summary image link <b>2016</b> to view the video feed or summary image (see below) associated with the particular event (such as a gas leak).</div>
<div class="description-paragraph" id="p-0365" num="0407">Turning to <figref idrefs="DRAWINGS">FIG. 16C</figref>, an example of a summary alarm image <b>2034</b> is illustrated, according to various embodiments. In the summary alarm image <b>2034</b> shown in <figref idrefs="DRAWINGS">FIG. 16C</figref>, a detected target species <b>2035</b> is overlaid over an image of the scene being monitored by the imaging system <b>2000</b>. The summary alarm image <b>2034</b> can comprise a color map representative of concentrations of the target species (e.g., red for high concentrations and blue for low concentrations, or vice versa). As explained herein, some petrochemical installations may be located in remote physical locations, miles away from any infrastructure or populated areas. In such locations, high speed internet connections (e.g., optical communications networks, or other high speed connections) may be unavailable. In such remote facilities, lower speed (and lower bandwidth) cellular networks may be used to provide wireless communications between remote imaging systems <b>2000</b> and the monitoring computer system <b>2001</b>. Such low speed cellular networks may not provide adequate bandwidth for the transmission of high resolution, high frame rate raw image data. Accordingly, various embodiments disclosed herein utilize summary alarm images <b>2034</b> (as opposed to, or in addition to, video feeds) which can be transmitted rapidly over low speed and/or low bandwidth wireless communications networks.</div>
<div class="description-paragraph" id="p-0366" num="0408">When a particular active FOV <b>2018</b> of the designated or stored FOVs <b>2020</b> is to be monitored, the imaging system <b>2000</b> moves to image the active FOV <b>2018</b> and the fluid leak detection methods disclosed herein can be utilized to detect and/or identify fluid leaks (e.g., gas leaks, oil leaks, etc.) for a predetermined period of time T, which may be any suitable time period, for example, 15 s, 30 s, 1 min, etc. Within the time period T, the imaging system <b>2000</b> may capture a multispectral image at a cycle time t (e.g., every 1 s, every 5 s, etc.), and the system <b>2000</b> may determine whether there are any fluid leaks within the multispectral image. As explained above, it can be challenging to transfer the entire recorded video stream to the monitoring computer system <b>2001</b> over low speed networks.</div>
<div class="description-paragraph" id="p-0367" num="0409">Accordingly, in various embodiments disclosed herein, if an event is detected by the imaging system <b>2000</b> (e.g., if a target gas concentration is detected at or above a predetermined threshold), then the system <b>2000</b> can create the summary alarm image <b>2034</b> by detecting the target species <b>2035</b> over multiple frames of the image data and combining the multiple frames of image data into the summary alarm image <b>2034</b> that presents the detection of the target species <b>2035</b> over a period of time. In various embodiments, for example, the concentration of the detected species <b>2035</b> (or image data representative of the concentration of the species <b>2035</b>) can be averaged or otherwise weighted over the frames captured during the time period T. For example, the summary alarm image <b>2034</b> can comprises an average of, e.g., 5, 10, 20, 30, 40, 50, 100 frames over a time period equal to the number of frames divided by the frame rate. This period may correspond to the time or dwell period, T, as referenced above. Although time averaging the concentration can be employed, other approaches that involve accumulating data and/or calculating values over an extended period of time, e.g., <u>T</u>, can be used. Also the number of frames can be different. The resulting summary alarm image <b>2034</b> can comprise a single alarm image or a plurality of separate alarm images. For example, in some embodiments, a single alarm image can be generated for each time period T or dwell time to provide a time-averaged illustration of an event over multiple frames of video feed. In other embodiments, multiple alarm images can be generated within the time period T, e.g., if the time period T is particularly long, or it is otherwise desirable to present multiple summary images. In some embodiments, the summary alarm image <b>2034</b> can be combined or averaged (or otherwise calculated) over the entire time period T or dwell time at which the FOV of interest is being monitored. In other embodiments, the summary alarm image <b>2034</b> can be combined or averaged (or otherwise calculated) over a portion of (i.e., less than) the entire time period T or dwell time at which the FOV of interest is being monitored. Beneficially, the summary alarm image <b>2034</b> can comprise a relatively low resolution illustration of the fluid leak during the time period T that can provide the user with a snapshot of the event being monitored. Furthermore, since every frame of the raw video data need not be transmitted over the communications network, the summary alarm image <b>2034</b> can provide an efficient way to transfer important information about an event over low speed networks from remote locations.</div>
<div class="description-paragraph" id="p-0368" num="0410">In various embodiments, the time period T or dwell time at which a particular FOV <b>2022</b> is being monitored can be in a range of 1 sec to 60 min, in a range of 1 sec to 10 min, in a range of 1 sec to 1 min, in a range of 1 sec to 45 sec, in a range of 1 sec to 30 sec, or in a range of 1 sec to 15 sec (e.g., about 15 s). Any suitable time period T or dwell time may be used. The frame rate of the video feed during a particular period T or dwell time can be any suitable frame rate, e.g., in a range of 0.1 frames per second (fps), to 60 fps, in a range of 0.1 fps to 30 fps, in a range of 0.5 fps to 30 fps, in a range of 0.5 fps to 15 fps, or in a range of 1 fps to 10 fps. For various embodiments disclosed herein, including some embodiments that utilize low bandwidth cellular networks, lower frame rates may be used. In some embodiments, the summary alarm image <b>2034</b> can be generated at the end of each time period T or dwell time. In some embodiments, the summary alarm image <b>2034</b> can be continuously generated after each frame, or after a plurality of frames, until the time period T expires.</div>
<div class="description-paragraph" id="p-0369" num="0411">The summary alarm image <b>2034</b> can also enable the user to quickly and accurately identify the approximate location of the source of the fluid leak, since the leaked fluid concentrations can be represented by a color scale overlaid on the image of the scene. For example, the user may associate areas of maximum concentration M with the source of the fluid leak. As explained above, when an event (such as a fluid leak) is detected and/or identified by imaging system <b>2000</b> (e.g., when a concentration of a target species exceeds a threshold), the associated imaging system <b>2000</b> can transmit an alert or alarm to the monitoring computer system <b>2001</b> indicating the type and concentration of the species detected. In addition, the imaging system <b>2000</b> can transmit the summary alarm image <b>2034</b> to the monitoring computer system <b>2001</b> so that the user can view a time-averaged summary of the concentration of the species over the time period T monitored by the imaging system <b>2000</b>. The event or alarm can be posted to the event log <b>2003</b> with details about the time, location, type, and concentration of the leak, and/or other information about the event, or any combination of these parameters. Furthermore, the user can access the summary alarm image <b>2034</b> by way of the event log <b>2003</b> (e.g., by way of the “S” button shown in <figref idrefs="DRAWINGS">FIG. 17A</figref>). In various embodiments, the user can access low resolution video feed of the event by way of the event log <b>2003</b> as well (e.g., by way of a triangular “play” button, which is shown in <figref idrefs="DRAWINGS">FIG. 17A</figref>).</div>
<div class="description-paragraph" id="p-0370" num="0412">In some embodiments, one or more imaging systems <b>2000</b> may produce multiple gas detection and/or quantification maps for a particular FOV at the monitored facility over a period of time (e.g., seconds, minutes, hours, days, etc.). These multiple gas detection and/or quantification maps may be generated at typical video frame rates (e.g., 30 frames per second), but maps or images may also be generated at wider time intervals. A processing device, such as an image processor, may analyze the multiple gas detection and/or quantification maps and generate a representative summary alarm image <b>2034</b>. In various embodiments, the processing device may be part of, locally connected to, or located near, the imaging system(s) <b>2000</b> at the facility where gas leak monitoring is being performed. In some embodiments, the local processing device analyzes the multiple gas detection and/or quantification maps to identify a subset of those images (e.g., one image) which are representative of a detected gas leak over the period of time. For example, the local processing device may analyze each of the multiple gas detection and/or quantification maps to identify one or more gas leaks in those images. The local processing device may then select one image (or a small plurality of images) that is representative of the leak. In other embodiments, the summary image <b>2034</b> may be a composite image that includes information from multiple gas detection and/or quantification maps or images detected over multiple frames. For example, as explained above, in such embodiments, the summary alarm image <b>2034</b> may comprise a time average of the multiple gas detection and/or quantification maps. This time average summary image can be generated by the local processing device by averaging the multiple maps on a pixel by pixel basis. This type of summary image <b>2034</b> can be effective at capturing gas leak activity over time, as gas clouds are ever-changing objects such that the blurring that may occur by averaging multiple images over time does not negatively impact the ability to observe useful information about a gas cloud from such an image. Accordingly, this and other types of summary images may be useful to operators for identifying gas leak activity within a monitored field of view over a period of time in a single image. In other embodiments, other statistical and/or image processing techniques can be used to create a composite image. Summary images <b>2034</b> offer a bandwidth- and time-efficient way for operators to quickly assess a site for potential gas leaks. A summary image <b>2034</b> can be transmitted to an operator alone, without the source data (e.g., a video) which was collected in order to generate the summary image. Or if enough data bandwidth exists to transmit the source data, such as a video, then the summary image <b>2034</b> can be used as a cover image for the recorded video it belongs to and can be viewed by clicking the summary button on the related event in the event log <b>2003</b>.</div>
<div class="description-paragraph" id="p-0371" num="0413">Accordingly, the embodiments disclosed herein can provide the user at the monitoring computer system <b>2001</b> with accurate and timely information about events (e.g., fluid leaks) that occur at installations being monitored by associated imaging systems <b>2000</b>. Moreover, the data transmitted to the user at the computer system <b>2001</b> can be rapidly and reliably transmitted over a suitable wireless communications network, e.g., having different speeds, including networks that are relatively low speed and low bandwidth networks, such as wireless cellular networks. As explained above, the user can engage the viewing mode bar <b>2024</b> to allow the user to select a desired viewing mode, e.g., a live-stream video mode, a progressive mode (discussed below), etc. If the user would like to see a live stream of the monitored FOV (whether visible or IR image data) in real-time, the user can select the live-stream mode. In some embodiments, such as those in which a high speed connection (e.g., Ethernet or WiFi) is available, the user may be able to view relatively high resolution and/or high frame rate videos by selecting the live-stream mode. In other embodiments, such as those in which only a low speed connection (e.g., cellular connection) is available, the user may be able to view relatively low resolution and/or low frame rate videos by selecting the live-stream mode.</div>
<div class="description-paragraph" id="p-0372" num="0414">In addition, the user can select the progressive mode to sequentially view summary alarm images <b>2034</b> for successive stored FOVs <b>2022</b>. For example, if there are N FOVs <b>2022</b> for a particular imaging system <b>2000</b>, the progressive mode can progressively present summary alarm images <b>2034</b> for FOV <b>1</b>, FOV <b>2</b>, FOV <b>3</b>, . . . FOV N, so that the user can view events such as fluid leaks at each FOV <b>2022</b> of the imaging system <b>2000</b>. In some embodiments, when the system is in the progressive mode, the system can present summary alarm images <b>2034</b> for a particular FOV <b>2022</b> only when an alarm is triggered for that particular FOV <b>2022</b> during the time period T being monitored. In some embodiments, if no alarm is triggered for the particular FOV <b>2022</b> during the time period T being monitored, then the system can present low resolution video image data (e.g., visual and/or IR image data) of the FOV <b>2022</b> being monitored. In other embodiments, if no alarm is triggered for the particular FOV <b>2022</b>, then no image data is presented and/or the imaging system <b>2000</b> with no alarm may be skipped in the cycle of the progressive mode.</div>
<div class="description-paragraph" id="p-0373" num="0415">Thus, as one example, during a cycle of the progressive mode, the imaging system <b>2000</b> being monitored detects a fluid leak at FOV <b>1</b> and FOV <b>3</b>, but does not detect any leaks or other events at FOV <b>2</b> and FOV <b>4</b>. In some embodiments utilizing the progressive mode, the cycle can illustrate a low resolution video (e.g., visible and/or IR image data) of FOV <b>1</b> for a first period followed by the summary alert image <b>2034</b> that illustrates the time averaged leak data for a second period (or vice versa). The progressive mode can alternately illustrate low resolution video of FOV <b>1</b> and the summary alert image <b>2034</b> of FOV <b>1</b> for the time period T that FOV <b>1</b> is being monitored. In other embodiments, the system can illustrate only the summary alert image(s) <b>2034</b> of FOV <b>1</b> during the time period T.</div>
<div class="description-paragraph" id="p-0374" num="0416">When the time period T for monitoring FOV <b>1</b> is complete, the progressive mode can progress to FOV <b>2</b>. Since no alarm is indicated for FOV <b>2</b>, the system can present low resolution video streams (e.g., visible and/or IR image data) for the time period T being monitored. When the time period T for monitoring FOV <b>2</b> is complete, the progressive mode can progress to FOV <b>3</b>. Because the system <b>2000</b> detects a fluid leak at FOV <b>3</b> during the time period T, the progressive mode can alternately present low resolution video of the FOV <b>3</b> with the summary alert image <b>2034</b> of the FOV <b>3</b> over the time period T, as explained above. When the monitoring period for FOV <b>3</b> is complete, the system can present low resolution video of FOV <b>4</b> for the time period T, since no alarm is indicated at FOV <b>4</b> during this period.</div>
<div class="description-paragraph" id="p-0375" num="0417">Thus, the progressive scanning mode of the systems disclosed herein can beneficially present the user with an accurate and rapid identification of events that occur during the time period T being monitored by a particular imaging system <b>2000</b>. The use of summary alarm images <b>2034</b> for presenting event data to the user can enable the use of low speed communications networks to obtain rich data from even remote petrochemical installations or facilities.</div>
<div class="description-paragraph" id="p-0376" num="0418">Moreover, in some embodiments, different events can be grouped together based on event type (e.g., gas or liquid leak, intrusion, etc.), type of detected gas or liquid, event time, message, and/or FOV (e.g. which imaging system <b>2000</b> detected the event, and/or positions within a scan of the imaging system <b>2000</b>). The interface can present several algorithm options that can be used to determine the path direction of the fluid leak and/or wind direction at the time of the event. Events can be added to a particular group of events if, for example, a time difference (delta T) between a current event and a most recent event within the group does not exceed a predetermined time limit (e.g., 5 minutes, and/or can be based on the FOVs in path and visit times). In another embodiment, a single event can be shown to generate a single, but longer than default, video clip that does not exceed the video maximum length. In some embodiments, multiple grouped events can comprise parent and child events, which can be caused by or follow after parent events. For example, for multiple generated grouped events, information about the parent event can be replaced with the most recent event in the group (including time stamp). The listed parent event can show a number of child events that occurred after or because of the parent event. In some embodiments, only the parent event can be displayed on the event panel and a “show all child events” button can be engaged to show the depending child events. When the user acknowledges the parent event or a grouped single event (e.g., changes the event status by pressing the “event status” button), all child events in this group may be automatically checked as acknowledged.</div>
<div class="description-paragraph" id="p-0377" num="0419">Thus, in various embodiments, the processing electronics can be configured to associate multiple events with one another and to form a group of the associated multiple events. The processing electronics can be configured to form the group of the associated multiple events based at least in part on at least one of event type, type of the detected one or more target species, event time, and a field of view (FOV) in which the one or more target species has been detected. For example, in various embodiments, if a particular imaging system <b>2000</b> detects a fluid leak in multiple FOVs, then the processing electronics can associate the detections identified in the multiple FOVs in a grouping of events. Additionally, if an imaging system <b>2000</b> detects a fluid leak during the same time period, the processing electronics may similarly group those detections into a grouping of events. Similarly, if an imaging system <b>2000</b> detects a fluid leak at different times that are grouped closely together, the processing electronics may similarly group those detections into a grouping of events. Further, if other types of events occur at similar times or locations, the processing electronics can likewise groups those events. For example, if a collection of systems <b>2000</b> are offline during the same time period, the processing electronics can determine that the systems <b>2000</b> are offline due to a related event (e.g., a power outage, etc.). If multiple events are grouped together by the electronics, an event (e.g., the first event detected) may be assigned as the parent event, and the other grouped events (e.g., subsequent event(s)) may be assigned as child events. The events may be grouped for a predetermined time period in some embodiments.</div>
<div class="description-paragraph" id="p-0378" num="0420">Beneficially, the grouping of events described herein can assist in reducing the occurrence of redundant or otherwise less relevant alarms. For example, if a single fluid leak is detected over multiple FOVs, and an alarm is generated after each detection of the single fluid leak, then the multiple alarms may bombard the user with redundant information and/or create confusion, a distraction, or nuisance if multiple alarms are indicated at once or repeatedly over short intervals. Grouping the single fluid leak into a single event (and, for example, a single alarm or alert) can advantageously simplify the monitoring process for the user, for example, by reducing extraneous alarms.</div>
<div class="description-paragraph" id="p-0379" num="0421">The imaging systems <b>2000</b> disclosed herein can detect fluid leaks that include gas leaks (e.g., methane leaks, hydrogen sulfide leaks, etc.) and/or liquid leaks (e.g., crude oil leaks). Gas leaks occur very rapidly, since the gases can freely escape into the atmosphere at a high rate. For tracking the progression of a gas leak, the imaging systems <b>2000</b> disclosed herein can overlay a color map onto the imaged FOV <b>2022</b> that relates the color map to an estimated concentration of the leaked gas (see, e.g., the summary image <b>2034</b> of <figref idrefs="DRAWINGS">FIG. 16C</figref>). Utilizing color maps to track gas leaks based on concentration is an effective way of monitoring the progression and/or identifying the source of the gas leak, because gases leak on relatively fast time scales.</div>
<div class="description-paragraph" id="p-0380" num="0422">By contrast, for liquid leaks (such as oil leaks), the liquid may seep into the ground or other surface at relatively low rates, e.g., over the periods of hours, days, or weeks. Due to the low rate of progression of liquid leaks, it can be challenging to monitor the progression of a liquid leak and/or to identify the source of the liquid leak. Accordingly, various embodiments disclosed herein enable the user to track the liquid leak over relatively long time periods and to identify the source of the leak utilizing the systems <b>2000</b> disclosed herein.</div>
<div class="description-paragraph" id="p-0381" num="0423">Turning to <figref idrefs="DRAWINGS">FIG. 16D</figref>, a time lapsed leak progression image <b>2036</b> that shows the progression of a liquid leak over a time period is illustrated, according to various embodiments. In <figref idrefs="DRAWINGS">FIG. 16D</figref> a color map of a liquid leak <b>2039</b> is overlaid on an image (e.g., a visible or IR image, in this case a visible image) of the FOV <b>2022</b> that is monitored. However, instead of the color map representing an estimated concentration of the target species (such as for gas leaks), the progression of the liquid leak <b>2039</b> of <figref idrefs="DRAWINGS">FIG. 16D</figref> is mapped based on an amount of time that the liquid leak <b>2039</b> is present at a particular location within the FOV <b>2022</b>. For example, as shown in <figref idrefs="DRAWINGS">FIG. 16D</figref>, the liquid leak <b>2039</b> can comprise a first region <b>2037</b> (e.g., ring or contour) that indicates a long residence time of the liquid leak <b>2039</b> and a second region <b>2038</b> (e.g., ring or contour) that indicates a short residence time of the liquid leak <b>2039</b>. Since the first region <b>2037</b> has a relatively long residence time in which the liquid (e.g., oil) has been present at that particular location, the user may infer that the source of the liquid leak is at or near the first region <b>2037</b>. Because the second region <b>2038</b> has a relatively short residence time in which the liquid (e.g., oil) has been present at the second region <b>2038</b>, the user may infer that the liquid leak has only recently progressed to the second region <b>2038</b>. Although two regions <b>2037</b>, <b>2038</b> are shown in <figref idrefs="DRAWINGS">FIG. 16D</figref>, it should be appreciated that the system <b>2000</b> can identify any suitable number of regions for tracking the progression of the liquid leak <b>2039</b>. Further in some embodiments, the system <b>2000</b> can automatically detect and/or indicate the estimated time periods at which the liquid leak <b>2039</b> has been present at each location (e.g., at the regions <b>2037</b>, <b>2038</b>). For example, the system <b>2000</b> can indicate that the liquid leak <b>2039</b> has been present in the first region <b>2037</b> for a first time period (e.g., 3 days, 2 hours, 33 minutes), and that the liquid leak <b>2039</b> has been present in the second region <b>2038</b> for a second, shorter time period (e.g., 2 hours, 15 minutes).</div>
<div class="description-paragraph" id="p-0382" num="0424">Accordingly, the embodiment of <figref idrefs="DRAWINGS">FIG. 16D</figref> can advantageously provide the user with rich information, in a relatively small image size, about the progression of a liquid leak <b>2039</b> over a long time period (e.g., on the order of hours, days, weeks, etc.). The color coded map of <figref idrefs="DRAWINGS">FIG. 16D</figref> can be efficiently and rapidly communicated over low speed communications networks and can enable the user to identify the source of the liquid leak, as well as its progression over time. For example, by illustrating leak residence time based on color-coded rings or contours, the user can easily determine at a glance the location of the source of the leak and/or the general spatial extent of the leak over time in a single image.</div>
<div class="description-paragraph" id="p-0383" num="0425"> <figref idrefs="DRAWINGS">FIG. 17A</figref> is a schematic diagram of the events log <b>2003</b>, according to various embodiments. <figref idrefs="DRAWINGS">FIG. 17B</figref> is a schematic diagram of an event guide <b>2040</b>, according to various embodiments. As explained above, the event log <b>2003</b> can be presented on the user interface that is rendered to the user at the central monitoring computer system <b>2001</b> (see also <figref idrefs="DRAWINGS">FIG. 16A</figref>). The event guide <b>2040</b> can provide a summary of possible event types and information about those event types. As shown in <figref idrefs="DRAWINGS">FIG. 17A</figref>, the event log <b>2003</b> can present a list of events detected by the imaging systems <b>2000</b> being monitored by the computer system <b>2001</b>. In some embodiments, the events can be automatically prioritized based on the urgency of the event. For example, the system can automatically determine that events such as fluid leaks have higher priority than other types of events, such as a high disk use notification, ping failure, or the other events listed in <figref idrefs="DRAWINGS">FIG. 17B</figref>. Thus in some embodiments, processing electronics (e.g., of the imaging system <b>2000</b>) can be configured to analyze the events log <b>2003</b>, and based on the analysis, to transmit a priority ranking of events to the computing system <b>2001</b>.</div>
<div class="description-paragraph" id="p-0384" num="0426">The event log <b>2003</b> can be sorted based on filters <b>2041</b> selected by the user. For example, the user can sort the events based on those starred by the user, based on priority of the event (which may be automatically generated by the system), based on timing of the event, based on which system <b>2000</b> detected the event, based on FOV <b>2022</b> of the event, or based on any other suitable filter or parameter. The events log <b>2003</b> can include links to the summary alarm image <b>2034</b> associated with the event (and or the progression image <b>2036</b> of a liquid leak) and/or links to video image data of the event, to provide the user with a convenient interface for accessing information about the events.</div>
<div class="description-paragraph" id="p-0385" num="0427">As shown in <figref idrefs="DRAWINGS">FIG. 17B</figref>, high priority events (such as a fluid leak) can be indicated with a high priority icon, medium priority events (such as disk failure) can be indicated with a medium priority icon, low priority events (such as high disk use) can be indicated with a low priority icon, a detection event can be indicated with a detection icon, and other events (such as threshold changes or mode changes) can be indicated with other types of icons or no icons. Beneficially, the priorities can also be illustrated to the user based on color-codes. For example, icons associated with high priority events can be red, icons associated with medium priority events can be orange, icons associated with low priority events can be yellow, detection events can be gray, and other events can be black. Any suitable color codes can be used, however, for the priorities. Further, as shown in <figref idrefs="DRAWINGS">FIG. 17B</figref>, the system <b>2000</b> can provide a brief description as well as a full message associated with the event.</div>
<div class="description-paragraph" id="p-0386" num="0428">In various embodiments disclosed herein, events (e.g., gas leaks, intrusion detection) can automatically generate a pop-up notification and/or sound that notifies the user of the event. Furthermore, the embodiments disclosed herein can be employed with other devices and sensors, such as other optical cameras, point sensors, temperature sensors, motion sensors, etc. Data from the other devices can be transmitted to and/or from the user computing systems over the communications networks disclosed herein.</div>
<div class="description-paragraph" id="p-0387" num="0429">References throughout this specification to “one embodiment,” “an embodiment,” “a related embodiment,” or similar language mean that a particular feature, structure, or characteristic described in connection with the referred to “embodiment” is included in at least one embodiment of the present invention. Thus, appearances of the phrases “in one embodiment,” “in an embodiment,” and similar language throughout this specification may, but do not necessarily, all refer to the same embodiment. It is to be understood that no portion of disclosure, taken on its own and in possible connection with a figure, is intended to provide a complete description of all features of the invention.</div>
<div class="description-paragraph" id="p-0388" num="0430">In the drawings like numbers are used to represent the same or similar elements wherever possible. The depicted structural elements are generally not to scale, and certain components are enlarged relative to the other components for purposes of emphasis and understanding. It is to be understood that no single drawing is intended to support a complete description of all features of the invention. In other words, a given drawing is generally descriptive of only some, and generally not all, features of the invention. A given drawing and an associated portion of the disclosure containing a description referencing such drawing do not, generally, contain all elements of a particular view or all features that can be presented is this view, for purposes of simplifying the given drawing and discussion, and to direct the discussion to particular elements that are featured in this drawing. A skilled artisan will recognize that the invention may possibly be practiced without one or more of the specific features, elements, components, structures, details, or characteristics, or with the use of other methods, components, materials, and so forth. Therefore, although a particular detail of an embodiment of the invention may not be necessarily shown in each and every drawing describing such embodiment, the presence of this detail in the drawing may be implied unless the context of the description requires otherwise. In other instances, well known structures, details, materials, or operations may be not shown in a given drawing or described in detail to avoid obscuring aspects of an embodiment of the invention that are being discussed. Furthermore, the described single features, structures, or characteristics of the invention may be combined in any suitable manner in one or more further embodiments.</div>
<div class="description-paragraph" id="p-0389" num="0431">Moreover, if the schematic flow chart diagram is included, it is generally set forth as a logical flow-chart diagram. As such, the depicted order and labeled steps of the logical flow are indicative of one embodiment of the presented method. Other steps and methods may be conceived that are equivalent in function, logic, or effect to one or more steps, or portions thereof, of the illustrated method. Additionally, the format and symbols employed are provided to explain the logical steps of the method and are understood not to limit the scope of the method. Although various arrow types and line types may be employed in the flow-chart diagrams, they are understood not to limit the scope of the corresponding method. Indeed, some arrows or other connectors may be used to indicate only the logical flow of the method. For instance, an arrow may indicate a waiting or monitoring period of unspecified duration between enumerated steps of the depicted method. Without loss of generality, the order in which processing steps or particular methods occur may or may not strictly adhere to the order of the corresponding steps shown.</div>
<div class="description-paragraph" id="p-0390" num="0432">The features recited in claims appended to this disclosure are intended to be assessed in light of the disclosure as a whole.</div>
<div class="description-paragraph" id="p-0391" num="0433">At least some elements of a device of the invention can be controlled—and at least some steps of a method of the invention can be effectuated, in operation—with a programmable processor governed by instructions stored in a memory. The memory may be random access memory (RAM), read-only memory (ROM), flash memory or any other memory, or combination thereof, suitable for storing control software or other instructions and data. Those skilled in the art should also readily appreciate that instructions or programs defining the functions of the present invention may be delivered to a processor in many forms, including, but not limited to, information permanently stored on non-writable storage media (e.g. read-only memory devices within a computer, such as ROM, or devices readable by a computer I/O attachment, such as CD-ROM or DVD disks), information alterably stored on writable storage media (e.g. floppy disks, removable flash memory and hard drives) or information conveyed to a computer through communication media, including wired or wireless computer networks. In addition, while the invention may be embodied in software, the functions necessary to implement the invention may optionally or alternatively be embodied in part or in whole using firmware and/or hardware components, such as combinatorial logic, Application Specific Integrated Circuits (ASICs), Field-Programmable Gate Arrays (FPGAs) or other hardware or some combination of hardware, software and/or firmware components.</div>
<div class="description-paragraph" id="p-0392" num="0434">While examples of embodiments of the system and method of the invention have been discussed in reference to the gas-cloud detection, monitoring, and quantification (including but not limited to greenhouse gases such as Carbon Dioxide, Carbon Monoxide, Nitrogen Oxide as well as hydrocarbon gases such as Methane, Ethane, Propane, n-Butane, iso-Butane, n-Pentane, iso-Pentane, neo-Pentane, Hydrogen Sulfide, Sulfur Hexafluoride, Ammonia, Benzene, p- and m-Xylene, Vinyl chloride, Toluene, Propylene oxide, Propylene, Methanol, Hydrazine, Ethanol, 1,2-dichloroethane, 1,1-dichloroethane, Dichlorobenzene, Chlorobenzene, to name just a few), embodiments of the invention can be readily adapted for other chemical detection applications. For example, detection of liquid and solid chemical spills, biological weapons, tracking targets based on their chemical composition, identification of satellites and space debris, ophthalmological imaging, microscopy and cellular imaging, endoscopy, mold detection, fire and flame detection, and pesticide detection are within the scope of the invention.</div>
<div class="description-paragraph" id="p-0393" num="0435">As used herein, a phrase referring to “at least one of” a list of items refers to any combination of those items, including single members. As an example, “at least one of: a, b, or c” is intended to cover: a, b, c, a-b, a-c, b-c, and a-b-c.</div>
<div class="description-paragraph" id="p-0394" num="0436">If implemented in software, the functions may be stored on or transmitted over as one or more instructions or code on a computer-readable medium. The steps of a method or algorithm disclosed herein may be implemented in a processor-executable software module which may reside on a computer-readable medium. Computer-readable media includes both computer storage media and communication media including any medium that can be enabled to transfer a computer program from one place to another. A storage media may be any available media that may be accessed by a computer. By way of example, and not limitation, such computer-readable media may include RAM, ROM, EEPROM, CD-ROM or other optical disk storage, magnetic disk storage or other magnetic storage devices, or any other medium that may be used to store desired program code in the form of instructions or data structures and that may be accessed by a computer. Also, any connection can be properly termed a computer-readable medium. Disk and disc, as used herein, includes compact disc (CD), laser disc, optical disc, digital versatile disc (DVD), floppy disk, and blu-ray disc where disks usually reproduce data magnetically, while discs reproduce data optically with lasers. Combinations of the above also may be included within the scope of computer-readable media. Additionally, the operations of a method or algorithm may reside as one or any combination or set of codes and instructions on a machine readable medium and computer-readable medium, which may be incorporated into a computer program product.</div>
<div class="description-paragraph" id="p-0395" num="0437">Various modifications to the implementations described in this disclosure may be readily apparent to those skilled in the art, and the generic principles defined herein may be applied to other implementations without departing from the spirit or scope of this disclosure. Thus, the claims are not intended to be limited to the implementations shown herein, but are to be accorded the widest scope consistent with this disclosure, the principles and the novel features disclosed herein.</div>
<div class="description-paragraph" id="p-0396" num="0438">Certain features that are described in this specification in the context of separate implementations also can be implemented in combination in a single implementation. Conversely, various features that are described in the context of a single implementation also can be implemented in multiple implementations separately or in any suitable subcombination. Moreover, although features may be described above as acting in certain combinations and even initially claimed as such, one or more features from a claimed combination can in some cases be excised from the combination, and the claimed combination may be directed to a subcombination or variation of a subcombination.</div>
</div>
</div>
</section><section itemprop="claims" itemscope="">
<h2>Claims (<span itemprop="count">15</span>)</h2>
<div html="" itemprop="content"><div class="claims" lang="EN" load-source="patent-office" mxw-id="PCLM284183950">
<claim-statement>What is claimed is:</claim-statement>
<div class="claim"> <div class="claim" id="CLM-00001" num="00001">
<div class="claim-text">1. A system for monitoring an installation, the system comprising:
<div class="claim-text">an optical imaging system comprising an array of optical detectors, the array of optical detectors comprising an infrared (IR) detector array; and</div>
<div class="claim-text">processing electronics configured to process image data detected by the optical imaging system, the processing electronics configured to:
<div class="claim-text">detect a target species based at least in part on the processed image data; and</div>
<div class="claim-text">based on a detected amount of the target species, transmit an alarm notification to an external computing device over a communications network indicating that the target species has been detected at the installation,</div>
<div class="claim-text">wherein the processing electronics are configured to detect the target species over multiple frames of the image data and to combine the multiple frames of image data into a summary alarm image that presents the detection of the target species over a period of time,</div>
<div class="claim-text">wherein the optical imaging system defines a plurality of optical channels being spatially and spectrally different from one another, each of the plurality of optical channels positioned to transfer radiation incident on the optical imaging system towards the array of optical detectors,</div>
<div class="claim-text">wherein the processing electronics are configured to generate a progressive mode to sequentially present summary alarm images for successive stored fields of view (FOVs).</div>
</div>
</div>
</div>
</div> <div class="claim-dependent"> <div class="claim" id="CLM-00002" num="00002">
<div class="claim-text">2. The system of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the summary alarm image comprises a single image.</div>
</div>
</div> <div class="claim-dependent"> <div class="claim" id="CLM-00003" num="00003">
<div class="claim-text">3. The system of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the processing electronics are configured to create the summary alarm image by calculating an average concentration of the target species and/or an average of the image data representative of the concentration over the period of time.</div>
</div>
</div> <div class="claim-dependent"> <div class="claim" id="CLM-00004" num="00004">
<div class="claim-text">4. The system of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the processing electronics are configured to create an events log comprising a plurality of events comprising one or more target species detected by the processing electronics.</div>
</div>
</div> <div class="claim-dependent"> <div class="claim" id="CLM-00005" num="00005">
<div class="claim-text">5. The system of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the processing electronics are configured to compare the detected amount of the target species to a threshold amount and, based on that comparison, transmit the alarm notification to the external computing device over the communications network indicating that the target species has been detected at the installation.</div>
</div>
</div> <div class="claim"> <div class="claim" id="CLM-00006" num="00006">
<div class="claim-text">6. A system for monitoring an installation, the system comprising:
<div class="claim-text">an optical imaging system comprising an array of optical detectors; and</div>
<div class="claim-text">processing electronics configured to process image data detected by the optical imaging system, the processing electronics configured to:
<div class="claim-text">detect a target species based at least in part on the processed image data over multiple frames of the processed image data;</div>
<div class="claim-text">combine the multiple frames of processed image data into a summary alarm image that presents the detection of the target species over a period of time; and</div>
<div class="claim-text">generate a progressive mode to sequentially present summary alarm images for successive fields of view (FOVs) of the optical imaging system.</div>
</div>
</div>
</div>
</div> <div class="claim-dependent"> <div class="claim" id="CLM-00007" num="00007">
<div class="claim-text">7. The system of <claim-ref idref="CLM-00006">claim 6</claim-ref>, wherein the summary alarm image comprises a single image.</div>
</div>
</div> <div class="claim-dependent"> <div class="claim" id="CLM-00008" num="00008">
<div class="claim-text">8. The system of <claim-ref idref="CLM-00006">claim 6</claim-ref>, wherein the processing electronics are configured to create the summary alarm image by calculating an average concentration of the target species and/or an average of the image data representative of the concentration over the period of time.</div>
</div>
</div> <div class="claim-dependent"> <div class="claim" id="CLM-00009" num="00009">
<div class="claim-text">9. The system of <claim-ref idref="CLM-00006">claim 6</claim-ref>, wherein the processing electronics are configured to create an events log comprising a plurality of events comprising one or mote target species detected by the processing electronics.</div>
</div>
</div> <div class="claim-dependent"> <div class="claim" id="CLM-00010" num="00010">
<div class="claim-text">10. The system of <claim-ref idref="CLM-00009">claim 9</claim-ref>, wherein the processing electronics are configured to analyze the events log, and based on the analysis, to generate a priority ranking of events.</div>
</div>
</div> <div class="claim-dependent"> <div class="claim" id="CLM-00011" num="00011">
<div class="claim-text">11. The system of <claim-ref idref="CLM-00006">claim 6</claim-ref>, wherein the processing electronics are configured to generate a mosaic image comprising a plurality of fields of view (FOVs) of the optical imaging system at the installation.</div>
</div>
</div> <div class="claim-dependent"> <div class="claim" id="CLM-00012" num="00012">
<div class="claim-text">12. The system of <claim-ref idref="CLM-00006">claim 6</claim-ref>, wherein the processing electronics are configured to monitor a progression of a liquid leak over a period of time.</div>
</div>
</div> <div class="claim-dependent"> <div class="claim" id="CLM-00013" num="00013">
<div class="claim-text">13. The system of <claim-ref idref="CLM-00006">claim 6</claim-ref>, wherein at least a portion of the processing electronics are located remote from the optical imaging system.</div>
</div>
</div> <div class="claim-dependent"> <div class="claim" id="CLM-00014" num="00014">
<div class="claim-text">14. The system of <claim-ref idref="CLM-00006">claim 6</claim-ref>, wherein the processing electronics are configured to generate a system overview image that illustrates locations of a plurality of optical imaging systems at the installation, each optical imaging system of the plurality of optical imaging systems associated with an identifier.</div>
</div>
</div> <div class="claim-dependent"> <div class="claim" id="CLM-00015" num="00015">
<div class="claim-text">15. The system of <claim-ref idref="CLM-00006">claim 6</claim-ref>, wherein the processing electronics are configured to generate a multi-view image that illustrates image data captured by multiple optical imaging systems at multiple sites of one or a plurality of installations.</div>
</div>
</div> </div>
</div>
</section>
                </article>
            </search-app>
        </body>
    </html>
    