
    <html>
        <body>
            <search-app>
                <article class="result" itemscope="" itemtype="http://schema.org/ScholarlyArticle">
    <h1 itemprop="pageTitle">US10989521B2 - Apparatus and methods for distance estimation using multiple image sensors 
        - Google Patents</h1><section itemprop="abstract" itemscope="">
<h2>Abstract</h2>
<div html="" itemprop="content"><abstract lang="EN" load-source="patent-office" mxw-id="PA444051487">
<div class="abstract" id="p-0001" num="0000">Data streams from multiple image sensors may be combined in order to form, for example, an interleaved video stream, which can be used to determine distance to an object. The video stream may be encoded using a motion estimation encoder. Output of the video encoder may be processed (e.g., parsed) in order to extract motion information present in the encoded video. The motion information may be utilized in order to determine a depth of visual scene, such as by using binocular disparity between two or more images by an adaptive controller in order to detect one or more objects salient to a given task. In one variant, depth information is utilized during control and operation of mobile robotic devices.</div>
</abstract>
</div>
</section><section itemprop="description" itemscope="">
<h2>Description</h2>
<div html="" itemprop="content"><div class="description" lang="EN" load-source="patent-office" mxw-id="PDES290889279">
<heading id="h-0001">CROSS-REFERENCE AND RELATED APPLICATIONS</heading>
<div class="description-paragraph" id="p-0002" num="0001">This application is a continuation of U.S. patent application Ser. No. 15/948,885 filed Apr. 9, 2018, which is related to co-pending and co-owned U.S. patent application Ser. No. 14/285,385, entitled “APPARATUS AND METHODS FOR REAL TIME ESTIMATION OF DIFFERENTIAL MOTION IN LIVE VIDEO”, filed on May 22, 2014, and co-owned U.S. patent application Ser. No. 14/285,466, entitled “APPARATUS AND METHODS FOR ROBOTIC OPERATION USING VIDEO IMAGERY”, filed on May 22, 2014, now U.S. Pat. No. 9,713,982, each of the foregoing incorporated herein by reference in its entirety.</div>
<heading id="h-0002">COPYRIGHT</heading>
<div class="description-paragraph" id="p-0003" num="0002">A portion of the disclosure of this patent document contains material that is subject to copyright protection. The copyright owner has no objection to the facsimile reproduction by anyone of the patent document or the patent disclosure, as it appears in the Patent and Trademark Office patent files or records, but otherwise reserves all copyright rights whatsoever.</div>
<heading id="h-0003">BACKGROUND</heading>
<heading id="h-0004">Field of the Disclosure</heading>
<div class="description-paragraph" id="p-0004" num="0003">The present disclosure relates to, inter alia, computerized apparatus and methods for processing imagery from multiple sources.</div>
<heading id="h-0005">Description of Related Art</heading>
<div class="description-paragraph" id="p-0005" num="0004">Object recognition in the context of computer vision relates to finding a given object in an image or a sequence of frames in a video segment. Typically, video frames may contain multiple objects moving in one or more directions on a still or moving background. Object representations, also referred to as the “view”, may change from frame to frame due to a variety of object transformations, such as rotation, movement, translation, change in lighting, background, noise, appearance of other objects, partial blocking and/or unblocking of the object, and/or other object transformations. Robotic devices often employ video for navigation, target selection and/or obstacle avoidance. Determining motion of object from a moving robotic platform may require implementation of differential motion detection in an energy efficient manner. Depth of visual scene (e.g., distance to one or more objects) may be useful for operation of mobile robots as well.</div>
<heading id="h-0006">SUMMARY</heading>
<div class="description-paragraph" id="p-0006" num="0005">One aspect of the disclosure relates to a non-transitory computer-readable storage medium having instructions embodied thereon, the instructions being executable to perform a method of determining a distance to an object.</div>
<div class="description-paragraph" id="p-0007" num="0006">In another aspect, a method of determining distance to an object is disclosed. In one implementation, the object is disposed within a visual scene, and the method includes: producing a video stream by interleaving images of a first plurality of images and a second plurality of images of the visual scene; and evaluating the video stream to determine the distance. In one variant, individual images of the first and second pluralities of images are provided by first and second cameras, respectively, the second camera being separated spatially from the first camera.</div>
<div class="description-paragraph" id="p-0008" num="0007">In another variant, the evaluation comprises determination of a binocular disparity between at least a portion of the second and the first pluralities of images, the disparity being related to the distance and the spatial separation; and the distance determination is based on the disparity determination.</div>
<div class="description-paragraph" id="p-0009" num="0008">In a further variant, the evaluation further comprises encoding the video stream using an encoder process comprising motion estimation, the motion estimation configured to provide information related to a displacement of a first representation of the object within a given image of the video stream relative a second representation of the object within a preceding image of the video stream. Individual ones of the first and second pluralities of images comprise for instance a plurality of pixels, and the encoder process includes one of e.g., MPEG-4, H.262, H.263, H.264, and H.265 encoders.</div>
<div class="description-paragraph" id="p-0010" num="0009">In another aspect, a non-transitory computer-readable storage medium having instructions embodied thereon is disclosed. In one implementation, the instructions are executable to produce a combined image stream from first and second sequences of images of a sensory scene by at least: selecting a first image and a second image from the second sequence to follow a first image from the first sequence, the second image from the second sequence following the first image from the second sequence; selecting second and third images from the first sequence to follow the second image from the second sequence, the third image from the first sequence following the second from the first sequence; and evaluating the combined image stream to determine a depth parameter of the scene.</div>
<div class="description-paragraph" id="p-0011" num="0010">In one variant, the first and the second image sequences are provided by a first and a second image sensor, respectively, the first image sensor being disposed spatially separated from the second image sensor. The first image sensor and the second image sensor are configured to provide images of the sensory scene, and the spatial separation is configured to produce a binocular disparity for the first image from the second sequence relative the first image from the first sequence; and the second image from the second sequence relative the second image from the first sequence. The depth is determined based on the disparity.</div>
<div class="description-paragraph" id="p-0012" num="0011">In another variant, individual images of the first image sequence and the second image sequence comprise a plurality of pixels; and the evaluating comprises encoding the combined stream using a motion estimation encoder.</div>
<div class="description-paragraph" id="p-0013" num="0012">In yet another variant, the combined image stream comprises the first image from the first sequence, followed by the first image from the second sequence followed by the second image from the second sequence followed by the second image from the first sequence followed by the third image from the first sequence. The motion estimation encoder is configured to, in one particular implementation: determine a first version of the disparity based on encoding the first image from the first sequence and the first image from the second sequence; determine a second version of the disparity based on encoding the second image from the second sequence and the second image from the first sequence, the second version of the disparity having an opposite sign relative the first version; determine a first motion of the second image sequence based on encoding the first image from the second sequence followed by the second image from the second sequence; and determine a second motion of the first image sequence based on encoding the second image from the first sequence followed by the third image from the first sequence.</div>
<div class="description-paragraph" id="p-0014" num="0013">In a further aspect, an image processing apparatus is disclosed. In one implementation, the apparatus includes: an input interface configured to receive a stereo image of a visual scene, the stereo image comprising a first frame and a second frame; a logic component configured to form a sequence of frames by arranging the first and the second frames sequentially with one another within the sequence; a video encoder component in data communication with the logic component and configured to encode the sequence of frames to produce a sequence of compressed frames; and a processing component in data communication with the video encoder and configured to obtain motion information based on an evaluation of the compressed frames.</div>
<div class="description-paragraph" id="p-0015" num="0014">In one variant, the sequence of compressed frames comprises a key frame characterized by absence of the motion information, and the processing component is configured to not utilize the key frame during the evaluation of the compressed frames.</div>
<div class="description-paragraph" id="p-0016" num="0015">In another variant, the processing component is further configured to determine, based on the motion information, a depth parameter associated with the visual scene. The input interface is further configured to receive another image comprising a third frame, the first, second and third frames being provided respectively by first, second, and third cameras disposed spatially separately from one another. The sequence of frames is formed by arranging the third frame sequentially with the first frame and the second frame.</div>
<div class="description-paragraph" id="p-0017" num="0016">These and other objects, features, and characteristics of the system and/or method disclosed herein, as well as the methods of operation and functions of the related elements of structure and the combination of parts and economies of manufacture, will become more apparent upon consideration of the following description and the appended claims with reference to the accompanying drawings, all of which form a part of this specification, wherein like reference numerals designate corresponding parts in the various figures. It is to be expressly understood, however, that the drawings are for the purpose of illustration and description only and are not intended as a definition of the limits of the disclosure. As used in the specification and in the claims, the singular form of “a”, “an”, and “the” include plural referents unless the context clearly dictates otherwise.</div>
<description-of-drawings>
<heading id="h-0007">BRIEF DESCRIPTION OF THE DRAWINGS</heading>
<div class="description-paragraph" id="p-0018" num="0017"> <figref idrefs="DRAWINGS">FIG. 1A</figref> is a graphical illustration depicting a top view of robotic apparatus configured to acquire stereo imagery, in accordance with one or more implementations.</div>
<div class="description-paragraph" id="p-0019" num="0018"> <figref idrefs="DRAWINGS">FIG. 1B</figref> is a graphical illustration depicting a side view of a robotic apparatus comprising an adaptive controller apparatus of the disclosure, configured for autonomous navigation, in accordance with one or more implementations.</div>
<div class="description-paragraph" id="p-0020" num="0019"> <figref idrefs="DRAWINGS">FIG. 2A</figref> is a graphical illustration depicting stereo imagery input obtained with two spatially displaced cameras for use with the disparity determination methodology, according to one or more implementations.</div>
<div class="description-paragraph" id="p-0021" num="0020"> <figref idrefs="DRAWINGS">FIG. 2B</figref> is a graphical illustration depicting disparity between representations of objects corresponding to the frames of stereo imagery shown in <figref idrefs="DRAWINGS">FIG. 2A</figref>, according to one or more implementations.</div>
<div class="description-paragraph" id="p-0022" num="0021"> <figref idrefs="DRAWINGS">FIG. 2C</figref> is a graphical illustration depicting input frames comprising a plurality of moving objects for use with the motion extraction, according to one or more implementations.</div>
<div class="description-paragraph" id="p-0023" num="0022"> <figref idrefs="DRAWINGS">FIG. 3A</figref> is a logical block diagram depicting a determination of an input stream for motion processing using an alternating interleaver of stereo imagery input, according to one or more implementations.</div>
<div class="description-paragraph" id="p-0024" num="0023"> <figref idrefs="DRAWINGS">FIG. 3B</figref> is a logical block diagram depicting a determination of an input stream for motion processing using an alternating interleaver of stereo imagery input, according to one or more implementations.</div>
<div class="description-paragraph" id="p-0025" num="0024"> <figref idrefs="DRAWINGS">FIG. 4A</figref> is a functional block diagram depicting a processing apparatus configured to determine disparity from a dual image source, according to one or more implementations.</div>
<div class="description-paragraph" id="p-0026" num="0025"> <figref idrefs="DRAWINGS">FIG. 4B</figref> is a functional block diagram depicting a processing apparatus configured to determine disparity from multiple image sources, according to one or more implementations.</div>
<div class="description-paragraph" id="p-0027" num="0026"> <figref idrefs="DRAWINGS">FIG. 5A</figref> is a graphical illustration depicting triple camera configuration used for disparity determination using image interleaving, according to one or more implementations.</div>
<div class="description-paragraph" id="p-0028" num="0027"> <figref idrefs="DRAWINGS">FIG. 5B</figref> is a graphical illustration depicting quad camera configuration used for disparity determination using image interleaving, according to one or more implementations.</div>
<div class="description-paragraph" id="p-0029" num="0028"> <figref idrefs="DRAWINGS">FIG. 5C</figref> is a graphical illustration depicting linear multiple camera configuration useful for determining multiple depths scales using image interleaving, according to one or more implementations.</div>
<div class="description-paragraph" id="p-0030" num="0029"> <figref idrefs="DRAWINGS">FIG. 6A</figref> is a graphical illustration depicting an alternating interleaving of triple image input for use with the motion extraction, according to one or more implementations.</div>
<div class="description-paragraph" id="p-0031" num="0030"> <figref idrefs="DRAWINGS">FIG. 6B</figref> is a graphical illustration depicting an alternating interleaving of quad image input for use with the motion extraction, according to one or more implementations.</div>
<div class="description-paragraph" id="p-0032" num="0031"> <figref idrefs="DRAWINGS">FIG. 7</figref> is a functional block diagram depicting a motion extraction apparatus, according to one or more implementations.</div>
<div class="description-paragraph" id="p-0033" num="0032"> <figref idrefs="DRAWINGS">FIG. 8</figref> is a functional block diagram depicting a video processing system, comprising a differential motion extraction apparatus, according to one or more implementations.</div>
<div class="description-paragraph" id="p-0034" num="0033"> <figref idrefs="DRAWINGS">FIG. 9A</figref> is a graphical illustration depicting an encoded object for use with the motion extraction, according to one or more implementations.</div>
<div class="description-paragraph" id="p-0035" num="0034"> <figref idrefs="DRAWINGS">FIG. 9B</figref> is a graphical illustration depicting motion of an encoded object for use with the motion extraction methodology, according to one or more implementations.</div>
<div class="description-paragraph" id="p-0036" num="0035"> <figref idrefs="DRAWINGS">FIG. 9C</figref> is a graphical illustration depicting spatial distribution of motion extracted from encoded video, according to one or more implementations.</div>
<div class="description-paragraph" id="p-0037" num="0036"> <figref idrefs="DRAWINGS">FIG. 10</figref> is a logical flow diagram illustrating a method of determining a salient feature using encoded video motion information, in accordance with one or more implementations.</div>
<div class="description-paragraph" id="p-0038" num="0037"> <figref idrefs="DRAWINGS">FIG. 11</figref> is a logical flow diagram illustrating a method of data processing useful for determining features, in accordance with one or more implementations.</div>
<div class="description-paragraph" id="p-0039" num="0038"> <figref idrefs="DRAWINGS">FIG. 12</figref> is a logical flow diagram illustrating a method of executing an action configured based on a gesture detected using motion information, in accordance with one or more implementations.</div>
<div class="description-paragraph" id="p-0040" num="0039"> <figref idrefs="DRAWINGS">FIG. 13</figref> is a logical flow diagram illustrating a method of determining a depth of visual scene using encoded interleaved stereo image information, in accordance with one or more implementations.</div>
<div class="description-paragraph" id="p-0041" num="0040"> <figref idrefs="DRAWINGS">FIG. 14</figref> is a logical flow diagram illustrating a method of determining distance to objects using motion of interleaved image stream, in accordance with one or more implementations.</div>
<div class="description-paragraph" id="p-0042" num="0041"> <figref idrefs="DRAWINGS">FIG. 15</figref> is a logical flow diagram illustrating a method of executing an action configured based on detecting an object in motion information, in accordance with one or more implementations.</div>
<div class="description-paragraph" id="p-0043" num="0042"> <figref idrefs="DRAWINGS">FIGS. 16A-16D</figref> illustrate gestures of a human operator used for communicating control indications to a robotic device (such as one comprising a distance determination apparatus as described herein), in accordance with one or more implementations.</div>
<div class="description-paragraph" id="p-0044" num="0043"> <figref idrefs="DRAWINGS">FIG. 17</figref> is a graphical illustration depicting an exemplary unmanned robotic apparatus comprising distance determination apparatus of the disclosure configured for autonomous navigation, in accordance with one or more implementations.</div>
</description-of-drawings>
<div class="description-paragraph" id="p-0045" num="0044">All Figures disclosed herein are © Copyright 2014 Brain Corporation. All rights reserved.</div>
<heading id="h-0008">DETAILED DESCRIPTION</heading>
<div class="description-paragraph" id="p-0046" num="0045">Implementations of the present disclosure will now be described in detail with reference to the drawings, which are provided as illustrative examples so as to enable those skilled in the art to practice the present technology. Notably, the figures and examples below are not meant to limit the scope of the present disclosure to a single implementation, but other implementations are possible by way of interchange of or combination with some or all of the described or illustrated elements. Wherever convenient, the same reference numbers will be used throughout the drawings to refer to same or like parts.</div>
<div class="description-paragraph" id="p-0047" num="0046">Although the system(s) and/or method(s) of this disclosure have been described in detail for the purpose of illustration based on what is currently considered to be the most practical and preferred implementations, it is to be understood that such detail is solely for that purpose and that the disclosure is not limited to the disclosed implementations, but, on the contrary, is intended to cover modifications and equivalent arrangements that are within the spirit and scope of the appended claims. For example, it is to be understood that the present disclosure contemplates that, to the extent possible, one or more features of any implementation may be combined with one or more features of any other implementation</div>
<div class="description-paragraph" id="p-0048" num="0047">In the present disclosure, an implementation showing a singular component should not be considered limiting; rather, the disclosure is intended to encompass other implementations including a plurality of the same component, and vice-versa, unless explicitly stated otherwise herein.</div>
<div class="description-paragraph" id="p-0049" num="0048">Further, the present disclosure encompasses present and future known equivalents to the components referred to herein by way of illustration.</div>
<div class="description-paragraph" id="p-0050" num="0049">As used herein, the term “bus” is meant generally to denote all types of interconnection or communication architecture that is used to access the synaptic and neuron memory. The “bus” could be optical, wireless, infrared or another type of communication medium. The exact topology of the bus could be for example standard “bus”, hierarchical bus, network-on-chip, address-event-representation (AER) connection, or other type of communication topology used for accessing, e.g., different memories in pulse-based system.</div>
<div class="description-paragraph" id="p-0051" num="0050">As used herein, the terms “computer”, “computing device”, and “computerized device”, include, but are not limited to, personal computers (PCs) and minicomputers, whether desktop, laptop, or otherwise, mainframe computers, workstations, servers, personal digital assistants (PDAs), handheld computers, embedded computers, programmable logic device, personal communicators, tablet or “phablet” computers, portable navigation aids, J2ME equipped devices, smart TVs, cellular telephones, smart phones, personal integrated communication or entertainment devices, or literally any other device capable of executing a set of instructions and processing an incoming data signal.</div>
<div class="description-paragraph" id="p-0052" num="0051">As used herein, the term “computer program” or “software” is meant to include any sequence or human or machine cognizable steps which perform a function. Such program may be rendered in virtually any programming language or environment including, for example, C/C++, C#, Fortran, COBOL, MATLAB™, PASCAL, Python, assembly language, markup languages (e.g., HTML, SGML, XML, VoXML), and the like, as well as object-oriented environments such as the Common Object Request Broker Architecture (CORBA), Java™ (including J2ME, Java Beans), Binary Runtime Environment (e.g., BREW), and other languages.</div>
<div class="description-paragraph" id="p-0053" num="0052">As used herein, the terms “connection”, “link”, “synaptic channel”, “transmission channel”, “delay line”, are meant generally to denote a causal link between any two or more entities (whether physical or logical/virtual), which enables information exchange between the entities.</div>
<div class="description-paragraph" id="p-0054" num="0053">As used herein the term feature may refer to a representation of an object edge, determined by change in color, luminance, brightness, transparency, texture, and/or curvature. The object features may comprise, inter alia, individual edges, intersections of edges (such as corners), orifices, and/or curvature</div>
<div class="description-paragraph" id="p-0055" num="0054">As used herein, the term “memory” includes any type of integrated circuit or other storage device adapted for storing digital data including, without limitation, ROM. PROM, EEPROM, DRAM, Mobile DRAM, SDRAM, DDR/2 SDRAM, EDO/FPMS, RLDRAM, SRAM, “flash” memory (e.g., NAND/NOR), memristor memory, and PSRAM.</div>
<div class="description-paragraph" id="p-0056" num="0055">As used herein, the terms “processor”, “microprocessor” and “digital processor” are meant generally to include all types of digital processing devices including, without limitation, digital signal processors (DSPs), reduced instruction set computers (RISC), general-purpose (CISC) processors, microprocessors, gate arrays (e.g., field programmable gate arrays (FPGAs)), PLDs, reconfigurable computer fabrics (RCFs), array processors, secure microprocessors, and application-specific integrated circuits (ASICs). Such digital processors may be contained on a single unitary IC die, or distributed across multiple components.</div>
<div class="description-paragraph" id="p-0057" num="0056">As used herein, the term “network interface” refers to any signal, data, or software interface with a component, network or process including, without limitation, those of the FireWire (e.g., FW400, FW800, and/or other FireWire implementation), USB (e.g., USB2), Ethernet (e.g., 10/100, 10/100/1000 (Gigabit Ethernet), 10-Gig-E, etc.), MoCA, Coaxsys (e.g., TVnet™), radio frequency tuner (e.g., in-band or OOB, cable modem, etc.), Wi-Fi (802.11), WiMAX (802.16), PAN (e.g., 802.15), cellular (e.g., 3G, LTE/LTE-A/TD-LTE, GSM, and/or other cellular interface implementation) or IrDA families.</div>
<div class="description-paragraph" id="p-0058" num="0057">As used herein, the terms “pulse”, “spike”, “burst of spikes”, and “pulse train” are meant generally to refer to, without limitation, any type of a pulsed signal, e.g., a rapid change in some characteristic of a signal, e.g., amplitude, intensity, phase or frequency, from a baseline value to a higher or lower value, followed by a rapid return to the baseline value and may refer to any of a single spike, a burst of spikes, an electronic pulse, a pulse in voltage, a pulse in electrical current, a software representation of a pulse and/or burst of pulses, a software message representing a discrete pulsed event, and any other pulse or pulse type associated with a discrete information transmission system or mechanism.</div>
<div class="description-paragraph" id="p-0059" num="0058">As used herein, the term “receptive field” is used to describe sets of weighted inputs from filtered input elements, where the weights may be adjusted.</div>
<div class="description-paragraph" id="p-0060" num="0059">As used herein, the term “Wi-Fi” refers to, without limitation, any of the variants of IEEE-Std. 802.11 or related standards including 802.11 a/b/g/n/s/v and 802.11-2012.</div>
<div class="description-paragraph" id="p-0061" num="0060">As used herein, the term “wireless” means any wireless signal, data, communication, or other interface including without limitation Wi-Fi, Bluetooth, 3G (3GPP/3GPP2), HSDPA/HSUPA, TDMA, CDMA (e.g., IS-95A, WCDMA, and/or other wireless interface implementation), FHSS, DSSS, GSM, PAN/802.15, WiMAX (802.16), 802.20, narrowband/FDMA, OFDM, PCS/DCS, LTE/LTE-A/TD-LTE, analog cellular, CDPD, RFID or NFC (e.g., EPC Global Gen. 2, ISO 14443, ISO 18000-3), satellite systems, millimeter wave or microwave systems, acoustic, and infrared (e.g., IrDA).</div>
<div class="description-paragraph" id="p-0062" num="0061">The present disclosure provides, among other things, apparatus and methods for determining depth of field of a scene based on processing information from multiple sources detecting motion of objects and/or features in video in real time. The video information may comprise for example multiple streams of frames received from a plurality of cameras disposed separate from one another. Individual cameras may comprise an image sensor (e.g., charge-coupled device (CCD), CMOS device, and/or an active-pixel sensor (APS), photodiode arrays, and/or other sensors). In one or more implementations, the stream of frames may comprise a pixel stream downloaded from a file. An example of such a file may include a stream of two-dimensional matrices of red green blue RGB values (e.g., refreshed at a 25 Hz or other suitable frame rate). It will be appreciated by those skilled in the art when given this disclosure that the above-referenced image parameters are merely exemplary, and many other image representations (e.g., bitmap, luminance-chrominance (YUV, YCbCr), cyan-magenta-yellow and key (CMYK), grayscale, and/or other image representations) are equally applicable to and useful with the various aspects of the present disclosure. Furthermore, data frames corresponding to other (non-visual) signal modalities such as sonograms, infrared (IR), radar or tomography images may be equally compatible with the processing methodology of the disclosure, or yet other configurations.</div>
<div class="description-paragraph" id="p-0063" num="0062">The video processing methodology described herein may enable a robotic controller to obtain motion and/or distance information using a specialized hardware video encoder. Use of dedicated video encoders provides a computationally efficient way to determine motion and/or distance using video signals compared to processing techniques that employ general purpose processors for performing computations (e.g., optical flow, block matching, phase correlations and/or other. Computational efficiency of hardware video encoders may be leveraged top reduce energy use, complexity, size, and/or cost of the processing component, increase autonomy of robotic device using the computationally efficient controller, and/or increase processing performance (e.g., image resolution, frame rate, number of cameras) for a given hardware specifications compared to the prior art.</div>
<div class="description-paragraph" id="p-0064" num="0063">Processing data from multiple spatially distributed sources may enable depth of field determination using a disparity methodology. In some implementations of stereo vision, distance d to an object may be determined using binocular disparity D as follows:</div>
<div class="description-paragraph" id="p-0065" num="0064">
<maths id="MATH-US-00001" num="00001">
<math overflow="scroll">
<mtable>
<mtr>
<mtd>
<mrow>
<mi>d</mi>
<mo>∝</mo>
<mfrac>
<mn>1</mn>
<mi>D</mi>
</mfrac>
</mrow>
</mtd>
<mtd>
<mrow>
<mo>(</mo>
<mrow>
<mi>Eqn</mi>
<mo>.</mo>
<mstyle>
<mspace height="0.8ex" width="0.8em"> </mspace>
</mstyle>
<mo>⁢</mo>
<mn>1</mn>
</mrow>
<mo>)</mo>
</mrow>
</mtd>
</mtr>
</mtable>
</math>
</maths>
</div>
<div class="description-paragraph" id="p-0066" num="0065"> <figref idrefs="DRAWINGS">FIG. 1A</figref> depicts a top view of mobile robotic apparatus comprising two cameras configured to provide sensory information for determining distance based on the disparity. The apparatus <b>100</b> may comprise for instance a robotic vehicle outfitted with a motion detection apparatus configured in accordance with one or more implementations, e.g., such as illustrated in <figref idrefs="DRAWINGS">FIGS. 4A-4B</figref>, below. The robotic apparatus <b>100</b> may comprise left and right cameras <b>106</b>, <b>108</b> disposed at a distance <b>102</b> from one another. The robotic apparatus <b>100</b> may navigate in a direction <b>104</b>. One or more obstacles may be present in path of the apparatus <b>100</b>, e.g., a ball <b>112</b> and a box <b>122</b>, disposed at distance <b>110</b>, <b>120</b>, respectively, from the apparatus <b>110</b>. Due to the spatial separation <b>102</b> between the cameras <b>106</b>, <b>108</b>, travel paths from a given object (e.g., <b>114</b>, <b>116</b> for the ball <b>112</b>), <b>124</b>, <b>126</b> for the box <b>122</b>) may be unequal to one another. As shown in <figref idrefs="DRAWINGS">FIG. 1A</figref>, the path <b>114</b> is longer compared to the path <b>116</b>, and the path <b>126</b> is longer than the path <b>124</b>.</div>
<div class="description-paragraph" id="p-0067" num="0066">Difference in path lengths may cause a difference in apparent position of the objects <b>112</b>, <b>122</b> in image frame(s) provided by the camera <b>106</b> relative the image frame provided by the camera <b>108</b>.</div>
<div class="description-paragraph" id="p-0068" num="0067"> <figref idrefs="DRAWINGS">FIG. 2A</figref> depicts a typical stereo imagery input for use with the disparity determination methodology, according to one or more implementations. The frames <b>200</b>, <b>210</b> in <figref idrefs="DRAWINGS">FIG. 2A</figref> may be acquired by the two spatially displaced cameras <b>106</b>, <b>108</b> of the apparatus <b>100</b> in <figref idrefs="DRAWINGS">FIG. 1A</figref>. Object representations <b>216</b>, <b>212</b> of the frame <b>210</b> may be displaced horizontally relative to object representations <b>206</b>, <b>202</b>, respectively, of the frame <b>200</b>. Object representations <b>202</b>, <b>212</b>, <b>206</b>, <b>216</b> may correspond to objects <b>112</b>, <b>122</b>, respectively, in <figref idrefs="DRAWINGS">FIG. 1A</figref>.</div>
<div class="description-paragraph" id="p-0069" num="0068"> <figref idrefs="DRAWINGS">FIG. 2B</figref> illustrates the disparity between representations of objects corresponding to the frames of stereo imagery shown in <figref idrefs="DRAWINGS">FIG. 2A</figref>. Object representations <b>236</b>, <b>246</b> in frame <b>23</b> may be characterized by a horizontal disparity <b>234</b>, and similarly object representations <b>222</b> <b>232</b> may be characterized by a corresponding horizontal disparity <b>224</b>. Disparity <b>224</b>, <b>234</b> may be inversely proportional to distance between the camera and the respective object (e.g., the distance <b>110</b>, <b>120</b> in <figref idrefs="DRAWINGS">FIG. 1A</figref>); i.e., the shorter the distance, the greater the disparity, due to the greater subtended arc.</div>
<div class="description-paragraph" id="p-0070" num="0069"> <figref idrefs="DRAWINGS">FIG. 1B</figref> depicts a mobile robotic apparatus comprising a motion detection apparatus configured, e.g., in accordance with the exemplary implementations illustrated in <figref idrefs="DRAWINGS">FIGS. 7-8</figref>, infra. The robotic apparatus <b>160</b> may comprise a camera <b>166</b>. The camera <b>166</b> may be characterized by a field of view <b>168</b> (e.g., an extent of the observable world that may be captured by the camera lens at a given moment). The camera <b>166</b> may provide information associated with objects within the field of view <b>168</b>. In some implementations, the camera <b>166</b> may provide frames of pixels of luminance and/or color, refreshed at 25 Hz frame rate. However, it will be appreciated that, in some implementations, other frame rates may be used (whether constant or variable), as may other types of information provided by the camera(s) <b>166</b>.</div>
<div class="description-paragraph" id="p-0071" num="0070">One or more objects (e.g., a floor <b>170</b>, a stationary object <b>176</b>, a moving object (e.g., ball <b>174</b>), and/or other objects) may be present in the camera field of view. The motion of the objects may result in a displacement of pixels representing the objects within successive frames, such as is described in U.S. patent application Ser. No. 13/689,717 filed on Nov. 30, 2012 and entitled “APPARATUS AND METHODS FOR OBJECT DETECTION VIA OPTICAL FLOW CANCELLATION”, incorporated, herein by reference in its entirety.</div>
<div class="description-paragraph" id="p-0072" num="0071">When the robotic apparatus <b>160</b> is in motion, such as shown by arrow <b>164</b> in <figref idrefs="DRAWINGS">FIG. 1B</figref>, motion of the objects within the camera <b>166</b> field if view <b>168</b> (e.g., denoted by arrows <b>172</b>, <b>178</b>, <b>180</b> in <figref idrefs="DRAWINGS">FIG. 1B</figref>) may comprise the self-motion component and the object motion component. By way of a non-limiting example, motion of objects in <figref idrefs="DRAWINGS">FIG. 1B</figref> may comprise apparent motion <b>180</b> of the stationary background <b>176</b> and the boundary (e.g., the component <b>172</b> associated with the floor boundary); (ii) component <b>178</b> associated with the moving ball <b>174</b> that comprises a superposition of the ball displacement and motion of the camera; and/or other components. As noted previously, determination of the ball <b>174</b> motion may be particularly challenging when the camera <b>160</b> is in motion (e.g., during panning) and/or when the field of view is changing (e.g., when zooming in/out).</div>
<div class="description-paragraph" id="p-0073" num="0072"> <figref idrefs="DRAWINGS">FIG. 2C</figref> depicts two exemplary frames (e.g., provided by the camera <b>166</b> in FIG. <b>1</b>A) comprising multiple moving objects useful with the motion estimation methodology described herein. The frames <b>240</b>, <b>250</b> may comprise an object <b>246</b>, <b>256</b> that may move in a given direction (e.g., <b>288</b>). The frames <b>240</b>, <b>250</b> may comprise an object <b>242</b>, <b>252</b> that may move back and forth in a direction indicated by arrow <b>244</b>. Motion along curved trajectories may be resolved by using linear piece-wise approximation, wherein motion between successive frames may be interpreted as linear. An increased frame rate and/or image resolution may be employed with complex motion trajectories. In some implementations of target approach by a robotic device, the object <b>242</b> may comprise a target (e.g., ball) that may be moved back and forth in order to indicate to a controller of, e.g., the robotic vehicle <b>160</b> in <figref idrefs="DRAWINGS">FIG. 1B</figref>, a target to follow. Frames <b>240</b>, <b>250</b> may represent position of objects at two time instances. Due to the presence of multiple motions, detection of object <b>242</b>, <b>252</b> may be not straightforward due to, for example, portions of the frames <b>250</b> being characterized by differential motion.</div>
<div class="description-paragraph" id="p-0074" num="0073">In some implementations of object detection in the presence of differential motion, background (and/or self-motion) may be determined using a statistical analysis of motion distribution within a given encoded frame. Various statistical parameters may be determined, e.g., median, mean plus/minus n standard deviations, and/or others, in order to determine one or more prevailing (dominant) motion vectors for the frame. The prevailing motion may be removed (e.g., via a vector subtraction) from the frame motion distribution in order to determine residual motion. The residual motion may be analyzed (e.g., using a threshold technique) in order to detect one or more features that may be moving differently from the prevailing motion.</div>
<div class="description-paragraph" id="p-0075" num="0074">In one or more implementations of object detection in the presence of differential motion, prevailing motion may be determined using a clustering technique. For example, a motion filed within the frame may be partitioned into a plurality of clusters based on analysis of motion distribution. The largest area cluster may be associated with the prevailing (dominant) motion, or may be removed (masked off) from the frame to obtain residual motion distribution. The residual motion may be analyzed in order to determine the presence of one or more features based on remaining clusters of motion.</div>
<div class="description-paragraph" id="p-0076" num="0075">In some implementations, image frames provided by a plurality of cameras (e.g., the cameras <b>106</b>, <b>108</b> in <figref idrefs="DRAWINGS">FIG. 1A</figref>) may be utilized in order to determine depth of field and/or distance to objects using the disparity information. Comparing object representations <b>206</b>, <b>202</b> of frame <b>200</b> to object representations <b>216</b>, <b>212</b> of frame <b>210</b>, the disparity may be considered as object motion occurring between the capture of frame <b>200</b> and the capture of frame <b>210</b> in <figref idrefs="DRAWINGS">FIG. 2A</figref>. The disparity (e.g., apparent motion) <b>224</b>, <b>234</b> in <figref idrefs="DRAWINGS">FIG. 32B</figref> may be obtained using, in the exemplary implementation, motion estimation. Various motion estimation algorithms exist (e.g., optical flow methodology, such as that described in U.S. patent application Ser. No. 13/689,717 filed on Nov. 30, 2012 and entitled “APPARATUS AND METHODS FOR OBJECT DETECTION VIA OPTICAL FLOW CANCELLATION”, incorporated herein by reference in its entirety, each of which may be used consistent with the various aspects of the present disclosure.</div>
<div class="description-paragraph" id="p-0077" num="0076">In some implementations, the apparent motion due to disparity may be determined using motion estimation information provided by a video encoder. In order to enable motion estimation by an encoder, frames provided by individual cameras (e.g., <b>106</b>, <b>108</b> in <figref idrefs="DRAWINGS">FIG. 1A</figref>) may be combined to form a common video stream. <figref idrefs="DRAWINGS">FIG. 3A</figref> illustrates determination of an input stream for motion processing using an alternating interleaver of stereo frame input, according to one or more implementations. In <figref idrefs="DRAWINGS">FIG. 3A</figref>, the frame sequences <b>300</b>,<b>310</b> (also referred to as channel A, B) may correspond to data provided by two cameras (e.g., left/right cameras <b>106</b>, <b>108</b> in <figref idrefs="DRAWINGS">FIG. 1A</figref>), and/or data loaded from a disc or other source, in one or more implementations. The frame sequences <b>300</b>, <b>310</b> comprising (e.g., frames <b>302</b>, <b>312</b>) may be processed by an alternating interleaver process <b>320</b> configured to produce an interleaved frame sequence <b>322</b>. The sequence <b>322</b> may comprise alternating frames (e.g., <b>302</b>, <b>312</b>) from left/right cameras, in some implementations. In some implementations, the frames from left/right cameras (e.g., <b>302</b>, <b>312</b>) may be acquired simultaneous with one another using, e.g., multiple camera synchronization.</div>
<div class="description-paragraph" id="p-0078" num="0077"> <figref idrefs="DRAWINGS">FIG. 3B</figref> illustrates determination of an input stream for motion processing using an alternating interleaver of stereo frame input, according to one or more implementations. The A, B frame sequences <b>300</b>, <b>310</b> comprising (e.g., frames <b>302</b>, <b>312</b>) may be processed by the alternating interleaver process <b>340</b> configured to produce an interleaved frame sequence <b>342</b>. The sequence <b>342</b> may be configured to comprise alternating pairs of frames from a given channel. As shown in <figref idrefs="DRAWINGS">FIG. 3A</figref>, frames B<b>1</b>, B<b>2</b> from channel B acquired at times t<b>1</b>, t<b>2</b>, may be followed by frames A<b>1</b>, A<b>2</b> from channel A acquired at times t<b>1</b>, t<b>2</b>, followed by frames B<b>3</b>, B<b>4</b> from channel B acquired at times t<b>3</b>, t<b>4</b>, wherein t<b>4</b>&gt;t<b>3</b>&gt;t<b>2</b>&gt;t<b>1</b>. In some implementations (not shown), the frame A<b>1</b> may be repeated and/or preceded by a blank frame in the interleaved sequence <b>342</b>. Use of an alternating approach may provide, inter alia, both motion and disparity information within a single encoded stream.</div>
<div class="description-paragraph" id="p-0079" num="0078">Streams of interleaved frames (e.g., <b>322</b>, <b>342</b> <figref idrefs="DRAWINGS">FIGS. 3A-3B</figref>, and/or shown in <figref idrefs="DRAWINGS">FIGS. 6A-6B</figref>, below) may be utilized in order to determine depth of field of view and/or distance to objects using motion encoding, as described in detail below with respect to <figref idrefs="DRAWINGS">FIGS. 4A-5</figref>.</div>
<div class="description-paragraph" id="p-0080" num="0079"> <figref idrefs="DRAWINGS">FIG. 4A</figref> illustrates a processing apparatus configured to determine disparity from two image sources, according to one or more implementations. The apparatus <b>400</b> may comprise two image sources <b>404</b>, <b>405</b> configured to provide information environment <b>402</b>. In some implementations of visual data processing, the sources <b>404</b>, <b>405</b> may comprise digital and/or analog cameras disposed separate from one another. Individual cameras may comprise an image sensor (CCD, CMOS device, and/or an APS, photodiode arrays, and/or other sensors). It will be appreciated that in some implementations, such separation between the image sensors may be achieved even when the sensors are disposed on the same substrate or “chip” (e.g., two sensors placed at opposite ends of the same substrate/chip). In one or more implementations, the image sources <b>4054</b>, <b>405</b> may comprise video files on a storage device. An example of such a file may include a stream of two-dimensional matrices of red green blue RGB values (e.g., refreshed at a 25 Hz or other suitable frame rate). It will be appreciated by those skilled in the art when given this disclosure that the above-referenced image parameters are merely exemplary, and many other image representations (e.g., bitmap, luminance-chrominance (YUV, YCbCr), cyan-magenta-yellow and key (CMYK), grayscale, and/or other image representations) are equally applicable to and useful with the various aspects of the present disclosure. Furthermore, data frames corresponding to other (non-visual) signal modalities such as sonograms, IR, radar, or tomography images may be equally compatible with the processing methodology of the disclosure, or yet other configurations.</div>
<div class="description-paragraph" id="p-0081" num="0080">Image frames <b>406</b>, <b>407</b> provided by the sources <b>404</b>, <b>405</b> may be interleaved by the interleaver apparatus <b>410</b>. In some implementations, the interleaver apparatus <b>410</b> may comprise 2×1 multiplexer configured to provide one of the input channels <b>406</b>, <b>407</b> at its output <b>412</b> at a given time. The output <b>412</b> may comprise an alternating interleaved stream (e.g., <b>322</b>), an alternating interleaved stream of frames (e.g., <b>342</b> in <figref idrefs="DRAWINGS">FIG. 3B</figref>), or yet other option, in accordance with one or more implementations.</div>
<div class="description-paragraph" id="p-0082" num="0081">The interleaved output <b>412</b> may be provided to a motion estimation component <b>420</b>. In one or more implementations, the motion estimation component may comprise a video encoder comprising one or more motion estimation operations. The component <b>420</b> may comprise for instance a dedicated integrated circuit (IC) disposed on a single or multiple die), a component of a processing system (e.g., video encoder block of a Snapdragon® system on a chip), an ASIC, an FPGA with a video encoder intellectual property (IP) core, an OEM printed circuit board, and/or other. Video encoding effectuated by the component <b>420</b> may comprise any applicable standard comprising motion estimation between one or more current images and one or more preceding images. Some exemplary encoding implementations include H.264/MPEG-4 advanced video coding described, e.g., in <i>ISO/IEC </i>14496-10, 2009<i>—MPEG</i>-4 <i>Part </i>10<i>, Advanced Video Coding</i>, H.263 standard described in, e.g., <i>ITU</i>-<i>T H.</i>263 <i>TELECOMMUNICATION STANDARDIZATION SECTOR OF ITU </i>(01/2005) <i>SERIES H: AUDIOVISUAL AND MULTIMEDIA SYSTEMS Infrastructure of audiovisual services—Coding of moving video, Video coding for low bit rate communication</i>; H.262/MPEG-2, described in e.g., <i>ISO/IEC </i>13818-2 2013-10-01 <i>Information technology—Generic coding of moving pictures and associated audio information—Part </i>2<i>: Video</i>, H.265 standard described in, e.g., <i>ITU</i>-<i>T H.</i>263 <i>TELECOMMUNICATION STANDARDIZATION SECTOR OF ITU </i>(04/2013), <i>SERIES H: AUDIOVISUAL AND MULTIMEDIA SYSTEMS Infrastructure of audiovisual services—Coding of moving video, High efficiency video coding</i>; each of the foregoing being incorporated herein by reference in its entirety. See also Exhibit I hereto, which contains exemplary computer code useful for processing image data consistent with, e.g., the ISO/IEC 1196-10 and H.265 Standards referenced above.</div>
<div class="description-paragraph" id="p-0083" num="0082">In some implementations, the motion estimation component <b>420</b> may comprise logic configured to determine motion using optical flow, and/or other motion estimation algorithms such as but not limited to: block-matching algorithm, phase correlation, as well as determining locations of one or more features and estimating the motion of individual detected features.</div>
<div class="description-paragraph" id="p-0084" num="0083">Output <b>422</b> of the motion estimation component may be provided to a processing component <b>430</b> configured to determine one or more parameters of interest, including e.g., depth of the scene <b>402</b> and/or distance to objects that may be present in the scene, using motion based disparity determination methodology.</div>
<div class="description-paragraph" id="p-0085" num="0084">Returning now to <figref idrefs="DRAWINGS">FIG. 3A</figref>, the encoding of the interleaved frame sequence <b>322</b> by the motion estimation component <b>420</b> of <figref idrefs="DRAWINGS">FIG. 4A</figref> is depicted by arrow <b>324</b>. In some implementations, the component <b>420</b> may comprise MPEG-4/H.264 encoder configured to produce the encoded stream <b>330</b>. The encoding of frame pair <b>302</b>, <b>312</b> may produce motion information for the encoded frame <b>334</b>. The motion information of the frame <b>334</b> may comprise e.g., horizontal and/or vertical displacement (dx, dy) of blocks of pixels (macroblocks) and be interpreted as caused by a disparity between scene representations of frame <b>302</b> and frame <b>304</b>. In some implementations of stereo vision (e.g., as described above with respect to <figref idrefs="DRAWINGS">FIG. 1A</figref>), analysis of motion information for the frame <b>334</b> (performed by the component <b>430</b> in <figref idrefs="DRAWINGS">FIG. 4A</figref>) may produce the disparity D between left and right image frames. As shown in <figref idrefs="DRAWINGS">FIG. 3A</figref>, the encoded frames <b>334</b>, <b>338</b> may provide disparity estimates D<b>1</b>, D<b>2</b> associated with frames acquired at times t<b>1</b>, t<b>2</b>. Frames <b>332</b>, <b>336</b> may be ignored (skipped) for the purposes of the disparity determination. Disparity estimates D<b>1</b>, D<b>2</b> may be used to determine distance to one or more objects within the frames <b>330</b>.</div>
<div class="description-paragraph" id="p-0086" num="0085">In one or more implementations, the component <b>430</b> may be configured to parse the compressed video stream <b>422</b> in order to obtain motion information (e.g., map of vectors <b>916</b> in <figref idrefs="DRAWINGS">FIG. 9A</figref>). By way of an illustration, the motion information may comprise a macroblock location L (e.g., index), x-component, and y-component of motion of pixels associated with the macroblock location L. The extracted motion information may be used for disparity and or distance determination. Output <b>432</b> of the component <b>430</b> may be provided to another component (e.g., a controller of a robot). Various uses of the depth information are contemplated such as, for example, object detection, object localization, distance estimation, trajectory planning, gesture detection, and/or others that will be recognized by those of ordinary skill when provided the present disclosure.</div>
<div class="description-paragraph" id="p-0087" num="0086">Returning now to <figref idrefs="DRAWINGS">FIG. 3B</figref>, encoding of the interleaved frame sequence <b>342</b> by the motion estimation component <b>420</b> of <figref idrefs="DRAWINGS">FIG. 4A</figref> is depicted by arrow <b>344</b> in <figref idrefs="DRAWINGS">FIG. 3B</figref>. In some implementations, the component <b>420</b> may comprise MPEG-4/H.264 encoder configured to produce encoded stream <b>350</b>. In encoding of frame pair <b>302</b>, <b>312</b> may produce motion information for the encoded frame <b>354</b>. The motion information of the frame <b>354</b> may comprise horizontal and/or vertical displacement (dx, dy) of blocks of pixels and be interpreted as caused by a disparity between scene representations of frame <b>302</b> and frame <b>304</b>. In some implementations, of stereo vision (e.g., as described above with respect to <figref idrefs="DRAWINGS">FIG. 1A</figref>) analysis of motion information for the frame <b>354</b> (performed by the component <b>430</b> in <figref idrefs="DRAWINGS">FIG. 4A</figref>) may produce the disparity D between left and right image frames. As shown in <figref idrefs="DRAWINGS">FIG. 3B</figref>, the encoded frames <b>354</b>, <b>362</b> may provide disparity estimates D<b>1</b>, D<b>3</b> associated with frames acquired at times t<b>1</b>, t<b>3</b>. The encoded frame <b>358</b> may provide negative disparity estimate (−D<b>2</b>) associated with frames acquired at time t<b>2</b>. Frames <b>356</b>, <b>362</b> may provide motion information associated with the frame sequence <b>310</b>. Frame <b>360</b> may provide motion information associated with the frame sequence <b>300</b>. Disparity estimates D<b>1</b>, D<b>2</b>, D<b>3</b> may be used to determine distance to one or more objects within the frames <b>350</b> using, e.g., Eqn. 1.</div>
<div class="description-paragraph" id="p-0088" num="0087">Although interleaving of frames from two sources is illustrated in <figref idrefs="DRAWINGS">FIGS. 3A-3B</figref>, the methodology described herein may be employed for any practical number of sources (e.g., three, four as shown and described with respect to <figref idrefs="DRAWINGS">FIGS. 5A-5C</figref> below, and/or a greater number of sources).</div>
<div class="description-paragraph" id="p-0089" num="0088"> <figref idrefs="DRAWINGS">FIG. 4B</figref> illustrates a processing apparatus configured to determine disparity from two or more image sources, according to one or more implementations. The apparatus <b>440</b> may comprise a plurality of image sources (e.g., <b>443</b>, <b>445</b>) configured to provide information related to the environment <b>402</b>. In some implementations of visual data processing, the sources <b>443</b>, <b>445</b>, <b>405</b> may comprise sources described with respect to <figref idrefs="DRAWINGS">FIG. 4A</figref> above.</div>
<div class="description-paragraph" id="p-0090" num="0089">Image frames <b>446</b>, <b>448</b> provided by the sources <b>443</b>, <b>445</b> may be interleaved by the interleaver apparatus <b>450</b>. In some implementations, the interleaver apparatus <b>450</b> may comprise N×1 multiplexer configured to provide date from one of the input channels <b>446</b>, <b>448</b> at its output <b>452</b> at a given time. The output <b>452</b> may comprise an alternating interleaved stream (e.g., <b>600</b> in <figref idrefs="DRAWINGS">FIG. 6A</figref>), an alternating interleaved stream of frames (e.g., constructed similar to the stream <b>342</b> in <figref idrefs="DRAWINGS">FIG. 3B</figref>) in accordance with one or more implementations.</div>
<div class="description-paragraph" id="p-0091" num="0090">The interleaved output <b>452</b> may be provided to a processing component <b>460</b>. The component <b>460</b> may comprise motion estimation logic. In one or more implementations, the motion estimation logic may comprise a video encoder comprising motion estimation operation. The component <b>460</b> may comprise a dedicated integrated circuit (IC) disposed on a single or multiple die), a component of a processing system (e.g., video encoder block of a Snapdragon® system on a chip), an ASIC, an FPGA with a video encoder intellectual property (IP) core, an OEM printed circuit board, and/or other. Video encoding effectuated by the component <b>460</b> may comprise any applicable standard comprising motion estimation between current frame and preceding frame. In some implementations, the motion estimation component <b>460</b> may comprise logic configured to determine motion using optical flow, and/or other motion estimation algorithms such as but not limited to: block-matching algorithm, phase correlation, as well as determining locations of features and estimating the motion of those features. In one or more implementations wherein the input <b>452</b> may be encoded using a video encoder (e.g., MPEG-4, H.265), the component <b>460</b> may be configured to parse the encoded video stream in order to obtain motion information (e.g., map of vectors <b>916</b> in <figref idrefs="DRAWINGS">FIG. 9A</figref>).</div>
<div class="description-paragraph" id="p-0092" num="0091">The component <b>460</b> may comprise logic configured to determine depth of the scene <b>402</b> and/or distance to objects that may be present in the scene using motion determined based disparity determination methodology. The extracted motion information may be used for disparity and or distance determination. Output <b>462</b> of the component <b>460</b> may be provided to another component (e.g., a controller of a robot). Various uses of the depth information may be contemplated such as, for example, object detection, object localization, distance estimation, trajectory planning, gesture detection, and/or others. Determining motion disparity and/or the distance may be performed for example using operations described above with respect to <figref idrefs="DRAWINGS">FIGS. 3A-4A</figref>. In some implementations of an encoder with motion estimation (e.g., MPEG-4/H.264), the encoded stream <b>330</b>, and/or stream <b>350</b> in <figref idrefs="DRAWINGS">FIGS. 3A-3B</figref> and/or stream <b>422</b> in <figref idrefs="DRAWINGS">FIG. 4A</figref> may comprise one or more frames (also referred to as keyframes) that may not contain motion information. The processing component <b>430</b> and/or <b>460</b> of <figref idrefs="DRAWINGS">FIGS. 4A-4B</figref> may be configured to detect and ignore (e.g., skip) frames that do not convey motion information.</div>
<div class="description-paragraph" id="p-0093" num="0092">The apparatus <b>440</b> of <figref idrefs="DRAWINGS">FIG. 4B</figref> may be utilized with multi-camera configurations, e.g., such as described below with respect to <figref idrefs="DRAWINGS">FIGS. 5A-5C</figref>. <figref idrefs="DRAWINGS">FIG. 5A</figref> illustrates a triple-camera configuration useful with disparity determination using image interleaving, according to one or more implementations. The camera configuration <b>500</b> may comprise three cameras <b>510</b>, <b>520</b>, <b>530</b>, denoted A, B, C, respectively. In one or more implementations, the configuration <b>500</b> may comprise a pair of horizontally spaced cameras (e.g., left <b>510</b>, right <b>520</b>) and a vertically spaced camera (<b>530</b>). In some implementations (e.g., such as illustrated in <figref idrefs="DRAWINGS">FIG. 5C</figref>), the cameras <b>510</b>, <b>520</b>, <b>530</b> may be disposed in a linear array, and/or another configuration. Frames provided by the cameras <b>510</b>, <b>520</b>, <b>530</b> may be interleaved using any applicable methodologies, including these described with respect to <figref idrefs="DRAWINGS">FIGS. 3A-3B</figref>, and/or <b>6</b>A herein.</div>
<div class="description-paragraph" id="p-0094" num="0093">Various interleaving sequences may be employed when processing frames provided by the cameras <b>510</b>, <b>520</b>, <b>530</b>. By way of illustration, encoding interleaved frame stream ABCA . . . (e.g., the stream <b>600</b> shown in <figref idrefs="DRAWINGS">FIG. 6A</figref>) comprising frames provided by the cameras <b>510</b>, <b>520</b>, <b>530</b> using a motion estimation encoder, may provide motion due to disparity shown by arrows <b>504</b>, <b>514</b>, <b>524</b> in <figref idrefs="DRAWINGS">FIG. 5A</figref>. Encoding interleaved frame stream ACBA . . . , (e.g., <b>610</b> in <figref idrefs="DRAWINGS">FIG. 6A</figref>) using a motion estimation encoder, may provide motion due to disparity shown by arrows <b>526</b>, <b>516</b>, <b>506</b> in <figref idrefs="DRAWINGS">FIG. 5A</figref>. Various other interleaving sequences may be utilized, such as, for example, ABBCCAABBCCAA . . . , BACA . . . and/or other.</div>
<div class="description-paragraph" id="p-0095" num="0094"> <figref idrefs="DRAWINGS">FIG. 5B</figref> illustrates a quad-camera configuration useful with disparity determination using image interleaving, according to one or more implementations. The camera configuration <b>550</b> may comprise four cameras <b>550</b>, <b>552</b>, <b>556</b>, <b>558</b>, denoted A, B, C, D, respectively. In one or more implementations, the configuration <b>550</b> may comprise two pairs of horizontally spaced cameras (e.g., left top <b>550</b>, right top <b>552</b> and left bottom <b>550</b>, right bottom <b>556</b>) vertically spaced from one another. Various other spatial camera configurations may be utilized as well. Frames provided by the cameras <b>552</b>, <b>554</b>, <b>556</b>, <b>558</b> may be interleaved using any applicable methodologies, including these described with respect to <figref idrefs="DRAWINGS">FIGS. 3A-3B</figref>, and/or <b>6</b>B.</div>
<div class="description-paragraph" id="p-0096" num="0095">Various interleaving sequences may be employed when processing frames provided by the cameras <b>552</b>, <b>554</b>, <b>556</b>, <b>558</b>. By way of illustration, encoding interleaved frame stream ABCDA . . . (e.g., the stream <b>620</b> shown in <figref idrefs="DRAWINGS">FIG. 6B</figref>) comprising frames provided by the cameras <b>552</b>, <b>554</b>, <b>556</b>, <b>558</b> using a motion estimation encoder may provide motion due to disparity shown by arrows <b>562</b>, <b>564</b>, <b>566</b>, <b>568</b> in <figref idrefs="DRAWINGS">FIG. 5B</figref>. Encoding the interleaved frame stream ADCBA . . . , (e.g., <b>638</b> in <figref idrefs="DRAWINGS">FIG. 6B</figref>) using a motion estimation encoder, may provide motion due to disparity shown by arrows <b>569</b>, <b>567</b>, <b>565</b>, <b>563</b> in <figref idrefs="DRAWINGS">FIG. 5B</figref>. Various other interleaving sequences may be utilized, such as, for example sequences <b>624</b>, <b>628</b>, <b>630</b>, <b>634</b> illustrated in, <figref idrefs="DRAWINGS">FIG. 6B</figref> may be utilized. Sequences comprising transitions between diagonally opposing cameras in <figref idrefs="DRAWINGS">FIG. 5B</figref> (e.g., AC, CA, BD, DB and/or other) may be used to, inter alia, determine disparity shown by arrows <b>544</b>, <b>546</b>.</div>
<div class="description-paragraph" id="p-0097" num="0096"> <figref idrefs="DRAWINGS">FIG. 5C</figref> illustrates a linear multiple sensor element configuration useful with determining multiple depths scales using image interleaving, according to one or more implementations. The configuration <b>570</b> may comprise sensor elements <b>572</b>, <b>574</b>, <b>576</b>, <b>578</b> disposed in a linear array. In one or more implementations, individual sensor elements may comprise cameras or camera sensors. Spacing between the elements <b>572</b>, <b>574</b>, <b>576</b>, <b>578</b> may be the same (uniform linear array) and/or varying (e.g., a power law, random, and/or other). In some implementations, non-uniform spacing may be used in order to implement e.g., a Vernier scale.</div>
<div class="description-paragraph" id="p-0098" num="0097">Various interleaving sequences may be employed when processing frames provided by the elements <b>572</b>, <b>574</b>, <b>576</b>, <b>758</b>, such as, for example sequences <b>620</b>, <b>624</b>, <b>628</b>, <b>630</b>, <b>634</b>, <b>638</b> illustrated in, <figref idrefs="DRAWINGS">FIG. 6B</figref> and/or other sequences (e.g. ABBCCDDAA . . . ). Use of multiple elements of the array <b>570</b> may enable determination of multiple disparity estimations, e.g., shown by arrows <b>580</b>, <b>582</b>, <b>584</b>, <b>586</b>, <b>588</b>, <b>590</b>. In some implementations, the frames from individual sensor elements shown and described above with respect to <figref idrefs="DRAWINGS">FIGS. 5A-5C</figref> (e.g., <b>510</b>, <b>520</b>, <b>530</b>) may be acquired simultaneous with one another using, e.g., multiple camera synchronization. The disparity estimations corresponding to different sensor spacing (e.g., shown by arrows <b>580</b>, <b>582</b>, <b>584</b>, <b>586</b>, <b>588</b>, <b>590</b> in <figref idrefs="DRAWINGS">FIG. 5C</figref>) may be characterized by different dynamic range, different resolution, and/or precision, e.g., in accordance with Eqn. 1. By way of an illustration, closely spaced sensing elements (e.g., <b>572</b>, <b>574</b>) may be capable of determining distance to objects disposed farther from the array as compared to wide spaced elements (e.g., <b>572</b>-<b>578</b>). Wide spaced elements (e.g., <b>572</b>-<b>578</b>) may be capable of determining distance to objects with greater precision (e.g., lower uncertainty) as compared to estimates produced by closely spaced sensing elements (e.g., <b>572</b>, <b>574</b>).</div>
<div class="description-paragraph" id="p-0099" num="0098">In some implementations, multiple elements (e.g., <b>572</b>, <b>574</b>, <b>576</b>, <b>758</b>) may be disposed in a non-linear array (e.g., rectangular and/or concave) thereby providing multiple perspectives and/or views of the scene to the processing component. Some views/perspectives may, e.g., reveal objects that may be hidden and/or partially obscured in other perspectives, thereby enabling more robust determination of object distance and/or object detection. In some implementations, individual distance estimates (associated with individual camera pairs) may be combined using any appropriate methodologies (e.g., averaging, thresholding, median filtering), and/or other techniques to obtain a resultant distance estimate, characterized by greater precision and/or accuracy compared to individual estimates. In one or more implementations, a distance estimate associated with one camera pair may be selected as the resultant distance estimate, thereby enabling robust distance determination in presence of occlusions that may (at least partly) block the object in a given set of frames.</div>
<div class="description-paragraph" id="p-0100" num="0099"> <figref idrefs="DRAWINGS">FIG. 7</figref> depicts a motion extraction apparatus, according to one or more implementations. The apparatus <b>700</b> may comprise an encoder component <b>706</b> configured to encode input video stream <b>702</b>. The input <b>702</b> may comprise one or more frames received from an image sensor (e.g., charge-coupled device (CCD), CMOS device, and/or an active-pixel sensor (APS), photodiode arrays, and/or other image sensors). In one or more implementations, the input may comprise a pixel stream downloaded from a file. An example of such a file may include a stream of two-dimensional matrices of red green blue RGB values (e.g., refreshed at a 25 Hz or other suitable frame rate). It will be appreciated by those skilled in the art when given this disclosure that the above-referenced image parameters are merely exemplary, and many other image representations (e.g., bitmap, luminance-chrominance (YUV, YCbCr), cyan-magenta-yellow and key (CMYK), grayscale, and/or other image representations) are equally applicable to and useful with the various aspects of the present disclosure. Furthermore, data frames corresponding to other (non-visual) signal modalities such as sonograms, IR, radar or tomography images are equally compatible with the processing methodology of the disclosure, or yet other configurations.</div>
<div class="description-paragraph" id="p-0101" num="0100">The component <b>706</b> may comprise a specialized video encoder configured to implement video encoding comprising a motion estimation operation. In one or more implementations, the component <b>706</b> may comprise a dedicated integrated circuit (IC) disposed on a single or multiple die), a component of a processing system (e.g., video encoder block of a Snapdragon® system on a chip), an ASIC, an FPGA with a video encoder intellectual property (IP) core, an OEM printed circuit board, and/or other. Video encoding effectuated by the component <b>706</b> may comprise any applicable standard comprising motion estimation between current frame and preceding frame. Some encoding implementations may comprise MPEG-4, H.262, H.263, H.264, H.265 video encoder such as described above with respect to <figref idrefs="DRAWINGS">FIG. 4A</figref> supra.</div>
<div class="description-paragraph" id="p-0102" num="0101">The component <b>706</b> may provide encoded video output <b>708</b>. The output <b>708</b> may be characterized by a lower data rate (e.g., as represented by fewer bits per frame) as compared to the input video signal <b>702</b>. The output <b>708</b> may comprise pixel luminance and/or chromaticity data. The output <b>708</b> may comprise motion information, e.g., as illustrated in <figref idrefs="DRAWINGS">FIG. 9A</figref> which depicts output of a video encoder useful with the motion extraction methodology. In one or more implementations, the output illustrated in <figref idrefs="DRAWINGS">FIG. 9A</figref> may correspond to occurrence of an object, e.g., moving ball represented by a hashed circle <b>900</b> in <figref idrefs="DRAWINGS">FIG. 9A</figref> in input <b>702</b> of <figref idrefs="DRAWINGS">FIG. 7</figref>. The encoded output <b>708</b> may comprise a luminance component (also referred to as “luma”) depicted by area <b>902</b> in <figref idrefs="DRAWINGS">FIG. 9A</figref>. The encoded luminance may be comprised of a plurality of macroblocks <b>904</b>. Size of the macroblock may be configured in accordance with specifications of an application (e.g., encoding standard, video frame size, resolution, quality, refresh rate, bit depth, channel (e.g., luma, chroma), and/or other and be selected, for example, at 16×16 for luma channel, 8×8 for chroma channel for H.264 encoder.</div>
<div class="description-paragraph" id="p-0103" num="0102">The encoded output <b>708</b> (that also may be referred to as the compressed video) may comprise motion information, denoted by area <b>910</b> in <figref idrefs="DRAWINGS">FIG. 9A</figref>. Motion information may comprise one or more vectors (e.g., <b>916</b>) associated with one or more macroblock (e.g., <b>914</b>).</div>
<div class="description-paragraph" id="p-0104" num="0103">Compressed video <b>708</b> in <figref idrefs="DRAWINGS">FIG. 7</figref> may be provided to a processing component <b>710</b>. The component <b>710</b> may be configured to parse the compressed video stream <b>708</b> in order to obtain motion information (e.g., map of vectors <b>916</b> in <figref idrefs="DRAWINGS">FIG. 9A</figref>). By way of an illustration, the motion information may comprise a macroblock location L (e.g., index), x-component, and y-component of motion of pixels associated with the macroblock location L. The extracted motion information <b>712</b> may be provided to another component. Various uses of the motion information may be contemplated such as, for example, object detection by recognizing the shape of the surface of the object, and/or by using depth to segment the scene, gesture detection by determining the orientation of the hands or other body parts, and/or other. In some implementations, the compressed video may be provided via a pathway <b>714</b> to a target destination (e.g., general purpose processor for streaming to a display and/or other components).</div>
<div class="description-paragraph" id="p-0105" num="0104"> <figref idrefs="DRAWINGS">FIG. 8</figref> depicts a video processing system, comprising a differential motion extraction apparatus, according to one or more implementations. The system <b>800</b> of <figref idrefs="DRAWINGS">FIG. 8</figref> may be configured to receive sensory input <b>802</b>. In some implementations, the input <b>802</b> may comprise the input <b>702</b> described above with respect to <figref idrefs="DRAWINGS">FIG. 7</figref>. The input <b>802</b> may be encoded by a video encoder component <b>806</b>. In one or more implementations, the component <b>806</b> may comprise the component <b>706</b> described above with respect to <figref idrefs="DRAWINGS">FIG. 7</figref>. The component <b>806</b> may be configured to encode the input <b>802</b> using one or more encoding formats (e.g., H.264). The encoded signal <b>808</b> may be provided to component <b>810</b>. In some implementations, the component <b>810</b> may be configured to parse the encoded signal <b>808</b> to extract motion information <b>812</b> by, e.g., extracting from the compressed video data the P slice (P-frame) data which contains the motion information (x and y components) or the macroblock motion for all macroblocks covering the current frame. The extracted motion information may be used in controlling a robotic device.</div>
<div class="description-paragraph" id="p-0106" num="0105">The extracted motion information (e.g., <b>712</b>, <b>812</b> in <figref idrefs="DRAWINGS">FIGS. 7-8</figref>, respectively) may comprise horizontal and/or vertical displacement (e.g., the motion vector components (dx, dy)) of a pixel group (e.g., a macroblock) between the current frame and a preceding frame. In some implementations of video encoding useful with a pipeline-based multimedia framework (see, e.g., GStreamer framework, http://gstreamer.freedesktop.org/) the parsed motion information may be represented using the YUV color model. In one such implementation, the (U,V) channels may represent the (dx,dy) displacement and the Y channel may be used for representing additional information (e.g., indicates as to whether the current frame is the keyframe, macroblock size (e.g., 16×16, 8×8 and/or other size, and/or other information). Using the (Y,U,V) model to represent motion information may advantageously reduce computational load on, e.g., the component <b>820</b>, and enable access to motion information without necessitating further decoding/encoding operations in order to extract the motion vector components.</div>
<div class="description-paragraph" id="p-0107" num="0106">The input <b>802</b> may be processed by a processing component <b>820</b>. The component <b>820</b> may comprise an artificial neuron network (ANN) comprising a plurality of nodes. Individual nodes of the component <b>820</b> network may comprise neuron units characterized by a receptive field, e.g., region of space in which a presence of a stimulus may affect response of the neuron. In some implementations, the units may comprise spiking neurons and the ANN may comprise a spiking neuron network, (SNN). Various implementations of SNNs may be utilized consistent with the disclosure, such as, for example, those described in co-owned, and co-pending U.S. patent application Ser. No. 13/774,934, entitled “APPARATUS AND METHODS FOR RATE-MODULATED PLASTICITY IN A SPIKING NEURON NETWORK” filed Feb. 22, 2013, Ser. No. 13/763,005, entitled “SPIKING NETWORK APPARATUS AND METHOD WITH BIMODAL SPIKE-TIMING DEPENDENT PLASTICITY” filed Feb. 8, 2013, Ser. No. 13/152,105, filed Jun. 2, 2011 and entitled “APPARATUS AND METHODS FOR TEMPORALLY PROXIMATE OBJECT RECOGNITION”, Ser. No. 13/487,533, filed Jun. 4, 2012 and entitled “STOCHASTIC SPIKING NETWORK LEARNING APPARATUS AND METHODS”, Ser. No. 14/020,376, filed Sep. 9, 2013 and entitled “APPARATUS AND METHODS FOR EVENT-BASED PLASTICITY IN SPIKING NEURON NETWORKS”, Ser. No. 13/548,071, filed Jul. 12, 2012 and entitled “SPIKING NEURON NETWORK SENSORY PROCESSING APPARATUS AND METHODS”, commonly owned U.S. patent application Ser. No. 13/152,119, filed Jun. 2, 2011, entitled “SENSORY INPUT PROCESSING APPARATUS AND METHODS”, Ser. No. 13/540,429, filed Jun. 29, 2012 and entitled “SENSORY PROCESSING APPARATUS AND METHODS”, Ser. No. 13/623,820, filed Sep. 20, 2012 and entitled “APPARATUS AND METHODS FOR ENCODING OF SENSORY DATA USING ARTIFICIAL SPIKING NEURONS”, Ser. No. 13/623,838, filed Sep. 20, 2012 and entitled “SPIKING NEURON NETWORK APPARATUS AND METHODS FOR ENCODING OF SENSORY DATA”, Ser. No. 12/869,573, filed Aug. 26, 2010 and entitled “SYSTEMS AND METHODS FOR INVARIANT PULSE LATENCY CODING”, Ser. No. 12/869,583, filed Aug. 26, 2010, entitled “INVARIANT PULSE LATENCY CODING SYSTEMS AND METHODS”, Ser. No. 13/117,048, filed May 26, 2011 and entitled “APPARATUS AND METHODS FOR POLYCHRONOUS ENCODING AND MULTIPLEXING IN NEURONAL PROSTHETIC DEVICES”, Ser. No. 13/152,084, filed Jun. 2, 2011, entitled “APPARATUS AND METHODS FOR PULSE-CODE INVARIANT OBJECT RECOGNITION”, Ser. No. 13/239,255 filed Sep. 21, 2011, entitled “APPARATUS AND METHODS FOR SYNAPTIC UPDATE IN A PULSE-CODED NETWORK”, Ser. No. 13/487,576 entitled “DYNAMICALLY RECONFIGURABLE STOCHASTIC LEARNING APPARATUS AND METHODS”, filed Jun. 4, 2012, and U.S. Pat. No. 8,315,305, entitled “SYSTEMS AND METHODS FOR INVARIANT PULSE LATENCY CODING” issued Nov. 20, 2012, each of the foregoing being incorporated herein by reference in its entirety.</div>
<div class="description-paragraph" id="p-0108" num="0107">Receptive fields of the network 820 units may be configured to span several pixels with the input <b>802</b> frames so as to effectuate sparse transformation of the input <b>802</b>. Various applicable methodologies may be utilized in order to effectuate the sparse transformation, including, for example, those described in co-pending and co-owned U.S. patent application Ser. No. 13/540,429, entitled “SENSORY PROCESSING APPARATUS AND METHODS”, filed Jul. 2, 2012, and U.S. patent application Ser. No. 13/623,820, entitled “APPARATUS AND METHODS FOR ENCODING OF SENSORY DATA USING ARTIFICIAL SPIKING NEURONS”, filed on Sep. 20, 2012, each of the foregoing being incorporated herein by reference in its entirety. In some implementations, the encoding may comprise a sparse transformation, described in, e.g., U.S. patent application Ser. No. 14/191,383, entitled “APPARATUS AND METHODS FOR TEMPORAL PROXIMITY DETECTION”, filed on Feb. 26, 2014, the foregoing being incorporated herein by reference in its entirety.</div>
<div class="description-paragraph" id="p-0109" num="0108">The output <b>812</b> of the encoder <b>820</b> may be provided to the processing component <b>820</b>. In some implementations, the component <b>820</b> may use the motion information <b>812</b> in order to determine characteristics (e.g., location, dimension, shape, and/or other) of one or more objects in sensory input <b>802</b>. In one or more implementations, the component <b>820</b> may comprise an adaptive predictor component configured to determine a control output <b>826</b> for a robotic device (e.g., the vehicle <b>100</b>, <b>160</b> in <figref idrefs="DRAWINGS">FIGS. 1A-1B</figref>) based on the input <b>812</b> and/or inputs <b>802</b>, <b>812</b>. In some implementations of autonomous vehicle navigation, the input <b>812</b> and/or <b>802</b> may be used by the component <b>820</b> in order to predict control signal configured to cause the vehicle <b>160</b> in <figref idrefs="DRAWINGS">FIG. 1B</figref> to execute an obstacle avoidance action. Various implementations of predictors may be employed with the motion detection approach described herein, including, e.g., U.S. patent application Ser. No. 13/842,530, entitled “ADAPTIVE PREDICTOR APPARATUS AND METHODS”, filed on Mar. 15, 2013, the foregoing being incorporated herein by reference in its entirety.</div>
<div class="description-paragraph" id="p-0110" num="0109"> <figref idrefs="DRAWINGS">FIG. 9B</figref> illustrates motion of an object obtained from encoded video, according to one or more implementations. Hashed area <b>922</b> in <figref idrefs="DRAWINGS">FIG. 9B</figref> may represent luminance component of an image of a ball (e.g., <b>900</b> in <figref idrefs="DRAWINGS">FIG. 9A</figref>). The encoded output of <figref idrefs="DRAWINGS">FIG. 9A</figref> may comprise motion information, denoted by area <b>920</b> in. Motion information may comprise one or more vectors (e.g., <b>926</b>) associated with one or more macroblock (e.g., <b>924</b>). Encoded representations of <figref idrefs="DRAWINGS">FIGS. 9A-9B</figref> may be used to determine temporal distribution of motion associated with the ball <b>900</b>. Motion pattern comprising alternating opposing motion vectors <b>916</b>, <b>926</b> may be employed to communicate an action indication to a robotic device. In some implementations, a user may shake an object from left to right in front of a camera of an autonomous vehicle in order to indicate a target to be followed.</div>
<div class="description-paragraph" id="p-0111" num="0110"> <figref idrefs="DRAWINGS">FIG. 9C</figref> illustrates spatial distribution of motion extracted from encoded video, according to one or more implementations. The representation shown in <figref idrefs="DRAWINGS">FIG. 9C</figref> may comprise portion <b>930</b> comprising a first plurality of macroblocks <b>932</b> characterized by first motion direction <b>936</b>. The representation shown in <figref idrefs="DRAWINGS">FIG. 9C</figref> may comprise portion <b>940</b> comprising a second plurality of macroblocks <b>942</b> characterized by second motion direction <b>946</b>. The spatial motion map illustrated in <figref idrefs="DRAWINGS">FIG. 9C</figref> may be employed to communicate an action indication to a robotic device. In some implementations, a user wave arms (in a crisscross manner) in order to indicate to a robotic device a stop, and/or other command.</div>
<div class="description-paragraph" id="p-0112" num="0111">In some implementations (not shown) motion information for a given frame may be characterized by a plurality of different motion vectors due to, e.g., motion of different objects, camera pan/zoom operation, and/or video acquisition from a moving platform. By way of an illustration of operation of the robotic vehicle <b>160</b> of <figref idrefs="DRAWINGS">FIG. 1B</figref>, video signal obtained by the camera <b>166</b> may comprise a representation of human making gestures superimposed on a moving background. Detection of one motion associated with the gestures on a background motion may be referred to as differential motion detection. In some implementations, the background may be characterized by spatially coherent (uniform) motion. Background motion for a given frame may be estimated and removed. The resultant motion field may be analyzed in order to determine, e.g., hand gesture(s) and/or objects. In one or more implementations, a sequence of frames may be characterized by the background motion that is temporally coherent over timescale associated with the frame sequence. Background motion for the sequence of frames may be estimated and removed from individual frames within the sequence. The resultant motion field may be analyzed in order to determine, e.g., hand gesture(s) and/or objects.</div>
<div class="description-paragraph" id="p-0113" num="0112"> <figref idrefs="DRAWINGS">FIGS. 10-15</figref> illustrate methods <b>1000</b>, <b>1100</b>, <b>1200</b>, <b>1300</b>, <b>1400</b>, <b>1500</b> for determining and using motion information from encoded video. The operations of methods <b>1000</b>, <b>1100</b>, <b>1200</b>, <b>1300</b>, <b>1400</b>, <b>1500</b> presented below are intended to be illustrative. In some implementations, method <b>1000</b>, <b>1100</b>, <b>1200</b>, <b>1300</b>, <b>1400</b>, <b>1500</b> may be accomplished with one or more additional operations not described, and/or without one or more of the operations discussed. Additionally, the order in which the operations of method <b>1000</b>, <b>1100</b>, <b>1200</b>, <b>1300</b>, <b>1400</b>, <b>1500</b> are illustrated in <figref idrefs="DRAWINGS">FIGS. 10-15</figref> and described below is not intended to be limiting.</div>
<div class="description-paragraph" id="p-0114" num="0113">In some implementations, methods <b>1000</b>, <b>1100</b>, <b>1200</b>, <b>1300</b>, <b>1400</b>, <b>1500</b> may be implemented in one or more processing devices (e.g., a digital processor, an analog processor, a digital circuit designed to process information, an analog circuit designed to process information, a state machine, and/or other mechanisms for electronically processing information). The one or more processing devices may include one or more devices executing some or all of the operations of methods <b>1000</b>, <b>1100</b>, <b>1200</b>, <b>1300</b>, <b>1400</b>, <b>1500</b> in response to instructions stored electronically on an electronic storage medium. The one or more processing devices may include one or more devices configured through hardware, firmware, and/or software to be specifically designed for execution of one or more of the operations of methods <b>1000</b>, <b>1100</b>, <b>1200</b>, <b>1300</b>, <b>1400</b>, <b>1500</b>.</div>
<div class="description-paragraph" id="p-0115" num="0114"> <figref idrefs="DRAWINGS">FIG. 10</figref> illustrates a method of determining a salient feature using encoded video motion information, in accordance with one or more implementations.</div>
<div class="description-paragraph" id="p-0116" num="0115">Operations of method <b>1000</b> may be applied to processing of sensory data (e.g., audio, video, RADAR imagery, SONAR imagery, and/or other imagery), observation data, motor command activity in a robotic system, and/or other systems or data.</div>
<div class="description-paragraph" id="p-0117" num="0116">At operation <b>1002</b> of method <b>1000</b>, one or more a consecutive input video frames may be encoded. In one or more implementations, the frames may be provided by an image sensor (e.g., CCD, CMOS device, and/or APS, photodiode arrays, and/or other image sensors). In some implementations, the input may comprise a pixel stream downloaded from a file, such as a stream of two-dimensional matrices of red green blue RGB values (e.g., refreshed at a 25 Hz or other suitable frame rate). It will be appreciated by those skilled in the art when given this disclosure that the above-referenced image parameters are merely exemplary, and many other image representations (e.g., bitmap, luminance-chrominance YUV, YCbCr, CMYK, grayscale, and/other image representations) may be applicable to and useful with the various implementations. Data frames corresponding to other (non-visual) signal modalities such as sonograms, IR, radar or tomography images may be compatible with the processing methodology of the disclosure, and/or other configurations. The frames may form real-time (live) video. In one or more implementations, the encoding may comprise operations performed in accordance with any applicable encoding standard comprising motion estimation between successive frames (e.g., H.263, H.264, and/or other).</div>
<div class="description-paragraph" id="p-0118" num="0117">At operation <b>1004</b> encoded video may be parsed in order to obtain motion information. In some implementations, the motion information may comprise a plurality of motion vectors and their locations as associated with one or more macroblocks within the encoded frame (e.g., the vector <b>916</b> of macroblock <b>914</b> in <figref idrefs="DRAWINGS">FIG. 9A</figref>).</div>
<div class="description-paragraph" id="p-0119" num="0118">At operation <b>1006</b> a salient feature may be determined using motion information. In one or more implementations, the feature determination may be based on analysis of motion spatial map within a given frame (e.g., the motion map comprising the area <b>930</b>, <b>940</b> in <figref idrefs="DRAWINGS">FIG. 9C</figref>). In one or more implementations, the feature determination may be configured based on analysis of motion temporal characteristics (e.g., persistence of motion features in a given location over multiple frames, comparing motion at a given location between two or more frames, and/or other).</div>
<div class="description-paragraph" id="p-0120" num="0119"> <figref idrefs="DRAWINGS">FIG. 11</figref> illustrates a method of data processing useful for determining features, in accordance with one or more implementations.</div>
<div class="description-paragraph" id="p-0121" num="0120">At operation <b>1102</b> live video may be obtained during execution of a task. In some implementations of robotic vehicle navigation, the video may be obtained with a video camera disposed on the vehicle. The video stream may be encoded using any applicable standard comprising motion estimation operation (e.g., H.263, H.264, and/or other).</div>
<div class="description-paragraph" id="p-0122" num="0121">At operation <b>1104</b> motion information may be determined from the encoded video stream. In some implementations, the encoded video stream may be parsed in order to obtain motion information. In some implementations, the motion information may comprise a plurality of motion vectors and their locations as associated with one or more macroblocks within the encoded frame (e.g., the vector <b>916</b> of macroblock <b>914</b> in <figref idrefs="DRAWINGS">FIG. 9A</figref>).</div>
<div class="description-paragraph" id="p-0123" num="0122">At operation <b>1106</b> a location of an object within video frame may be determined using motion information obtained at operation <b>1104</b>. In one or more implementations, the location determination may be based on temporal and/or spatial persistence (coherence) of motion over a given area and/or over several frames. By way of an illustration, occurrence of a plurality of macroblocks characterized by motion vectors within a given margin from one another (e.g., 5-20% in one implementation) in a given frame may indicate a moving object.</div>
<div class="description-paragraph" id="p-0124" num="0123">At operation <b>1108</b> the object associated with the location determined at operation <b>806</b> may be related to a task action. Based on the action determination, a control signal may be provided. In some implementations, the control signal provision may be configured based on operation of an adaptive predictor, e.g., such as described in U.S. patent application Ser. No. 13/842,530, entitled “ADAPTIVE PREDICTOR APPARATUS AND METHODS”, filed on Mar. 15, 2013, incorporated supra.</div>
<div class="description-paragraph" id="p-0125" num="0124">At operation <b>1110</b>, the action may be executed. By way of an illustration, the object may comprise a ball <b>174</b> in <figref idrefs="DRAWINGS">FIG. 1A</figref>, the motion information may indicate the ball moving to the left of the vehicle, the task may comprise target pursuit, and the action may comprise a left turn by the vehicle.</div>
<div class="description-paragraph" id="p-0126" num="0125"> <figref idrefs="DRAWINGS">FIG. 12</figref> is a logical flow diagram illustrating a method of executing an action configured based on a gesture detected using motion information, in accordance with one or more implementations.</div>
<div class="description-paragraph" id="p-0127" num="0126">At operation <b>1202</b> motion information may be determined using one or more encoded video frames. In some implementations, the motion information may comprise motion vectors due to gestures of a human (e.g., vectors <b>936</b>, <b>946</b> in <figref idrefs="DRAWINGS">FIG. 9B</figref>).</div>
<div class="description-paragraph" id="p-0128" num="0127">At operation <b>1204</b> a spatio-temporal distribution of the motion information may be determined. In some implementations of spatial motion distribution, the motion map may comprise more areas of macroblocks (e.g., the area <b>910</b> in <figref idrefs="DRAWINGS">FIG. 9A and/or 90</figref> in <figref idrefs="DRAWINGS">FIG. 9C</figref>) characterized by similar motion vector components. (e.g., components of vector <b>946</b> in <figref idrefs="DRAWINGS">FIG. 6C</figref>). In some implementations, temporal motion distribution may be determined by analyzing motion associated with a portion of the frame (e.g., the area <b>940</b> in <figref idrefs="DRAWINGS">FIG. 6C</figref>) over a plurality of consecutive frames.</div>
<div class="description-paragraph" id="p-0129" num="0128">At operation <b>1206</b> a gesture may be determined based on a spatio-temporal pattern within the motion distribution. By way of an illustration, a pattern of alternating motion vectors of a rectangular area within the frame may correspond to a crisscross motion of arms by the user indicating an alert (e.g., a stop) command to the robotic device. In some implementations, motion information for a given frame may be characterized by a plurality of different motion vectors due to, e.g., motion of different objects, camera pan/zoom operation, and/or video acquisition from a moving platform. By way of an illustration of operation of the robotic vehicle <b>160</b> of <figref idrefs="DRAWINGS">FIG. 1B</figref>, video signal obtained by the camera <b>166</b> may comprise a representation of human making gestures superimposed on a moving background.</div>
<div class="description-paragraph" id="p-0130" num="0129">At operation <b>1208</b>, an action may be executed in accordance with the gesture determined at operation <b>1206</b>. For example, upon detecting the crisscross arm motion the robotic device may stop trajectory navigation.</div>
<div class="description-paragraph" id="p-0131" num="0130">The motion-based gesture detection methodology described herein may be employed for operation of a robotic appliance and/or remotely operated device. In some implementations, gesture detection may be effectuated by a spoofing controller, e.g., such as described in U.S. patent application Ser. No. 14/244,892, entitled “ADAPTIVE PREDICTOR APPARATUS AND METHODS”, filed on Apr. 3, 2014, incorporated herein by reference in its entirety. The spoofing controller may be trained to develop associations between the detected gestures and one or more remote control commands (by e.g., an IR remote operating a home appliance (TV)). The developed associations may enable the spoofing controller to operate the TV in accordance with gestured of a user in lieu of the remote controller commands.</div>
<div class="description-paragraph" id="p-0132" num="0131">A commercially available off-the shelf hardware video encoder (e.g., <b>1006</b> in <figref idrefs="DRAWINGS">FIG. 10</figref>) may be used to provide a compressed video stream. Typically, hardware encoders may be utilized in order to reduce video data rate in order to reduce storage, and/or bandwidth load associated with manipulation of video information. Motion extraction methodology described herein may advantageously enable determination of motion information by an application device using available compressed video albeit that is used for other purposes (e.g., reduction in storage and/or bandwidth). Use of available compressed video, comprising motion estimation data (e.g., MPEG-4) may substantially reduce computational load associated with motion determination, compared to existing techniques such as optic flow, and/or motion estimation algorithms such as but not limited to: block-matching algorithm, phase correlation, as well as determining locations of features and estimating the motion of those features.</div>
<div class="description-paragraph" id="p-0133" num="0132"> <figref idrefs="DRAWINGS">FIG. 13</figref> illustrates a method of determining a depth of visual scene using encoded interleaved stereo image information, in accordance with one or more implementations.</div>
<div class="description-paragraph" id="p-0134" num="0133">At operation <b>1302</b> of method <b>1300</b>, a monocular frame configuration may be obtained using a stereo image of a visual scene. In some implementations, the monocular frame configuration may comprise an interleaved frame sequence <b>324</b>, <b>342</b> described above with respect to <figref idrefs="DRAWINGS">FIGS. 3A-3B</figref>. In one or more implementations of multi-camera image acquisition, the monocular frame configuration may comprise an interleaved frame sequence such as shown in <figref idrefs="DRAWINGS">FIGS. 6A and/or 6B</figref>.</div>
<div class="description-paragraph" id="p-0135" num="0134">At operation <b>1304</b> monocular frame sequence may be encoded using a motion estimation encoder. In some implementations, the encoding may be performed by a specialized video encoder comprising a motion estimation operation (e.g., MPEG-4, H.264, or other).</div>
<div class="description-paragraph" id="p-0136" num="0135">At operation <b>1306</b> depth of visual scene may be determined using motion information of the encoded data obtained at operation <b>1304</b>. In one or more implementations, the motion information may be obtained by to parsing the compressed video stream (e.g., <b>422</b> in <figref idrefs="DRAWINGS">FIG. 4A</figref>). By way of an illustration, the motion information may comprise a macroblock location L (e.g., index), x-component, and y-component of motion of pixels associated with the macroblock location L. The extracted motion information may be used for disparity and or distance determination. Various uses of the depth information may be contemplated such as, for example, object detection, object localization, distance estimation, trajectory planning, gesture detection, and/or other.</div>
<div class="description-paragraph" id="p-0137" num="0136"> <figref idrefs="DRAWINGS">FIG. 14</figref> illustrates a method of determining distance to objects using motion of interleaved image stream, in accordance with one or more implementations.</div>
<div class="description-paragraph" id="p-0138" num="0137">At operation <b>1402</b> of method <b>1400</b>, frames from multiple cameras may be combined into an interleaved frame stream. In one or more implementations the interleaved frame stream may comprise a frame sequence such as shown in <figref idrefs="DRAWINGS">FIGS. 6A and/or 6B</figref>.</div>
<div class="description-paragraph" id="p-0139" num="0138">At operation <b>1404</b> the interleaved frame sequence may be encoded using a motion estimation encoder. In some implementations, the encoding may be performed by a specialized video encoder comprising a motion estimation operation (e.g., MPEG-4, H.264, or other).</div>
<div class="description-paragraph" id="p-0140" num="0139">At operation <b>1406</b> an object may be detected based on a spatio-temporal pattern within the motion distribution of the encoded data. In one or more implementations, the motion information may be obtained by to parsing the compressed video stream (e.g., <b>422</b> in <figref idrefs="DRAWINGS">FIG. 4A</figref> comprising, e.g., encoded frames <b>356</b>, <b>360</b>, <b>364</b> shown and described with respect to <figref idrefs="DRAWINGS">FIG. 3B</figref>). Object detection may be effectuated using any applicable methodologies including these described above with respect to <figref idrefs="DRAWINGS">FIGS. 9A-9C</figref>.</div>
<div class="description-paragraph" id="p-0141" num="0140">At operation <b>1408</b>, distance to the object identified at operation <b>1406</b> may be determined. The distance determination may be configured based on the disparity data that may be obtained from the motion information of the encoded data (e.g., the frames <b>354</b>, <b>358</b>, <b>362</b> in <figref idrefs="DRAWINGS">FIG. 3B</figref>). Various uses of the distance information may be contemplated such as, for example, object detection, trajectory planning, gesture detection, obstacle avoidance, and/or other.</div>
<div class="description-paragraph" id="p-0142" num="0141"> <figref idrefs="DRAWINGS">FIG. 15</figref> illustrates a method of executing an action configured based on detecting an object in motion information, in accordance with one or more implementations.</div>
<div class="description-paragraph" id="p-0143" num="0142">At operation <b>1502</b> of method <b>1500</b> interleaved frame stream may be encoded using a motion estimation encoder. In some implementations, the encoding may be performed by a specialized video encoder comprising a motion estimation operation (e.g., MPEG-4, H.264, or other). into encoded data using encoder with motion estimation</div>
<div class="description-paragraph" id="p-0144" num="0143">At operation <b>1504</b> distance to the object may be determined using disparity determined from the motion information of the encoded data. The distance determination may be configured based on the disparity data that may be obtained from the motion information of the encoded data (e.g., the frames <b>354</b>, <b>358</b>, <b>362</b> in <figref idrefs="DRAWINGS">FIG. 3B</figref>).</div>
<div class="description-paragraph" id="p-0145" num="0144">At operation <b>1506</b> an action may be associated with the object parameters determined at operation <b>1504</b>. In some implementations, the object parameters may comprise object features (e.g., shape, color, identity), location, distance, speed, and/or other. By way of an illustration, the object may comprise a ball <b>112</b> in <figref idrefs="DRAWINGS">FIG. 1A</figref> rolling across the path of the vehicle <b>100</b>. The distance to the ball <b>112</b> and the ball motion data may indicate that the vehicle <b>100</b> may collide with the ball <b>112</b>. The action may comprise a turn left/right and/or reducing the speed of the vehicle <b>100</b>.</div>
<div class="description-paragraph" id="p-0146" num="0145">At operation <b>1510</b> the action may be executed. Action execution may be configured based on output of an adaptive predictor apparatus configured to predict control signal for the robotic vehicle <b>100</b>. In some implementations, the predictor may be operated in accordance with a learning process such as described, for example, in U.S. patent application Ser. No. 13/842,530, entitled “ADAPTIVE PREDICTOR APPARATUS AND METHODS”, filed on Mar. 15, 2013, the foregoing being incorporated supra.</div>
<div class="description-paragraph" id="p-0147" num="0146"> <figref idrefs="DRAWINGS">FIGS. 16A-16D</figref> illustrate gestures of a human operator used for communicating control indications to a robotic device comprising distance determination apparatus described herein, in accordance with one or more implementations.</div>
<div class="description-paragraph" id="p-0148" num="0147"> <figref idrefs="DRAWINGS">FIG. 16A</figref> is a top view of a user and may illustrate a base posture of the user. <figref idrefs="DRAWINGS">FIG. 16B</figref> may depict user gestures <b>1600</b> communicating a right turn action to a robotic device (e.g., the vehicle <b>100</b> in <figref idrefs="DRAWINGS">FIG. 1A</figref>. The robotic device <b>100</b> may utilize stereo images provided by the cameras <b>106</b>, <b>108</b> in order to detect position of the user arms <b>1608</b>, <b>1608</b>. In some implementations, the arm <b>1608</b>, <b>1604</b> position may be determined using the distance determination methodology configured based on encoding interleaved left/right portions of the stereo imagery. By way of an illustration, the gesture in <figref idrefs="DRAWINGS">FIG. 16B</figref> may be determining based on a comparison of distance between the robot and the user arms in positions <b>1604</b>, <b>1608</b> in <figref idrefs="DRAWINGS">FIG. 16B</figref> relative the user arms in position <b>1624</b> in <figref idrefs="DRAWINGS">FIG. 16A</figref>. In one or more implementations, the gesture in <figref idrefs="DRAWINGS">FIG. 16B</figref> may be determining based on a comparison of distance between the robot and the user arms in positions <b>1604</b>, <b>1608</b> relative the user head <b>1602</b> in <figref idrefs="DRAWINGS">FIG. 16B</figref>.</div>
<div class="description-paragraph" id="p-0149" num="0148"> <figref idrefs="DRAWINGS">FIG. 16C</figref> is a side view of the user and may depict user gesture <b>1610</b> communicating a stop action to a robotic device (e.g., the vehicle <b>100</b> in <figref idrefs="DRAWINGS">FIG. 1A</figref>). The robotic device <b>100</b> may utilize stereo images provided by the cameras <b>106</b>, <b>108</b> in order to detect position of the user arms, head <b>1642</b>, <b>1612</b>, and/or hands <b>1614</b>, <b>1644</b>. In some implementations, the hand <b>1642</b>, <b>1644</b> position may be determined using the distance determination methodology configured based on encoding interleaved left/right portions of the stereo imagery. By way of an illustration, the gesture in <figref idrefs="DRAWINGS">FIG. 16C</figref> may be obtained based on a comparison of distance between the robot and the user hands in position <b>1614</b> in <figref idrefs="DRAWINGS">FIG. 16C</figref> relative the user hand in position <b>1644</b> in <figref idrefs="DRAWINGS">FIG. 16D</figref>. In one or more implementations, the gesture in <figref idrefs="DRAWINGS">FIG. 16C</figref> may be determined based on a comparison of distance between the robot and the user hand in position <b>1614</b> relative the user head <b>1612</b> in <figref idrefs="DRAWINGS">FIG. 16C</figref>. In some implementations (not shown) the user may communicate an indication to the robotic device by, e.g., appearing in view of the camera. By way of an illustrating, the user stepping in front of the vehicle may indicated to the vehicle a stop action</div>
<div class="description-paragraph" id="p-0150" num="0149">The present disclosure also contemplates a computerized controller apparatus for implementing, inter alia, motion and/or distance determination methodology in accordance with one or more implementations.</div>
<div class="description-paragraph" id="p-0151" num="0150">The controller apparatus (not shown) may comprise a processing module configured to receive sensory input from sensory block (e.g., cameras <b>106</b>, <b>108</b> in <figref idrefs="DRAWINGS">FIG. 1A</figref>). In some implementations, the sensory module may comprise audio input/output portion. The processing module may be configured to implement signal processing functionality (e.g., distance estimation, object detection based on motion maps, and/or other).</div>
<div class="description-paragraph" id="p-0152" num="0151">The controller apparatus may comprise memory configured to store executable instructions (e.g., operating system and/or application code, raw and/or processed data such as raw image fames and/or object views, teaching input, information related to one or more detected objects, and/or other information).</div>
<div class="description-paragraph" id="p-0153" num="0152">In some implementations, the processing module may interface with one or more of the mechanical, sensory, electrical, power components, communications interface, and/or other components via driver interfaces, software abstraction layers, and/or other interfacing techniques. Thus, additional processing and memory capacity may be used to support these processes. However, it will be appreciated that these components may be fully controlled by the processing module. The memory and processing capacity may aid in processing code management for the controller apparatus (e.g. loading, replacement, initial startup and/or other operations). Consistent with the present disclosure, the various components of the device may be remotely disposed from one another, and/or aggregated. For example, the instructions operating the haptic learning process may be executed on a server apparatus that may control the mechanical components via network or radio connection. In some implementations, multiple mechanical, sensory, electrical units, and/or other components may be controlled by a single robotic controller via network/radio connectivity.</div>
<div class="description-paragraph" id="p-0154" num="0153">The mechanical components of the controller apparatus may include virtually any type of device capable of motion and/or performance of a desired function or task. Examples of such devices may include one or more of motors, servos, pumps, hydraulics, pneumatics, stepper motors, rotational plates, micro-electro-mechanical devices (MEMS), electroactive polymers, shape memory alloy (SMA) activation, and/or other devices. The sensor devices may interface with the processing module, and/or enable physical interaction and/or manipulation of the device.</div>
<div class="description-paragraph" id="p-0155" num="0154">The sensory devices may enable the controller apparatus to accept stimulus from external entities. Examples of such external entities may include one or more of video, audio, haptic, capacitive, radio, vibrational, ultrasonic, infrared, motion, and temperature sensors radar, lidar and/or sonar, and/or other external entities. The module may implement logic configured to process user commands (e.g., gestures) and/or provide responses and/or acknowledgment to the user.</div>
<div class="description-paragraph" id="p-0156" num="0155">The electrical components may include virtually any electrical device for interaction and manipulation of the outside world. Examples of such electrical devices may include one or more of light/radiation generating devices (e.g. LEDs, IR sources, light bulbs, and/or other devices), audio devices, monitors/displays, switches, heaters, coolers, ultrasound transducers, lasers, and/or other electrical devices. These devices may enable a wide array of applications for the apparatus in industrial, hobbyist, building management, medical device, military/intelligence, and/or other fields.</div>
<div class="description-paragraph" id="p-0157" num="0156">The communications interface may include one or more connections to external computerized devices to allow for, inter alia, management of the controller apparatus. The connections may include one or more of the wireless or wireline interfaces discussed above, and may include customized or proprietary connections for specific applications. The communications interface may be configured to receive sensory input from an external camera, a user interface (e.g., a headset microphone, a button, a touchpad, and/or other user interface), and/or provide sensory output (e.g., voice commands to a headset, visual feedback, and/or other sensory output).</div>
<div class="description-paragraph" id="p-0158" num="0157">The power system may be tailored to the needs of the application of the device. For example, for a small hobbyist robot or aid device, a wireless power solution (e.g. battery, solar cell, inductive (contactless) power source, rectification, and/or other wireless power solution) may be appropriate. However, for building management applications, battery backup/direct wall power may be superior, in some implementations. In addition, in some implementations, the power system may be adaptable with respect to the training of the apparatus <b>1800</b>. Thus, the controller apparatus may improve its efficiency (to include power consumption efficiency) through learned management techniques specifically tailored to the tasks performed by the controller apparatus.</div>
<div class="description-paragraph" id="p-0159" num="0158">Various aspects of the disclosure may advantageously be applied to design and operation of apparatus configured to process sensory data. Implementations of the principles of the disclosure may be applicable to detecting objects by a wide variety of stationary and portable video devices, such as, for example, smart phones, portable communication devices, notebook, netbook and tablet computers, surveillance camera systems, and practically any other computerized device configured to process vision data. The motion information may be used as a proxy for optic flow (estimated motion (dx,dy) on a grid across the frame of the video). Use of available hardware encoders to obtain motion data may reduce energy use by portable devices, enable motion detection on higher resolution video (e.g., resolutions greater than 320×240), improve motion detection resolution in order to, e.g., detect gestures, compared to optic flow detection techniques.</div>
<div class="description-paragraph" id="p-0160" num="0159">Interleaving of frames from multiple spatially displaced cameras may enable determination of binocular disparity between pairs of camera images using motion estimation. Use of an off-the shelf commercially available hardware video encoder (e.g., MPEG-4, H.265 and/or other encoder) comprising motion estimation, may substantially reduce cost, size, energy use of a motion estimation component, compared to use of optical flow for determining motion. Encoded into video may be parsed to obtain motion information. Motion corresponding to a pair of frames from displaced cameras may be interpreted as a measure of disparity. The disparity may be utilized in order to determine depth of visual scene and/or distance to objects within visual scene. By way of an illustration, embodying a motion determination component and/or a distance determination component of the disclosure in a robotic vehicle (e.g., <b>100</b>, <b>1700</b> in FIGS. <b>1</b>A, <b>17</b>) may extend duration of autonomous operation of the robotic apparatus due to, in part, lower energy use that may be associated with motion/distance detection based on video encoded using hardware encoder, as compared to using video processing in a CPU (e.g., optical flow, and/or pixel block matching). The increased autonomy may be characterized by the robotic device capability to perform a given action (e.g., a flight route and/or surveillance route) an additional number of times without recharging, and/or being capable of completing longer routes on a given charge as compared to the prior art solutions. In one or more implementations, the reduced energy use may be leveraged for producing a smaller, lighter and/or less costly robot that may be capable of performing the action (e.g., navigate a given route) compared to the comparable device of the prior art.</div>
<div class="description-paragraph" id="p-0161" num="0160">An autonomous robotic device comprising a hardware video encoder may be capable to perform motion estimation for obstacle avoidance, tracking moving objects, stabilization, platform and/or enabling the robot to learn its own self motion. By way of an illustration, a robotic device may be configured to follow a target (e.g., a person, a ball <b>112</b> in <figref idrefs="DRAWINGS">FIG. 1A</figref>, and/other object) at a distance (e.g., <b>110</b> in <figref idrefs="DRAWINGS">FIG. 1A</figref>). In one or more implementations of tracking, the robotic device may be configured to maintain distance from target within a range (e.g., not to exceed 50 meters, and not to approach closer than 2 meters when following a for vehicle, and/or not to exceed 5 meters, and not to approach closer than 0.25 meters when following the ball <b>112</b> in <figref idrefs="DRAWINGS">FIG. 1A</figref>. In one or more implementations of object tracking, approach, avoid, and/or other, controller of the robotic device may be configured to determine distance to the target and motion of the target using, e.g., the alternating interleaving methodology shown and described with respect to <figref idrefs="DRAWINGS">FIG. 3B</figref>. In some implementations, the distance may be determined using the interleaving methodology; the motion may be determined using the video encoding methodology.</div>
<div class="description-paragraph" id="p-0162" num="0161"> <figref idrefs="DRAWINGS">FIG. 17</figref> illustrates use of distance determination methodology by an unmanned robotic apparatus configured for autonomous navigation, in accordance with one or more implementations. The unmanned autonomous vehicle (AUV) <b>1700</b> may comprise a plurality of cameras <b>1702</b> disposed spatially from one another. Video stream provided by the cameras <b>1702</b> may be interleaved and encoded using any applicable methodology described herein (e.g., with respect to <figref idrefs="DRAWINGS">FIGS. 3A-3B, 6A-6B</figref>, and/or <b>9</b>A-<b>9</b>C). The encoding may enable controller of the vehicle <b>1700</b> (e.g., <b>400</b>, <b>440</b> in <figref idrefs="DRAWINGS">FIGS. 4A-4B</figref>, and/or <b>700</b>, <b>800</b> <figref idrefs="DRAWINGS">FIGS. 7-8</figref>) do determine distance <b>1706</b> between the vehicle <b>1700</b> and the landing location <b>1712</b>, and/or distance <b>1718</b> to obstacles (e.g., <b>1710</b>). The controller may utilize the distance and/or vehicle motion information to control actuators <b>1704</b> when landing, during take-off and or navigating around obstacles.</div>
<div class="description-paragraph" id="p-0163" num="0162">In some, implementations, the motion detection methodology described herein may be employed for detecting salient objects in video input. The saliency of an item (such as an object, a person, a pixel, and/or other) may be described by a characteristic by which the item may stand out relative to its neighbors. For example, a salient vehicle may comprise a vehicle that may be moving differently (e.g., going slower/faster than the rest of the traffic, weaving from lane to lane) compared to the rest of the traffic. A salient object for target approach may comprise a stationary and/or moving ball on a moving background due to self-motion by the vehicle.</div>
<div class="description-paragraph" id="p-0164" num="0163">Implementations of the principles of the disclosure may be further applicable to a wide assortment of applications including computer-human interaction (e.g., recognition of gestures, voice, posture, face, and/or other interactions), controlling processes (e.g., processes associated with an industrial robot, autonomous and other vehicles, and/or other processes), augmented reality applications, access control (e.g., opening a door based on a gesture, opening an access way based on detection of an authorized person), detecting events (e.g., for visual surveillance or people or animal counting, tracking).</div>
<div class="description-paragraph" id="p-0165" num="0164">A video processing system of the disclosure may be implemented in a variety of ways such as, for example, a software library, an IP core configured for implementation in a programmable logic device (e.g., FPGA), an ASIC, a remote server, comprising a computer readable apparatus storing computer executable instructions configured to perform feature detection. Myriad other applications exist that will be recognized by those of ordinary skill given the present disclosure.</div>
<div class="description-paragraph" id="p-0166" num="0165">Although the system(s) and/or method(s) of this disclosure have been described in detail for the purpose of illustration based on what is currently considered to be the most practical and preferred implementations, it is to be understood that such detail is solely for that purpose and that the disclosure is not limited to the disclosed implementations, but, on the contrary, is intended to cover modifications and equivalent arrangements that are within the spirit and scope of the appended claims. For example, it is to be understood that the present disclosure contemplates that, to the extent possible, one or more features of any implementation can be combined with one or more features of any other implementation.</div>
<div class="description-paragraph" id="p-0167" num="0166">
<tables id="TABLE-US-00001" num="00001">
<patent-tables colsep="0" frame="none" pgwide="1" rowsep="0">
<table align="left" class="description-table" cols="1" colsep="0" rowsep="0" width="100%">
<thead>
<tr class="description-tr">
<td align="center" class="description-td" colspan="1" nameend="1" namest="1" rowsep="1"> </td>
</tr>
<tr class="description-tr">
<td class="description-td">EXHIBIT I—EXEMPLARY COMPUTER CODE</td>
</tr>
<tr class="description-tr">
<td class="description-td">© Copyright 2014 Brain Corporation, All rights reserved</td>
</tr>
<tr class="description-tr">
<td align="center" class="description-td" colspan="1" nameend="1" namest="1" rowsep="1"> </td>
</tr>
</thead>
<tbody><tr class="description-tr">
<td class="description-td"> </td>
</tr>
</tbody></table>
<table align="left" class="description-table" cols="1" colsep="0" rowsep="0" width="100%">
<tbody><tr class="description-tr">
<td class="description-td">void ff_h263_update_motion_val(MpegEncContext * s){</td>
</tr>
<tr class="description-tr">
<td class="description-td"> const int mb_xy = s-&gt;mb_y * s-&gt;mb_stride + s-&gt;mb_x;</td>
</tr>
<tr class="description-tr">
<td class="description-td">    //FIXME a lot of that is only needed for !low_delay</td>
</tr>
<tr class="description-tr">
<td class="description-td"> const int wrap = s-&gt;b8_stride;</td>
</tr>
<tr class="description-tr">
<td class="description-td"> const int xy = s-&gt;block_index[0];</td>
</tr>
<tr class="description-tr">
<td class="description-td"> int motion_x=0, motion_y=0;</td>
</tr>
<tr class="description-tr">
<td class="description-td"> const int block_size= 8&gt;&gt;s-&gt;avctx-&gt;lowres;</td>
</tr>
<tr class="description-tr">
<td class="description-td"> s-&gt;current_picture.mbskip_table[mb_xy]= s-&gt;mb_skipped;</td>
</tr>
<tr class="description-tr">
<td class="description-td"> if(s-&gt;mv_type != MV_TYPE_8X8){</td>
</tr>
<tr class="description-tr">
<td class="description-td">  if (s-&gt;mb_intra) {</td>
</tr>
<tr class="description-tr">
<td class="description-td">   motion_x = 0;</td>
</tr>
<tr class="description-tr">
<td class="description-td">   motion_y = 0;</td>
</tr>
<tr class="description-tr">
<td class="description-td">  } else if (s-&gt;mv_type == MV_TYPE_16X16) {</td>
</tr>
<tr class="description-tr">
<td class="description-td">   motion_x = s-&gt;mv[0][0][0];</td>
</tr>
<tr class="description-tr">
<td class="description-td">   motion_y = s-&gt;mv[0][0][1];</td>
</tr>
<tr class="description-tr">
<td class="description-td">  } else /*if (s-&gt;mv_type == MV_TYPE_FIELD)*/ {</td>
</tr>
<tr class="description-tr">
<td class="description-td">   int i;</td>
</tr>
<tr class="description-tr">
<td class="description-td">   motion_x = s-&gt;mv[0][0][0] + s-&gt;mv[0][1][0];</td>
</tr>
<tr class="description-tr">
<td class="description-td">   motion_y = s-&gt;mv[0][0][1] + s-&gt;mv[0][1][1];</td>
</tr>
<tr class="description-tr">
<td class="description-td">   motion_x = (motion_x&gt;&gt;1) | (motion_x&amp;1);</td>
</tr>
<tr class="description-tr">
<td class="description-td">   for(i=0; i&lt;2; i++){</td>
</tr>
<tr class="description-tr">
<td class="description-td">    s-&gt;p_field_mv_table[i][0][mb_xy][0]= s-&gt;mv[0][i][0];</td>
</tr>
<tr class="description-tr">
<td class="description-td">    s-&gt;p_field_mv_table[i][0][mb_xy][1]= s-&gt;mv[0][i][1];</td>
</tr>
<tr class="description-tr">
<td class="description-td">   }</td>
</tr>
<tr class="description-tr">
<td class="description-td">   s-&gt;current_picture.ref_index[0][4*mb_xy ]=</td>
</tr>
<tr class="description-tr">
<td class="description-td">   s-&gt;current_picture.ref_index[0][4*mb_xy + 1]= s-&gt;field_select[0][0];</td>
</tr>
<tr class="description-tr">
<td class="description-td">   s-&gt;current_picture.ref_index[0][4*mb_xy + 2]=</td>
</tr>
<tr class="description-tr">
<td class="description-td">   s-&gt;current_picture.ref_index[0][4*mb_xy + 3]= s-&gt;field_select[0][1];</td>
</tr>
<tr class="description-tr">
<td class="description-td">  }</td>
</tr>
<tr class="description-tr">
<td class="description-td">  /* no update if 8X8 because it has been done during parsing */</td>
</tr>
<tr class="description-tr">
<td class="description-td">  s-&gt;current_picture.motion_val[0][xy][0] = motion_x;</td>
</tr>
<tr class="description-tr">
<td class="description-td">  s-&gt;current_picture.motion_val[0][xy][1] = motion_y;</td>
</tr>
<tr class="description-tr">
<td class="description-td">  s-&gt;current_picture.motion_val[0][xy + 1][0] = motion_x;</td>
</tr>
<tr class="description-tr">
<td class="description-td">  s-&gt;current_picture.motion_val[0][xy + 1][1] = motion_y;</td>
</tr>
<tr class="description-tr">
<td class="description-td">  s-&gt;current_picture.motion_val[0][xy + wrap][0] = motion_x;</td>
</tr>
<tr class="description-tr">
<td class="description-td">  s-&gt;current_picture.motion_val[0][xy + wrap][1] = motion_y;</td>
</tr>
<tr class="description-tr">
<td class="description-td">  s-&gt;current_picture.motion_val[0][xy + 1 + wrap][0] = motion_x;</td>
</tr>
<tr class="description-tr">
<td class="description-td">  s-&gt;current_picture.motion_val[0][xy + 1 + wrap][1] = motion_y;</td>
</tr>
<tr class="description-tr">
<td class="description-td">  if(s-&gt;avctx-&gt;debug_mv) {</td>
</tr>
<tr class="description-tr">
<td class="description-td">   for (int i=0;i&lt;2*block_size;i++) memset(s-&gt;dest[0] + i * s-&gt;linesize, 120 + s-</td>
</tr>
<tr class="description-tr">
<td class="description-td">&gt;current_picture.key_frame * 5, 2*block_size);</td>
</tr>
<tr class="description-tr">
<td class="description-td">   for (int i=0;i&lt;block_size;i++) memset(s-&gt;dest[1] + i * s-&gt;uvlinesize, 128 + motion_x,</td>
</tr>
<tr class="description-tr">
<td class="description-td">block_size);</td>
</tr>
<tr class="description-tr">
<td class="description-td">   for (int i=0;i&lt;block_size;i++) memset(s-&gt;dest[2] + i * s-&gt;uvlinesize, 128 + motion_y,</td>
</tr>
<tr class="description-tr">
<td class="description-td">block_size);</td>
</tr>
<tr class="description-tr">
<td class="description-td">  }</td>
</tr>
<tr class="description-tr">
<td class="description-td"> } else {</td>
</tr>
<tr class="description-tr">
<td class="description-td">  if(s-&gt;avctx-&gt;debug_mv) {</td>
</tr>
<tr class="description-tr">
<td class="description-td">   for (int i=0;i&lt;block_size*2;i++) memset(s-&gt;dest[0] + i * s-&gt;uvlinesize, 130 + block_size*2);</td>
</tr>
<tr class="description-tr">
<td class="description-td">   for (int ywrap=0, y=0;y&lt;2;ywrap+=wrap,y++) {</td>
</tr>
<tr class="description-tr">
<td class="description-td">    for (int x=0;x&lt;2;x++) {</td>
</tr>
<tr class="description-tr">
<td class="description-td">     motion_x = s-&gt;current_picture.motion_val[0][xy + x + ywrap][0];</td>
</tr>
<tr class="description-tr">
<td class="description-td">     motion_y = s-&gt;current_picture.motion_val[0][xy + x + ywrap][1];</td>
</tr>
<tr class="description-tr">
<td class="description-td">     for (int i=0;i&lt;block_size/2;i++) memset(s-&gt;dest[1] + x*block_size/2 + (i +</td>
</tr>
<tr class="description-tr">
<td class="description-td">y*block_size/2) * s-&gt;uvlinesize, 128 + motion_x, block_size/2);</td>
</tr>
<tr class="description-tr">
<td class="description-td">     for (int i=0;i&lt;block_size/2;i++) memset(s-&gt;dest[2] + x*block_size/2 + (i +</td>
</tr>
<tr class="description-tr">
<td class="description-td">y*block_size/2) * s-&gt;uvlinesize, 128 + motion_y, block_size/2);</td>
</tr>
<tr class="description-tr">
<td class="description-td">    }</td>
</tr>
<tr class="description-tr">
<td class="description-td">   }</td>
</tr>
<tr class="description-tr">
<td class="description-td">  }</td>
</tr>
<tr class="description-tr">
<td class="description-td"> }</td>
</tr>
<tr class="description-tr">
<td class="description-td"> if(s-&gt;encoding){ //FIXME encoding MUST be cleaned up</td>
</tr>
<tr class="description-tr">
<td class="description-td">  if (s-&gt;mv_type == MV_TYPE_8X8)</td>
</tr>
<tr class="description-tr">
<td class="description-td">   s-&gt;current_picture.mb_type[mb_xy]= MB_TYPE_L0 | MB_TYPE_8x8;</td>
</tr>
<tr class="description-tr">
<td class="description-td">  else if(s-&gt;mb_intra)</td>
</tr>
<tr class="description-tr">
<td class="description-td">   s-&gt;current_picture.mb_type[mb_xy]= MB_TYPE_INTRA;</td>
</tr>
<tr class="description-tr">
<td class="description-td">  else</td>
</tr>
<tr class="description-tr">
<td class="description-td">   s-&gt;current_picture.mb_type[mb_xy]= MB_TYPE_L0 | MB_TYPE_16x16;</td>
</tr>
<tr class="description-tr">
<td class="description-td"> }</td>
</tr>
<tr class="description-tr">
<td class="description-td">}</td>
</tr>
<tr class="description-tr">
<td align="center" class="description-td" colspan="1" nameend="1" namest="1" rowsep="1"> </td>
</tr>
</tbody></table>
</patent-tables>
</tables>
</div>
</div>
</div>
</section><section itemprop="claims" itemscope="">
<h2>Claims (<span itemprop="count">20</span>)</h2>
<div html="" itemprop="content"><div class="claims" lang="EN" load-source="patent-office" mxw-id="PCLM287035896">
<claim-statement>What is claimed:</claim-statement>
<div class="claim"> <div class="claim" id="CLM-00001" num="00001">
<div class="claim-text">1. A system for instructing a robot to execute a physical task, comprising:
<div class="claim-text">at least one processor; and</div>
<div class="claim-text">a non-transitory computer readable media having computer executable instructions stored therein, which, when executed by the at least processor configure the at least one processor to,
<div class="claim-text">receive a plurality of images associated with a plurality of spatially separated cameras, at least a portion of the plurality of images depicting an object,</div>
<div class="claim-text">determine, from the at least the portion of the plurality of images, a pattern of movement associated with the object based on disparity information derived from two or more of the plurality of images, the pattern of movement within the plurality of images is partitioned into a plurality of clusters wherein a largest area cluster is associated with a prevailing motion and is removed from the frame to obtain residual motion distribution associated with the object, the prevailing motion corresponding to apparent motion caused by at least one of a moving background or motions of the robot,</div>
<div class="claim-text">generate, based on the pattern of movement, a command signal configured to instruct the robot to execute a physical action, and</div>
<div class="claim-text">send the command signal to the robot,</div>
</div>
<div class="claim-text">wherein the non-transitory computer readable media comprises an artificial neural network configured to, receive as input the plurality of images and the pattern of movement associated with the object to produce the command signal.</div>
</div>
</div>
</div> <div class="claim-dependent"> <div class="claim" id="CLM-00002" num="00002">
<div class="claim-text">2. The system of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the robot is configured to execute the physical action responsive to receiving the command signal.</div>
</div>
</div> <div class="claim-dependent"> <div class="claim" id="CLM-00003" num="00003">
<div class="claim-text">3. The system of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the plurality of images comprise a monocular frame sequence comprising an interleaved frame sequence derived from frame sequences obtained from the plurality of spatially separated cameras.</div>
</div>
</div> <div class="claim-dependent"> <div class="claim" id="CLM-00004" num="00004">
<div class="claim-text">4. The system of <claim-ref idref="CLM-00003">claim 3</claim-ref>, further comprising:
<div class="claim-text">a motion estimation encoder, wherein the monocular frame sequence is encoded in part using the motion estimation encoder.</div>
</div>
</div>
</div> <div class="claim-dependent"> <div class="claim" id="CLM-00005" num="00005">
<div class="claim-text">5. The system of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the artificial neural network generates a sparse transformation of the one or more images.</div>
</div>
</div> <div class="claim-dependent"> <div class="claim" id="CLM-00006" num="00006">
<div class="claim-text">6. The system of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the plurality of encoded images are generated by encoding an interleaved sequence of images from three or more cameras of the plurality of spatially separate cameras.</div>
</div>
</div> <div class="claim-dependent"> <div class="claim" id="CLM-00007" num="00007">
<div class="claim-text">7. The system of <claim-ref idref="CLM-00006">claim 6</claim-ref>, wherein the interleaved sequence of images comprises a transition between diagonally opposing cameras.</div>
</div>
</div> <div class="claim"> <div class="claim" id="CLM-00008" num="00008">
<div class="claim-text">8. A non-transitory computer readable media having computer executable instructions stored therein, which, when executed by the at least processor coupled to a robot configure the at least one processor to,
<div class="claim-text">receive a plurality of images associated with a plurality of spatially separated cameras, at least a portion of the plurality of images depicting an object;</div>
<div class="claim-text">determine, from the at least the portion of the plurality of images, a pattern of movement associated with the object based on disparity information derived from two or more of the plurality of images, the pattern of movement within the plurality of images is partitioned into a plurality of clusters wherein a largest area cluster is associated with a prevailing motion and is removed from the frame to obtain residual motion distribution associated with the object, the prevailing motion corresponding to apparent motion caused by at least one of a moving background or motions of the robot;</div>
<div class="claim-text">generate, based on the pattern of movement, a command signal configured to instruct the robot to execute a physical action; and</div>
<div class="claim-text">send the command signal to the robot,</div>
<div class="claim-text">wherein the non-transitory computer readable media comprises an artificial neural network configured to, receive as input the plurality of images and the pattern of movement associated with the object to produce the command signal.</div>
</div>
</div>
</div> <div class="claim-dependent"> <div class="claim" id="CLM-00009" num="00009">
<div class="claim-text">9. The non-transitory computer readable media of <claim-ref idref="CLM-00008">claim 8</claim-ref>, wherein the robot is configured to execute the physical action responsive to receiving the command signal.</div>
</div>
</div> <div class="claim-dependent"> <div class="claim" id="CLM-00010" num="00010">
<div class="claim-text">10. The non-transitory computer readable media of <claim-ref idref="CLM-00008">claim 8</claim-ref>, wherein the plurality of images comprise a monocular frame sequence comprising an interleaved frame sequence derived from frame sequences obtained from the plurality of spatially separated cameras.</div>
</div>
</div> <div class="claim-dependent"> <div class="claim" id="CLM-00011" num="00011">
<div class="claim-text">11. The non-transitory computer readable media of <claim-ref idref="CLM-00010">claim 10</claim-ref>, wherein the monocular frame sequence is encoded in part using a motion estimation encoder.</div>
</div>
</div> <div class="claim-dependent"> <div class="claim" id="CLM-00012" num="00012">
<div class="claim-text">12. The non-transitory computer readable media of <claim-ref idref="CLM-00008">claim 8</claim-ref>, wherein the artificial neural network generates produces a sparse transformation of the one or more images.</div>
</div>
</div> <div class="claim-dependent"> <div class="claim" id="CLM-00013" num="00013">
<div class="claim-text">13. The non-transitory computer readable media of <claim-ref idref="CLM-00008">claim 8</claim-ref>, wherein the plurality of encoded images are generated by encoding an interleaved sequence of images from three or more cameras of the plurality of spatially separate cameras.</div>
</div>
</div> <div class="claim-dependent"> <div class="claim" id="CLM-00014" num="00014">
<div class="claim-text">14. The non-transitory computer readable media of <claim-ref idref="CLM-00013">claim 13</claim-ref>, wherein the interleaved sequence of images comprises a transition between diagonally opposing cameras.</div>
</div>
</div> <div class="claim"> <div class="claim" id="CLM-00015" num="00015">
<div class="claim-text">15. A method for instructing a robot to execute a physical task, comprising:
<div class="claim-text">receiving as input a plurality of images associated with a plurality of spatially separated cameras, at least a portion of the plurality of images depicting an object;</div>
<div class="claim-text">generating a sparse transformation of one or more of the plurality of images;</div>
<div class="claim-text">determining, from the at least the portion of the plurality of images, a pattern of movement associated with the object based on disparity information derived from two or more of the plurality of images wherein the pattern of movement associated with the object is determined by partitioning the plurality of images into a plurality of clusters wherein a largest area cluster is associated with a prevailing motion, removing the largest area cluster from the frame and obtaining residual motion distribution associated with the object, the prevailing motion corresponding to apparent motion caused by at least one of a moving background or motions of the robot;</div>
<div class="claim-text">generating, based on the pattern of movement, a command signal configured to instruct the robot to execute a physical action; and</div>
<div class="claim-text">sending the command signal to the robot;</div>
<div class="claim-text">wherein the non-transitory computer readable media comprises an artificial neural network configured to, receive as input the plurality of images and the pattern of movement associated with the object to produce the command signal.</div>
</div>
</div>
</div> <div class="claim-dependent"> <div class="claim" id="CLM-00016" num="00016">
<div class="claim-text">16. The method of <claim-ref idref="CLM-00015">claim 15</claim-ref>, wherein the robot is configured to execute the physical action responsive to receiving the command signal.</div>
</div>
</div> <div class="claim-dependent"> <div class="claim" id="CLM-00017" num="00017">
<div class="claim-text">17. The method of <claim-ref idref="CLM-00015">claim 15</claim-ref>, wherein the plurality of images comprise a monocular frame sequence comprising an interleaved frame sequence derived from frame sequences obtained from the plurality of spatially separated cameras.</div>
</div>
</div> <div class="claim-dependent"> <div class="claim" id="CLM-00018" num="00018">
<div class="claim-text">18. The method of <claim-ref idref="CLM-00017">claim 17</claim-ref>, wherein the monocular frame sequence is encoded in part using a motion estimation encoder.</div>
</div>
</div> <div class="claim-dependent"> <div class="claim" id="CLM-00019" num="00019">
<div class="claim-text">19. The method of <claim-ref idref="CLM-00015">claim 15</claim-ref>, wherein the artificial neural network generates produces a sparse transformation of the one or more images.</div>
</div>
</div> <div class="claim-dependent"> <div class="claim" id="CLM-00020" num="00020">
<div class="claim-text">20. The method of <claim-ref idref="CLM-00015">claim 15</claim-ref>, wherein,
<div class="claim-text">the plurality of encoded images are generated by encoding an interleaved sequence of images from three or more cameras of the plurality of spatially separate cameras, and</div>
<div class="claim-text">the interleaved sequence of images comprises a transition between diagonally opposing cameras.</div>
</div>
</div>
</div> </div>
</div>
</section>
                </article>
            </search-app>
        </body>
    </html>
    