
    <html>
        <body>
            <search-app>
                <article class="result" itemscope="" itemtype="http://schema.org/ScholarlyArticle">
    <h1 itemprop="pageTitle">US11176213B2 - Systems and methods for identifying electronic content using video graphs 
        - Google Patents</h1><section itemprop="abstract" itemscope="">
<h2>Abstract</h2>
<div html="" itemprop="content"><abstract lang="EN" load-source="patent-office" mxw-id="PA471444076">
<div class="abstract" id="p-0001" num="0000">Systems and methods are provided for identifying and recommending electronic content to consumers. In accordance with an implementation, one or more elements of electronic content are identified based on video graph data. In an exemplary method, information associated with a first element of video content is received, and corresponding video graph data is obtained. One or more second elements of video content that are similar to the first element of video content are identified based on the obtained video graph data. A subset the first and second elements of video content is subsequently identified for delivery to the user.</div>
</abstract>
</div>
</section><section itemprop="description" itemscope="">
<h2>Description</h2>
<div html="" itemprop="content"><div class="description" lang="EN" load-source="patent-office" mxw-id="PDES310883366">
<heading id="h-0001">CROSS-REFERENCE TO RELATED APPLICATIONS</heading>
<div class="description-paragraph" id="p-0002" num="0001">This patent application is a continuation of and claims the benefit of priority to U.S. application Ser. No. 15/600,823 filed on May 22, 2017, which is a continuation of and claims the benefit of priority to U.S. application Ser. No. 14/730,140 filed on Jun. 3, 2015 (now U.S. Pat. No. 9,679,069), which is a continuation of and claims the benefit of priority to U.S. application Ser. No. 13/533,398 filed on Jun. 26, 2012 (now U.S. Pat. No. 9,058,385), the entireties of which are incorporated herein by reference.</div>
<heading id="h-0002">BACKGROUND</heading>
<heading id="h-0003">Technical Field</heading>
<div class="description-paragraph" id="p-0003" num="0002">The present disclosure generally relates to systems and methods for identifying electronic content in a network environment, such as the Internet. More particularly, and without limitation, the present disclosure relates to systems and methods that leverage video graph data to identify and/or provide recommendations of video content to a user.</div>
<heading id="h-0004">Background Information</heading>
<div class="description-paragraph" id="p-0004" num="0003">Today, the discovery of electronic content, such as online video content, presents challenges and opportunities not present within traditional broadcast television or cable television environments. For example, in a traditional broadcast television environment, a program may only be available at a particular time and on a particular channel. In contrast, electronic content is generally not distributed by a single channel or website within a network environment, such as the Internet. Instead, the electronic content, e.g., a video clip or movie, may be distributed through as many websites and other outlets as possible in order to maximize the number of viewers exposed to the electronic content. Furthermore, popular or premium electronic content is often reproduced (both legally or illegally) and widely distributed across many websites and portals, particularly as the demand or interest for the content increases with more and more viewers.</div>
<div class="description-paragraph" id="p-0005" num="0004">As a result, a large amount of duplicative videos and other electronic content is available across the Internet. The wide availability of duplicative electronic content, including duplicative segments of video clips, may render it difficult for a user to readily identify content of interest based on, for example, characteristics of the content, preferences of the user, and/or preference of the user's friends in a social networking environment.</div>
<div class="description-paragraph" id="p-0006" num="0005">In view of the foregoing, there is a need for improved systems and methods for efficiently discovering and identifying desired electronic content in a network environment, such as the Internet. Moreover, there is a need for improved systems and methods for identifying electronic content, including video content, that is dispersed across multiple websites. There is also a need for such systems and methods that can be implemented in a computer-based environment.</div>
<heading id="h-0005">SUMMARY</heading>
<div class="description-paragraph" id="p-0007" num="0006">Consistent with embodiments of the present disclosure, computer-implemented systems and methods are provided for identifying electronic content, including video content, based on video graph data. In one exemplary embodiment, a method is provided that receives information associated with a first element of video content. The method includes obtaining, in response to the received information, data associated with at least one video graph, and identifying, using a processor, one or more second elements of video content that are similar to the first element of video content. The identification is based on the obtained video graph data, and the video graph data includes information indicative of the similarity between the first and second elements video content. The method includes identifying at least a subset of the first and second elements of video content for delivery to a user.</div>
<div class="description-paragraph" id="p-0008" num="0007">Consistent with further embodiments of the present disclosure, a system is provided having a storage device and at least one processor coupled to the storage device. The storage device stores a set of instructions for controlling the at least one processor, and wherein the at least one processor, being operative with the set of instructions, is configured to receive information associated with a first element of video content. The processor is further configured to obtain, in response to the received information, data associated with at least one video graph, and identify one or more second elements of video content that are similar to the first element of video content. The identification is based on the obtained video graph data, and the video graph data includes information indicative of the similarity between the first and second elements of video content. The processor is further configured to identify at least a subset of the first and second elements of video content for delivery to a user.</div>
<div class="description-paragraph" id="p-0009" num="0008">Other embodiments of the present disclosure relate to a tangible, non-transitory computer-readable medium that stores a set of instructions that, when executed by a processor, perform a method for identifying electronic content. The method includes receiving information associated with a first element of video content and obtaining, in response to the received information, data associated with at least one video graph. The method also includes identifying, using a processor, one or more second elements of video content that are similar to the first element of video content. The identification is based on the obtained video graph data, and the video graph data includes information indicative of the similarity between the first and second elements video content. In addition, the method includes identifying at least a subset of the first and second elements of video content for delivery to a user.</div>
<div class="description-paragraph" id="p-0010" num="0009">It is to be understood that both the foregoing general description and the following detailed description are exemplary and explanatory only, and are not restrictive of the invention as claimed. Further, the accompanying drawings, which are incorporated in and constitute a part of this specification, illustrate embodiments of the present disclosure and together with the description, serve to explain principles of the invention as set forth in the accompanying claims.</div>
<description-of-drawings>
<heading id="h-0006">BRIEF DESCRIPTION OF THE DRAWINGS</heading>
<div class="description-paragraph" id="p-0011" num="0010"> <figref idrefs="DRAWINGS">FIG. 1</figref> is a diagram of an exemplary computing environment within which embodiments of the present disclosure may be practiced.</div>
<div class="description-paragraph" id="p-0012" num="0011"> <figref idrefs="DRAWINGS">FIG. 2</figref> is a diagram of an exemplary computer system, consistent with disclosed embodiments.</div>
<div class="description-paragraph" id="p-0013" num="0012"> <figref idrefs="DRAWINGS">FIGS. 3A-3C</figref> are flowcharts of an exemplary methods for generating measures of similarity between elements of video content, according to disclosed embodiments.</div>
<div class="description-paragraph" id="p-0014" num="0013"> <figref idrefs="DRAWINGS">FIG. 4</figref> is a flowchart of an exemplary method for associating elements of video content, according to disclosed embodiments.</div>
<div class="description-paragraph" id="p-0015" num="0014"> <figref idrefs="DRAWINGS">FIG. 5</figref> is a flowchart of an exemplary method for identifying similar pairs of video segments, according to disclosed embodiments.</div>
<div class="description-paragraph" id="p-0016" num="0015"> <figref idrefs="DRAWINGS">FIGS. 6-9</figref> are diagrams of exemplary video graphs, according to disclosed embodiments.</div>
<div class="description-paragraph" id="p-0017" num="0016"> <figref idrefs="DRAWINGS">FIG. 10</figref> is a flowchart of an exemplary method for associating users based patterns of video consumption, according to disclosed embodiments.</div>
<div class="description-paragraph" id="p-0018" num="0017"> <figref idrefs="DRAWINGS">FIG. 11</figref> is a flowchart of an exemplary method for identifying similar elements of video content, according to disclosed embodiments.</div>
<div class="description-paragraph" id="p-0019" num="0018"> <figref idrefs="DRAWINGS">FIGS. 12A and 12B</figref> are diagrams of exemplary interfaces for displaying video content, according to disclosed embodiments.</div>
<div class="description-paragraph" id="p-0020" num="0019"> <figref idrefs="DRAWINGS">FIG. 13</figref> is a flowchart of an exemplary method for identifying similar elements of video content, according to disclosed embodiments.</div>
<div class="description-paragraph" id="p-0021" num="0020"> <figref idrefs="DRAWINGS">FIGS. 14A and 14B</figref> are diagrams of exemplary interfaces for displaying video content, according to disclosed embodiments.</div>
</description-of-drawings>
<heading id="h-0007">DESCRIPTION OF THE EMBODIMENTS</heading>
<div class="description-paragraph" id="p-0022" num="0021">Reference will now be made in detail to embodiments of the present disclosure, examples of which are illustrated in the accompanying drawings. The same reference numbers will be used throughout the drawings to refer to the same or like parts.</div>
<div class="description-paragraph" id="p-0023" num="0022">In this application, the use of the singular includes the plural unless specifically stated otherwise. In this application, the use of “or” means “and/or” unless stated otherwise. Furthermore, the use of the term “including,” as well as other forms such as “includes” and “included,” is not limiting. In addition, terms such as “element” or “component” encompass both elements and components comprising one unit, and elements and components that comprise more than one subunit, unless specifically stated otherwise. Additionally, the section headings used herein are for organizational purposes only, and are not to be construed as limiting the subject matter described.</div>
<div class="description-paragraph" id="p-0024" num="0023"> <figref idrefs="DRAWINGS">FIG. 1</figref> illustrates an exemplary computing environment <b>100</b> within which embodiments consistent with the present disclosure may be practiced. In <figref idrefs="DRAWINGS">FIG. 1</figref>, a recommendations system <b>140</b> and a plurality of user devices <b>102</b> and <b>112</b> are interconnected via a communications network <b>120</b>. As further disclosed herein, recommendations system <b>140</b> and user devices <b>102</b>, <b>112</b> may exchange information associated with one or more elements of electronic content, e.g., video clips or segments of video clips.</div>
<div class="description-paragraph" id="p-0025" num="0024">In an embodiment, user devices <b>102</b> and <b>112</b> can be implemented with a processor or computer-based system. For example, user devices <b>102</b> and <b>112</b> can include, but are not limited to, a personal computer, a laptop computer, a notebook computer, a hand-held computer, a personal digital assistant, a portable navigation device, a mobile phone, a smart phone, a set top box, a third party portals, an optical disk player (e.g., a DVD player), a digital video recorder (DVR), and any additional or alternate computing device operable to transmit and receive data across network <b>120</b>.</div>
<div class="description-paragraph" id="p-0026" num="0025">Although computing environment <b>100</b> is illustrated in <figref idrefs="DRAWINGS">FIG. 1</figref> with two user devices <b>102</b> and <b>112</b> in communication with recommendations system <b>140</b>, persons of ordinary skill in the art will recognize that environment <b>100</b> may include any number of additional number of mobile or stationary user devices, any number of additional search engines, and any additional number of computers, systems, or servers without departing from the spirit or scope of the disclosed embodiments.</div>
<div class="description-paragraph" id="p-0027" num="0026">Communications network <b>120</b> may represent any form or medium of digital data communication. Examples of communication network <b>120</b> include a local area network (“LAN”), a wireless LAN, e.g., a “WiFi” network, a wireless Metropolitan Area Network (MAN) that connects multiple wireless LANs, and a wide area network (“WAN”), e.g., the Internet. Consistent with embodiments of the present disclosure, network <b>120</b> may comprise the Internet and include any publicly-accessible network or networks interconnected via one or more communication protocols, including, but not limited to, hypertext transfer protocol (HTTP) and transmission control protocol/internet protocol (TCP/IP). Moreover, communications network <b>120</b> may also include one or more mobile device networks, such as a GSM network or a PCS network, that allow user devices, such as user device <b>102</b>, to send and receive data via applicable communications protocols, including those described above.</div>
<div class="description-paragraph" id="p-0028" num="0027">Recommendations system <b>140</b> may include a recommendations server <b>142</b> and a data repository <b>144</b>. Recommendations server <b>142</b> may include a front end <b>142</b>A, and a back end <b>142</b>B, which is disposed in communication with front end <b>142</b>A. In the exemplary embodiment of <figref idrefs="DRAWINGS">FIG. 1</figref>, front end <b>142</b>A and back end <b>142</b>B of recommendations server <b>142</b> may be incorporated into a hardware unit, for example, a single computer, a single server, or any additional or alternate computing device apparent to one or skill in the art. Further, in such an exemplary embodiment, front end <b>142</b>A may be a software application, such as a web service, executing on recommendations server <b>142</b>. However, recommendations server <b>142</b> is not limited to such configurations, and, in additional embodiments, front end <b>142</b>A may be executed on any computer or server separate from back end <b>1428</b>.</div>
<div class="description-paragraph" id="p-0029" num="0028">Data repository <b>144</b> may include a content data store <b>144</b>A and a video graph data store <b>144</b>B. In an embodiment, content data store <b>144</b>A may include elements of electronic content that, for example, may be delivered to a user device (e.g., one of user devices <b>102</b> and <b>112</b>) in response requests and/or queries provided to recommendations server <b>142</b>. For example, the electronic content within content data store <b>144</b>A may include, but is not limited to, textual content, video content (e.g., video clips or segments of video clips), audio content, executable programs (e.g., Java scripts), and/or any additional content that is appropriate for delivery to a user device across communications network <b>120</b>.</div>
<div class="description-paragraph" id="p-0030" num="0029">In an embodiment, content data store <b>144</b>A may further include metadata associated with one or more of the elements of electronic content stored within content data store <b>144</b>A. For example, the metadata may include, but is not limited to, information identifying a source of the content (e.g., a source uniform resource locator (URL) or an address of a source repository), structural information associated with the content (e.g., a type of the content and a size of the content), editorial and contextual information that describes the content, and information associated with a viewership of the content (e.g., a number of times users or particular users have accessed the content).</div>
<div class="description-paragraph" id="p-0031" num="0030">For example, the editorial and contextual information associated with an element of electronic content, e.g., a video clip, may include, but is not limited to, a title of the video clip, information identifying a creator of the video clip, information identifying one or more performers associated with portions of the video clip, a date on which the video clip was created, and keywords or text describing the video clip. Further, for example, the metadata associated with the video clip may also identify an event associated with or referenced by the video clip, an additional element of electronic content explicitly related to or referenced within the video clip (e.g., one or more additional episodes within a particular television series), and/or information identifying a product referenced by the video clip.</div>
<div class="description-paragraph" id="p-0032" num="0031">Referring back to <figref idrefs="DRAWINGS">FIG. 1</figref>, data repository <b>144</b> may also include video graph data store <b>144</b>B. In an embodiment, video graph data store <b>144</b>B may include information associated with one or more video graphs that describe relationships and similarities between video clips or elements of video content stored within content data store <b>144</b>A and additional video content accessible to recommendations system <b>140</b> across network <b>120</b> based on, for example, audio and/or visual content associated with the video clips and users who have previously viewed the video clips.</div>
<div class="description-paragraph" id="p-0033" num="0032">In an embodiment, recommendations server <b>142</b> may leverage the video graph data to improve the discoverability of digital video content accessible across communications network <b>120</b> and to improve a relevance of digital video content presented to a user in response to a search query received over communications network <b>120</b>. For example, recommendations server <b>142</b> may leverage the video graph data to enhance metadata about a particular video by including data from closely associated videos, to improve a ranking of results of a keyword search of videos, to recommend videos related to a video watched by a user, to discover the source videos used within a video, and/or to follow events as videos are uploaded and distributed across communications network <b>120</b>.</div>
<div class="description-paragraph" id="p-0034" num="0033"> <figref idrefs="DRAWINGS">FIG. 2</figref> is an exemplary computer system <b>200</b> with which embodiments consistent with the present disclosure may be implemented. Computer system <b>200</b> includes one or more processors, such as processor <b>202</b>. Processor <b>202</b> is connected to a communication infrastructure <b>206</b>, such as a bus or communications network, e.g., network <b>120</b> of <figref idrefs="DRAWINGS">FIG. 1</figref>.</div>
<div class="description-paragraph" id="p-0035" num="0034">Computer system <b>200</b> also includes a main memory <b>208</b>, for example, random access memory (RAM), and may include a secondary memory <b>210</b>. Secondary memory <b>210</b> may include, for example, a hard disk drive <b>212</b> and/or a removable storage drive <b>214</b>, representing a magnetic tape drive, an optical disk drive, CD/DVD drive, etc. The removable storage drive <b>214</b> reads from and/or writes to a removable storage unit <b>218</b> in a well-known manner. Removable storage unit <b>218</b> represents a magnetic tape, optical disk, or other storage medium that is read by and written to by removable storage drive <b>214</b>. As will be appreciated, the removable storage unit <b>218</b> can represent a computer readable medium having stored therein computer programs, sets of instructions, code, or data to be executed by processor <b>202</b>.</div>
<div class="description-paragraph" id="p-0036" num="0035">In alternate embodiments, secondary memory <b>210</b> may include other means for allowing computer programs or other program instructions to be loaded into computer system <b>200</b>. Such means may include, for example, a removable storage unit <b>222</b> and an interface <b>220</b>. An example of such means may include a removable memory chip (e.g., EPROM, RAM, ROM, DRAM, EEPROM, flash memory devices, or other volatile or non-volatile memory devices) and associated socket, or other removable storage units <b>222</b> and interfaces <b>220</b>, which allow instructions and data to be transferred from the removable storage unit <b>222</b> to computer system <b>200</b>.</div>
<div class="description-paragraph" id="p-0037" num="0036">Computer system <b>200</b> may also include one or more communications interfaces, such as communications interface <b>224</b>. Communications interface <b>224</b> allows software and data to be transferred between computer system <b>200</b> and external devices. Examples of communications interface <b>224</b> may include a modem, a network interface (e.g., an Ethernet card), a communications port, a PCMCIA slot and card, etc. Software and data may be transferred via communications interface <b>224</b> in the form of signals <b>226</b>, which may be electronic, electromagnetic, optical or other signals capable of being received by communications interface <b>224</b>. These signals <b>226</b> are provided to communications interface <b>224</b> via a communications path (i.e., channel <b>228</b>). Channel <b>228</b> carries signals <b>226</b> and may be implemented using wire, cable, fiber optics, RF link, and/or other communications channels. In an embodiment of the invention, signals <b>226</b> comprise data packets sent to processor <b>202</b>. Information representing processed packets can also be sent in the form of signals <b>226</b> from processor <b>202</b> through communications path <b>228</b>.</div>
<div class="description-paragraph" id="p-0038" num="0037">The terms “storage device” and “storage medium” may refer to particular devices including, but not limited to, main memory <b>208</b>, secondary memory <b>210</b>, a hard disk installed in hard disk drive <b>212</b>, and removable storage units <b>218</b> and <b>222</b>. Further, the term “computer readable medium” may refer to devices including, but not limited to, a hard disk installed in hard disk drive <b>212</b>, any combination of main memory <b>208</b> and secondary memory <b>210</b>, and removable storage units <b>218</b> and <b>222</b>, which respectively provide computer programs and/or sets of instructions to processor <b>202</b> of computer system <b>200</b>. Such computer programs and sets of instructions can be stored within one or more computer readable media. Additionally or alternatively, computer programs and sets of instructions may also be received via communications interface <b>224</b> and stored on the one or more computer readable media.</div>
<div class="description-paragraph" id="p-0039" num="0038">Such computer programs and instructions, when executed by processor <b>202</b>, enable processor <b>202</b> to perform the computer-implemented methods described herein. Examples of program instructions include, for example, machine code, such as code produced by a compiler, and files containing a high-level code that can be executed by processor <b>202</b> using an interpreter.</div>
<div class="description-paragraph" id="p-0040" num="0039">Furthermore, the computer-implemented methods described herein can be implemented on a single processor of a computer system, such as processor <b>202</b> of system <b>200</b>. However, in additional embodiments, these computer-implemented methods may be implemented using one or more processors within a single computer system, and additionally or alternatively, these computer-implemented methods may be implemented on one or more processors within separate computer systems linked via a network.</div>
<div class="description-paragraph" id="p-0041" num="0040">As described above, a web server (e.g., recommendations server <b>142</b>) may receive information associated with a video clip, and additionally or alternatively, a search query, from a user device (e.g., user device <b>102</b>) across communications network <b>120</b>. Recommendations server <b>142</b> may subsequently leverage data associated with one or more video graphs (e.g., as stored within video graph data store <b>144</b>B) to identify additional video content similar to the video clip and/or relevant to at least a portion of the received search query.</div>
<div class="description-paragraph" id="p-0042" num="0041">In an embodiment, a video graph may illustrate a network of videos or video clips that include identical or similar portions of audio content, visual content, or combinations of audio and video content. For example, such video graphs may be represented as a bi-partite graph having nodes that represent video clips and edges that connect the videos clips and that are indicative of a degree of similarity between the connected video clips. For example, and as discussed above, such video clips may be associated with corresponding metadata (e.g., within content data store <b>144</b>B) that includes, but is not limited to, titles of the video clips, durations of the video clips, sources of the video clips, producers of the content associated with the video clips, a quality of the video clips, an indication of an originality of the video clips, and any additional or alternate information apparent to one of ordinary skill in the art and appropriate to the video clips.</div>
<div class="description-paragraph" id="p-0043" num="0042">The edges that connect video clips within a video graph may be indication of an association between the two video clips, as determined by measures of similarities between corresponding segments of the video clips. For example, an association A(i, j) between video clips i and j may be represented as a list of discrete association elements c<sub>k</sub>(i, j) corresponding to similar pairs of segments of clips i and j, as follows:
<br/>
<i>A</i>(<i>i,j</i>)={<i>c</i> <sub>k</sub>(<i>i,j</i>)}.  (1)
</div>
<div class="description-paragraph" id="p-0044" num="0043">An association element c<sub>k</sub>(i, j) references a pair k of “similar” segments of the video clips having similar durations, one from video clip i and the other from video clip j, and is defined as follows:
<br/>
<i>c</i> <sub>k</sub>(<i>i,j</i>)=(<i>p,d</i> <sub>k</sub> <i>,t</i> <sub>k,i</sub> <i>,t</i> <sub>k,j</sub> <i>,s</i> <sub>k,ij</sub>),  (2)
<br/>
where p is a type of similarity between video clips i and j (e.g., an auditory similarity, a visual similarity, and a combination of auditory and visual similarity), d<sub>k </sub>is a duration of the k<sup>th </sup>segment of video clips i and j, t<sub>k,i </sub>is a start time of the k<sup>th </sup>segment in video clip i, t<sub>k,j </sub>is a start time of the k<sup>th </sup>segment within video clip j, and s<sub>k,ij </sub>is a measure of the similarity between the k<sup>th </sup>segment of video clips i and j.
</div>
<div class="description-paragraph" id="p-0045" num="0044">In such an embodiment, video clips i and j may be may be considered “associated” when video clips i and j are characterized by at least one non-zero association element. In such an embodiment, video clips i and j, and additionally or alternatively, the similar segments of video clips i and j, may be connected by corresponding edges in a video graph. Furthermore, if video clips i and j fall to share a common, non-zero association element, then these video clips are not associated and would not be linked within the video graph.</div>
<div class="description-paragraph" id="p-0046" num="0045">As discussed above, the association between two video clips may be determined based on, among other things, a measure indicative of a similarity between corresponding segments of the video clips, and on a determination that the similarity measure satisfies one or more associations rules. For example, the similarity between portions of two video clips or elements of video content may be based on a visual similarity, an auditory similarity, and/or a combination of the auditory and visual similarities, as described below in reference to <figref idrefs="DRAWINGS">FIGS. 3A-3C</figref>.</div>
<div class="description-paragraph" id="p-0047" num="0046"> <figref idrefs="DRAWINGS">FIG. 3A</figref> illustrates an exemplary method <b>300</b> for generating a measure of auditory similarity (i.e., p=3 in Equation (2)) between segments of a pair of video clips, in accordance with disclosed embodiments. In <figref idrefs="DRAWINGS">FIG. 3A</figref>, a pair of video clips, e.g., first video clip i and second video clip j, are accessed in step <b>302</b>. For example, video clips i and j may be stored within content data store <b>144</b>A and capable of being delivered to a user device, e.g., user device <b>102</b>, by recommendations server <b>142</b>.</div>
<div class="description-paragraph" id="p-0048" num="0047">In step <b>304</b>, first and second auditory samples are extracted from corresponding portions of the first and second video clips. For example, in step <b>304</b>, a first auditory sample a<sub>i</sub>(t<sub>i</sub>,d) may be extracted from first video clip i starting at a temporal position t<sub>i </sub>and extending for a temporal duration d. Similarly, in step <b>304</b>, a second auditory sample a<sub>j</sub>(t<sub>j</sub>,d) may be extracted from second video clip j starting at a temporal position t<sub>j </sub>and extending for duration d.</div>
<div class="description-paragraph" id="p-0049" num="0048">Auditory fingerprints of first auditory sample a<sub>i</sub>(t<sub>i</sub>,d) and second auditory sample a<sub>j</sub>(t<sub>j</sub>,d) may be generated in step <b>306</b>. For example, to generate a first auditory fingerprint in step <b>306</b>, first auditory sample a<sub>i</sub>(t<sub>i</sub>,d) may be divided into a plurality of frames, e.g., of twenty millisecond duration, and the spectrum features may be computed at each of the frames. The calculated spectrum features of the first auditory sample a<sub>i</sub>(t<sub>i</sub>,d) and second auditory sample a<sub>j</sub>(t<sub>j</sub>,d) may form the corresponding first and second audio fingerprints in step <b>306</b>.</div>
<div class="description-paragraph" id="p-0050" num="0049">For example, the spectrum features of the first and second auditory samples may correspond to mel-frequency cepstrum (MFC) representation of short-term power spectrums of first and second auditory samples. In such an embodiment, the spectrum features computed at each of the frames may correspond to a set of mel-frequency cepstral coefficients (MFCCs) that collectively form the MFC representation. However, the techniques of <figref idrefs="DRAWINGS">FIG. 3A</figref> are not limited to such exemplary algorithms, and in additional embodiments, the spectrum features of an auditory sample of a digital video clip may be computed using any additional or alternate technique appropriate to the auditory sample, without departing from the spirit or scope of the disclosed embodiments.</div>
<div class="description-paragraph" id="p-0051" num="0050">Referring back to <figref idrefs="DRAWINGS">FIG. 3A</figref>, a metric s<sub>ij </sub>of the similarity between the first and second auditory fingerprints may be computed in step <b>308</b>. In an embodiment, similarity metric s<sub>ij </sub>may be computed in step <b>308</b> based on an “average” signal-to-noise ratio across the frames of first auditory sample a<sub>i</sub>(t<sub>i</sub>,d) and second auditory sample a<sub>j</sub>(t<sub>j</sub>,d). For example, the spectrum features associated with first auditory sample a<sub>i</sub>(t<sub>i</sub>,d) may be treated as “signal,” and the spectrum features of second auditory sample a<sub>j</sub>(t<sub>j</sub>,d) may be treated as “signal plus noise.” An average signal-to-noise ratio may be computed for the frames of the first and second auditory samples and assigned to similarity metric s<sub>ij </sub>in step <b>308</b>. Further, in an embodiment, the value of similarity metric s<sub>ij </sub>may be stored within data repository <b>144</b>, for example, within metadata associated with video clips i and j within content data store <b>144</b>A. Exemplary method <b>300</b> is finished and complete in step <b>310</b>.</div>
<div class="description-paragraph" id="p-0052" num="0051">As discussed above, an association between a pair of video clips need not be limited to a similarity in auditory content. In additional embodiments, the associated between the pair of video clips may be based on a similarity between visual content of the video clips, as described in reference to <figref idrefs="DRAWINGS">FIG. 3B</figref>.</div>
<div class="description-paragraph" id="p-0053" num="0052"> <figref idrefs="DRAWINGS">FIG. 3B</figref> illustrates an exemplary method <b>320</b> for generating a metric of visual similarity (i.e., p=2 In Equation (2)) between segments of a pair of video clips, in accordance with disclosed embodiments. In <figref idrefs="DRAWINGS">FIG. 36</figref>, a pair of digital video clips, e.g., first video clip land second video clip j, are accessed in step <b>322</b>. As discussed above, video clips i and j may represent video clips stored within content data store <b>144</b>A of data repository <b>144</b> and capable of being delivered a user device, e.g., user device <b>102</b>, by recommendations server <b>142</b>.</div>
<div class="description-paragraph" id="p-0054" num="0053">In step <b>324</b>, first and second samples of visual content are extracted from corresponding portions of the first and second video clips. For example, in step <b>324</b>, a first visual sample v<sub>i</sub>(t<sub>i</sub>,d) may be extracted from first video i starting at a temporal position t<sub>i </sub>and extending for a temporal duration d. Similarly, in step <b>324</b>, a second visual sample v<sub>j</sub>(t<sub>j</sub>,d) may be extracted from second video j starting at a temporal position t<sub>j </sub>and extending for duration d.</div>
<div class="description-paragraph" id="p-0055" num="0054">Visual fingerprints associated with the first visual sample v<sub>i</sub>(t<sub>i</sub>,d) and second visual sample v<sub>j</sub>(t<sub>j</sub>,d) may be generated in step <b>326</b>. For example, to generate a fingerprint associated with a visual sample (e.g., one or more of first visual sample v<sub>i</sub>(t<sub>i</sub>,d) and second visual sample v<sub>j</sub>(t<sub>j</sub>,d)), step <b>326</b> may initially decompose the visual sample into a plurality of frames, e.g., having a duration of twenty milliseconds. Histograms may be computed for the frames of the visual sample, and differences between the histograms at consecutive frames may be determined. In such an embodiment, step <b>326</b> may assign the sequence of histogram differences as the fingerprint of the visual sample.</div>
<div class="description-paragraph" id="p-0056" num="0055">In step <b>328</b>, a measure of a similarity s<sub>ij </sub>between first visual sample v<sub>i</sub>(t<sub>i</sub>,d) and second visual sample v<sub>j</sub>(t<sub>j</sub>,d) may be computed based on the visual fingerprints computed in step <b>326</b>. For example, the similarity measure s<sub>ij </sub>may be computed as a correlation between the histogram differences associated with the first and second visual samples. Further, in an embodiment, the value of similarity metric s<sub>ij </sub>may be stored within data repository <b>144</b>, for example, within metadata associated with video clips i and j within content data store <b>144</b>A. Method <b>320</b> is then finished and completed in step <b>330</b>.</div>
<div class="description-paragraph" id="p-0057" num="0056">Further, in additional embodiments, the association between the first and second video clips may computed based on measures of both an auditory similarity and a visual similarity between the pair of video clips. <figref idrefs="DRAWINGS">FIG. 3C</figref> illustrates an exemplary method <b>340</b> for generating a measure of auditory and visual similarity (i.e., p=1 in Equation (2)) between segments of a pair of digital video clips, in accordance with disclosed embodiments.</div>
<div class="description-paragraph" id="p-0058" num="0057">In <figref idrefs="DRAWINGS">FIG. 3C</figref>, a pair of digital video clips, e.g., first video clip i and second video clip j, are accessed in step <b>342</b>. As discussed above, video clips i and j may represent videos stored within content data store <b>144</b>A of data repository <b>144</b> and capable of being delivered to a user device, e.g., user device <b>102</b>, by recommendations server <b>142</b>.</div>
<div class="description-paragraph" id="p-0059" num="0058">In step <b>344</b>, samples of the first and second video clips are extracted for analysis. For example, the first sample may be extracted from first video clip i starting at a temporal position t<sub>i </sub>and extending for a temporal duration d. Similarly, the second sample may be extracted from second video clip j starting at a temporal position t<sub>j </sub>and extending for duration d.</div>
<div class="description-paragraph" id="p-0060" num="0059">Measures of auditory and visual similarity are obtained for the first and second samples in step <b>346</b>. In an embodiment, the auditory similarity between the first and second samples may be computed using exemplary method <b>300</b>, as described above in reference to <figref idrefs="DRAWINGS">FIG. 3A</figref>, and the visual similarity may be computed using exemplary method <b>320</b>, as described above in reference to <figref idrefs="DRAWINGS">FIG. 3B</figref>. However, the techniques of <figref idrefs="DRAWINGS">FIG. 3C</figref> are not limited to such exemplary measures of auditory and visual similarity, and in additional embodiments, the auditory and visual similarity measures may be computed using any additional or alternate technique, or may be retrieved from any appropriate source accessible to communications network <b>120</b>, without departing from the spirit or scope of the disclosed embodiments.</div>
<div class="description-paragraph" id="p-0061" num="0060">In step <b>348</b>, the auditory and visual similarities may be weighted using corresponding weight factors, and a measure of audio-visual similarity between the first and second samples may be computed based on a linear combination of the weighted auditory and visual similarities. In an embodiment, the weight factors applied to the auditory and visual similarities may be adaptively determined based on one or more characteristics of the first and second video clips. For example, such characteristics may be identified based on information within corresponding metadata (e.g., metadata within content data store <b>144</b>A), and such characteristics include, but are not limited to, types of the first and second video clips, sizes of the first and second video clips, and any additional or alternate information apparent to one of skill in the art. Method <b>340</b> is subsequently finished and complete in step <b>350</b>.</div>
<div class="description-paragraph" id="p-0062" num="0061">The exemplary techniques of <figref idrefs="DRAWINGS">FIG. 3C</figref> are not limited to such adaptively-determined weight factors. In additional embodiments, the weight factors may be pre-determined by a user associated with one of the user devices (e.g., user device <b>102</b> and <b>112</b>), pre-determined based on the source or creator of the first and second videos, or established in accordance with any additional or alternate algorithm or rationale, without departing from the spirit or scope of the disclosed embodiments.</div>
<div class="description-paragraph" id="p-0063" num="0062">In an embodiment, an association between two video clips may be based on a determination that a similarity measure corresponding to paired segments of the video clips satisfies one or more association rules. In such embodiments, an association rule may represent a set of minimum requirements for two video clips to connected within a video graph, and the association rule may enable a user to define a proper video graph based on its application.</div>
<div class="description-paragraph" id="p-0064" num="0063">For example, an association rule may be associated with a type of similarity measure (e.g., auditory similarity, visual similarity, or combinations thereof), a requirement on the similarity measure of an element, as described above, and a specific algorithm to calculate the measure. For example, to define a video graph that represents duplicated content among a list of video clips, an association rule may be defined to: (i) consider a similarity measure based on auditory and visual similarities; (ii) for each potential associated clip, fingerprint two corresponding video portions and compare the fingerprints to calculate a probability of whether the portions are duplicates; and (iii) include only those video portions having a similarity measure that exceeds a threshold value. By including only video clip portions that exceed the threshold value, the association rule may determine that the corresponding video clip portions are likely to be duplicates. In such embodiments, the resulting video graphs may represent a network of duplicate content throughout the list of videos.</div>
<div class="description-paragraph" id="p-0065" num="0064">In additional embodiments, association rules consistent with the disclosed embodiments may be used to construct video graphs linking video clips associated with common sets of auditory content, but different sets of visual content. For example, two clips of music videos for a single song performed by the Jonas Brothers may have a common audio track, but may have different visual content. Further, in such an example, the music videos may be characterized by an auditory similarity of 0.98, a visual similarity of 0.04, and a combined auditory and visual similarity of 0.51.</div>
<div class="description-paragraph" id="p-0066" num="0065">As discussed above, one or more association rules may be applied to the music video clips to determine whether these music video clips are connected within a corresponding video graph. For example, an association rule may determine that two video clips are connected if a corresponding similarity measure, e.g., an auditory similarity, exceeds a value of 0.9. Using a such a rule, the two music video clips would be connected within a corresponding video graph.</div>
<div class="description-paragraph" id="p-0067" num="0066">Association rules, consistent with the disclosed embodiments, are not limited to a single measure of similarity, and in additional embodiments, an association rule may link video clips that satisfy limitations on a plurality of types of similarity. For example, such an association rule may deem two video clips, or segments of video clips, as connected when a visual similarity exceeds a first threshold value (e.g., 0.9) and a combined auditory and visual similarity exceeds a second threshold value (e.g., 0.9). Using a such a rule, the two music video clips would not connected within a corresponding video graph.</div>
<div class="description-paragraph" id="p-0068" num="0067">In further embodiments, association rules consistent with the disclosed embodiments may connect segments of video clips within a corresponding video graph. For example, a first video clip may include a complete speech delivered by President Obama, and a second video clip may include portions of the speech interspersed with media commentary. Segments may be extracted from the first and second video clips, and the first and second video segments may be associated with corresponding initial positions within the first and second video clips and may share a common duration, e.g., thirty seconds.</div>
<div class="description-paragraph" id="p-0069" num="0068">As discussed above, an association rule may determine that the first and second video clips are connected if a corresponding similarity measure, e.g., an auditory similarity, between the segments exceeds a value of 0.9. Using a such a rule, the first and second video clips would be connected within a corresponding video graph.</div>
<div class="description-paragraph" id="p-0070" num="0069">In further embodiments, association rules consistent with the disclosed embodiments may incorporate limitations on types of similarity measures, and additionally or alternatively, requirements associated with one or more parameters of the video clips. For example, an association rule may determine that two video dips are connected when a value of a combined auditory and visual similarity exceeds a first threshold value (e.g., 0.9) and a duration of the two video clips exceeds a second threshold value (e.g., thirty seconds). Using such a rule, the first and second video clips would not be connected within a video graph, as the durations of the first and second video clips fail to exceed thirty seconds.</div>
<div class="description-paragraph" id="p-0071" num="0070">Although described in terms of a duration of a video clip, association rules consistent with the disclosed embodiments are not limited to such an exemplary characteristic. Additionally or alternatively, such association rules may impose requirements on other characteristics of the video clips, such as a requirement for a common title or producer, a requirement for a common type or quality of video, and any other parameter apparent to one of skill in the art and appropriate to the video clips. Further, for example, such association rules may leverage metadata associated with pairs of video clips (e.g., metadata within content data store <b>144</b>A) to determine whether the video clips are connected within a corresponding video graph.</div>
<div class="description-paragraph" id="p-0072" num="0071"> <figref idrefs="DRAWINGS">FIG. 4</figref> illustrates an exemplary method <b>400</b> for computing an association between a pair of video clips, in accordance with a disclosed embodiment. Method <b>400</b> may provide functionality that enables a recommendations server (e.g., recommendations server <b>142</b>) to compute values indicative of an association between the video clips based on, for example, an application of one or more association rules to measures of similarity between segments of the video clips. As discussed above, and based on the computed association values, an edge within a corresponding video graph may then connect the pair of associated video clips within the video graph.</div>
<div class="description-paragraph" id="p-0073" num="0072">In <figref idrefs="DRAWINGS">FIG. 4</figref>, recommendations server <b>142</b> may obtain information associated with a first video clip and a second video clip in step <b>402</b>. In an embodiment, the obtained information may include metadata associated with the first and second video dips, which may be obtained from a corresponding data repository (e.g., content data store <b>144</b>A). The obtain metadata may include, for example, values of one or more characteristics associated with the first and second video clips.</div>
<div class="description-paragraph" id="p-0074" num="0073">In step <b>404</b>, an association rule may be applied to the metadata associated with the first and second video clips. As discussed above, the association rule may determine a connection between a pair of video clips based on a type of similarity and based on a magnitude of a measure of that type of similarity. Further, in such embodiments, the association rule may be associated with a video graph that links videos having one or more specified relationships, including, but not limited to, video that include duplicative content. For example, the association rule identified in step <b>404</b> determine that two video clips are connected within a video graph when a measure of a corresponding auditory similarity, visual similarity, or combination of auditory and visual similarities exceeds a threshold value.</div>
<div class="description-paragraph" id="p-0075" num="0074">The association rule identified in step <b>404</b> may also impose limitations of one or more characteristics of connected video clips. For example, and as discussed above, such limitations may include, but are not limited to, a requirement that a duration of the video clips exceeds a threshold value, that the video clips be characterized by a specific type, quality, or file format, a source of the video clips, or any additional or alternate characteristics appropriate to the video clips.</div>
<div class="description-paragraph" id="p-0076" num="0075">In step <b>406</b>, recommendation server <b>142</b> may determine whether the metadata associated with the first and second video clips satisfies the association rule. For example, and as discussed above, the association rule may require that connected video clips have a duration that exceeds thirty seconds. In such an embodiment, the metadata associated with the first and second videos may be processed in step <b>406</b> to extract data identifying corresponding durations, and the corresponding durations may be compared against the requirement imposed by the association rule.</div>
<div class="description-paragraph" id="p-0077" num="0076">If it is determined in step <b>406</b> that the metadata fails to satisfy the requirement set forth in the association rule, then no connection exists between the first and second video clips, and in step <b>408</b>, recommendations server <b>142</b> may assign an association value of “null” to the association data for the first and second video clips (i.e., the vector of association values A(i, j)). For example, if the metadata indicates that a source of the first video clip is CNN.com, and a source of the second video clip is YouTube.com, then an association rule requiring that the video clips share a common source would not be satisfied, and no connection would exist between the first and second video dips. In such an embodiment, the association data for the first and second video clips may be output and stored by recommendation server <b>142</b> in step <b>410</b>, and exemplary method <b>400</b> is finished and complete in step <b>411</b>.</div>
<div class="description-paragraph" id="p-0078" num="0077">If, however, step <b>406</b> determines that the metadata of the first and second video clips is consistent with the requirements of the association rule, then the first and second video clips are decomposed into corresponding segments in step <b>412</b> for similarity analysis. For example, as discussed above, if both the first and second video clips were obtained from YouTube.com, then the association rule requiring that the video clips share a common source would be satisfied, and a potential association may exist between segments of the first and second video dips.</div>
<div class="description-paragraph" id="p-0079" num="0078">In an embodiment, the decomposition process of step <b>412</b> may partition the first and second video dips into corresponding video segments having a predetermined duration and being associated with a predetermined shift between consecutive segments. For example, the first and second video clips may be decomposed into corresponding segments having a five second duration and a shift of 0.1 seconds, a fifteen second duration and a shift of 0.3 seconds, or any additional or alternate duration and shift apparent to one of skill in the art and appropriate to the first and second video dips.</div>
<div class="description-paragraph" id="p-0080" num="0079">However, the decomposition processes of step <b>412</b> are not limited to segments associated with such predetermined durations and predetermined shifts. In further embodiments, the association rule identified in step <b>404</b> may identify at least one of a duration or shift into which the first and second video clips are decomposed. In additional embodiments, auditory and visual content associated with the first and second video clips may be analyzed to algorithmically and adaptively decompose the first and second vides into a plurality of segments.</div>
<div class="description-paragraph" id="p-0081" num="0080">In step <b>414</b>, recommendations server <b>142</b> may analyze the first and second video segments to identify pairs of similar first and second video segments. For example, in step <b>414</b>, measures of similarity may be determined for pairs of the first and second video segments based on, for example, an auditory similarity, a visual similarity, or a combination of auditory similarity and visual similarity. The identified association rule may subsequently be applied to the computed similarity measures to identify one or more pairs of similar first and second video segments, as described below in reference to <figref idrefs="DRAWINGS">FIG. 5</figref>.</div>
<div class="description-paragraph" id="p-0082" num="0081"> <figref idrefs="DRAWINGS">FIG. 5</figref> illustrates an exemplary method <b>500</b> for identifying pairs of similar video segments, in accordance with disclosed embodiments. Method <b>500</b> may provide functionality that enables a recommendations server (e.g., recommendations server <b>142</b>) to compute a measure indicative of a similarity between pairs of video segments and to determine whether the similarity measure satisfies a corresponding association rule.</div>
<div class="description-paragraph" id="p-0083" num="0082">In step <b>502</b>, recommendations server <b>142</b> may obtain information identifying a plurality of segments of a first video clip and a second video clip. For example, as described above in reference to <figref idrefs="DRAWINGS">FIG. 4</figref>, the first and second video clips may be partitioned into corresponding segments having a predetermined duration and being associated with a predetermined shift between consecutive segments. For example, the first and second video clips may be decomposed into corresponding segments having a five second duration and a shift of 0.1 seconds, a fifteen second duration and a shift of 0.3 seconds, or any additional or alternate duration and shift apparent to one of skill in the art and appropriate to the first and second video segments.</div>
<div class="description-paragraph" id="p-0084" num="0083">In steps <b>504</b> and <b>506</b>, recommendations server <b>142</b> may select one of the first video segments and one of the second video segments for further similarity analysis. In an embodiment, the selected first and second video segments may have starting times that correspond to starting times of the respective video clips. However, the selected first and second video segment are not limited to such exemplary starting times, and in further embodiments, recommendations server <b>142</b> may select first and second video segments disposed in any additional or alternate temporal location within the respective video clips, without departing from the spirit or scope of the disclosed embodiments.</div>
<div class="description-paragraph" id="p-0085" num="0084">Recommendations server <b>142</b> may subsequently compute measures indicative of a similarity between the first and second video segments in step <b>508</b>. For example, as outlined above, the computed similarity measures may be based on an auditory similarity between the segments, a visual similarity between the segments, or a combination of auditory similarity and visual similarity between the segments, as discussed above in reference to <figref idrefs="DRAWINGS">FIGS. 3A-3C</figref>.</div>
<div class="description-paragraph" id="p-0086" num="0085">In step <b>510</b>, recommendation server <b>142</b> may apply one or more association rules to the computed similarity measures. As described above, and in an embodiment, the one or more association rules may represent a set of minimum requirements for two video segments to connected within a video graph based on, for example, a type of similarity and based on a magnitude of a measure of that type of similarity. For example, the application of the association rule in step <b>510</b> may determine that the pair of video segments are connected within when a measure of a corresponding auditory similarity, visual similarity, or combination of auditory and visual similarities exceeds a threshold value.</div>
<div class="description-paragraph" id="p-0087" num="0086">Recommendation server <b>142</b> may determine in step <b>512</b> whether the pair of first and second video segments are similar, based on the applied association rule and the computed similarity measure. For example, in step <b>512</b>, recommendation server <b>142</b> may determine whether the computed similarity measure exceeds a predetermined threshold value of the associated rule, and additionally or alternatively, whether one or more characteristics of the first and second video clips (e.g., video source, video quality, or duration) satisfy the association rule.</div>
<div class="description-paragraph" id="p-0088" num="0087">If it is determined in step <b>512</b> that the first and second video segments are not similar, the recommendation server <b>142</b> may assign a value of zero to the computed similarity value in step <b>514</b>. In such an embodiment, the assignation of the zero value indicates that the first and second video segments are dissimilar, and recommendations server <b>142</b> may subsequently output and store the assigned value in step <b>516</b>. For example, the assigned value may be stored in metadata associated with the first video clip and additionally or alternatively, with the second video clip, within content data store <b>144</b>A of <figref idrefs="DRAWINGS">FIG. 1</figref>.</div>
<div class="description-paragraph" id="p-0089" num="0088">If, however, it is determined in step <b>512</b> that the first and second video segments are similar, method <b>500</b> passes to step <b>516</b>, in which recommendation server <b>142</b> stores the computed similarity measure in metadata associated with the first video clip and additionally or alternatively, with the second video clip. Method <b>500</b> subsequently passes to step <b>518</b>, which determines whether additional second video segments are available for analysis. If additional second video segments are available for analysis, then method <b>500</b> passes back to step <b>506</b>, which selects an additional second video segment for similarity analysis.</div>
<div class="description-paragraph" id="p-0090" num="0089">Alternatively, if no additional second video segments are available for analysis, then method <b>500</b> passes to step <b>520</b>, which determines whether additional first video segments exists for similarity analysis. If additional first video segments are available for analysis, then method <b>500</b> passes back to step <b>504</b>, which selects an additional first video segment for similarity analysis. If, however, recommendations server <b>142</b> determines in step <b>520</b> that no additional first segments exist, then method <b>500</b> is finished and complete in step <b>522</b>, and the similarity values for the pairs of first and second video segments are output to step <b>414</b> of method <b>400</b>, which identifies similar pairs of first and second video segments based on the outputted similarity measures.</div>
<div class="description-paragraph" id="p-0091" num="0090">Referring back to <figref idrefs="DRAWINGS">FIG. 4</figref>, recommendations server may process the identified pairs of similar first and second video segments to merge neighboring similar video segments into expanded segments in step <b>416</b>. For example, in step <b>416</b>, recommendations server <b>142</b> may determine that the m<sup>th </sup>segment of the first video is similar to the n<sup>th </sup>segment of the second video, and that the (m+1)<sup>st </sup>segment of the first video is similar to the (n+1)<sup>st </sup>segment of the second video. In such an embodiment, the adjacent m<sup>th </sup>and (m+1)<sup>st </sup>segments of the first video may be merged into an expanded first video segment, and adjacent the n<sup>th </sup>and (n+1)<sup>st </sup>segments of the second video may be merged to form an expanded second video segment. Further, in step <b>416</b>, boundaries of the merged first and second video segments may be determined based on, for example, at least one of auditory or visual content of the merged first and second video segments.</div>
<div class="description-paragraph" id="p-0092" num="0091">A similarity measure associated with each of the pairs of merged first video segments and the merged second video segments may be determined in step <b>418</b>. For example, the merged similarity values may be computed as a linear combination of weighted similarity measures corresponding to the pairs of video segments that form the merged pairs. In such an embodiment, the similarity measure for one of pairs of video segments may be weighted in accordance with one or more characteristics of the video segments, including but not limited to a duration of the segments. However, the processes of step <b>418</b> are not limited to such exemplary techniques, and in additional embodiments, recommendations server <b>142</b> may compute the similarity measures for the merged pairs using any additional or alternate technique apparent to one of skill in the art and appropriate to the video segments.</div>
<div class="description-paragraph" id="p-0093" num="0092">Based on the similarity measures computed for the merged segments in step <b>418</b>, and additionally or alternatively, on the similarity measures associated with the similar video segments identified in step <b>414</b>, recommendations server <b>142</b> may determine association values that correspond to the pairs of video segments in step <b>420</b>. For example, and as discussed above, the association value, c<sub>k</sub>(i,j), for the k<sup>th </sup>pair of video segments may be defined based on a type of similarity, a duration of the video segments, start times of the segments within their respective video clips, and the corresponding similarity measures.</div>
<div class="description-paragraph" id="p-0094" num="0093">Upon determination of the association values for the pairs of similar first and second video segments, the association values may be leveraged to generate association data for the first and second video clips (e.g., a vector of association values A(i, j), as outlined above in Equation (1)), and the association data for the first and second video clips may be output and stored in step <b>410</b>. For example, the association data may be stored in video graph data store <b>1448</b> or <figref idrefs="DRAWINGS">FIG. 1</figref>, and additionally or alternatively, within corresponding metadata of content data store <b>144</b>A of <figref idrefs="DRAWINGS">FIG. 1</figref>. Method <b>400</b> is subsequently finished and completed in step <b>411</b>.</div>
<div class="description-paragraph" id="p-0095" num="0094">In the embodiment described above, methods <b>400</b> and <b>500</b> may identify pairs of similar video segments from first and second video clips, and may subsequently merge adjacent pairs to similar video segments. However, the exemplary processes of <figref idrefs="DRAWINGS">FIGS. 4 and 5</figref> are not limited to the identification and subsequent merger of adjacent pairs of similar video segments. In additional embodiments, the processes of <figref idrefs="DRAWINGS">FIGS. 4 and 5</figref> may identify groups or clusters of similar first and second video segments, based on, for example, auditory and/or visual content within the first and second video segments. In such embodiments, the identified groups or clusters of video segments may be merged, and the boundaries of such merged segments may be determined by recommendations server <b>142</b>, as outlined above in reference to step <b>416</b> of <figref idrefs="DRAWINGS">FIG. 4</figref>.</div>
<div class="description-paragraph" id="p-0096" num="0095">Using the processes described above, the association data generated by recommendation server <b>142</b> may indicate a relationship between the pairs of similar segments of the video clips. For example, a first video clip V<sub>1 </sub>includes a complete speech delivered by President Obama, and a second video dip V<sub>2 </sub>includes a portion of the speech having a duration of 15.61 seconds that is interspersed with media commentary. In video clip V<sub>2</sub>, the portion of the speech starts at 11.92 seconds, and a corresponding portion of the speech begins at 30.84 seconds into video clip V<sub>1</sub>. The processes of <figref idrefs="DRAWINGS">FIGS. 4 and 5</figref> may be applied to the video clips V<sub>1 </sub>and V<sub>2 </sub>to generate a set of association values representative of segments of video clips V<sub>1 </sub>and V<sub>2 </sub>that exhibit both an auditory and a visual similarity.</div>
<div class="description-paragraph" id="p-0097" num="0096">As discussed above, in step <b>402</b>, information associated with video clips V<sub>1 </sub>and V<sub>2 </sub>may be obtained from content data store <b>144</b>A, and a corresponding association rule may be identified in step <b>404</b>. For example, the association rule may require that similar video clips be associated with a combined measure of auditory and visual similarity that exceeds 0.9. However, the processes of <figref idrefs="DRAWINGS">FIG. 4</figref> are not limited to such exemplary association rules, and in additional embodiments, the association rule or rules identified in step <b>404</b> may include any additional or alternate limitation on similarity type or video parameter, without departing from the spirit of the disclosed embodiments.</div>
<div class="description-paragraph" id="p-0098" num="0097">Video clips V<sub>1 </sub>and V<sub>2 </sub>may subsequently be decomposed into corresponding segments in step <b>412</b> of <figref idrefs="DRAWINGS">FIG. 4</figref>. For example, video clips V<sub>1 </sub>and V<sub>2 </sub>may be divided into segments having a duration of five seconds and a shift between consecutive segments of 0.1 seconds. Thus, for a relevant portion of twenty seconds, the processes of step <b>412</b> will generate <b>200</b> video segments that include the relevant portions of the first and second video clips.</div>
<div class="description-paragraph" id="p-0099" num="0098">For each video segment in V<sub>1</sub>, the processes of step <b>412</b> identify all the similar segments in V<sub>2</sub>, i.e., the segments association with a corresponding measure of auditory and visual similarity that exceeds 0.9. During such processes, one or more of the similar video segments in V<sub>2 </sub>may temporally overlap, and in such embodiments, the processes of steps <b>412</b> may identify one of the overlapping similar video segments in V<sub>2 </sub>is associated with the largest measure of similarity, and retain that identify overlapping video segment in V<sub>2 </sub>for further analysis.</div>
<div class="description-paragraph" id="p-0100" num="0099">For example, step <b>414</b> may determine that a segment from V<sub>1 </sub>starting at 30.8 seconds is similar to three segments from V<sub>2 </sub>respectively starting at 11.8 seconds, 11.9 seconds, and 12.0 seconds within video V<sub>2 </sub>and respectively being associated with similarity values of 0.91, 0.95, and 0.93. As portions of these segments overlap in temporal space, the processes of step <b>412</b> retain the video segment in V<sub>2 </sub>starting at 11.9 seconds, as this segment is associated with the largest similarity measure, i.e. 0.95.</div>
<div class="description-paragraph" id="p-0101" num="0100">As discussed above, in step <b>416</b>, adjacent segments in V<sub>1 </sub>may be continuously merged to form an expanded V<sub>1 </sub>segment if the adjacent segments in V<sub>1 </sub>are deemed similar to corresponding adjacent segments in V<sub>2</sub>. Further, as discussed above, the corresponding adjacent segments in V<sub>2 </sub>may also be continuously merged to yield a similar pair of merged video segments from V<sub>1 </sub>and V<sub>2</sub>.</div>
<div class="description-paragraph" id="p-0102" num="0101">For example, a first segment from V<sub>1 </sub>starting at 30.8 seconds may be similar to a first segment from V<sub>2 </sub>starting at 11.9 seconds. A consecutive segment in V<sub>2 </sub>starts at 30.9 seconds, and a consecutive segment from V<sub>2 </sub>starts at 12.0 seconds. Assuming the two pairs of segments are associated within similarity measures that satisfy the association rule (i.e., similarity measures that exceed 0.9), the first and consecutive segments in V<sub>1 </sub>are merged into an expanded V<sub>1 </sub>segment, and the first and consecutive segments in V<sub>2 </sub>are merged into an expanded V<sub>2 </sub>segment.</div>
<div class="description-paragraph" id="p-0103" num="0102">As such, the merging processes of step <b>416</b> generate an expanded V<sub>1 </sub>segment of 5.1 seconds starting at 30.8 seconds in V<sub>1 </sub>matching an expanded V<sub>2 </sub>segment of 5.1 starting at 11.9 seconds within V<sub>2</sub>. Further, as the merging process is applied to each of the segments of video clips V<sub>1 </sub>and V<sub>2</sub>, a duration of the expanded segment may increase from 5.1 seconds to 15.6 seconds, i.e., the length of the common portion of the video clip.</div>
<div class="description-paragraph" id="p-0104" num="0103">After merging, the exemplary processes of step <b>416</b> may determine the boundaries of each matching expanded segments based on analysis of audio-visual scenes for video clips V<sub>1 </sub>and V<sub>2</sub>. For example, an abrupt visual scene change at may be identified 11.92 and 27.53 second into V<sub>2</sub>, which has the highest similarity measure of 0.98 with a scene starting at 30.84 seconds into V<sub>1</sub>. In such embodiment, the association between video clips V<sub>1 </sub>and V<sub>2</sub>, A(1,2), takes the following form:
<br/>
<i>A</i>(1,2)={(1,15.61,30.84,11.92,0.98)}.  (3)
<br/>
In Equation (3), a similarity type p is equivalent to unity, indicating that the similarity measure relates to a combined auditory and visual similarity, a duration d of the similar segments is 15.61 seconds, a start time t<sub>1 </sub>of the portion in video clip V<sub>1 </sub>is 30.84 seconds, a start time t<sub>2 </sub>of the portion within video clip V<sub>2 </sub>is 11.92 seconds, and the resulting similarity measure s<sub>12 </sub>is 0.98. The association value A(1,2) is subsequently stored in step <b>410</b> and method <b>400</b> is finished and completed in step <b>412</b>.
</div>
<div class="description-paragraph" id="p-0105" num="0104">Further, based on the stored association data, one or more video graphs may be generated to illustrate a network of video clips, and additionally or alternatively, segments of the video clips, that include identical or similar portions of audio and/or visual content. For example, video graphs consistent with the disclosed embodiments can be represented as bi-partite graphs having nodes that represent video clips, and edges that connects the video clips and represent an association between the two video dips, as determined by similarities between corresponding segments of the video clips, as discussed below in reference to <figref idrefs="DRAWINGS">FIGS. 6-9</figref>.</div>
<div class="description-paragraph" id="p-0106" num="0105"> <figref idrefs="DRAWINGS">FIG. 6</figref> is an exemplary video graph <b>600</b> of a network of similar video clips, in accordance with one of the disclosed embodiments. In video graph <b>600</b>, the exemplary network includes video clips “Video 1,” “Video 2,” “Video 3,” “Video 4,” and “Video 5,” which are represented, respectively, by nodes <b>602</b>, <b>604</b>, <b>606</b>, <b>608</b>, and <b>610</b>. Further, in video graph <b>600</b>, edges <b>622</b>, <b>624</b>, <b>626</b>, <b>628</b>, <b>630</b>, and <b>632</b> connect corresponding ones of the nodes, and are indicative of an association between the video clips associated with the nodes.</div>
<div class="description-paragraph" id="p-0107" num="0106">For example, in <figref idrefs="DRAWINGS">FIG. 6</figref>, edge <b>622</b> connects nodes <b>602</b> and <b>604</b>, and edge <b>624</b> connects nodes <b>602</b> and <b>606</b>. As such, an association exists between “Video 1” and “Video 2,” and between “Video 1” and “Video 3,” and at least a portion of “Video 1” is similar to corresponding segments of “Video 2” and “Video 3.”</div>
<div class="description-paragraph" id="p-0108" num="0107">Further, in <figref idrefs="DRAWINGS">FIG. 6</figref>. edge <b>626</b> connects nodes <b>604</b> and <b>608</b>, edge <b>628</b> connects node <b>606</b> to node <b>608</b>, and edge <b>630</b> connects node <b>606</b> to <b>610</b>. Thus, in view of video graph <b>600</b>, an association exists between “Video 2” and “Video 4,” between “Video 3” and “Video 4,” and between “Video” 3” and “Video 6.” Further, in view of edge <b>632</b> that connects nodes <b>608</b> and <b>610</b>, an association exists between “Video 4” and “Video 5.”</div>
<div class="description-paragraph" id="p-0109" num="0108">In an embodiment, the associations described by video graph <b>600</b> may be determined through an application of one or more association rules to metadata associated with the video clips and to measures of similarity between segments of the video clips, as discussed above in reference for <figref idrefs="DRAWINGS">FIGS. 4 and 5</figref>. For example, the similarity measures may be associated with an auditory similarity between segments of video clips, a visual similarity between segments of video clips, or a combination of auditory and visual similarities, as discussed above in reference to <figref idrefs="DRAWINGS">FIGS. 3A-3C</figref>. Further, in an embodiment, information associated with video graph <b>600</b>, including, but not limited to information identifying nodes, corresponding edges, and associations between video clips associated with the nodes, may be stored in a data repository accessible over network <b>120</b>, e.g., video graph data store <b>144</b>B.</div>
<div class="description-paragraph" id="p-0110" num="0109">As discussed above, metadata associated with a corresponding video clip may specify structural information associated with the video clip, such as a creation date, and contextual information associated with the video clip, such as an event or events referenced by the video clip. In such embodiments, an association rule may leverage the structural and contextual information specified within the metadata to contract a video graph that associated video clips based not only on an auditory or a visual similarity, but also on the basis of a common event referenced by the video clips and/or a chronological order of the associated video clips.</div>
<div class="description-paragraph" id="p-0111" num="0110"> <figref idrefs="DRAWINGS">FIG. 7</figref> illustrates an exemplary video graph <b>700</b> associated with a single chronological event, in accordance with a disclosed exemplary embodiment. In video graph <b>700</b>, the exemplary network includes video clips “Video 1,” “Video 2,” “Video 3,” “Video 4,” “Video 5,” “Video 6,” and “Video 7,” which are represented, respectively, by nodes <b>702</b>, <b>704</b>, <b>706</b>, <b>708</b>, <b>710</b>, <b>712</b>, and <b>714</b>. Further, in video graph <b>700</b>, unidirectional edges <b>722</b>, <b>724</b>, <b>726</b>, <b>728</b>, <b>730</b>, and <b>732</b> connect corresponding pairs of the nodes, and indicate an association between video clips associated with the nodes.</div>
<div class="description-paragraph" id="p-0112" num="0111">In contrast to exemplary video graph <b>600</b> of <figref idrefs="DRAWINGS">FIG. 6</figref>, the associations between linked video clips in video graph <b>700</b> indicate not only an auditory and/or a visual similarity between linked video clips, but also a common event associated with the linked video clips and a chronological order in which the linked video clips were created. For example, as discussed above, an association rule may leverage metadata associated with the linked video clips to identify a date on which the video was created, and additionally, an event associated with or referred to within the video. For example, a unidirectional edge two linking nodes within video graph <b>700</b> indicates not only an association between the video clips associated with the nodes, but also an order in which the video clips corresponding to the linked nodes were produced or made available to users of communications network <b>120</b>.</div>
<div class="description-paragraph" id="p-0113" num="0112">For example, in <figref idrefs="DRAWINGS">FIG. 7</figref>, unidirectional edge <b>722</b> between node <b>702</b> and node <b>706</b> indicates an association between “Video 1” and “Video 3” and further, that “Video 3” was produced subsequent to “Video 1.” Similarly, for example, unidirectional edges <b>724</b>, <b>726</b>, and <b>728</b> between node <b>704</b> and nodes <b>706</b>, <b>708</b>, and <b>710</b>, respectively, indicate respectively associations between “Video 2” and “Video 3,” “Video 4,” and “Video 5,” and additionally, that “Video 3,” “Video 4,” and “Video 5” were produced subsequent to “Video 2.” Further, in <figref idrefs="DRAWINGS">FIG. 7</figref>, unidirectional edge <b>730</b> between nodes <b>708</b> and <b>714</b> and unidirectional edge <b>732</b> between nodes <b>710</b> and <b>714</b> indicate, respectively, associations between “Video 4” and” “Video 7,” and between “Video 6” and “Video 7.” Unidirectional edges <b>730</b> and <b>732</b> further indicate that “Video 7” was produced subsequent to “Video 4” and “Video 6.”</div>
<div class="description-paragraph" id="p-0114" num="0113">Further, in additional embodiments, a video graph can have multiple association rules, with each rule applying to a subset of video clips. For example, a video graph may represent a combination of multiple video graphs, which may be referred to as “sub-video graphs.” In such an embodiment, the video graph can have association rules defining the association of videos within sub-video graphs, (e.g., sub-association rules), and additional association rules that define defining associations between the sub-video graphs (e.g., global association rules). For example, the sub-association rules and the global association rules may be based on, but are not limited to, auditory similarities, visual similarities, combinations of auditory and visual similarities, an association with a common event, and any additional or alternate parameter apparent to one of skill in the art and appropriate to the videos.</div>
<div class="description-paragraph" id="p-0115" num="0114"> <figref idrefs="DRAWINGS">FIG. 8</figref> is a diagram of an exemplary multiple-event video graph <b>800</b>, consistent with disclosed embodiments. In <figref idrefs="DRAWINGS">FIG. 8</figref>, multiple-event video graph <b>800</b> may include single-event video graphs <b>802</b>, <b>804</b>, and <b>806</b>. Further, in <figref idrefs="DRAWINGS">FIG. 8</figref>, edge <b>822</b> connect single-event video graphs <b>802</b> and <b>804</b>, and edge <b>824</b> connects single-event video graphs <b>804</b> and <b>806</b>. As discussed above, edges <b>822</b> and <b>824</b> are indicative of an association between video clips or segments of video clips within corresponding ones of single-event video graphs <b>802</b>, <b>804</b>, and <b>806</b>. For example, the associations indicated by the edges of video graph <b>800</b> may be determined using a global association rule in accordance with one or more of the exemplary processes of <figref idrefs="DRAWINGS">FIGS. 3A-3C, 4, and 5</figref>.</div>
<div class="description-paragraph" id="p-0116" num="0115">For example, individual single-event video graphs <b>802</b>, <b>804</b>, and <b>806</b> may respectively represent networks of video clips that includes identical or similar segments of audio and/or visual content, and/or that are associated with corresponding events, as described above in reference to <figref idrefs="DRAWINGS">FIGS. 6 and 7</figref>. Further, single-event video graphs <b>802</b>, <b>804</b>, and <b>806</b> may be defined according to one or more sub-association rules that determine associations between linked videos and may establish a production chronology within the linked video clips.</div>
<div class="description-paragraph" id="p-0117" num="0116">Although described in terms of multiple, single-event video graphs, sub-video graphs consistent with the disclosed embodiments are not limited to such exemplary configurations. In additional embodiments, a video graph may include any additional or alternate set of sub-video graphs, defined in accordance with any appropriate sub-association rule, without departing from the spirit or scope of the disclosed embodiments. For example, such sub-video graphs may be defined based on sub-association rules requiring that similar video clips share a common source, are of a common file type, are of a common quality, reference a common product, were accessed by members of a pre-determined group of users, or any additional or alternate requirements apparent to one of skill in the art and determinable using metadata associated with the videos.</div>
<div class="description-paragraph" id="p-0118" num="0117">Moreover, the disclosed video graphs are not limited to representations of networks of similar video clips or segments of video clips. In additional embodiments, an extended video graph may include a video graph that represents of a linked network of video clips, a graph representing a social network of users (e.g., through Facebook, MySpace, LinkedIn, or Google+), and connections between the video graph and the social network graph that identify video clips viewed by specific users of the social network, as described below in reference to <figref idrefs="DRAWINGS">FIG. 9</figref>.</div>
<div class="description-paragraph" id="p-0119" num="0118"> <figref idrefs="DRAWINGS">FIG. 9</figref> is a diagram of an exemplary extended video graph <b>900</b>, in accordance with a disclosed embodiment. In the embodiment of <figref idrefs="DRAWINGS">FIG. 9</figref>, extended video graph <b>900</b> includes a first “layer” of association, which corresponds to a video graph describing a network of associated video clips, and a second “layer” of association, which corresponds to set of users linked within a social network. For example, the video graph describing the first layer of association may correspond to one of exemplary video graphs described above in reference to <figref idrefs="DRAWINGS">FIGS. 6-8</figref>, and may be generated using any of the exemplary processes described above.</div>
<div class="description-paragraph" id="p-0120" num="0119">In <figref idrefs="DRAWINGS">FIG. 9</figref>, the first layer of extended video graph <b>900</b> includes “Video 1,” “Video 2,” “Video 3,” “Video 4,” and “Video 5,” which are represented, respectively, by nodes <b>902</b>, <b>904</b>, <b>906</b>, <b>908</b>, and <b>910</b>. Further, in extended video graph <b>900</b>, edges <b>912</b>, <b>914</b>, <b>916</b>, <b>918</b>, and <b>920</b> connect corresponding ones of the nodes, and are indicative of an association between video clips associated with the nodes. For example, the association between the video clips in extended video graph <b>900</b> may be based on an auditory similarity, a visual similarity, a combination of auditory and visual similarities, or on a value of one or more parameters characterizing the video clips, including, but not limited to, sources of the video clips, durations of the video clips, types and qualities of the video clips, and any additional or alternate parameter appropriate to the video clips. In such embodiments, the values of such parameters may be obtained from metadata associated with the video clips, e.g., as stored within content data store <b>144</b>A.</div>
<div class="description-paragraph" id="p-0121" num="0120">The second layer of extended video graph <b>900</b> includes “User A,” “User B,” and “User C,” which are represented by nodes <b>942</b>, <b>944</b>, and <b>946</b>. Further, edges <b>952</b> and <b>954</b> connect corresponding ones of the nodes and are indicative of a link between the users associated with the linked nodes. For example, edge <b>952</b> indicates that “User A” and “User C” are linked within the social network, and edge <b>954</b> indicates that “User B” and “User C” are linked within the social network.</div>
<div class="description-paragraph" id="p-0122" num="0121">In the embodiment of <figref idrefs="DRAWINGS">FIG. 9</figref>, the first and second layers of extended video graph <b>900</b> are linked through a pattern of video consumption exhibited by members of the social network. For example, edges <b>962</b> and <b>964</b> indicate that “User A” has accessed “Video 1” and “Video 2,” edges <b>966</b> and <b>968</b> indicate that “User B” has accessed “Video 3” and “Video 4,” and edges <b>970</b> and <b>972</b> indicate that “User C” has accessed “Video 4” and “Video 5.”</div>
<div class="description-paragraph" id="p-0123" num="0122">Further, in additional to describing that users have accessed particular videos, information associated with extended video graph <b>900</b> may also characterize a nature of the users' interaction with videos. For example, information association with edge <b>962</b> may indication that “User A” has accessed “Video 1,” and may further indicate a number of times that “User A” has accessed “Video 1,” segments of “Video 1” that are popular with “User A,” and a date or time at which “User A” last accessed “Video 1.” Such information is not, however, limited to such exemplary access information, and in additional embodiments, any additional or alternate information characterizing a user's access of a video may be stored within video graph data <b>144</b>A, without departing from the spirit or scope of the disclosed embodiments.</div>
<div class="description-paragraph" id="p-0124" num="0123">In <figref idrefs="DRAWINGS">FIG. 9</figref>, edges that connect users within the social network may be explicitly established by users through a procedure specified by the corresponding social networking application. For example, edge <b>952</b> connection “User A” and “User C” may indicate that “User A” requested a connection with “User C” within the social networking application, that the “User C” subsequently affirmed the request to establish the connection. However, the association between users of the social networking application is not limited to such an exemplary procedure, and in additional embodiments, an association between users may be established automatically based on a pattern of video consumption, as described below in reference to <figref idrefs="DRAWINGS">FIG. 10</figref>.</div>
<div class="description-paragraph" id="p-0125" num="0124"> <figref idrefs="DRAWINGS">FIG. 10</figref> is a flowchart of an exemplary method <b>1000</b> for associating users based patterns of video consumption, in accordance with a disclosed embodiment. Method <b>1000</b> may provide functionality that enables a recommendations server (e.g., recommendations server <b>142</b>) to automatically associate of members of a social network based on corresponding patterns of multimedia content consumption. However, the exemplary processes of <figref idrefs="DRAWINGS">FIG. 10</figref> are not limited to members of social networks, and in additional embodiments, method <b>1000</b> may be applied to multimedia consumption patterns associated with any additional or alternate linking of users, or to any number of arbitrary users.</div>
<div class="description-paragraph" id="p-0126" num="0125">In <figref idrefs="DRAWINGS">FIG. 10</figref>, information associated with a video consumption pattern of a first user and a video consumption pattern of a second user is obtained in step <b>1002</b>. In an embodiment, the information may indicate a list of videos accessed by a user and a number of times the user accessed the videos. For example, such information may be stored within a data repository accessible via communications network <b>120</b>, e.g., data repository <b>144</b> of <figref idrefs="DRAWINGS">FIG. 1</figref>.</div>
<div class="description-paragraph" id="p-0127" num="0126">In step <b>1004</b>, the video consumption data associated with the first and second users may be processed to identify one or more pairs of video clips accessed by the first user and the second user. Video graph data for the identified pairs of video clips may be access in step <b>1006</b> from, for example, video graph data store <b>144</b>B of <figref idrefs="DRAWINGS">FIG. 1</figref>. For example, the video graph data may correspond to an extended video graph that identifies associations between video clips and additionally, associations between individual users capable of accessing the video clips, as described above in reference to <figref idrefs="DRAWINGS">FIG. 9</figref>.</div>
<div class="description-paragraph" id="p-0128" num="0127">A value indicative of an association between the video consumption patterns of the first and second users may be computed in step <b>1008</b> based on, for example, the video consumption data obtained in step <b>1002</b> and the video graph data obtained in step <b>1004</b>. For example, the computed association value may represent a an average association value computed across the pairs of video clips identified in step <b>1004</b>. In such an embodiment, each of the pairs of video clips identified in step <b>1004</b> contributes equally to the average association value, and may be associated with corresponding weight factors of unity.</div>
<div class="description-paragraph" id="p-0129" num="0128">In additional embodiments, the computation of the association value in step <b>1008</b> may represent a weighted average of the association values of the pairs of video clips identified in step <b>1004</b>. For example, the association values for the pairs of video clips may be weighted in accordance with a frequency at which the pairs of video clips have been accessed by the first and second users, and additionally or alternatively, an indication of a time or date at which the pairs of video clips were last accessed by the first and second users. Further, in additional embodiments, the association values for the video clips may be further weighted to account for any additional or alternate factor relevant to the video consumption patterns of the first and second users, without departing from the spirit or scope of the disclosed embodiments.</div>
<div class="description-paragraph" id="p-0130" num="0129">Step <b>1010</b> subsequently determines whether the association value computed in step <b>1008</b> exceeds a threshold value. If it is determined in step <b>1010</b> that the association value for the first and second users does not exceed the threshold value, then step <b>1012</b> determines that no association exists between the first and second users. In such an embodiment, method <b>1000</b> is finished and completed in step <b>1016</b>.</div>
<div class="description-paragraph" id="p-0131" num="0130">Alternatively, if it is determined in step <b>1010</b> that the association value for the first and second users exceeds the threshold value, then an association is established in step <b>1014</b> between the first and second users, and a corresponding edge connects the first and second users in a video graph, e.g., edge <b>952</b> of extended video graph <b>900</b>. The video graph information stored in a corresponding data repository, e.g., video data store <b>144</b> of <figref idrefs="DRAWINGS">FIG. 1</figref>, may subsequently be updated to reflect the association between the first and second users. Exemplary method <b>1000</b> is finished and completed in step <b>1016</b>.</div>
<div class="description-paragraph" id="p-0132" num="0131">The exemplary processes of <figref idrefs="DRAWINGS">FIG. 10</figref> may identify an association between individual users that access and consume multimedia, and in particular, video content. Further, the processes of <figref idrefs="DRAWINGS">FIG. 10</figref> may facilitate the association of individual users, regardless of whether the individual users are members of a social network, and additionally or alternatively, without an affirmative action by the individual users (e.g., a request to associate).</div>
<div class="description-paragraph" id="p-0133" num="0132">To illustrate the exemplary processes of <figref idrefs="DRAWINGS">FIG. 10</figref>, consider that a first user x accesses a set of video clips X, and user y accesses a set of video clips Y. In step <b>1002</b>, video consumption information for user x and video consumption information for user y is accessed. For example, user x may be associated with video consumption information X={x<sub>1</sub>(1, 1), x<sub>2</sub>(3, 3), x<sub>3</sub>(1, 0)}, and user y may be associated with video consumption information Y={y<sub>1</sub>(1, 1), y<sub>2</sub>(2, 1), y<sub>3</sub>(3, 2), y<sub>4</sub>(1, 0), y<sub>5</sub>(1, 0)}. In such an exemplary embodiment, video consumption information takes the form z(i, j), in which i represents a number of times a video clip was accessed by a user, and j represents a number of times the accessed video clip was consumed by the user.</div>
<div class="description-paragraph" id="p-0134" num="0133">Pairs of video clips possibly viewed by user x and user y may be identified in step <b>1004</b>, and video graph data associated with the identified pairs of videos may be obtained in step <b>1006</b>. As described above, the video graph data may include association values A corresponding to the identified pairs, and the obtained video graph data for the identified pairs takes the following form:
<br/>
<i>A</i>(<i>x</i> <sub>1</sub> <i>,y</i> <sub>1</sub>)={(1,21.1,1.9,2.1,0.99)};  (4)
<br/>
<i>A</i>(<i>x</i> <sub>2</sub> <i>,y</i> <sub>2</sub>)={(1,12.3,4.1,32.5,0.93)};  (5)
<br/>
<i>A</i>(<i>x</i> <sub>2</sub> <i>,y</i> <sub>3</sub>)={(1,12.3,20.6,11.7,0.96)};  (6)
<br/>
<i>A</i>(<i>x</i> <sub>1</sub> <i>,y</i> <sub>2</sub>)=<i>A</i>(<i>x</i> <sub>1</sub> <i>,y</i> <sub>3</sub>)=<i>A</i>(<i>x</i> <sub>1</sub> <i>,y</i> <sub>4</sub>)=<i>A</i>(<i>x</i> <sub>1</sub> <i>,y</i> <sub>5</sub>)={ };  (7)
<br/>
<i>A</i>(<i>x</i> <sub>2</sub> <i>,y</i> <sub>1</sub>)=<i>A</i>(<i>x</i> <sub>2</sub> <i>,y</i> <sub>4</sub>)=<i>A</i>(<i>x</i> <sub>2</sub> <i>,y</i> <sub>5</sub>)={ }; and  (8)
<br/>
<i>A</i>(<i>x</i> <sub>3</sub> <i>,y</i> <sub>1</sub>)=<i>A</i>(<i>x</i> <sub>3</sub> <i>,y</i> <sub>2</sub>)=<i>A</i>(<i>x</i> <sub>3</sub> <i>,y</i> <sub>3</sub>)=<i>A</i>(<i>x</i> <sub>3</sub> <i>,y</i> <sub>4</sub>)=<i>A</i>(<i>x</i> <sub>3</sub> <i>,y</i> <sub>5</sub>)={ },   (9)
<br/>
in which “{ }” corresponds to a null set.
</div>
<div class="description-paragraph" id="p-0135" num="0134">In step <b>1008</b>, a summary value describing an association between user x and user y may be computed based, for example, the video graph data obtained for the pairs of videos in step <b>1006</b>. Summary association values B may be initially calculated for the pairs of videos based on the association values. For example, the summary association values may be set to unity if a pair of videos are associated, and zero is the pair of videos is not association, as follows:
<br/>
<i>B</i>(<i>x</i> <sub>1</sub> <i>,y</i> <sub>1</sub>)=<i>B</i>(<i>x</i> <sub>2</sub> <i>,y</i> <sub>2</sub>)=<i>B</i>(<i>x</i> <sub>2</sub> <i>,y</i> <sub>3</sub>)=1;  (10)
<br/>
<i>B</i>(<i>x</i> <sub>1</sub> <i>,y</i> <sub>2</sub>)=<i>B</i>(<i>x</i> <sub>1</sub> <i>,y</i> <sub>3</sub>)=<i>B</i>(<i>x</i> <sub>1</sub> <i>,y</i> <sub>4</sub>)=<i>B</i>(<i>x</i> <sub>1</sub> <i>,y</i> <sub>5</sub>)=0;  (11)
<br/>
<i>B</i>(<i>x</i> <sub>2</sub> <i>,y</i> <sub>1</sub>)=<i>B</i>(<i>x</i> <sub>2</sub> <i>,y</i> <sub>4</sub>)=<i>B</i>(<i>x</i> <sub>2</sub> <i>,y</i> <sub>5</sub>)=0; and  (12)
<br/>
<i>B</i>(<i>x</i> <sub>3</sub> <i>,y</i> <sub>1</sub>)=<i>B</i>(<i>x</i> <sub>3</sub> <i>,y</i> <sub>2</sub>)=<i>B</i>(<i>x</i> <sub>3</sub> <i>,y</i> <sub>3</sub>)=<i>B</i>(<i>x</i> <sub>3</sub> <i>,y</i> <sub>4</sub>)=<i>B</i>(<i>x</i> <sub>3</sub> <i>,y</i> <sub>5</sub>)=0.  (13)
</div>
<div class="description-paragraph" id="p-0136" num="0135">The association value for user x and user y may be computed based on the summary association values. For example, as discussed above, the association value may be computed as a simple average of the summary association values of the pairs of videos. In such an embodiment, each accessed video clip pair is weighted equally, and the average association value between user x and user y takes the following form:</div>
<div class="description-paragraph" id="p-0137" num="0136"> <maths id="MATH-US-00001" num="00001"> <math overflow="scroll"> <mtable> <mtr> <mtd> <mrow> <mrow> <mfrac> <mrow> <mo>∑</mo> <mrow> <mi>B</mi> <mo>⁡</mo> <mrow> <mo>(</mo> <mrow> <msub> <mi>x</mi> <mi>i</mi> </msub> <mo>,</mo> <msub> <mi>y</mi> <mi>j</mi> </msub> </mrow> <mo>)</mo> </mrow> </mrow> </mrow> <mrow> <msub> <mi>N</mi> <mi>X</mi> </msub> <mo>⁢</mo> <msub> <mi>N</mi> <mi>Y</mi> </msub> </mrow> </mfrac> <mo>=</mo> <mrow> <mfrac> <mn>3</mn> <mrow> <mn>3</mn> <mo>×</mo> <mn>5</mn> </mrow> </mfrac> <mo>=</mo> <mn>0.2</mn> </mrow> </mrow> <mo>,</mo> </mrow> </mtd> <mtd> <mrow> <mo>(</mo> <mn>14</mn> <mo>)</mo> </mrow> </mtd> </mtr> </mtable> </math> </maths> <br/>
where N<sub>X </sub>and N<sub>Y </sub>are the number of video clips in lists X and Y.
</div>
<div class="description-paragraph" id="p-0138" num="0137">In additional embodiments, and as discussed above, accessed video clips that are watched more frequently may be assigned a larger weight factor in the computation of the average association value in step <b>1008</b>. For example, weight factors for video clips x<sub>i </sub>and y<sub>j </sub>based on video consumption may be computed as follows:</div>
<div class="description-paragraph" id="p-0139" num="0138"> <maths id="MATH-US-00002" num="00002"> <math overflow="scroll"> <mtable> <mtr> <mtd> <mrow> <mrow> <mrow> <mi>w</mi> <mo>⁡</mo> <mrow> <mo>(</mo> <msub> <mi>x</mi> <mi>i</mi> </msub> <mo>)</mo> </mrow> </mrow> <mo>=</mo> <mfrac> <msub> <mi>c</mi> <mi>i</mi> </msub> <msub> <mi>C</mi> <mi>X</mi> </msub> </mfrac> </mrow> <mo>;</mo> <mi>and</mi> </mrow> </mtd> <mtd> <mrow> <mo>(</mo> <mn>15</mn> <mo>)</mo> </mrow> </mtd> </mtr> <mtr> <mtd> <mrow> <mrow> <mrow> <mi>w</mi> <mo>⁡</mo> <mrow> <mo>(</mo> <msub> <mi>y</mi> <mi>j</mi> </msub> <mo>)</mo> </mrow> </mrow> <mo>=</mo> <mfrac> <msub> <mi>c</mi> <mi>j</mi> </msub> <msub> <mi>C</mi> <mi>Y</mi> </msub> </mfrac> </mrow> <mo>,</mo> </mrow> </mtd> <mtd> <mrow> <mo>(</mo> <mn>16</mn> <mo>)</mo> </mrow> </mtd> </mtr> </mtable> </math> </maths> <br/>
where c is the times a particular video being watched, and C<sub>X</sub>=1+3+1=5 and C<sub>Y</sub>=1+2+3+1+1=8 represent the total times user x and y watched videos, respectively. The corresponding weighted average association takes the following form:
<br/>
Σ<i>w</i>(<i>x</i> <sub>i</sub>)<i>w</i>(<i>y</i> <sub>j</sub>)<i>B</i>(<i>x</i> <sub>i</sub> <i>,y</i> <sub>j</sub>)=⅕×⅛×1+⅗× 2/8×1+⅗×⅜×1=0.4.  (17)
</div>
<div class="description-paragraph" id="p-0140" num="0139">As discussed above, step <b>1010</b> determines whether the association value between the users x and y exceeds a threshold value. An association between users x and y may be established in step <b>1014</b> if the association value exceeds the threshold, and alternatively, step <b>1012</b> determines that no association exists between users x and y when the association value does not exceed the threshold. As described above, the association of users x and y may proceed automatically without user intervention, or alternatively, the association may require an affirmative validation by one or both of users x and y.</div>
<div class="description-paragraph" id="p-0141" num="0140">Using the exemplary techniques outlined above, recommendations system <b>140</b> may generate video graphs that represent networks of video clips that include identical or similar segments of audio and/or visual content, and additionally or alternatively, that are accessed by similar viewers or groups of viewers. In such embodiments, recommendations system <b>140</b> may leverage the generated video graphs to share metadata between linked first and second video clips, and additionally or alternatively, to enhance the metadata associated with the linked first and second video clips using information available to recommendations system <b>140</b> across communications network <b>120</b>.</div>
<div class="description-paragraph" id="p-0142" num="0141">For example, the first and second video clips may be associated with a particular episode of broadcast or cable television series. In such an embodiment, recommendations system <b>140</b> may obtain information associated with the particular episode, including but not limited to, a title of the series, a title of the particular episode, and actors associated with the episode, and may update the metadata associated with the first and second video clips to include the obtained information. Similarly, the first and second video clips may be associated with a movie, and recommendations system <b>140</b> may update the metadata associated with the first and second video clips to include information associated with the movie, including but not limited to, a title of the movie, a textual description of the movie, and one or more actors associated with the movie.</div>
<div class="description-paragraph" id="p-0143" num="0142">Further, for example, recommendations server may update the metadata of the first and second video clips of a music video to add information associated with the music video, which may includes, but is not limited to: a name of a song; a name of a corresponding album; and/or data associated with one or more performers. Additionally, if the linked first and second video clips reference a common geographic location or a common event, recommendation system <b>140</b> may enhance the metadata of the linked first and second videos to including information associated with the common geographic location or event.</div>
<div class="description-paragraph" id="p-0144" num="0143">However, such exemplary enhancement processes are not limited to metadata associated with linked videos, and in additional embodiments, recommendations system may enhance metadata associated with individual consumers of video content, as identified within the second layer of association within the video graph of <figref idrefs="DRAWINGS">FIG. 9</figref>. In such an embodiment, a user of a social networking application may be associated with corresponding metadata identifying one or more video clips consumed by the user, and recommendations system <b>140</b> may leverage video graph data to enhance the metadata of the user to identify additional videos of potential interest to the user. For example, the metadata may be augmented to include information identifying additional video clips associated with an event (e.g., a common television show or movie), a scene (e.g., sports scenes, action scenes, and funny scenes), an individual (e.g., a musician, an actor/actress, or a politician), or other video characteristics (e.g., a channels, a language, a category, or an available time) referenced by the video clips consumed by the user.</div>
<div class="description-paragraph" id="p-0145" num="0144">Further, using the exemplary techniques outlined above, recommendations system <b>140</b> may leverage the generated video graphs to enhance a set of recommended videos that may be provided to a user. For example, a content provider (e.g., a news organization, such as nytimes.com, and a provider of streaming video, such as hulu.com and youtube.com) may provide a user with one or more “default” lists of video content, which may be displayed to the user at a user device (e.g., user device <b>102</b>). In such embodiments, a web site associated with a new organization may organize available video content into lists corresponding to popularity and content, e.g., economic news and/or international news, and may provide information associated with these lists to the user.</div>
<div class="description-paragraph" id="p-0146" num="0145">The web site may further enable the user to select one of the displayed lists, and to browse through a plurality of videos associated with the selected list to select a video of interest. Upon selection of the video by the user, user device <b>102</b> may, either programmatically or in response to a user instruction, establish a connection with video recommendations system <b>140</b> over network <b>120</b>, and may subsequently transmit information identify the selected video, and additionally or alternatively, the selected video list, to a web server associated with recommendations server <b>142</b>. In such an embodiment, video recommendations system <b>140</b> may leverage the generated video graphs to identify additional video content that is contextually similar to the selected video and/or the selected video list, and additionally or alternatively, that has been viewed by other users having viewing habits similar to the user, as described below in reference to <figref idrefs="DRAWINGS">FIG. 11</figref>.</div>
<div class="description-paragraph" id="p-0147" num="0146"> <figref idrefs="DRAWINGS">FIG. 11</figref> is a flowchart of an exemplary method <b>1100</b> for providing identifying video content based on video graph data, according to disclosed embodiments. Method <b>1100</b> may provide functionality that enables a recommendations system (e.g., recommendations system <b>140</b>) to identify additional video content having contextual similarity with video content selected by a user, and additionally or alternatively, that has been viewed by other users whose viewing habits are similar to the user, and to provide the additional video content to the user as a set of ranked recommendations.</div>
<div class="description-paragraph" id="p-0148" num="0147">In step <b>1102</b>, recommendations server <b>142</b> may receive information identifying one or more first elements of video content (e.g., one or more first video clips) of interest to a user. For example, and as described above, the information may identify one or more video clips that include, but are not limited to, a video clip selected by the user for viewing, and additionally or alternatively, video clips associated with a selected list of video clips.</div>
<div class="description-paragraph" id="p-0149" num="0148">In an embodiment, the information associated with the selected video clips may include, but is not limited to, identifiers of the selected video clips and metadata associated with the selected video clips. For example, the metadata associated may include, but is not limited to: information identifying sources of the selected video clips (e.g., a source uniform resource locator (URL) or an address of a source repository); structural information associated with the selected video clips (e.g., a quality of the video clip and a size of the video clip; editorial and contextual information associated with the selected video clips; and/or information associated with a viewership of the selected video clips (e.g., a number of times users or particular users have accessed the video).</div>
<div class="description-paragraph" id="p-0150" num="0149">Further, in additional embodiments, the information received in step <b>1102</b> may identify the user that selected the video clips. For example, the information identifying the user may include: a alpha-numeric identifier associated with the user; authentication information association with the user; a login and password that enables a user to access one or more social networking application; and/or any additional or alternate information identifying information. Further, in an embodiment, the information identify a user may specify one or more social networking applications with which the user is associated, and additionally or alternatively, one or more additional users that are associated with the user within the social networking applications.</div>
<div class="description-paragraph" id="p-0151" num="0150">Upon receipt of the information, in step <b>1104</b>, recommendations server <b>142</b> may access data associated with one or more video graphs that reference the selected video clips (e.g., as stored within video graph data store <b>144</b>B of <figref idrefs="DRAWINGS">FIG. 1</figref>). In such embodiments, the one or more video graphs may indicate an association between portions of the selected video clips and corresponding portions of additional video clips based on, for example, an auditory similarity, a visual similarity, a combination of an auditory and a visual similarity, similar values of characteristic parameters (e.g., one or more of the metadata parameters outlined above), and additionally or alternatively, based on popularity of the additional video clips with linked users (e.g., as expressed through an expanded video graph that links associated videos to corresponding associated users).</div>
<div class="description-paragraph" id="p-0152" num="0151">In step <b>1106</b>, recommendations server <b>142</b> may leverage the video graph data to select one or more of additional video clips that are associated with the selected video clips. For example, the video graph data may include association values that express the degree of similarity between segments of the selected video clip and corresponding segments of the additional video clips. In such embodiments, recommendations server <b>142</b> may select a subset of these potentially-associated video clips having association values that exceed a threshold association value for presentation to the user. Information associated with these additional video clips may be combined with information associated with the selected video clips to form a candidate video list in step <b>1106</b>.</div>
<div class="description-paragraph" id="p-0153" num="0152">In an embodiment, a predetermined number of the selected video clips and the additional video clips may be selected in step <b>1106</b> for inclusion within the candidate list. For example, the selected and additional video clips may be assigned initial ranks in accordance with one or more factors, and recommendations server <b>142</b> may select a predetermined number of the selected and additional video clips in step <b>1106</b> for inclusion within the candidate video list. In such embodiments, the factors facilitating the initial ranking of the video clips may include, but are not limited to, a number videos linked to each of the video clips, a popularity of the video clips, and any additional or alternate factor available from the accessed video graph data.</div>
<div class="description-paragraph" id="p-0154" num="0153">Recommendations server <b>142</b> may filter the candidate video list of videos in step <b>1108</b>. In an embodiment, the filtering processes of step <b>1108</b> may leverage the accessed video graph data to identify a group or groups of videos that are identical or substantially similar, and to retain a video clip from the group that is associated with a highest quality and/or a quality that is suited to the user. For example, although a video clip suitable for playback on a high-definition display unit is available within a group of identical or substantially-similar video clips, a video clip of lower quality may be retained in step <b>1108</b> when that video clip is more consistent with the needs of the user. Similarly, among a group of identical or substantially similar video clips, a video may be discarded if that video clip has been previously viewed by the selecting user, or if that video clip was accessed by the selecting user but never viewed.</div>
<div class="description-paragraph" id="p-0155" num="0154">Additionally or alternatively, the filtering processes of step <b>1108</b> may also leverage video graph data to identify video clips within the candidate list that are similar or identical to video clips watched by users associated with the selecting user, i.e., users connected to the selecting user through a corresponding social network, as described above in reference to <figref idrefs="DRAWINGS">FIG. 9</figref>. In such an embodiment, videos within the augmented list that are identical or similar to those consumed by one or more of the users associated with the selecting user may be retained within the filtered list.</div>
<div class="description-paragraph" id="p-0156" num="0155">In step <b>1110</b>, recommendations server <b>142</b> may adjust the rankings assigned the video clips within the filtered video list. For example, and as discussed above, recommendations server <b>142</b> may access video graph data associated with the filtered video list. In such an embodiment, recommendation server may adjust an existing ranking of, or assign a new ranking to, a particular video clip in step <b>1110</b> based on a number of video clips associated with the particular video clip within the video graph data.</div>
<div class="description-paragraph" id="p-0157" num="0156">For example, a large number of such associations may indicate that the particular video clip is especially important or interesting to other users, as other users of recommendations system <b>140</b> may have copied auditory and/or visual content of the particular video clip to other video clips identified within, for example, the accessed video graph data. In such embodiments, the ranking assigned to the particular video clip may be directly proportional to the number of associations within the video graph data.</div>
<div class="description-paragraph" id="p-0158" num="0157">However, the ranking of the video clips within step <b>1110</b> need not be based only on the accessed video graph data. In additional embodiments, recommendations server <b>142</b> may leverage metadata associated with the particular video clip to determine a number of videos associated with that video clip, and may subsequently assign a ranking to the video clip in step <b>1110</b> based on the determined number of associations, as described above.</div>
<div class="description-paragraph" id="p-0159" num="0158">For example, as discussed above, the video clip may be associated with metadata that indicates a degree of similarity between the particular video clip and other video clips. In such embodiments, the degree of similarity may be determined based on a number of videos that describe a common event (e.g., a broadcast television program, a cable program, or a movie), that include a common scene or type of scene (e.g., sports, action, or comedy scene), or that are associated with a common individual (e.g., an actor, a politician, or a musician). The degree of similarity is not, however, limited to such exemplary indicia, and in further embodiments, the degree of similarity between video content may be based on any additional or alternate element of metadata, including, but not limited to, a broadcast channel, a country of origin, a video category, or a time slot.</div>
<div class="description-paragraph" id="p-0160" num="0159">The exemplary ranking process of step <b>1110</b> are not limited to rankings based on associations between videos within corresponding video graphs, and in additional embodiments, recommendations server <b>142</b> may assign a ranking to a selected video clip based on a consumption of the selected video clip by other users, and additionally or alternatively, a consumption of video clips having content similar to that of the selected video clip. Further, a segment of a video clip may be deemed important or interesting when video clips that incorporate the segment have been accessed by many users. Accordingly, a ranking assigned to a video clip may be directly proportional to a number of users that have consumed the video clip, a number of user that have accessed the video clip, that have accessed the or that have accessed or consumed video content similar to the video clip.</div>
<div class="description-paragraph" id="p-0161" num="0160">Referring back to <figref idrefs="DRAWINGS">FIG. 11</figref>, in step <b>1112</b>, recommendations server <b>142</b> may transmit information associated with the ranked video clips to user device <b>102</b>, which may receive the receive the information, render the received information, and display the information associated with the video clips to the requesting user in accordance with the ranking. Further, in addition to the ranked video list, recommendations server <b>142</b> may also generate video graph information corresponding to the filtered video set for transmission to user device <b>102</b>. For example, such video graph data may include, but is not limited to, a list of video clips associated with at least one of the ranked video clips. In such embodiments, method <b>1100</b> is finished and completed in step <b>1114</b>.</div>
<div class="description-paragraph" id="p-0162" num="0161">Upon receipt of the filtered video list, user device <b>102</b> may present the filtered video list to the user, as depicted below in <figref idrefs="DRAWINGS">FIGS. 12A and 12B</figref>. <figref idrefs="DRAWINGS">FIG. 12A</figref> illustrates an exemplary interface <b>1200</b> through which a user device may present a ranked list of video clips to a user, according to disclosed embodiments. In the exemplary interface of <figref idrefs="DRAWINGS">FIG. 12A</figref>, the user has selected “Video A,” and the selected video is displayed to the user within an embedded display window <b>1202</b>A disposed within region <b>1202</b>.</div>
<div class="description-paragraph" id="p-0163" num="0162">Further, in region <b>1202</b>, additional information identifying the selected video clip, including a title of the video clip and a textual description of the video clip, may be included within in portion <b>1202</b>B of region <b>1202</b>. For example, such information may be obtained from metadata associated with the selected video clip and provided by the content provider or recommendations server <b>142</b>.</div>
<div class="description-paragraph" id="p-0164" num="0163">In the exemplary embodiment of <figref idrefs="DRAWINGS">FIG. 12A</figref>, the filtered video set generated by recommendations server <b>142</b> includes video clips “Video B,” “Video C,” “Video D,” and “Video E,” which may be displayed in corresponding locations <b>1204</b>, <b>1206</b>, <b>1208</b>, and <b>1210</b> within interface <b>1200</b>. Further, in <figref idrefs="DRAWINGS">FIG. 12A</figref>, previews and video graph information of “Video B,” “Video C,” “Video D,” and “Video E” may be displayed within corresponding ones of locations <b>1204</b>, <b>1206</b>, <b>1208</b>, and <b>1210</b> within interface <b>1200</b>.</div>
<div class="description-paragraph" id="p-0165" num="0164">For example, in location <b>1206</b>, a preview of “Video C” may be displayed within to the user within region <b>1206</b>A, and video graph data associated with the “Video C” may be displayed in region <b>1206</b>B. For example, the displayed preview may include, but is not limited to, a portion of “Video C,” a still image associated with a portion of “Video C,” or any additional or alternate representation of the subject matter of “Video C” apparent to one of skill in the art. Further, for example, the video graph data corresponding to “Video C” may include, but is not limited to, one or more embedded hyperlinks that direct a user to video clips associated with “Video C.” In such an embodiment, the user may click or otherwise activate a portion of the one of the embedded hyperlinks to access the associated video clip. For example, the user may click or otherwise activate a portion of region <b>1206</b>A, in which the preview of “Video C” is displayed, to gain access for video content associated with “Video C.”</div>
<div class="description-paragraph" id="p-0166" num="0165">In additional embodiments, not depicted in <figref idrefs="DRAWINGS">FIG. 12A</figref>, locations <b>1204</b>, <b>1208</b>, and <b>1210</b> may include regions that display respective previews of “Video B,” “Video D,” and “Video E,” and further regions that display video graph data including embedded hyperlinks to video content associated, respectively, with “Video B,” “Video D,” and “Video E.” In such embodiments, the user may click on a preview to gain access to a corresponding video clip, or may click on a hyperlink to gain access to a video clip associated with the corresponding video, as described above.</div>
<div class="description-paragraph" id="p-0167" num="0166"> <figref idrefs="DRAWINGS">FIG. 12B</figref> illustrates an additional exemplary interface <b>1250</b> through which a user device may present a ranked list of video clips to a user, according to disclosed embodiments. In contrast to exemplary interface <b>1200</b> of <figref idrefs="DRAWINGS">FIG. 12A</figref>, interface <b>1250</b> graphically illustrates a hierarchical relationship between videos within the ranked list, for example, as specified within video graph data.</div>
<div class="description-paragraph" id="p-0168" num="0167">For example, as described above, a user may have selected “Video A” from a corresponding list of displayed video clips, and recommendations server <b>142</b> may generate a ranked list based on an analysis of video graph data, as described above in reference to <figref idrefs="DRAWINGS">FIG. 11</figref>. In such an embodiment, the ranked list may include video clip “Video A,” as well as video clips “Video B,” “Video C,” “Video D,” and “Video E” identified using the exemplary method <b>1100</b>.</div>
<div class="description-paragraph" id="p-0169" num="0168">In <figref idrefs="DRAWINGS">FIG. 12B</figref>, video clips “Video A,” “Video B,” “Video D,” “Video E,” and “Video F” are associated with corresponding ones of nodes <b>1260</b>, <b>1262</b>, <b>1264</b>, <b>1266</b>, and <b>1268</b> within interface <b>1250</b>. Further, interface <b>1250</b> also indicates graphically a relationship between video clips “Video A,” “Video B,” “Video D,” “Video E,” and “Video F” within the corresponding video graph data. For example, interface <b>1250</b> indicates that node <b>1260</b> is linked to nodes <b>1262</b> and <b>1264</b>, and as such, “Video A” is associated with “Video B” and “Video D” within one or more video graphs. Similarly, interface <b>1250</b> also indicates that node <b>1264</b> linked to nodes <b>1266</b> and <b>1268</b>, and as such, “Video D” is associated with “Video E” and “Video F” within one or more video graphs.</div>
<div class="description-paragraph" id="p-0170" num="0169">Further, in <figref idrefs="DRAWINGS">FIG. 12B</figref>, previews and video graph information of video graphs “Video B,” “Video E,” and “Video F” may be displayed within corresponding ones of nodes <b>1262</b>, <b>1266</b>, and <b>1268</b> within interface <b>1250</b>. For example, in node <b>1262</b>, a preview of “Video B” may be displayed within to the user within region <b>1262</b>A, and video graph data associated with the “Video B” may be displayed in region <b>1262</b>B. For example, the displayed preview may include, but is not limited to, a portion of “Video B,” a still image associated with a portion of “Video B,” or any additional or alternate representation of the subject matter of “Video B” apparent to one of skill in the art. Further, for example, the video graph data corresponding to “Video B” may include, but is not limited to, one or more embedded hyperlinks that direct a user to video clips associated with “Video B.” In such an embodiment, the user may click or otherwise activate a portion of the one of the embedded hyperlinks to access the corresponding video clip. Further, the user may click or otherwise activate a portion of region <b>1262</b>A, in which the preview of “Video B” is displayed, to gain access for video content associated with “Video B.”</div>
<div class="description-paragraph" id="p-0171" num="0170">In additional embodiments, not depicted in <figref idrefs="DRAWINGS">FIG. 12B</figref>, nodes <b>1260</b> and <b>1264</b> may include regions that display respective previews of “Video A” and “Video D,” and further regions that display video graph data including embedded hyperlinks to video content associated, respectively, with “Video A” and “Video D.” In such embodiments, the user may click on a preview to gain access to a corresponding video clip, or may click on a hyperlink to gain access to a video clip associated with the corresponding video, as described above.</div>
<div class="description-paragraph" id="p-0172" num="0171">In the exemplary embodiment of <figref idrefs="DRAWINGS">FIG. 11</figref>, recommendations system <b>140</b> generates a ranked list of video clips that are relevant to a user's selection of an video clip, and additionally or alternative, to a user's selection of a list of video clips. As such, <figref idrefs="DRAWINGS">FIG. 11</figref> enables recommendations system <b>140</b> to enhance an ability of the user to browse and subsequently view video clips that are consistent with the user's interest, or alternatively, with the interests of other users having consumption patterns similar to the user.</div>
<div class="description-paragraph" id="p-0173" num="0172">The disclosed embodiments are, however, not limited to recommendations system that enhance a user's browsing experience. In additional embodiments, recommendations system <b>140</b> may facilitate a discovery of video content relevant to the user's interest during search and retrieval processes, as described below in reference to <figref idrefs="DRAWINGS">FIG. 13</figref>.</div>
<div class="description-paragraph" id="p-0174" num="0173"> <figref idrefs="DRAWINGS">FIG. 13</figref> is a flowchart of an additional exemplary method <b>1300</b> for providing recommendations of video content based on video graph data, according to disclosed embodiments. Method <b>1300</b> may provide functionality that enables a recommendations system (e.g., recommendations system <b>140</b>) to identify additional video content having contextual similarity with video content related to a textual search query, and additionally or alternatively, that has been viewed by users whose viewing habits are similar to the requesting user.</div>
<div class="description-paragraph" id="p-0175" num="0174">In an embodiment, a user of a user device (e.g., user device <b>102</b>) may access a web page or other interface associated with recommendations system <b>140</b>, and may enter a textual search query into a corresponding region of the accessed web page and submit the textual search query by clicking on a corresponding region of the web page, entering a keystroke, or through any additional or alternate activation technique appropriate to the web page. For example, as depicted in <figref idrefs="DRAWINGS">FIG. 14A</figref>, the user may access web page <b>1400</b> and enter a corresponding textual search query in region <b>1402</b>. The user may subsequently submit the textual search query by clicking or otherwise activating “Submit” button <b>1403</b>. Upon submission of the request, user device <b>102</b> may establish a connection with recommendations system <b>140</b> over communications network <b>120</b>, and may subsequently transmit information associated with the textual search query to recommendations server <b>142</b>.</div>
<div class="description-paragraph" id="p-0176" num="0175">Referring back to <figref idrefs="DRAWINGS">FIG. 13</figref>, recommendations server <b>142</b> may receive the information associated with the textual search query from user device <b>102</b> in step <b>1302</b>. In an embodiment, the received information may include the textual search query entered by the user into the website associated with recommendations server <b>142</b>, and additionally, information that identifies one or more characteristics of the user. For example, such identifying information may include demographic information (e.g., age), employment information, one or more social networking applications associated with the user, geographic information associated with the user (e.g., an employment location, a residential location, or a preferred geographic location), and one or more contextual or structural preferences of the user. Further, in an embodiment, the identifying information may specify one or more social networking applications with which the user is associated, and additionally or alternatively, one or more additional users that are linked to the user within the social networking applications.</div>
<div class="description-paragraph" id="p-0177" num="0176">In step <b>1304</b>, and upon receipt of the textual search query and/or the user information, recommendations server <b>142</b> may identify one or more first video clips that are relevant to the textual search query. For example, recommendations server <b>142</b> may access a content data repository (e.g., content data store <b>144</b>A of <figref idrefs="DRAWINGS">FIG. 1</figref>) that stores a plurality of video clips and metadata corresponding to the video clips. In such an embodiment, at least a portion of the textual search query may be compared against metadata associated with the available video clips to identify the first video clips that are contextually related to the search query.</div>
<div class="description-paragraph" id="p-0178" num="0177">The identification of the first video clips is not, however, limited to such exemplary processes. In additional embodiments, recommendations server <b>142</b> may identify the first video clips relevant to the received search query within any additional or alternate data repository accessible across communications network <b>120</b>, or using any additional or alternate technique appropriate to the video clips and the received search query, without departing from the spirit or scope of the disclosed embodiments.</div>
<div class="description-paragraph" id="p-0179" num="0178">Upon identification of the first video clips, in step <b>1306</b>, recommendations server <b>142</b> may access data associated with one or more video graphs that reference the video content of the first video clips. For example, and as described above, recommendation server <b>142</b> may access a video graph repository (e.g., video graph data store <b>1448</b> of <figref idrefs="DRAWINGS">FIG. 1</figref>) to obtain the video graph data.</div>
<div class="description-paragraph" id="p-0180" num="0179">Further, in such embodiments, the video graph data may indicate an association between portions of the first video clips and corresponding segments of second video clips based on, for example, an auditory similarity, a visual similarity, a combination of an auditory and a visual similarity, similar values of characteristic parameters (e.g., one or more of the metadata parameters outlined above), and additionally or alternatively, based on popularity of the additional video clips with linked users (e.g., as expressed through an expanded video graph that links associated video clips to corresponding associated users within a social network). For example, such video graphs may be generated through an application of one or more association rules that express limitations on auditory similarity, visual similarity, parameter values, and popularity, as described above.</div>
<div class="description-paragraph" id="p-0181" num="0180">In step <b>1308</b>, recommendations server <b>142</b> may leverage the video graph data to identify one or more second video clips that are associated with the candidate list. For example, as described above, the video graph data may include association values that express the degree of similarity between segments of the candidate video clips and corresponding segments of the second video clips. Information associated with the second video clips may be combined with information associated with the first video clips to form a candidate video list in step <b>1310</b>.</div>
<div class="description-paragraph" id="p-0182" num="0181">In an embodiment, a predetermined number of the first and second video clips may be selected in step <b>1310</b> for inclusion within the candidate list. For example, the first and second video clips may be assigned initial ranks in accordance with one or more factors, and recommendations server <b>142</b> may select a predetermined number of the first and second video clips in step <b>1310</b> for inclusion within the candidate video list. In such embodiments, the factors facilitating the initial ranking of the video clips may include, but are not limited to, a number videos linked to each of the video clips, a popularity of the video clips, and any additional or alternate factor available from the accessed video graph data.</div>
<div class="description-paragraph" id="p-0183" num="0182">In step <b>1312</b>, the candidate video clips may be filtered by recommendations server <b>142</b> to generate a filtered video list. For example, and as described above, the filtering processes of step <b>1310</b> may leverage the accessed video graph data to identify a group or groups of video clips that are identical or substantially similar, and to retain a video clip from the group that is associated with a highest quality and/or a quality that is suited to the user. Similarly, among a group of identical or substantially similar videos, a video may be discarded if that video has been previously viewed by the selecting user, or if that video was accessed by the user but never viewed. Additionally or alternatively, the filtering processes of step <b>1310</b> may also leverage video graph data to identify video clips within the candidate list that are similar or identical to videos watched by one or more additional users associated with the user of user device <b>102</b>, i.e., users connected to the user through a corresponding social network, as described in reference to <figref idrefs="DRAWINGS">FIG. 9</figref>.</div>
<div class="description-paragraph" id="p-0184" num="0183">In step <b>1314</b>, recommendations server <b>142</b> may adjust or assign rankings to the video clips within the filtered video list. For example, recommendations server <b>142</b> may access video graph data associated with the video clips of the filtered video list, and may adjust an existing ranking or assign a new ranking to a particular video clip in step <b>1312</b> based on a number of video clips associated with the particular video clip within the accessed video graph data.</div>
<div class="description-paragraph" id="p-0185" num="0184">However, the ranking of a video clip within step <b>1314</b> need not be based solely on video graph data, and in additional embodiments, recommendations server <b>142</b> adjust the ranking or assign the new ranking to the particular video clip based on metadata associated with the particular video clip. For example, such metadata may indicate a number of additional video clips associated with the particular video clip, a number of additional video clips that describe a event, a scene or type of scene, an individual referenced by the particular video clip, or any additional or alternative information, as described above in reference to <figref idrefs="DRAWINGS">FIG. 11</figref>. Further, in step <b>1314</b>, recommendations server <b>142</b> may assign a ranking to the particular video clip based on a consumption of the particular video clip by other users, and additionally or alternatively, a consumption of video clips having content similar to that of the particular video clip.</div>
<div class="description-paragraph" id="p-0186" num="0185">Referring back to <figref idrefs="DRAWINGS">FIG. 13</figref>, in step <b>1316</b>, recommendations server <b>142</b> may transmit information associated with the ranked video clips to user device <b>102</b>, which may receive the receive the information, render the received information, and display the information associated with the video clips to the user in accordance with the ranking. Further, in addition to the ranked video list, recommendations server <b>142</b> may also transmit video graph information corresponding to one or more of the ranked video clips to user device <b>102</b> in step <b>1316</b>. Method <b>1300</b> is finished and completed in step <b>1318</b>.</div>
<div class="description-paragraph" id="p-0187" num="0186">Upon receipt of the filtered video list, user device <b>102</b> may present the ranked video clips to the user in list form as depicted in <figref idrefs="DRAWINGS">FIG. 14B</figref>. In <b>148</b>, user device <b>102</b> presents the ranked list of video clips to the user within interface <b>1400</b> in regions <b>1404</b>, <b>1406</b>, <b>1408</b>, and <b>1410</b>. For example, region <b>1404</b>, which is associated with “Video Result 1,” includes information <b>1404</b>A identifying the video clip associated with “Video Result 1,” information <b>14048</b> related to the video graph data associated with “Video Result 1,” and a preview <b>1404</b>C of the video clip associated with “Video Result 1.” Similar Information and corresponding previews are included within locations <b>1406</b>, <b>1408</b>, and <b>1410</b> corresponding to “Video Result 2,” “Video Result 3,” and “Video Result N,” i.e., the N<sup>th </sup>of the plurality of search results.</div>
<div class="description-paragraph" id="p-0188" num="0187">For example, in region <b>1404</b>, preview <b>1404</b>C and information <b>1404</b>A identifying the “Video Result 1” may represent hyperlinks embedded into interface <b>1400</b>. In such embodiments, the user may access a video clip associated with “Video Result 1” by clicking or otherwise activating a corresponding one of the embedded hyperlinks. Similarly, Information <b>1404</b>B related to the video graph data associated with “Video Result 1” may include one or more embedded hyperlinks that, upon activation by the user, direct the user to one or more video clips linked to “Video Result 1” within the video graph data. In such embodiments, upon activation of one or more of the hyperlinks, the user may directly access the corresponding video clip, or may alternatively be directed to a website or other interface that displays the content to the user.</div>
<div class="description-paragraph" id="p-0189" num="0188">In the embodiments described above, a user device (e.g., user devices <b>102</b> or <b>112</b>) transmits information associated with a video clip of interest to a user, and additionally or alternatively, a textual search query, to recommendations system <b>140</b> over communications network <b>120</b>. Based on the received information and/or textual search query, recommendations system <b>140</b> may leverage video graph data to identify one or more video clips of potential interest to the user, and may transmit the recommended video clips to the user device over communications network <b>120</b>. In such embodiments, the user may interact directly with recommendations system <b>140</b> via user devices <b>102</b> or <b>112</b>, the recommendations system <b>140</b> may function as either a provider of electronic content or a search engine.</div>
<div class="description-paragraph" id="p-0190" num="0189">The disclosed embodiments are not, however, limited to direct interaction between recommendations system <b>140</b> and a user at a user device. For example, the user at the user device may access a website or other suitable interface associated with a search engine or a content provider (e.g., a news organization, such as nytimes.com, and a provider of streaming video, such as hulu.com and youtube.com). In such embodiments, the search engine or content provider may be associated with an additional web server in communications with recommendations system <b>140</b> and user devices <b>102</b> and <b>112</b> across network <b>120</b>.</div>
<div class="description-paragraph" id="p-0191" num="0190">In response to an input from the user (e.g., a user entry of a textual search query or a user selection of a video clip from a displayed list), the additional web server may programmatically establish a connection with recommendations system <b>140</b> and may subsequently transmit information associated with the selected video clip and/or the textual search query to recommendations system <b>140</b>. Recommendations system <b>140</b> may leverage the video graph data to identify one or more video clips of potential interest to the user, and may transmit information associated with recommended video clips to the additional web server, which may subsequently provide the information to the user device for rendering and display to the user.</div>
<div class="description-paragraph" id="p-0192" num="0191">In the exemplary embodiments described above, reference is made to systems and methods that process, identify, and associate similar video clips and/or videos. However, the disclosed embodiments are not limited to such exemplary video clips and videos. In further embodiments, the processes described above may process, identify, and associate any additional or alternate element of video content or element of multimedia content apparent to one of skill in the art and associated with a corresponding duration, corresponding starting and completion times, and corresponding contextual and structural characteristics.</div>
<div class="description-paragraph" id="p-0193" num="0192">Various embodiments have been described herein with reference to the accompanying drawings. It will, however, be evident that various modifications and changes may be made thereto, and additional embodiments may be implemented, without departing from the broader scope of the invention as set forth in the claims that follow.</div>
<div class="description-paragraph" id="p-0194" num="0193">Further, other embodiments will be apparent to those skilled in the art from consideration of the specification and practice of one or more embodiments of the present disclosure. It is intended, therefore, that this disclosure and the examples herein be considered as exemplary only, with a true scope and spirit of the invention being indicated by the following listing of exemplary claims.</div>
</div>
</div>
</section><section itemprop="claims" itemscope="">
<h2>Claims (<span itemprop="count">16</span>)</h2>
<div html="" itemprop="content"><div class="claims" lang="EN" load-source="patent-office" mxw-id="PCLM306523907">
<claim-statement>What is claimed is:</claim-statement>
<div class="claim"> <div class="claim" id="CLM-00001" num="00001">
<div class="claim-text">1. An apparatus, comprising:
<div class="claim-text">a storage device that stores a set of instructions; and</div>
<div class="claim-text">at least one processor coupled to the storage device and operative with the instructions to:
<div class="claim-text">obtain video graph data for first video content accessed by a first user and second video content accessed by a second user;</div>
<div class="claim-text">generate updated video graph data indicative of a link between the first user and the second user based on an association between video consumption of the first user and video consumption of the second user;</div>
<div class="claim-text">compute one or more measures of similarity between the first video content and the second video content;</div>
<div class="claim-text">determine one or more weight factors for the one or more measures of similarity;</div>
<div class="claim-text">calculate a value indicative of the association between the video consumption of the first user and the second user based on the one or more weighted measures of similarity;</div>
</div>
<div class="claim-text">determine whether the calculated value exceeds a threshold value; and</div>
<div class="claim-text">generate the updated video graph data when the calculated value is determined to exceed the threshold value.</div>
</div>
</div>
</div> <div class="claim-dependent"> <div class="claim" id="CLM-00002" num="00002">
<div class="claim-text">2. The apparatus of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the at least one processor is further operative with the instructions to:
<div class="claim-text">identify additional video content based on the updated video graph data; and</div>
<div class="claim-text">transmit at least a portion of the additional video content to at least one of a first device of the first user and a second device of the second user.</div>
</div>
</div>
</div> <div class="claim-dependent"> <div class="claim" id="CLM-00003" num="00003">
<div class="claim-text">3. The apparatus of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the video graph data comprises links between a first video clip of the first video content and at least a second video clip.</div>
</div>
</div> <div class="claim-dependent"> <div class="claim" id="CLM-00004" num="00004">
<div class="claim-text">4. The apparatus of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the association between the video consumption of the first user and the video consumption of the second user is based on the obtained video graph data and video consumption information of the first user and the second user.</div>
</div>
</div> <div class="claim-dependent"> <div class="claim" id="CLM-00005" num="00005">
<div class="claim-text">5. The apparatus of <claim-ref idref="CLM-00004">claim 4</claim-ref>, wherein the weight factors comprise at least one of a frequency of access for the video content, a time of day of access for the video content, and a date of access for the video content.</div>
</div>
</div> <div class="claim-dependent"> <div class="claim" id="CLM-00006" num="00006">
<div class="claim-text">6. The apparatus of <claim-ref idref="CLM-00004">claim 4</claim-ref>, wherein the video consumption information for the first user and the second user comprises at least one of a list of accessed video content, a list of consumed video content, a frequency of accessed video content, and a frequency of consumed video content.</div>
</div>
</div> <div class="claim-dependent"> <div class="claim" id="CLM-00007" num="00007">
<div class="claim-text">7. The apparatus of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the first user and the second user are associated with a social network.</div>
</div>
</div> <div class="claim-dependent"> <div class="claim" id="CLM-00008" num="00008">
<div class="claim-text">8. The apparatus of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein each of the first video content and the second video content comprises video clips.</div>
</div>
</div> <div class="claim"> <div class="claim" id="CLM-00009" num="00009">
<div class="claim-text">9. A computer-implemented method comprising the following operations performed by at least one processor:
<div class="claim-text">obtaining video graph data for first video content accessed by a first user and second video content accessed by a second user;</div>
<div class="claim-text">generating updated video graph data indicative of a link between the first user and the second user based on an association between video consumption of the first user and video consumption of the second user,</div>
<div class="claim-text">computing one or more measures of similarity between the first video content and the second video content;</div>
<div class="claim-text">determining one or more weight factors for the one or more measures of similarity;</div>
<div class="claim-text">calculating a value indicative of the association between the video consumption of the first user and the second user based on the one or more weighted measures of similarity;</div>
<div class="claim-text">determining whether the calculated value exceeds a threshold value; and</div>
<div class="claim-text">generating the updated video graph data when the calculated value is determined to exceed the threshold value.</div>
</div>
</div>
</div> <div class="claim-dependent"> <div class="claim" id="CLM-00010" num="00010">
<div class="claim-text">10. The method of <claim-ref idref="CLM-00009">claim 9</claim-ref>, further comprising:
<div class="claim-text">identifying additional video content based on the updated video graph data; and</div>
<div class="claim-text">transmitting at least a portion of the additional video content to at least one of a first device of the first user and a second device of the second user.</div>
</div>
</div>
</div> <div class="claim-dependent"> <div class="claim" id="CLM-00011" num="00011">
<div class="claim-text">11. The method of <claim-ref idref="CLM-00009">claim 9</claim-ref>, wherein the video graph data comprises links between a first video clip of the first video content and at least a second video clip.</div>
</div>
</div> <div class="claim-dependent"> <div class="claim" id="CLM-00012" num="00012">
<div class="claim-text">12. The method of <claim-ref idref="CLM-00009">claim 9</claim-ref>, wherein the association between the video consumption of the first user and the video consumption of the second user is based on the obtained video graph data and video consumption information of the first user and the second user.</div>
</div>
</div> <div class="claim-dependent"> <div class="claim" id="CLM-00013" num="00013">
<div class="claim-text">13. The method of <claim-ref idref="CLM-00009">claim 9</claim-ref>, wherein the weight factors comprise at least one of a frequency of access for the video content, a time of day of access for the video content, and a date of access for the video content.</div>
</div>
</div> <div class="claim-dependent"> <div class="claim" id="CLM-00014" num="00014">
<div class="claim-text">14. The method of <claim-ref idref="CLM-00012">claim 12</claim-ref>, wherein the video consumption information for the first user and the second user comprises at least one of a list of accessed video content and a frequency of accessed video content.</div>
</div>
</div> <div class="claim-dependent"> <div class="claim" id="CLM-00015" num="00015">
<div class="claim-text">15. The method of <claim-ref idref="CLM-00009">claim 9</claim-ref>, wherein the first user and the second user are associated with a social network.</div>
</div>
</div> <div class="claim"> <div class="claim" id="CLM-00016" num="00016">
<div class="claim-text">16. A tangible, non-transitory computer-readable medium that stores a set of instructions that, when executed by at least one processor, cause the at least one processor to perform operations comprising:
<div class="claim-text">obtaining video graph data for first video content accessed by a first user and second video content accessed by a second user;</div>
<div class="claim-text">generating updated video graph data indicative of a link between the first user and the second user based on an association between video consumption of the first user and video consumption of the second user,</div>
<div class="claim-text">computing one or more measures of similarity between the first video content and the second video content;</div>
<div class="claim-text">determining one or more weight factors for the one or more measures of similarity;</div>
<div class="claim-text">calculating a value indicative of the association between the video consumption of the first user and the second user based on the one or more weighted measures of similarity;</div>
<div class="claim-text">determining whether the calculated value exceeds a threshold value; and</div>
<div class="claim-text">generating the updated video graph data when the calculated value is determined to exceed the threshold value.</div>
</div>
</div>
</div> </div>
</div>
</section>
                </article>
            </search-app>
        </body>
    </html>
    