
    <html>
        <body>
            <search-app>
                <article class="result" itemscope="" itemtype="http://schema.org/ScholarlyArticle">
    <h1 itemprop="pageTitle">US9357117B2 - Photographing device for producing composite image and method using the same 
        - Google Patents</h1><section itemprop="abstract" itemscope="">
<h2>Abstract</h2>
<div html="" itemprop="content"><abstract lang="EN" load-source="patent-office" mxw-id="PA173274030">
<div class="abstract" id="p-0001" num="0000">A photographing device includes a photographing unit, an image processor which separates an object from a first photographing image obtained by the photographing unit, a display which displays a background live view obtained by superimposing the separated object on a live view of a background, and a controller which obtains a second photographing image corresponding to the live view of the background when a command to shoot the background is input and generates a composite image based on the separated object and the second photographing image.</div>
</abstract>
</div>
</section><section itemprop="description" itemscope="">
<h2>Description</h2>
<div html="" itemprop="content"><div class="description" lang="EN" load-source="patent-office" mxw-id="PDES103526876">
<heading id="h-0001">CROSS-REFERENCE TO RELATED APPLICATION</heading>
<div class="description-paragraph" id="p-0002" num="0001">This application claims priority from Korean Patent Application No. 10-2012-0157065, filed on Dec. 28, 2012, in the Korean Intellectual Property Office, the disclosure of which is incorporated herein by reference in its entirety.</div>
<heading id="h-0002">BACKGROUND</heading>
<div class="description-paragraph" id="p-0003" num="0002">1. Field</div>
<div class="description-paragraph" id="p-0004" num="0003">Apparatuses and methods consistent with the exemplary embodiments relate to a photographing device and a method for producing a composite image, and more particularly, to a photographing device capable of producing a composite image based on an object separated from a photographed image and a live view of a background, and a method for producing a composite image using the same.</div>
<div class="description-paragraph" id="p-0005" num="0004">2. Description of the Related Art</div>
<div class="description-paragraph" id="p-0006" num="0005">Due to development of electronic technology, various portable devices have been developed and distributed. Recent portable devices are provided with a display means and a photographing means, thereby supporting a photographing function. A photographing device supporting the photographing function displays a live view on a display means using light coming through a lens. Thus, a user may capture pictures while viewing the live view.</div>
<div class="description-paragraph" id="p-0007" num="0006">As photographing becomes common, photographs may be captured in various locations and the captured photographs may be used in various ways. For example, users may upload the captured pictures to their blogs, Internet forums, or social network services (SNSs) and share the captured pictures with others.</div>
<div class="description-paragraph" id="p-0008" num="0007">Meanwhile, the user may wish to capture pictures of himself or herself in famous places or buildings. In this case, when there is no one to assist to photograph the picture of the user, the user needs to photograph the picture by using a self shooting mode or a timer.</div>
<div class="description-paragraph" id="p-0009" num="0008">In order to use the timer, however, the user needs to put the photographing device on the ground or floor, adjust a shooting angle or direction, and fix the photographing device to a certain position. In addition, when taking a picture of the user in the self shooting mode, the picture may be mostly filled with the user's face due to a limit of the user's arm's length. Thus, it is difficult to recognize a background of the picture.</div>
<div class="description-paragraph" id="p-0010" num="0009">Accordingly, there is a need for a technology for photographing a picture of the user and the background in the self shooting mode while maintaining a good ratio therebetween.</div>
<heading id="h-0003">SUMMARY</heading>
<div class="description-paragraph" id="p-0011" num="0010">Exemplary embodiments of the present disclosure overcome the above disadvantages and other disadvantages not described above. Also, the present disclosure is not required to overcome the disadvantages described above, and an exemplary embodiment of the present disclosure may not overcome any of the problems described above.</div>
<div class="description-paragraph" id="p-0012" num="0011">An aspect of exemplary embodiments relates to a photographing device capable of producing a composite image by separating an object from a captured image and superimposing the separated object on a live view, and a method for producing a composite image using the same.</div>
<div class="description-paragraph" id="p-0013" num="0012">According to an exemplary embodiment, a photographing device includes a photographing unit, an image processor which separates an object from a first photographing image obtained by the photographing unit, a display which displays a background live view obtained by superimposing the separated object on a live view of a background, and a controller which obtains a second photographing image corresponding to the live view of the background when a command to shoot the background is input and generates a composite image based on the separated object and the second photographing image.</div>
<div class="description-paragraph" id="p-0014" num="0013">When a composite shooting mode is selected, the controller may control the display to display a live view of the object and search for and track the object by monitoring respective frames of the live view of the object, and when a command to shoot the object is input, the controller may generate the first photographing image corresponding to the live view of the object and control the image processor to separate the tracked object from the first photographing image.</div>
<div class="description-paragraph" id="p-0015" num="0014">The photographing unit may include a first photographing unit which captures a photograph in a first shooting direction, and a second photographing unit which captures a photograph in a second shooting direction opposite to the first shooting direction.</div>
<div class="description-paragraph" id="p-0016" num="0015">When the composite shooting mode is entered, the controller may activate the first photographing unit to provide the live view of the object to the display, and when the first photographing image is obtained by the first photographing unit, the controller may activate the second photographing unit to provide the live view of the background, on which the separated object is superimposed and provided to the display.</div>
<div class="description-paragraph" id="p-0017" num="0016">The photographing device may further include a storage unit which stores a previously learned object model, the object model being obtained based on a plurality of images of the object to determine an object area in which the object is located, wherein the controller determines that the object is located in the object area determined by the object model on the first photographing image.</div>
<div class="description-paragraph" id="p-0018" num="0017">The display may display the first photographing image, and when a user's manipulation for the first photographing image is performed, the controller may change at least one of a size and a shape of the object area according to the user's manipulation.</div>
<div class="description-paragraph" id="p-0019" num="0018">The controller may control the image processor to restore a shape of the object separated from the first photographing image, and superimpose and display the object having the restored shape on the live view of the background.</div>
<div class="description-paragraph" id="p-0020" num="0019">The controller may control the image processor to adjust at least one of a display position, a size, a color and a brightness of the object on the background live view according to a user's selection.</div>
<div class="description-paragraph" id="p-0021" num="0020">The controller may adjust at least one of a display position, a size, a color and a brightness of the separated object based on the live view of the background.</div>
<div class="description-paragraph" id="p-0022" num="0021">According to another exemplary embodiment, a method for producing a composite image includes obtaining a first photographing image and separating an object from the first photographing image, displaying a background live view obtained by superimposing the object on a live view of a background, obtaining a second photographing image corresponding to the live view of the background when a command to shoot a background is input, and generating a composite image based on the separated object and the second photographing image.</div>
<div class="description-paragraph" id="p-0023" num="0022">The separating the object may include, when a composite shooting mode is selected, displaying a live view of the object, searching for and tracking the object by monitoring respective frames of the live view of the object, and when a command to shoot the object is input, generating the first photographing image corresponding to the live view of the object, and separating the tracked object from the first photographing image.</div>
<div class="description-paragraph" id="p-0024" num="0023">The searching for and tracking the object may include obtaining an object model based on a plurality of images of the object to determine an object area in which the object is located and determining that the object is located in the object area determined by the object model on the first photographing image.</div>
<div class="description-paragraph" id="p-0025" num="0024">The separating the object may include displaying the first photographing image, and when a user's manipulation for the first photographing image is performed, changing at least one of a size and a shape of the object area according to the user's manipulation.</div>
<div class="description-paragraph" id="p-0026" num="0025">The method may further include restoring a shape of the object separated from the first photographing image.</div>
<div class="description-paragraph" id="p-0027" num="0026">The method may further include adjusting at least one of a display position, a size, a color and a brightness of the separated object on the background live view according to a user's selection.</div>
<div class="description-paragraph" id="p-0028" num="0027">The method may further include adjusting at least one of a display position, a size, a color and a brightness of the separated object based on the live view of the background.</div>
<div class="description-paragraph" id="p-0029" num="0028">According to an exemplary embodiment, an image processing apparatus includes a first image input unit which receives a first image including an object; a second image input unit which receives a second image different from the first image; an object processing unit which separates the object from the first image; and an image composing unit which superimposes the separated object on the second image to be provided as a preview image to a user and generates a composite image based on the separated object and the second image when the preview image is selected by the user.</div>
<div class="description-paragraph" id="p-0030" num="0029">The first image and the second image may be received simultaneously.</div>
<div class="description-paragraph" id="p-0031" num="0030">The first image and the second image may be received in a subsequent order.</div>
<div class="description-paragraph" id="p-0032" num="0031">The object processing unit may restore a shape of the object separated from the first image and the image composing unit may generate the composite image based on the object having the restored shape and the second image.</div>
<div class="description-paragraph" id="p-0033" num="0032">The object processing unit may adjust at least one of a display position, a size, a color and a brightness of the object on the preview image according to a user's selection.</div>
<div class="description-paragraph" id="p-0034" num="0033">The controller may adjust at least one of a display position, a size, a color and a brightness of the separated object based on the second image.</div>
<div class="description-paragraph" id="p-0035" num="0034">According to an exemplary embodiment, a non-transitory computer readable recording medium having recorded thereon a program executable by a computer for performing the above method is provided.</div>
<description-of-drawings>
<heading id="h-0004">BRIEF DESCRIPTION OF THE DRAWINGS</heading>
<div class="description-paragraph" id="p-0036" num="0035">The above and/or other aspects of the present disclosure will be more apparent by describing certain exemplary embodiments of the present disclosure with reference to the accompanying drawings, in which:</div>
<div class="description-paragraph" id="p-0037" num="0036"> <figref idrefs="DRAWINGS">FIG. 1</figref> is a block diagram illustrating a configuration of a photographing apparatus according to an exemplary embodiment of the present disclosure;</div>
<div class="description-paragraph" id="p-0038" num="0037"> <figref idrefs="DRAWINGS">FIG. 2</figref> illustrates a method for producing a composite image according to an exemplary embodiment of the present disclosure;</div>
<div class="description-paragraph" id="p-0039" num="0038"> <figref idrefs="DRAWINGS">FIGS. 3 and 4</figref> illustrate various methods for displaying an object on a live view;</div>
<div class="description-paragraph" id="p-0040" num="0039"> <figref idrefs="DRAWINGS">FIG. 5</figref> illustrates a method for separating an object from a captured image;</div>
<div class="description-paragraph" id="p-0041" num="0040"> <figref idrefs="DRAWINGS">FIG. 6</figref> illustrates a method for adjusting an object area according to a user's manipulation;</div>
<div class="description-paragraph" id="p-0042" num="0041"> <figref idrefs="DRAWINGS">FIG. 7</figref> illustrates a method for separating an object from a captured image and restoring a shape of the object;</div>
<div class="description-paragraph" id="p-0043" num="0042"> <figref idrefs="DRAWINGS">FIG. 8</figref> is a block diagram illustrating a configuration of a photographing device according to an exemplary embodiment of the present disclosure;</div>
<div class="description-paragraph" id="p-0044" num="0043"> <figref idrefs="DRAWINGS">FIG. 9</figref> illustrates an example of a method for producing a composite image;</div>
<div class="description-paragraph" id="p-0045" num="0044"> <figref idrefs="DRAWINGS">FIG. 10</figref> illustrates a detailed example of a method for producing a composite image;</div>
<div class="description-paragraph" id="p-0046" num="0045"> <figref idrefs="DRAWINGS">FIG. 11</figref> is a flow chart illustrating a method for producing a composite image according to an exemplary embodiment of the present disclosure; and</div>
<div class="description-paragraph" id="p-0047" num="0046"> <figref idrefs="DRAWINGS">FIG. 12</figref> is a flow chart illustrating a method for separating an object according to an exemplary embodiment of the present disclosure.</div>
</description-of-drawings>
<heading id="h-0005">DETAILED DESCRIPTION</heading>
<div class="description-paragraph" id="p-0048" num="0047">Certain exemplary embodiments of the present disclosure will now be described in greater detail with reference to the accompanying drawings.</div>
<div class="description-paragraph" id="p-0049" num="0048">In the following description, like drawing reference numerals are used for like elements, even in different drawings. The matters defined in the description, such as detailed construction and elements, are provided to assist in a comprehensive understanding of the disclosure. However, it should be understood that the exemplary embodiments of the present disclosure can be carried out without those specifically defined matters. Also, well-known functions or constructions are not described in detail since they would obscure the disclosure with unnecessary detail.</div>
<div class="description-paragraph" id="p-0050" num="0049"> <figref idrefs="DRAWINGS">FIG. 1</figref> is a block diagram illustrating a configuration of a photographing apparatus according to an exemplary embodiment of the present disclosure. The photographing device <b>100</b> may be implemented in various types of devices, such as a digital camera, a camcorder, a mobile phone, a tablet computer, a laptop computer, a personal digital assistant (PDA), an MP3 player, and the like.</div>
<div class="description-paragraph" id="p-0051" num="0050">As shown in <figref idrefs="DRAWINGS">FIG. 1</figref>, the photographing device <b>100</b> may include a photographing unit <b>110</b>, an image processor <b>120</b>, a display <b>130</b>, and a controller <b>140</b>.</div>
<div class="description-paragraph" id="p-0052" num="0051">The photographing unit <b>110</b> performs a shooting operation. The photographing unit <b>110</b> may include a lens and an image sensor. Types of lens used in the photographing unit <b>110</b> may be a general-use lens, a wide-angle lens, a zoom lens, and the like. The type of lens may be decided according to a type and a feature of the photographing device <b>100</b> and user environment. The image sensor may be a complementary metal oxide semiconductor (CMOS), a charge coupled device (CCD), or the like.</div>
<div class="description-paragraph" id="p-0053" num="0052">The image processor <b>120</b> processes an image sensed by the photographing unit <b>110</b>. More specifically, the image processor <b>120</b> separates an object, which is a specific subject of the shooting operation, from an image obtained by the photographing unit <b>110</b>.</div>
<div class="description-paragraph" id="p-0054" num="0053">Separation of the object may be performed using various algorithms.</div>
<div class="description-paragraph" id="p-0055" num="0054">In an exemplary embodiment, the controller <b>140</b> divides, for example, a photographed image captured by the photographing unit <b>110</b> or a live view image displayed on the display <b>130</b> in a unit of pixel or pixel block comprising a plurality of pixels. The controller <b>140</b> compares pixel values of respective pixels or representative pixel values of respective pixel blocks, and detects a pixel area of which pixel values or representative pixel values has a difference therebetween which is greater than a predetermined threshold value. The controller <b>140</b> determines that a detected pixel area is at an edge and determines an object area based on the determined edge.</div>
<div class="description-paragraph" id="p-0056" num="0055">The controller <b>140</b> determines that an object area satisfying a preset condition is a subject for compositing. The controller <b>140</b> controls the image processor <b>120</b> to separate the object area satisfying the preset condition from the photographed image. In an exemplary embodiment, when composing a person's face, the controller <b>140</b> may determine that an object area having pixel values corresponding to a facial color is a subject for compositing.</div>
<div class="description-paragraph" id="p-0057" num="0056">In another example, the controller <b>140</b> may determine the object area using a previously learned object model. The object model is a model which analyzes a plurality of images obtained by taking photographs of the object multiple times or analyzes a plurality of live view images of the object and defines the object area according to statistics of an area in which the object is located. The object model may be continuously updated by repeated learning. The controller <b>140</b> may apply the object model to the photographed image and determine that the object is located in an area corresponding to the object model. More specifically, the object area may be specified based on an edge defining the area corresponding to the object model. The controller <b>140</b> controls the image processor <b>120</b> to separate the object area which is determined by the object model.</div>
<div class="description-paragraph" id="p-0058" num="0057">The display <b>130</b> displays a live view obtained by the photographing unit <b>110</b>. The live view is an image provided to identify a subject through the display <b>130</b>, not a viewfinder. More specifically, light entering through the lens of the photographing unit <b>110</b> is introduced into the image sensor, and the image sensor outputs an electrical signal corresponding to the incident light to the display <b>130</b>. Accordingly, an image of the subject to be photographed is displayed on a live view area. The live view may be provided in various ways such as a contrast auto focus (AF) live view method, a phase difference AF live view method, and a method of using a separate image sensor for processing the live view. When the image processor <b>120</b> separates the object, the separated object is superimposed onto the live view and is displayed, which will be described later.</div>
<div class="description-paragraph" id="p-0059" num="0058">The controller <b>140</b> controls an overall operation of the photographing device <b>100</b>. More specifically, when a shooting command is input, the controller <b>140</b> controls the photographing unit <b>110</b> to capture a photograph. Accordingly, when an image corresponding to a background is photographed, the controller <b>140</b> generates a composite image by applying the object to the photographed image. The controller <b>140</b> controls the image processor <b>120</b> to perform various image processing on the object so that the photographed image composited with the object <b>11</b> may appear natural.</div>
<div class="description-paragraph" id="p-0060" num="0059">More specifically, the image processor <b>120</b> may perform a tone mapping to reduce a difference in brightness between the object <b>11</b> and a background image and to adjust color distribution. In addition, the image processor <b>120</b> may perform an image matting to remove a distinctive boundary between the object and the background image. As a result, more natural composite images may be generated.</div>
<div class="description-paragraph" id="p-0061" num="0060">The operation of generating the composite image as described above may be performed when the photographing device <b>100</b> operates in a composite shooting mode. That is, the photographing device <b>100</b> may operate in various shooting modes, such as a general shooting mode, a self shooting mode, the composite shooting mode, a special shooting mode, and the like, according to the user's selection.</div>
<div class="description-paragraph" id="p-0062" num="0061"> <figref idrefs="DRAWINGS">FIG. 2</figref> illustrates a method for producing a composite image. With reference to <figref idrefs="DRAWINGS">FIG. 2</figref>, when the composite shooting mode is selected, the controller <b>140</b> activates the photographing unit <b>110</b> to display a live view. Here, when the shooting command is input, a photographing image <b>10</b> including an object <b>11</b> is obtained. The image processor <b>120</b> separates the object <b>11</b> from the photographing image <b>10</b>.</div>
<div class="description-paragraph" id="p-0063" num="0062">When the photographing image <b>10</b> is obtained, the controller <b>140</b> activates the photographing unit <b>110</b> again to display a live view <b>20</b> on the display <b>150</b>. The display <b>150</b> superimposes the object <b>11</b> on the live view <b>20</b> and displays the superimposed object <b>11</b> on the live view <b>20</b> on a screen. The live view <b>20</b> may change according to a shooting direction of the photographing device <b>100</b> in real time, but the same object <b>11</b> is displayed on the live view <b>20</b>. Accordingly, while the user views the background image and the object <b>11</b> on the screen, the user may input the shooting command at a desired shooting angle and shooting time in consideration of composing the object <b>11</b> and the background image. When the shooting command is input, the controller <b>140</b> controls the photographing unit <b>110</b> to obtain a photographing image and composites the photographing image and the object <b>11</b> to become a single image. For illustrative purposes, the firstly photographed image is referred to as a first photographing image or an object image, and the subsequently photographed image is referred to as a second photographing image or the background image. In addition, the live view <b>20</b> displayed with the object <b>11</b>, which is superimposed thereon, is referred to as a background live view.</div>
<div class="description-paragraph" id="p-0064" num="0063"> <figref idrefs="DRAWINGS">FIG. 3</figref> illustrates a composite method according to another exemplary embodiment of the present disclosure. With reference to <figref idrefs="DRAWINGS">FIG. 3</figref>, the photographing device <b>100</b> may change at least one of display properties such as a display position, a size, a color, and a brightness of the object on the background live view.</div>
<div class="description-paragraph" id="p-0065" num="0064">For example, when the object <b>11</b> is separated from the first photographing image, the object <b>11</b> is superimposed and displayed on the background live view <b>20</b> as shown in <figref idrefs="DRAWINGS">FIG. 3(A)</figref>. Here, when the user touches the object <b>11</b> and drags the object <b>11</b> to another position, the controller <b>140</b> analyzes a coordinate value of a touched point and determines a trajectory and a direction of the dragging using values sensed by a touch sensor provided in the display <b>130</b>. The controller <b>140</b> controls the image processor <b>120</b> to change a display position of the object <b>11</b> according to the trajectory and the direction of the dragging. Therefore, as shown in <figref idrefs="DRAWINGS">FIG. 3(B)</figref>, the object <b>11</b> moves to another position on the background live view <b>20</b> and is displayed.</div>
<div class="description-paragraph" id="p-0066" num="0065">In addition, the user may perform a manipulation such as pinch-in and pinch-out. When two or more points in an area in which the object <b>11</b> is displayed are touched by, for example, fingers and a distance between the touched points increases by spreading the fingers apart, the controller <b>140</b> determines that a pinch-in manipulation to enlarge a size of the object <b>11</b> is performed. On the contrary, when the distance between the touched points decreases by bringing the fingers closer, the controller <b>140</b> determines that a pinch-out manipulation to reduce the size of the object <b>11</b> is performed. The controller <b>140</b> controls the image processor <b>120</b> to enlarge or reduce the size of the object <b>11</b> according to the determination result. <figref idrefs="DRAWINGS">FIG. 3(C)</figref> shows that the size of the object <b>11</b> is reduced.</div>
<div class="description-paragraph" id="p-0067" num="0066">In <figref idrefs="DRAWINGS">FIGS. 3(B) and 3(C)</figref>, the display position and the size of the object change according to the user's manipulation. However, display properties of the object may be automatically adjusted according to a feature of the background live view. That is, the controller <b>140</b> may analyze the background live view <b>20</b> and control the image processor <b>120</b> to move the object <b>11</b> such that the object <b>11</b> and other objects in the background live view <b>20</b> do not overlap each other or to adjust the size of the object <b>11</b> such that the object <b>11</b> and other objects in the background live view <b>20</b> do not overlap each other.</div>
<div class="description-paragraph" id="p-0068" num="0067">In <figref idrefs="DRAWINGS">FIG. 3</figref>, an exemplary embodiment of changing the display position and the size of the object <b>11</b> is described. In an alternative embodiment, properties such as the brightness and the color of the object <b>11</b> may be adjusted based on the feature of the background live view.</div>
<div class="description-paragraph" id="p-0069" num="0068"> <figref idrefs="DRAWINGS">FIG. 4</figref> illustrates an exemplary embodiment of adjusting the brightness of the separated object <b>11</b> to correspond to properties such as the brightness or color of the background live view <b>20</b>. The controller <b>140</b> analyzes a property such as the brightness or color of the background live view <b>20</b> and controls the image processor <b>120</b> to adjust the brightness or color of the object <b>11</b> corresponding to the analyzed property of the background live view <b>20</b>. <figref idrefs="DRAWINGS">FIG. 4</figref> shows that the brightness of the object <b>11</b> increases according to that of the background live view <b>20</b>.</div>
<div class="description-paragraph" id="p-0070" num="0069">In another exemplary embodiment of the present disclosure, the photographing device <b>100</b> may track and separate the object based on the live view.</div>
<div class="description-paragraph" id="p-0071" num="0070"> <figref idrefs="DRAWINGS">FIG. 5</figref> illustrates a method for separating an object from an image according to another exemplary embodiment of the present disclosure.</div>
<div class="description-paragraph" id="p-0072" num="0071">In the composite shooting mode, the controller <b>140</b> activates the photographing unit <b>110</b> to display a live view <b>30</b>. The live view <b>30</b> displays the object <b>11</b>, which is a subject to be photographed.</div>
<div class="description-paragraph" id="p-0073" num="0072">The controller <b>140</b> searches for the object <b>11</b> by monitoring respective frames of the live view <b>30</b>. In an exemplary embodiment, the controller <b>140</b> may extract the live view <b>30</b> in a frame unit and analyze the extracted frames to detect an edge. The controller <b>140</b> determines that, among pixel areas defined by the edge, a pixel area including pixels which have similar pixel values and are adjacent to one another correspond to the object <b>11</b>.</div>
<div class="description-paragraph" id="p-0074" num="0073">In this case, when a size of the determined pixel area is smaller than a threshold value, it may be difficult to determine the pixel area as the object <b>11</b>. For example, when a back of the user's head is displayed as shown in <figref idrefs="DRAWINGS">FIG. 5(A)</figref>, or when the user's face in a side profile is displayed as shown in <figref idrefs="DRAWINGS">FIG. 5(B)</figref>, a pixel area corresponding to the user's face is relatively small and thus it may be difficult to determine a face area of the user. Accordingly, the controller <b>140</b> monitors the live view <b>30</b> continuously until the face area is determined.</div>
<div class="description-paragraph" id="p-0075" num="0074">When the user faces toward the photographing unit as shown in <figref idrefs="DRAWINGS">FIG. 5(C)</figref>, the size of the face area may be greater than the threshold value. Thus, the face area and a body area which is connected thereto are determined to be the object <b>11</b>. Once the object <b>11</b> is determined, the controller <b>140</b> may track the object <b>11</b> continuously even when the size of the object <b>11</b> decreases or the object <b>11</b> moves. Accordingly, when the user moves and change a position of the user's face as shown in <figref idrefs="DRAWINGS">FIGS. 5(D) and 5(E)</figref>, the object <b>11</b> may still be correctly determined. Here, when shooting is performed, the photographing image <b>10</b> is obtained as shown in <figref idrefs="DRAWINGS">FIG. 5(F)</figref>. Subsequently, the object <b>11</b> is separated from the photographing image <b>10</b>.</div>
<div class="description-paragraph" id="p-0076" num="0075">As described above, when the user firstly captures a photograph of his or her own, auto segmentation to separate the image of the user from the captured image is performed. In <figref idrefs="DRAWINGS">FIG. 5</figref>, the object <b>11</b> is automatically separated, however, in order to increase precision of the auto segmentation, the user may additionally adjust an object area to be separated, which will be described below.</div>
<div class="description-paragraph" id="p-0077" num="0076"> <figref idrefs="DRAWINGS">FIG. 6</figref> illustrates a method for adjusting an object area in a photographing device according to another exemplary embodiment of the present disclosure. When a first photographing image <b>10</b> is obtained, the controller <b>140</b> displays the first photographing image <b>10</b> on a screen of the display <b>130</b>. Here, a guide image <b>61</b> for expressing a boundary of the object may be shown together on the screen. The controller <b>140</b> performs the auto segmentation to automatically specify the object area, determines a boundary of the object area, and renders the guide image <b>61</b> in a dotted line or a solid line at the boundary.</div>
<div class="description-paragraph" id="p-0078" num="0077">Here, the user may perform various manipulations on the screen, such as a touch, a drag, a flick, a scribble, and a lug. In this case, a line image <b>62</b> may be displayed on the screen according to a trajectory of the user's manipulation. Consequently, a size and a shape of the object area, i.e., an area which is determined to be the object, may change according to a direction or an extent of the user's manipulation on the screen. Accordingly, the object may be precisely determined by adjusting the object area.</div>
<div class="description-paragraph" id="p-0079" num="0078">In addition, the composition of the photographing image may vary according to a distance from a subject to the photographing device and the size of the subject. For example, when the user captures a photograph of his or her own, the photographing image may not show the whole body of the user but show only his face and a part of his upper body due to a limit of the user's arm's length. In this case, when the object is separated, the other part of the user's upper body and lower body are not included in the object. As a result, when the object is composited with the background image, the composite image may look unnatural. Therefore, before composing the object with the background image, it may be desirable to restore a shape of the separated object.</div>
<div class="description-paragraph" id="p-0080" num="0079"> <figref idrefs="DRAWINGS">FIG. 7</figref> illustrates a method for restoring an object using a photographing device according to another exemplary embodiment of the present disclosure. With reference to <figref idrefs="DRAWINGS">FIG. 7</figref>, in the object <b>11</b>, a width of a restoration area may be determined using widths of the face area and the body area. For example, as shown in <figref idrefs="DRAWINGS">FIG. 7(A)</figref>, when a half of a width of the face area is represented as Wf and a half of a width of the body area is represented as Ws, a size of the restoration area may be determined using a log function.</div>
<div class="description-paragraph" id="p-0081" num="0080">More specifically, the controller <b>140</b> may determine the restoration area using the following expression.
<br/>
[Expression 1]
<br/>
<i>Wf/Ws*</i>7 log<sub>2</sub> <i>|x|</i>  1
</div>
<div class="description-paragraph" id="p-0082" num="0081">A value calculated by Expression 1 may be used to determine the width of the restoration area. In Expression 1, x denotes a coordinate value on a horizontal axis. That is, modeling is performed on an x-y coordinate system in which a coordinate of a center point <b>13</b> of a top side of the first photographing image <b>10</b> in <figref idrefs="DRAWINGS">FIG. 7(A)</figref> is, for example, (0,0), an x coordinate increases in a direction from left to right, and a y coordinate increases in a downward direction. In this case, a part which forms a shape of a shoulder may be expressed by a coordinate (x,y). In <figref idrefs="DRAWINGS">FIG. 7</figref>, in order to restore the shoulder, a shape of the shoulder is determined first. In order to restore a right shoulder, the controller <b>140</b> calculates a y coordinate value using Expression 1, by sequentially increasing an x coordinate value from a point <b>14</b>, which is a top right corner of the photographing image <b>10</b>, to the right until the y coordinate value becomes a maximum value, i.e., a height of the first photographing image <b>10</b> in a vertical direction thereof. In order to restore a left shoulder, the controller <b>140</b> calculates the y value by sequentially decreasing the x value from a point <b>15</b>, which is a top left corner of the first photographing image <b>10</b>, to the left until the y value becomes the maximum value. As a result, when an area corresponding to the shoulders is determined, the image processor <b>120</b> draws a boundary line of shoulder portions <b>12</b> to be continuous from an edge line of the body area and provides an inner area of the drawn boundary line with a determined pixel value, so that the shoulder area may be restored. The pixel value of the restored area may be determined in a mirroring method which uses the same color of an adjacent pixel.</div>
<div class="description-paragraph" id="p-0083" num="0082">It should be noted that Expression 1 described above is given only for illustrative purposes and the disclosure is not limited thereto. Therefore, the controller <b>140</b> may also restore the shoulders using various mathematical function graphs. For example, instead of the log function, other functions which are similar to the log function may be used.</div>
<div class="description-paragraph" id="p-0084" num="0083">Consequently, as shown in <figref idrefs="DRAWINGS">FIGS. 7(B) and 7(C)</figref>, the image processor <b>120</b> may separate the object <b>11</b> from the first photographing image and restore the shoulder portions <b>12</b>. <figref idrefs="DRAWINGS">FIG. 7(C)</figref> shows the shape of the object <b>11</b> including the restored shoulder portions <b>12</b>.</div>
<div class="description-paragraph" id="p-0085" num="0084">As described above, the photographing device <b>100</b> shoots a plurality of photographing images using the photographing unit <b>110</b> and generates a composite image. The number of photographing units may vary. For example, a plurality of photographing images may be obtained by performing a photographing operation multiple times using a single photographing unit or by sequentially or concurrently performing photographing operations using two or more photographing units. Hereinafter, a configuration and an operation of a photographing device including a plurality of photographing units according to an exemplary embodiment of the present disclosure are described.</div>
<div class="description-paragraph" id="p-0086" num="0085"> <figref idrefs="DRAWINGS">FIG. 8</figref> is a block diagram illustrating a configuration of a photographing device according to an exemplary embodiment of the present disclosure. With reference to <figref idrefs="DRAWINGS">FIG. 8</figref>, the photographing device <b>100</b> may include the photographing unit <b>110</b>, the image processor <b>120</b>, the display <b>130</b>, the controller <b>140</b>, and a storage unit <b>150</b>.</div>
<div class="description-paragraph" id="p-0087" num="0086">The storage unit <b>150</b> may store various programs and data used in the photographing device <b>100</b>. In addition, the storage unit <b>150</b> may store photographing images captured by the photographing unit <b>110</b> and composite images generated by the image processor <b>120</b>. Furthermore, the storage unit <b>150</b> may store the object model as described above. The storage unit <b>150</b> may be a flash memory, hard disk drive (HDD), or the like which is built in the photographing device <b>100</b> or may be a memory card, a universal serial bus (USB) memory stick, a removable HDD, or the like which is mounted onto or is connectable to the photographing device.</div>
<div class="description-paragraph" id="p-0088" num="0087">The photographing unit <b>110</b> may include a plurality of photographing units such as a first photographing unit <b>111</b> and a second photographing unit <b>112</b>. The first photographing unit <b>111</b> and the second photographing unit <b>112</b> may be provided in a main body of the photographing device <b>100</b> in opposite portions thereof. For example, when the first photographing unit <b>111</b> is provided on a first surface of the photographing device <b>100</b> to capture photographs in a first shooting direction, the second photographing unit <b>112</b> is provided on a second surface to capture photographs in a second shooting direction opposite to the first shooting direction.</div>
<div class="description-paragraph" id="p-0089" num="0088">When the composite shooting mode is entered, the controller <b>140</b> activates the first photographing unit <b>111</b> and displays a live view of the first photographing unit <b>111</b> on the screen of the display <b>130</b>. Here, when a shooting command is input, the controller <b>140</b> controls the first photographing unit <b>111</b> to capture a photograph and obtains a first photographing image. The image processor <b>120</b> separates an image of an object from the first photographing image.</div>
<div class="description-paragraph" id="p-0090" num="0089">When the first photographing image is obtained by the first photographing unit <b>111</b>, the controller <b>140</b> activates the second photographing unit <b>112</b>. Accordingly, a live view acquired by the second photographing unit <b>112</b> on which the separated image of the object is superimposed, i.e. the background live view, is provided to the display <b>130</b>.</div>
<div class="description-paragraph" id="p-0091" num="0090">The image processor <b>120</b> may include a first buffer <b>121</b>, an object processor <b>122</b>, a second buffer <b>123</b>, and an adder <b>124</b>. The first buffer <b>121</b> buffers image data sensed by the first photographing unit <b>111</b>, and the second buffer <b>123</b> buffers image data sensed by the second photographing unit <b>112</b>.</div>
<div class="description-paragraph" id="p-0092" num="0091">The object processor <b>122</b> separates data of an area which is determined to be an object from the image data stored in the first buffer <b>121</b>. The separated data is transferred to the adder <b>124</b>.</div>
<div class="description-paragraph" id="p-0093" num="0092">The adder <b>124</b> superimposes the object separated by the object processor <b>122</b> on an image buffered by the second buffer <b>123</b> and outputs the superimposed image to the display <b>130</b>.</div>
<div class="description-paragraph" id="p-0094" num="0093">The display <b>130</b> may selectively display image data output from the first buffer <b>121</b>, the second buffer <b>123</b>, and the adder <b>124</b>.</div>
<div class="description-paragraph" id="p-0095" num="0094">In a situation where a live view of the superimposed object and the background, i.e., the background live view, is displayed, when a shooting command is input, the controller <b>140</b> controls the second photographing unit <b>112</b> to capture a photograph and obtains a second photographing image. Subsequently, the controller <b>140</b> composites the first and second photographing images and stores the composite image in the storage unit <b>150</b>.</div>
<div class="description-paragraph" id="p-0096" num="0095">Although it is described in this exemplary embodiment that the photographing unit <b>110</b> includes two photographing units, it should be noted that the number of photographing units may be varied depending on a particular application. In an alternative embodiment, the photographing unit <b>110</b> may include one photographing unit which can be rotated to photograph an image in different shooting directions. For example, the photographing unit <b>110</b> may capture the first photographing image in a first shooting direction and may be rotated by a predetermined angle to capture the second photographing image in a second shooting direction.</div>
<div class="description-paragraph" id="p-0097" num="0096">Also, in this exemplary embodiment, it is described that the display <b>130</b> displays image data provided from the image processor <b>120</b>. However, it should be noted that the display <b>130</b> may also display image data received directly from the photographing unit <b>110</b>.</div>
<div class="description-paragraph" id="p-0098" num="0097"> <figref idrefs="DRAWINGS">FIG. 9</figref> illustrates an example of a method for producing a composite image. With reference to <figref idrefs="DRAWINGS">FIG. 9</figref>, the display <b>130</b> is provided on a surface of the photographing device <b>100</b>. The first photographing unit <b>111</b> is provided on the same surface on which the display <b>130</b> is provided, and the second photographing unit <b>112</b> is provided on a surface opposite to the display <b>130</b>.</div>
<div class="description-paragraph" id="p-0099" num="0098">When performing a composite shooting, the first photographing unit <b>111</b> and the second photographing unit <b>112</b> may be sequentially activated as described above. However, <figref idrefs="DRAWINGS">FIG. 9</figref> shows an exemplary embodiment in which the first photographing unit <b>111</b> and the second photographing unit <b>112</b> are simultaneously activated. In this case, the display <b>130</b> displays a live view <b>910</b> of the first photographing unit <b>111</b> and a live view <b>920</b> of the second photographing unit <b>112</b> together. Thus, while viewing the two live views <b>910</b> and <b>920</b> at the same time, the user may capture photographs.</div>
<div class="description-paragraph" id="p-0100" num="0099">For example, when the user inputs the shooting command, the controller <b>140</b> controls the first photographing unit <b>111</b> to take a photograph. The image processor <b>120</b> separates an object <b>911</b> from the photographed image. The screen is switched to a live view <b>930</b>, in which the separated object <b>911</b> is displayed together on the screen. Here, when the user inputs the shooting command again, a composite image including the object <b>911</b> on the live view <b>930</b> is generated.</div>
<div class="description-paragraph" id="p-0101" num="0100">In another example, the user may select and photograph a background image first. That is, in a state where the two live views <b>910</b> and <b>920</b> are displayed, when the user touches the second live view <b>920</b>, the background image is shot. In this case, the background image may be fixed in a portion of the screen in which the second live view <b>920</b> is displayed, and the first live view <b>910</b> may be continuously displayed. Alternatively, the background image may be displayed on an entire screen, and the first live view <b>910</b> may be displayed in a picture in picture (PIP) form or be transparently superimposed on the background image.</div>
<div class="description-paragraph" id="p-0102" num="0101">In another example, in a state where the two live views <b>910</b> and <b>920</b> are displayed, when the user inputs the shooting command, the controller <b>140</b> controls the first and second photographing unit <b>111</b> and <b>112</b> simultaneously to take photographs at the same time. Thus, when the background image and the object are obtained at the same time, the object is superimposed and displayed on the background image and the display position, the size, the shape, and other display properties of the object may vary according to the user's selection.</div>
<div class="description-paragraph" id="p-0103" num="0102">These various exemplary embodiments may be performed by various types of devices having a photographing function. When the photographing device <b>100</b> is a device which supports various applications such as a mobile phone and a table PC, the user may select a shooting mode before taking a photograph.</div>
<div class="description-paragraph" id="p-0104" num="0103"> <figref idrefs="DRAWINGS">FIG. 10</figref> illustrates a process of selecting a composite shooting mode and producing a composite image. With reference to <figref idrefs="DRAWINGS">FIG. 10</figref>, when the photographing device <b>100</b> is turned on or unlocked, an icon screen <b>1000</b> including icons for preset applications or folders may be displayed. Among the icons, the user may select a camera icon (not shown), i.e., an icon for a camera application.</div>
<div class="description-paragraph" id="p-0105" num="0104">When the camera icon is selected, the display <b>130</b> displays a selection screen <b>1010</b> for selecting various modes. The selection screen <b>1010</b> displays information about various modes such as a general shooting, a self shooting, a composite shooting, and a special shooting.</div>
<div class="description-paragraph" id="p-0106" num="0105">Here, when the composite shooting mode is selected, the first photographing unit <b>111</b> is activated first so that a live view <b>1020</b> of the first photographing unit <b>111</b> is displayed. On a screen of the live view <b>1020</b>, a shooting menu <b>1022</b> may be displayed. When the user selects the shooting menu <b>1022</b>, a first photographing image corresponding to the live view <b>1020</b> is generated and an object <b>1021</b> is separated from the first photographing image. Subsequently, the second photographing unit <b>112</b> is activated so that the object <b>1021</b> is superimposed on a live view of the second photographing unit <b>112</b> and displayed on a background live view <b>1030</b>. The shooting menu <b>1022</b> may be displayed on the background live view <b>1030</b>.</div>
<div class="description-paragraph" id="p-0107" num="0106">The user may rotate the photographing device <b>100</b> and select a background which the user wants. Accordingly, when the object <b>1021</b> is placed on the desired background live view <b>1030</b>, the user may select the shooting menu <b>1022</b> so that a background image is photographed.</div>
<div class="description-paragraph" id="p-0108" num="0107">When the background image is photographed, the controller <b>140</b> displays a composite image <b>1040</b> including the object <b>1021</b> on the screen and also displays a question message <b>1050</b> for asking whether to store the composite image <b>1040</b>. The controller <b>140</b> may store or may not store the composite image <b>1040</b> in the storage unit <b>150</b> according to the user's response to the question message <b>1050</b>. When the composite image <b>1040</b> is not stored, the controller <b>140</b> returns to an operation of displaying the background live view <b>1030</b> including the object <b>1021</b>.</div>
<div class="description-paragraph" id="p-0109" num="0108"> <figref idrefs="DRAWINGS">FIG. 11</figref> is a flow chart illustrating a method for producing a composite image according to an exemplary embodiment of the present disclosure. With reference to <figref idrefs="DRAWINGS">FIG. 11</figref>, the photographing device <b>100</b> obtains a first photographing image in operation S<b>1110</b> and separates an object from the first photographing image in operation S<b>1120</b>.</div>
<div class="description-paragraph" id="p-0110" num="0109">In operation S<b>1130</b>, the photographing device <b>100</b> displays the background live view on which the separated object is superimposed. The live views of the object and the background may be obtained by different photographing units or be obtained in sequence by a single photographing unit. Various display properties of the object such as the position, the size, the shape, the color and the brightness of the object may vary on the background live view according to the user's manipulation or display properties of the live view of the background. In addition, for the object, shape restoration may be performed and subsequent processing such as the tone mapping and the image matting may be performed.</div>
<div class="description-paragraph" id="p-0111" num="0110">When the shooting command is input in operation S<b>1140</b>, the photographing device <b>100</b> generates a photographing image corresponding to the background, composites the object and the photographing image, and generates a composite image in operation S<b>1150</b>.</div>
<div class="description-paragraph" id="p-0112" num="0111"> <figref idrefs="DRAWINGS">FIG. 12</figref> is a flow chart illustrating a method for separating an object according to an exemplary embodiment of the present disclosure. With reference to <figref idrefs="DRAWINGS">FIG. 12</figref>, when the composite photographing mode is entered in operation S<b>1210</b>, a live view is displayed in operation S<b>1220</b>. In operation S<b>1230</b>, the photographing device <b>100</b> monitors respective frames of the live view and searches for an object. The object may be found using a previously learned object model. For example, the photographing device <b>100</b> may determine that an area corresponding to the object model on the photographing image corresponds to the object. When the object is determined and specified, the photographing device <b>100</b> tracks the object continuously in operation S<b>1240</b>.</div>
<div class="description-paragraph" id="p-0113" num="0112">Subsequently, when a command to take a photograph of the object is input in S<b>1250</b>, a first photographing image corresponding to the live view is taken in operation S<b>1260</b>, and the object is separated from the first photographing image in operation S<b>1270</b>. The separated object is superimposed on background live view so that the user may predict a composite image before photographing the background.</div>
<div class="description-paragraph" id="p-0114" num="0113">According to the various exemplary embodiments of the present disclosure, the user's image may be naturally composited with a background image. For example, since the position, the shape, and other display properties of the user's image may be conveniently adjusted on the background live view prior to photographing the background image, disadvantageous of the self shooting may be overcome.</div>
<div class="description-paragraph" id="p-0115" num="0114">The user may store the composite image in the photographing device <b>100</b> or upload the composite image to an external cloud server or web server so as to be shared with others.</div>
<div class="description-paragraph" id="p-0116" num="0115">In the exemplary embodiments as described above, the object is separated from the first photographing image and is superimposed on the live view of the background. However, the object may not be separated from a still image. For example, the object may be a dynamic object which is separated from each frame of moving image data or from a live view thereof.</div>
<div class="description-paragraph" id="p-0117" num="0116">More specifically, in the exemplary embodiment shown in <figref idrefs="DRAWINGS">FIG. 8</figref>, a first live view provided by the first photographing unit <b>111</b> may be buffered in the first buffer <b>121</b>. The object processor <b>122</b> may perform an object separating operation on the live view buffered in the first buffer <b>121</b> for a preset period. Accordingly, according to a change to the first live view provided by the first photographing unit <b>111</b>, the shape and the position of the separated object varies constantly. Such dynamic object may be superimposed on a second live view provided by the second photographing unit <b>112</b>. Watching the second live view on which the dynamic object is superimposed, the user may input a shooting command at an appropriate time. When the shooting command is input, the controller <b>140</b> controls the first and second photographing unit <b>111</b> and <b>112</b> to perform shooting and adds the object separated from a first image shot by the first image photographing unit <b>111</b> to an image shot by the second photographing unit <b>112</b>, thereby generating a composite image.</div>
<div class="description-paragraph" id="p-0118" num="0117">In addition, it may also be possible to separate an object from a moving image content provided by the storage unit <b>150</b> or an external storage medium and superimpose the object on another photographing image. Accordingly, the user may separate a desired object from previously shot moving image data and composite the object and the other photographing image, which is a newly shot image. Since the object separating method and the compositing method are the same as the exemplary embodiments described above, a description thereof will be omitted.</div>
<div class="description-paragraph" id="p-0119" num="0118">According to exemplary embodiments of the present disclosure, a separated image of an object is superimposed on a live view of a background to be provided to a user so that the user may obtain a composite image including the object and the background.</div>
<div class="description-paragraph" id="p-0120" num="0119">Thus, the user may easily take photographs of the user without other people's assistance.</div>
<div class="description-paragraph" id="p-0121" num="0120">The method for producing a composite image and method for separating an object according to the various exemplary embodiments described above may be coded as software and be stored in a non-transitory readable medium. The non-transitory readable medium may be built in various types of photographing devices and support the photographing devices to carry out the methods as described above.</div>
<div class="description-paragraph" id="p-0122" num="0121">A non-transitory readable medium is a medium which does not store data temporarily such as a register, cash, and memory but stores data semi-permanently and is readable by devices. More specifically, the aforementioned various applications or programs may be stored and provided in a non-transitory readable medium such as a compact disk (CD), digital video disk (DVD), hard disk, Blu-ray disk, universal serial bus (USB), memory card, and read-only memory (ROM).</div>
<div class="description-paragraph" id="p-0123" num="0122">The foregoing exemplary embodiments and advantages are merely exemplary and are not to be construed as limiting the present disclosure. The present teaching can be readily applied to other types of apparatuses. Also, the description of the exemplary embodiments of the present disclosure is intended to be illustrative, and not to limit the scope of the claims, and many alternatives, modifications, and variations will be apparent to those skilled in the art.</div>
</div>
</div>
</section><section itemprop="claims" itemscope="">
<h2>Claims (<span itemprop="count">22</span>)</h2>
<div html="" itemprop="content"><div class="claims" lang="EN" load-source="patent-office" mxw-id="PCLM96129112">
<claim-statement>What is claimed is:</claim-statement>
<div class="claim"> <div class="claim" id="CLM-00001" num="00001">
<div class="claim-text">1. A photographing device comprising:
<div class="claim-text">a photographing unit;</div>
<div class="claim-text">an image processor which separates an object from a first photographing image obtained by the photographing unit;</div>
<div class="claim-text">a display which displays a background live view obtained by superimposing the separated object on a live view of a background; and</div>
<div class="claim-text">a controller which obtains a second photographing image corresponding to the live view of the background when a command to shoot the background is input, and generates a composite image based on the separated object and the second photographing image,</div>
<div class="claim-text">wherein the controller controls the image processor to restore a shape of the separated object by adding a restoration area of the object, the restoration area being not included in the first photographing image and not overlapping with an area of the separated object, and superimposes the separated object having the restored shape on the live view of the background.</div>
</div>
</div>
</div> <div class="claim-dependent"> <div class="claim" id="CLM-00002" num="00002">
<div class="claim-text">2. The photographing device as claimed in <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein, when a composite shooting mode is selected, the controller controls the display to display a live view of the object and searches for and tracks the object by monitoring respective frames of the live view of the object, and
<div class="claim-text">when a command to shoot the object is input, the controller generates the first photographing image corresponding to the live view of the object and controls the image processor to separate the tracked object from the first photographing image.</div>
</div>
</div>
</div> <div class="claim-dependent"> <div class="claim" id="CLM-00003" num="00003">
<div class="claim-text">3. The photographing device as claimed in <claim-ref idref="CLM-00002">claim 2</claim-ref>, wherein the photographing unit comprises:
<div class="claim-text">a first photographing unit which captures a photograph in a first shooting direction; and</div>
<div class="claim-text">a second photographing unit which captures a photograph in a second shooting direction opposite to the first shooting direction, and</div>
<div class="claim-text">when the composite shooting mode is entered, the controller activates the first photographing unit to provide the live view of the object to the display, and when the first photographing image is obtained by the first photographing unit, the controller activates the second photographing unit to provide the live view of the background, on which the separated object is superimposed and provided to the display.</div>
</div>
</div>
</div> <div class="claim-dependent"> <div class="claim" id="CLM-00004" num="00004">
<div class="claim-text">4. The photographing device as claimed in <claim-ref idref="CLM-00001">claim 1</claim-ref>, further comprising:
<div class="claim-text">a storage unit which stores a previously learned object model, the object model being obtained based on a plurality of images of the object to determine an object area in which the object is located,</div>
<div class="claim-text">wherein the controller determines that the object is located in the object area determined by the object model on the first photographing image.</div>
</div>
</div>
</div> <div class="claim-dependent"> <div class="claim" id="CLM-00005" num="00005">
<div class="claim-text">5. The photographing device as claimed in <claim-ref idref="CLM-00004">claim 4</claim-ref>, wherein the display displays the first photographing image, and
<div class="claim-text">when a user's manipulation for the first photographing image is performed, the controller changes at least one of a size and a shape of the object area according to the user's manipulation.</div>
</div>
</div>
</div> <div class="claim-dependent"> <div class="claim" id="CLM-00006" num="00006">
<div class="claim-text">6. The photographing device as claimed in <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the controller determines the restoration area based on widths of a face area and a body area of the separated object.</div>
</div>
</div> <div class="claim-dependent"> <div class="claim" id="CLM-00007" num="00007">
<div class="claim-text">7. The photographing device as claimed in <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the controller controls the image processor to adjust at least one of a display position, a size, a color and a brightness of the object on the background live view according to a user's selection.</div>
</div>
</div> <div class="claim-dependent"> <div class="claim" id="CLM-00008" num="00008">
<div class="claim-text">8. The photographing device as claimed in <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the controller adjusts at least one of a display position, a size, a color and a brightness of the separated object based on the live view of the background.</div>
</div>
</div> <div class="claim"> <div class="claim" id="CLM-00009" num="00009">
<div class="claim-text">9. A method for producing a composite image, the method comprising:
<div class="claim-text">obtaining a first photographing image and separating an object from the first photographing image;</div>
<div class="claim-text">restoring a shape of the separated object by adding a restoration area of the object, the restoration area being not included in the first photographing image and not overlapping with an area of the separated object;</div>
<div class="claim-text">displaying a background live view obtained by superimposing the separated object having the restored shape on a live view of a background;</div>
<div class="claim-text">obtaining a second photographing image corresponding to the live view of the background when a command to shoot a background is input; and</div>
<div class="claim-text">generating a composite image based on the separated object and the second photographing image.</div>
</div>
</div>
</div> <div class="claim-dependent"> <div class="claim" id="CLM-00010" num="00010">
<div class="claim-text">10. The method as claimed in <claim-ref idref="CLM-00009">claim 9</claim-ref>, wherein the separating the object comprises:
<div class="claim-text">when a composite shooting mode is selected, displaying a live view of the object;</div>
<div class="claim-text">searching for and tracking the object by monitoring respective frames of the live view of the object, and</div>
<div class="claim-text">when a command to shoot the object is input, generating the first photographing image corresponding to the live view of the object; and</div>
<div class="claim-text">separating the tracked object from the first photographing image.</div>
</div>
</div>
</div> <div class="claim-dependent"> <div class="claim" id="CLM-00011" num="00011">
<div class="claim-text">11. The method as claimed in <claim-ref idref="CLM-00010">claim 10</claim-ref>, wherein the searching for and tracking the object comprises:
<div class="claim-text">obtaining an object model based on a plurality of images of the object to determine an object area in which the object is located; and</div>
<div class="claim-text">determining that the object is located in the object area determined by the object model on the first photographing image.</div>
</div>
</div>
</div> <div class="claim-dependent"> <div class="claim" id="CLM-00012" num="00012">
<div class="claim-text">12. The method as claimed in <claim-ref idref="CLM-00011">claim 11</claim-ref>, wherein the separating the object comprises:
<div class="claim-text">displaying the first photographing image; and</div>
<div class="claim-text">when a user's manipulation for the first photographing image is performed, changing at least one of a size and a shape of the object area according to the user's manipulation.</div>
</div>
</div>
</div> <div class="claim-dependent"> <div class="claim" id="CLM-00013" num="00013">
<div class="claim-text">13. The method as claimed in <claim-ref idref="CLM-00009">claim 9</claim-ref>, wherein the restoring the shape of the separated object comprises determining the restoration area based on widths of a face area and a body area of the separated object.</div>
</div>
</div> <div class="claim-dependent"> <div class="claim" id="CLM-00014" num="00014">
<div class="claim-text">14. The method as claimed in <claim-ref idref="CLM-00009">claim 9</claim-ref>, further comprising:
<div class="claim-text">adjusting at least one of a display position, a size, a color and a brightness of the separated object on the background live view according to a user's selection.</div>
</div>
</div>
</div> <div class="claim-dependent"> <div class="claim" id="CLM-00015" num="00015">
<div class="claim-text">15. The method as claimed in <claim-ref idref="CLM-00009">claim 9</claim-ref>, further comprising:
<div class="claim-text">adjusting at least one of a display position, a size, a color and a brightness of the separated object based on the live view of the background.</div>
</div>
</div>
</div> <div class="claim"> <div class="claim" id="CLM-00016" num="00016">
<div class="claim-text">16. An image processing apparatus comprising:
<div class="claim-text">a first image input unit which receives a first image including an object;</div>
<div class="claim-text">a second image input unit which receives a second image different from the first image;</div>
<div class="claim-text">an object processing unit which separates the object from the first image; and</div>
<div class="claim-text">an image composing unit which superimposes the separated object on the second image to be provided as a preview image to a user and generates a composite image based on the separated object and the second image when the preview image is selected by the user,</div>
<div class="claim-text">wherein the object processing unit restores a shape of the separated object by adding a restoration area of the object, the restoration area being not included in the first image and not overlapping with an area of the separated object, and the image composing unit generates the composite image based on the object having the restored shape and the second image.</div>
</div>
</div>
</div> <div class="claim-dependent"> <div class="claim" id="CLM-00017" num="00017">
<div class="claim-text">17. The image processing apparatus as claimed in <claim-ref idref="CLM-00016">claim 16</claim-ref>, wherein the first image and the second image are received simultaneously.</div>
</div>
</div> <div class="claim-dependent"> <div class="claim" id="CLM-00018" num="00018">
<div class="claim-text">18. The image processing apparatus as claimed in <claim-ref idref="CLM-00016">claim 16</claim-ref>, wherein the first image and the second image are received in a subsequent order.</div>
</div>
</div> <div class="claim-dependent"> <div class="claim" id="CLM-00019" num="00019">
<div class="claim-text">19. The image processing apparatus as claimed in <claim-ref idref="CLM-00016">claim 16</claim-ref>, wherein the object processing unit determines the restoration area of the object based on widths of a face area and a body area of the separated object.</div>
</div>
</div> <div class="claim-dependent"> <div class="claim" id="CLM-00020" num="00020">
<div class="claim-text">20. The image processing apparatus as claimed in <claim-ref idref="CLM-00016">claim 16</claim-ref>, wherein the object processing unit adjusts at least one of a display position, a size, a color and a brightness of the object on the preview image according to a user's selection.</div>
</div>
</div> <div class="claim-dependent"> <div class="claim" id="CLM-00021" num="00021">
<div class="claim-text">21. The image processing apparatus as claimed in <claim-ref idref="CLM-00016">claim 16</claim-ref>, wherein the controller adjusts at least one of a display position, a size, a color and a brightness of the separated object based on the second image.</div>
</div>
</div> <div class="claim-dependent"> <div class="claim" id="CLM-00022" num="00022">
<div class="claim-text">22. A non-transitory computer readable recording medium having recorded thereon a program executable by a computer for performing the method of <claim-ref idref="CLM-00009">claim 9</claim-ref>.</div>
</div>
</div> </div>
</div>
</section>
                </article>
            </search-app>
        </body>
    </html>
    