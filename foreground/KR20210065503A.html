
    <html>
        <body>
            <search-app>
                <article class="result" itemscope="" itemtype="http://schema.org/ScholarlyArticle">
    <h1 itemprop="pageTitle">KR20210065503A - 딥러닝을 적용한 탁구 로봇 시스템 
        - Google Patents</h1><section itemprop="abstract" itemscope="">
<h2>Abstract</h2>
<div html="" itemprop="content"><abstract lang="KO" load-source="patent-office" mxw-id="PA449876317">
<div class="abstract" num="0001a">본 발명의 실시예들에 따르면, 딥러닝을 적용한 탁구 로봇 시스템은 딥러닝과 비전 시스템을 이용하여 사용자와 양방향 랠리가 가능하고, 제시된 방식의 탁구 로봇은 복잡한 수학 계산이나 탁구공의 궤적을 모두 감지할 필요 없고, 사용자의 운동 능력에 맞춰 성능 향상이 가능하여 기존에 제안된 로봇들과 차별화되며, 뿐만 아니라, 스테레오 타입 카메라 한 대 만으로 충분히 작동이 가능하여 금전적으로 사용자의 접근성이 쉽고, 딥 러닝을 이용한 탁구 로봇은 향후 노인과 환자 및 장애인의 재활 문제에 큰 기여를 부여할 수 있다. </div>
<div class="abstract" num="0002a">
<figref num="1a"> </figref>
</div>
</abstract>
</div>
</section><section itemprop="description" itemscope="">
<h2>Description</h2>
<div html="" itemprop="content"><div class="description" lang="KO" load-source="patent-office" mxw-id="PDES294341913">
<invention-title lang="KO">딥러닝을 적용한 탁구 로봇 시스템{TABLE TENNIS ROBOT SYSTEM APPLYING DEEP LEARNING} </invention-title>
<technical-field>
<div class="description-paragraph" num="0001">본 발명은 탁구 로봇 시스템에 대한 것으로, 보다 상세하게는 딥 러닝을 적용하여 환자의 운동능력에 맞춰 탁구로봇의 성능을 향상시킬 수 있어 효율적인 재활훈련을 가능하게 하는 딥러닝을 적용한 탁구 로봇 시스템에 관한 것이다. </div>
</technical-field>
<background-art>
<div class="description-paragraph" num="0002">재활 환자들을 치료하는 과정에서 효과적인 운동의 선택은 재활 치료에서 가장 중요하다. 수영과 요가, 스쿼트와 같은 재활 운동 중, 실내에서도 쉽게 즐길 수 있으며 민첩성을 길러줄 수 있는 탁구는 효과적인 재활 운동으로 주목 받고 있다. </div>
<div class="description-paragraph" num="0003">혼자서는 즐길 수 없는 탁구라는 종목의 특성상, 재활 환자들은 공을 발사해주는 기계에 의존하여 이를 상대로 탁구를 즐기고 있는 실정이다. 그러나 단방향적인 탁구 연습은 운동 범위의 제약과 더불어 재활 환자들의 능력을 고려하지 않아 효과적인 재활 훈련이 불가하다. </div>
<div class="description-paragraph" num="0004">로봇은 산업용으로 개발되어 공장 자동화의 일 부분을 담당하여 왔으나, 최근에는 로봇을 응용한 분야가 더욱 확대되어, 의료용 로봇, 재활 로봇, 우주 항공 로봇 등이 개발되고, 일반 가정에서 사용할 수 있는 가정용 로봇도 만들어지고 있다. </div>
<div class="description-paragraph" num="0005">로봇과 더불어, 최근에는 딥러닝을 포함한 기계학습은 IoT 기술의 발달과 빅데이터 처리를 뒷받침할 수 있는 GPU를 비롯한 각종 하드웨어 발전으로 정확성에 큰 기여를 하고 있다. </div>
<div class="description-paragraph" num="0006">이와 관련하여, 대한민국 등록특허 제10-1719278호에는, 딥러닝 기술을 모듈화하며, 각 모듈별 In/Out 파라미터 속성 추출 및 훈련 데이터셋 분석, 딥러닝 시나리오의 자동화하는 콘텐츠 기반 딥러닝 분석도구를 탑재하며, 상기 콘텐츠 기반 딥러닝 분석도구를 통한 모듈간 IN/OUT의 파라미터 속성을 연동시키는 파라미터 속성 연동모듈과, 모듈간 연동이 가능한 동적 호출 인터페이스 연동모듈과, 상기 모듈간의 표준API 인터페이스 통합모듈과, 모듈의 태스킹 분석, 결과, 확인을 하나로 통합하는 One-pass 통합모듈 및 딥러닝 분석도구를 통한 분석 결과를 저장하는 분석 결과 저장소를 포함하는 통합GUI프레임워크가 개시되어 있다. </div>
<div class="description-paragraph" num="0007">특히, 스탠포드대학의 앤드류 응과 구글이 함께한 딥 러닝 프로젝트에서는 16,000개의 컴퓨터 프로세서와 10억 개 이상의 neural networks 그리고 DNN(deep neural networks)을 이용하여 유튜브에 업로드 되어 있는 천만 개 넘는 비디오 중 고양이 인식에 성공하였다. </div>
<div class="description-paragraph" num="0008">위에서 언급된 배경기술에 기재된 내용들은 단지 본 발명의 배경기술을 이해시키기 위해서 작성된 것이므로, 이 기술의 당업자에게 이미 알려진 공지기술 외의 내용을 포함할 수 있다. </div>
</background-art>
<div>
<div>
<div class="description-paragraph" num="0009">
<patcit>
<text>출원번호 10-2017-0081211</text>
</patcit>
<patcit>
<text>출원번호 10-2018-0081228</text>
</patcit>
<patcit>
<text>출원번호 10-2018-0081228</text>
</patcit>
<patcit>
<text>출원번호 10-2019-0106848</text>
</patcit>
</div>
</div>
</div>
<summary-of-invention>
<tech-problem>
<div class="description-paragraph" num="0010">본 발명은 기존에 딥 러닝을 통해 환자의 운동능력에 맞춰 탁구로봇의 성능을 향상시킬 수 있어 효율적인 재활훈련을 가능하게 하는 딥러닝을 적용한 탁구 로봇 시스템을 제공하고자 한다. </div>
</tech-problem>
<tech-solution>
<div class="description-paragraph" num="0011">본 발명의 실시예에 따른 딥러닝을 적용한 탁구 로봇 시스템은, 카메라를 통해서 탁구대에서 일측에서 타측으로 날아가는 탁구공의 감지하고, 실시간 위치시간정보를 연산하는 비전 시스템; 가이드레일을 따라서 상기 탁구대의 폭방향으로 움직이고, 탁구채를 이용하여 상기 탁구공을 일측으로 타격하도록 배치되는 로봇암; 상기 가이드레일에서 상기 로봇암의 위치와 상기 로봇암의 작동을 제어하는 로봇 제어기; 상기 탁구공의 실시간 위치시간정보를 미리 설정된 형태로 입력 받고, 이를 학습하여 상기 탁구공의 예상 위치시간정보를 연산하는 딥러닝부; 및 상기 비전 시스템 및 상기 딥러닝부와 정보를 송수신하고, 실시간 위치시간정보와 예상 위치시간정보를 통해서 상기 로봇 제어기를 제어하는 메인 컨트롤러를 포함할 수 있다. </div>
<div class="description-paragraph" num="0012">본 발명의 실시예에 따른 딥러닝을 적용한 탁구 로봇 시스템에서, 상기 메인 컨트롤러는, 상기 비전 시스템으로부터 입력된 탁구공의 실시간 위치시간정보로부터 상기 딥러닝부에서 연산된 예상 위치시간정보를 선택하고, 선택된 예상 위치시간정보를 기준으로 상기 로봇암이 상기 탁구공을 타격하도록 제어할 수 있다. </div>
<div class="description-paragraph" num="0013">본 발명의 실시예에 따른 딥러닝을 적용한 탁구 로봇 시스템에서, 상기 로봇 제어기는, 상기 가이드레일 상에서 상기 로봇암의 위치를 제어하는 레일 제어기; 및 상기 로봇암이 탁구공을 타격하도록 제어하는 암 제어기를 포함할 수 있다. </div>
<div class="description-paragraph" num="0014">본 발명의 실시예에 따른 딥러닝을 적용한 탁구 로봇 시스템에서, 상기 딥러닝부는 텐서플로우(tensorflow)를 이용한 학습을 적용할 수 있다. </div>
<div class="description-paragraph" num="0015">본 발명의 실시예에 따른 딥러닝을 적용한 탁구 로봇 시스템에서, 상기 비전 시스템은 탁구대의 일측 상부 및 타측 상부에 배치되는 제1, 2 카메라를 포함할 수 있다. </div>
<div class="description-paragraph" num="0016">본 발명의 실시예에 따른 딥러닝을 적용한 탁구 로봇 시스템에서, 상기 메인 컨트롤러는 opencv 및 cuda를 활용한 gpu를 활용하여 상기 비전 시스템에서 촬영된 영상데이터를 처리할 수 있다. </div>
<div class="description-paragraph" num="0017">본 발명의 실시예에 따른 딥러닝을 적용한 탁구 로봇 시스템에서, 상기 메인 컨트롤러는 raspberry pi의 i2c 통신을 통해서 상기 암 제어기를 제어할 수 있다. </div>
<div class="description-paragraph" num="0018">본 발명의 실시예에 따른 딥러닝을 적용한 탁구 로봇 시스템에서, 상기 메인 컨트롤러는 raspberry pi의 gpio pwm 통신을 통해서 상기 레일 제어기를 제어할 수 있다. </div>
<div class="description-paragraph" num="0019">본 발명의 실시예에 따른 딥러닝을 적용한 탁구 로봇 시스템에서, 상기 메인 컨트롤러는, 상기 비전 시스템으로부터 촬영된 데이터로부터 탁구공의 전체윤곽을 추출하고, 추출된 전체윤곽의 중심점을 연산하며, 이 중심점을 기준으로 탁구공의 위치시간정보를 연산할 수 있다. </div>
<div class="description-paragraph" num="0020">본 발명의 실시예에 따른 딥러닝을 적용한 탁구 로봇 시스템에서, 상기 메인 컨트롤러는, 상기 비전 시스템으로부터 촬영된 데이터에서, 렌즈 왜곡을 보정하기 위해서 fisheye 렌즈왜곡모델을 사용할 수 있다. </div>
<div class="description-paragraph" num="0021">본 발명의 실시예에 따른 딥러닝을 적용한 탁구 로봇 시스템에서, 상기 비전 시스템은 적어도 두 개의 카메라를 포함하고, 이들을 시간적으로 동기화 시키기 위해서 serializer 및 deserializer가 사용될 수 있다. </div>
<div class="description-paragraph" num="0022">본 발명의 실시예에 따른 딥러닝을 적용한 탁구 로봇 시스템에서, 상기 비전 시스템은 빛에너지를 조사하여 탁구공과의 거리를 감지하는 TOF(time of flight)방식을 이용할 수 있다. </div>
<div class="description-paragraph" num="0023">상기 비전 시스템, 상기 메인 컨트롤러, 상기 딥러닝부, 및 상기 로봇 제어기 중에서 적어도 하나는 소켓통신을 이용하여 데이터를 송수신하는 것을 특징으로 하는 딥러닝을 적용한 탁구 로봇 시스템. </div>
</tech-solution>
<advantageous-effects>
<div class="description-paragraph" num="0024">본 발명의 실시예들에 따르면, 딥러닝을 적용하여 사용자의 패턴분석을 통해 맞춤형 능력향상이 가능한 딥러닝을 적용한 탁구 로봇 시스템은 딥러닝과 비전 시스템을 이용하여 사용자와 양방향 랠리가 가능한다. </div>
<div class="description-paragraph" num="0025">또한, 제시된 방식의 탁구 로봇은 복잡한 수학 계산이나 탁구공의 궤적을 모두 감지할 필요 없고, 사용자의 운동 능력에 맞춰 성능 향상이 가능하여 기존에 제안된 로봇들과 차별화된다. </div>
<div class="description-paragraph" num="0026">뿐만 아니라, 스테레오 타입 카메라 한 대 만으로 충분히 작동이 가능하여 금전적으로 사용자의 접근성이 쉽고, 딥 러닝을 이용한 탁구 로봇은 향후 노인과 환자 및 장애인의 재활 문제에 큰 기여를 부여할 수 있다. </div>
<div class="description-paragraph" num="0027">위에서 언급된 본 발명의 실시예에 따른 효과는 기재된 내용에만 한정되지 않고, 명세서 및 도면으로부터 예측 가능한 모든 효과를 더 포함할 수 있다. </div>
</advantageous-effects>
</summary-of-invention>
<description-of-drawings>
<div class="description-paragraph" num="0028">도 1a는 본 발명의 실시예에 따른 탁구 로봇 시스템의 개략적인 구성도이다. <br/>
도 1b는 본 발명의 실시예에 따른 탁구 로봇 시스템의 개략적인 사시도이다. <br/>
도 2는 본 발명의 실시예에 따른 탁구 로봇 시스템의 개략적인 측면도이다. <br/>
도 3은 본 발명의 실시예에 따른 탁구 로봇 시스템에서 데이터의 형태를 보여주는 테이블이다. <br/>
도 4는 본 발명의 실시예에 따른 탁구 로봇 시스템에서 개략적인 학습방법을 보여준다. <br/>
도 5는 본 발명의 실시예에 따른 탁구 로봇 시스템에서 부가적인 내용을 보여준다. </div>
</description-of-drawings>
<description-of-embodiments>
<div class="description-paragraph" num="0029">본 발명은 이하에서 설명되는 특정 실시예에 한정되지 않으며, 본 발명의 사상 및 기술 범위에 포함되는 모든 변형, 균등물 내지 대체물을 포함하는 것으로 이해되어야 한다. </div>
<div class="description-paragraph" num="0030">본 명세서에서 사용한 용어는 특정 실시예를 설명하기 위해 사용된 것으로, 본 발명을 한정하려는 의도가 아니다. 단수의 표현은 문맥상 명백하게 뜻하지 않는 한 복수의 표현을 포함할 수 있다. </div>
<div class="description-paragraph" num="0031">"포함하다" 또는 "가지다" 등의 용어는 명세서상에 기재된 구성이나 작용 등 외에도 다른 구성이나 작용 등을 더 포함할 가능성이 있음을 의미한다. 제1, 제2 등의 용어는 구성요소들을 한정하기 위한 것이 아니며, 하나의 구성요소를 다른 구성요소로부터 구별하는 목적이다. </div>
<div class="description-paragraph" num="0032">이하, 본 발명에 따른 딥러닝을 적용한 탁구 로봇 시스템의 실시예를 첨부도면을 참조하여 상세히 설명한다.</div>
<div class="description-paragraph" num="0033">도 1a는 본 발명의 실시예에 따른 탁구 로봇 시스템의 개략적인 구성도이고, 도 1b는 본 발명의 실시예에 따른 탁구 로봇 시스템의 개략적인 사시도이다. </div>
<div class="description-paragraph" num="0034">도 1a 및 1b를 참조하면, 탁구 로봇 시스템은 비전 시스템(100), 메인 컨트롤러(105), 딥러닝부(110), 통신부(115), 로봇 제어기(120), 로봇암(130), 및 레일(135)을 포함하고, 상기 비전 시스템(100)은 두 개의 카메라(102)를 포함하고, 상기 로봇 제어기(120)는 암 제어기(122)와 레일 제어기(124)를 포함한다. </div>
<div class="description-paragraph" num="0035">먼저, 상기 카메라(102)는 날아가는 탁구공(150)을 감지하고, 상기 비전 시스템(100)은 날아가는 탁구공(150)의 실시간 위치시간정보를 감지한다. </div>
<div class="description-paragraph" num="0036">좀 더 상세하게는, 딥러닝부(110)는 날아가는 공의 1프레임과 로봇암(130)에 도달 시 2프레임을 습득하고, 이러한 데이터를 모아 회귀 분석 기법을 사용한 지도 학습으로 예측 값을 산출한다. </div>
<div class="description-paragraph" num="0037">특히, 비전 시스템(100)은 스테레오 카메라 영상처리 기술, opencv(open source computer vision)를 활용하여 이미지를 처리하고, cuda를 활용한 gpu(graphic processing unit)활용할 수 있다. </div>
<div class="description-paragraph" num="0038">본 실시예에서 제시된 탁구공 추적 방식은 실 사용시 한 대의 카메라만으로 작동이 가능하며, 이는 다수의 카메라의 규격화를 통해 거리를 측정할 필요가 없으며 초기 딥러닝시에만 사용될 수 있다. </div>
<div class="description-paragraph" num="0039">상기 딥러닝부(110)는 탁구공(150)의 실시간 위치시간정보를 입력 받고, 예상 위치시간정보를 습득할 수 있으며, 본 실시예에서는 tensorflow를 이용한 딥러닝 기술을 적용할 수 있다. </div>
<div class="description-paragraph" num="0040">상기 딥러닝부(110)에서 적용하는 딥 러닝 또는 심층학습(深層學習, 영어: deep structured learning, deep learning 또는 hierarchical learning)은 여러 비선형 변환기법의 조합을 통해 높은 수준의 추상화(abstractions, 다량의 데이터나 복잡한 자료들 속에서 핵심적인 내용 또는 기능을 요약하는 작업)를 시도하는 기계학습 알고리즘의 집합으로 정의되며, 큰 틀에서 사람의 사고방식을 컴퓨터에게 가르치는 기계학습의 한 분야라고 이야기할 수 있다.</div>
<div class="description-paragraph" num="0041">어떠한 데이터가 있을 때, 이를 컴퓨터가 알아 들을 수 있는 형태(예를 들어 이미지의 경우는 픽셀정보를 열벡터로 표현하는 등)로 표현(representation)하고 이를 학습에 적용하는 것으로, deep neural networks, convolutional deep neural networks, deep belief networks와 같은 딥 러닝 기법들이 있으며, 컴퓨터비젼, 음성인식, 자연어처리, 음성/신호처리 등의 분야에 적용될 수 있다. </div>
<div class="description-paragraph" num="0042">상기 딥러닝부(110)는 탁구공(150)의 연속적 실시간 위치시간정보를 이용하여 예상위치정보를 습득 및 학습하며, 예측 모델을 구현할 수 있으며, 상기 로봇암(130)에 장착된 탁구채의 타격시점에 대응하는 위치시간정보도 예측할 수 있다. </div>
<div class="description-paragraph" num="0043">상기 메인 컨트롤러(105)는 상기 비전 시스템(100), 및 상기 딥러닝부(110)와 정보를 송수신하고, 통신부(115)를 통해서 상기 로봇 제어기(120)의 암 제어기(122)를 통해서 상기 로봇암(130)의 작동을 제어하고, 상기 레일 제어기(124)를 통해서 상기 레일(135)을 제어하여 상기 로봇암(130)의 위치를 제어할 수 있다. </div>
<div class="description-paragraph" num="0044">상기 메인 컨트롤러(105)는 raspberry pi의 i2c 통신을 통해서 상기 로봇암(130)을 제어할 수 있고, raspberry pi의 gpio pwm 통신을 통해서 상기 레일(135)을 제어하여 상기 로봇암(130)의 위치를 제어할 수 있으며, socket통신을 통한 하드웨어간의 정보를 송수신할 수 있으며, 통신방법은 이에 한정되지 않으며, 실시예에 따라서 다른 통신수단으로 변경될 수 있다. </div>
<div class="description-paragraph" num="0045">도 1b를 참조하면, 탁구 로봇 시스템은 탁구대(140), 네트(145), 탁구공(150), 탁구채(155), 로봇암(130), 레일(135), 및 카메라(102)를 포함한다. 상기 탁구대(140)의 일측에는 사용자(재활 치료환자)가 배치되고, 타측에 폭방향으로 레일(135)이 배치되며, 상기 레일(135) 위에 상기 로봇암(130)이 배치되고, 상기 로봇암(130)의 일측에 탁구채(155)가 고정된다. </div>
<div class="description-paragraph" num="0046">상기 로봇암(130)은 제어기의 제어에 따라서 날아오는 탁구공을 타격하여 네트(145)를 넘어서 탁구공을 탁구대(140)의 일측으로 넘기는 기능을 수행할 수 있는데, 타격을 위한 위치시간정보가 필요하다. </div>
<div class="description-paragraph" num="0047">먼저, 비전 시스템(100)은 카메라를 통해서 날라가는 탁구공의 실시간 위치시간정보를 연산하며, 상기 딥러닝부(110)는 입력된 탁구공(150)의 실시간 위치시간정보를 학습하고, 예상 위치시간정보를 연산한다. </div>
<div class="description-paragraph" num="0048">그리고, 설정된 횟수만큼 학습이 진행되면, 메인 컨트롤러(105)는 탁구공(150)의 실시간 위치시간정보에 대응하는 예상 위치시간정보를 선택하고, 상기 로봇암(130)과 레일(135)이 작동하여 날라가는 탁구공(150)을 다시 보낼 수 있다. 아울러, 탁구공(150)의 예상 위치시간정보는 딥러닝부(110)의 학습을 통해서 다시 업데이트될 수 있다. </div>
<div class="description-paragraph" num="0049">본 실시예에서, 탁구공을 받아 치는 로봇암(130)은 역기구학을 적용한 6축 관절이고, 상기 레일(135)은 리니어 가이드 타입을 사용하였으며, 학습된 데이터 정보를 바탕으로 날아가는 탁구공의 프레임을 통해 예측 방향과 도달 시간(예측 위치시간정보)을 결정하고 소켓통신을 이용하여 로봇암(130) 및 레일(135)과 정보를 주고 받을 수 있다. </div>
<div class="description-paragraph" num="0050">도 2는 본 발명의 실시예에 따른 탁구 로봇 시스템의 개략적인 측면도이고, 도 3은 본 발명의 실시예에 따른 탁구 로봇 시스템에서 데이터의 형태를 보여주는 테이블이다. </div>
<div class="description-paragraph" num="0051">도 2 및 도 3을 참조하면, feature data와 label data를 포함하며, 이들은 모두 탁구공의 위치시간정보를 포함한다. 학습을 적용하기 위해서 이러한 데이터들이 상기 딥러닝부(110)로 설정된 형태로 설정된 순서대로 입력되고, 상기 딥러닝부(110)는 이러한 데이터들을 학습한다. </div>
<div class="description-paragraph" num="0052">따라서, 탁구공(150)의 이동경로 상의 위치시간정보가 입력되면, 탁구공(150)의 타격지점에서의 예상 위치시간정보가 선택될 수 있고, 예상 위치시간정보를 기초로 상기 로봇암(130)과 레일(135)을 제어하여 탁구공(150)을 적절한 시기와 위치에서 타격할 수 있다. </div>
<div class="description-paragraph" num="0053">도 4는 본 발명의 실시예에 따른 탁구 로봇 시스템에서 개략적인 학습방법을 보여준다. 도 4에서, 데이터수집단계에서, 비전 시스템은 탁구공의 프레임을 통해서 경로상에서 위치시간정보를 수집한다. 데이터입력단계에서, 미리 설정된 프로그램이나 라이브러리를 통해서 수집된 데이터들이 상기 딥러닝부로 입력될 수 있다. </div>
<div class="description-paragraph" num="0054">학습단계에서, 딥러닝부(110)는 입력된 데이터를 통해서 날아가는 공의 1프레임과 로봇암에 도달 시 2프레임을 습득하고, 이러한 데이터를 모아 회귀 분석 기법을 이용하여 데이터들을 학습한다. </div>
<div class="description-paragraph" num="0055">그리고, 예측 단계에서는 비전 시스템(100)에서 감지된 실시간 위치시간정보에 따라서 학습된 데이터들에서 예측 위치시간정보를 선택하고, 이렇게 선택된 예측 위치시간정보를 기초로 상기 로봇암(130)과 레일(135)이 제어되어 탁구공을 제대로 타격할 수 있도록 한다. </div>
<div class="description-paragraph" num="0056">도 5는 본 발명의 실시예에 따른 탁구 로봇 시스템에서 부가적인 내용을 보여준다. </div>
<div class="description-paragraph" num="0057">도 5를 참조하면, 비전 시스템(100)의 프로세스에 있어서, 탁구공(150)의 이동속도가 빨라서, 카메라에 감지된 화면데이터에 잔상이 일어나며, 잔상이 일어날 경우 탁구공의 좌표가 현재 위치와는 다른 위치로 왜곡되어 표시될 수 있다. 따라서, 본 실시예에서는 잔상을 포함한 전체적인 탁구공의 윤곽을 추출하고, 타원형 윤곽의 중심점을 계산하여 탁구공(150)의 중심점으로 계산한다. </div>
<div class="description-paragraph" num="0058">또한, 스테레오 카메라에 있는 렌즈는 광각 렌즈로써 촬영 시 왜곡이 발생하는데, 왜곡은 화각이 넓으면 넓을수록 심하게 발생하여, 카메라 왜곡에 대한 보정이 필요하며, 이를 해결하기 위해 본 실시예에서는 fisheye 렌즈왜곡 모델을 사용하였다. </div>
<div class="description-paragraph" num="0059">또한, 두 대의 카메라를 동시에 사용하기 위해 각 카메라의 동기화 (Synchronization) 작업이 필수적인데, 만일 동기화가 이루어지지 않았을 경우, 탁구공의 진행 방향을 시간차 없이 정확히 인지하여 예측 값을 얻어낼 수 없으며, 두 개의 카메라는 서로 다른 reference clock 을 보유하고 있어 clock을 일치시켜주는 작업이 필요하기에, 각각의 serializer 및 deserializer는 카메라로부터 clock을 추출을 도와준다.</div>
<div class="description-paragraph" num="0060">뿐만 아니라, 레일(135) 상에서 로봇암(130)이 탁구공(150)이 날아오는 방향으로 이동하려면 무엇보다 카메라로부터 탁구공까지의 거리를 구하는 것이 필수이다. 따라서 본 실시예에서 사용한 거리 측정 방식은 빛을 쏘아서 반사되어 오는 시간을 측정하여 거리를 계산하는 방식 (Time of Flight, TOF) 을 이용하여 카메라로부터 탁구공까지의 거리를 측정하였다. </div>
<div class="description-paragraph" num="0061">본 발명의 실시예에서, 탁구공의 타점/시간을 예측하기 위해 LSTM 딥러닝 모델을 고려하였으며, 모델들을 생성해 주기 위하여 tensorflow 라이브러리를 사용하였다. </div>
<div class="description-paragraph" num="0062">또한, python coding을 이용하였고, GPU를 사용하기 위해 CUDA를 설치하여 tensorflow gpu 버전으로 모델을 제작하였으며, DNN model은 Estimator API의 DNN-Regressor라는 고수준 API를 통해 구현하였다. </div>
<div class="description-paragraph" num="0063">그리고, 구현한 모델이 예측을 적용하기 위해서 먼저 학습을 시켜주어야 하고, 구현한 모델은 전부 label값이 필요한 supervised learning이므로 학습데이터는 feature 데이터와 label데이터가 필요하다. </div>
<div class="description-paragraph" num="0064">따라서, 탁구를 직접 치면서 비전 시스템을 통해 제공되는 feature data와 label data를 저장하여 이용하였으며, 데이터세트를 이용하여 이를 구현한 모델에 학습을 시켜주었으며, RNN 및 LSTM 기준으로, 학습량을 단계별로 시켜본 결과 batch Size가 500일시 약 4000번의 학습단계에서 가장 적은 손실 값을 보여주었다. </div>
<div class="description-paragraph" num="0065">이상에서 본 발명의 실시예들에 대하여 설명하였으나, 본 발명의 사상은 본 명세서에 제시되는 실시 예에 제한되지 아니하며, 본 발명의 사상을 이해하는 당업자는 동일한 사상의 범위 내에서, 구성요소의 부가, 변경, 삭제, 추가 등에 의해서 다른 실시 예를 용이하게 제안할 수 있을 것이나, 이 또한 본 발명의 사상범위 내에 든다고 할 것이다. </div>
</description-of-embodiments>
<reference-signs-list>
<div class="description-paragraph" num="0066">100: 비전 시스템            102: 카메라<br/>
105: 메인 컨트롤러          110: 딥러닝부<br/>
115: 통신부                 120: 로봇 제어기<br/>
122: 암 제어기              124: 레일 제어기<br/>
130: 로봇암                 135: 레일<br/>
140: 탁구대                 145: 네트<br/>
150: 탁구공                 155: 탁구채</div>
</reference-signs-list>
</div>
</div>
</section><section itemprop="claims" itemscope="">
<h2>Claims (<span itemprop="count">13</span>)</h2>
<div html="" itemprop="content"><ol class="claims" lang="KO" load-source="patent-office" mxw-id="PCLM290496412">
<li class="claim"> <div class="claim" num="1">
<div class="claim-text">카메라를 통해서 탁구대에서 일측에서 타측으로 날아가는 탁구공의 위치시간정보를 연산하는 비전 시스템; <br/>가이드레일을 따라서 움직이고, 탁구채를 이용하여 상기 탁구공을 일측으로 타격하도록 배치되는 로봇암; <br/>상기 가이드레일에서 상기 로봇암의 위치와 상기 로봇암의 작동을 제어하는 로봇 제어기;  <br/>상기 탁구공의 위치시간정보를 미리 설정된 형태로 입력 받고, 이를 학습하여 상기 탁구공의 예상 위치시간정보를 연산하는 딥러닝부; 및 <br/>상기 비전 시스템 및 상기 딥러닝부와 정보를 송수신하고, 상기 비전 시스템에서 감지된 위치시간정보와 상기 딥러닝부에서 연산된 예상 위치시간정보를 통해서 상기 로봇 제어기를 제어하는 메인 컨트롤러; 를 포함하는 것을 특징으로 하는 딥러닝을 적용한 탁구 로봇 시스템. </div>
</div>
</li> <li class="claim-dependent"> <div class="claim" num="2">
<div class="claim-text">제 1 항에 있어서, <br/>상기 메인 컨트롤러는, 상기 비전 시스템으로부터 입력된 탁구공의 위치시간정보로부터 상기 딥러닝부에서 연산된 예상 위치시간정보를 선택하고, <br/>선택된 예상 위치시간정보를 기준으로 상기 로봇암이 상기 탁구공을 타격하도록 제어하는 것을 특징으로 하는 딥러닝을 적용한 탁구 로봇 시스템.<b> </b> </div>
</div>
</li> <li class="claim-dependent"> <div class="claim" num="3">
<div class="claim-text">제 2 항에 있어서, <br/>상기 로봇 제어기는, <br/>상기 가이드레일 상에서 상기 로봇암의 위치를 제어하는 레일 제어기; 및 <br/>상기 로봇암이 탁구공을 타격하도록 제어하는 암 제어기; 를 포함하는 것을 특징으로 하는 딥러닝을 적용한 탁구 로봇 시스템. </div>
</div>
</li> <li class="claim-dependent"> <div class="claim" num="4">
<div class="claim-text">제 2 항에 있어서, <br/>상기 딥러닝부는 텐서플로우(tensorflow)를 이용한 학습을 적용하는 것을 특징으로 하는 딥러닝을 적용한 탁구 로봇 시스템. </div>
</div>
</li> <li class="claim-dependent"> <div class="claim" num="5">
<div class="claim-text">제 2 항에 있어서, <br/>상기 비전 시스템은 탁구대의 일측 상부 및 타측 상부에 배치되는 제1, 2 카메라; 를 포함하는 딥러닝을 적용한 탁구 로봇 시스템.<b> </b> </div>
</div>
</li> <li class="claim-dependent"> <div class="claim" num="6">
<div class="claim-text">제 2 항에 있어서, <br/>상기 메인 컨트롤러는 opencv 및 cuda를 활용한 gpu를 활용하여 상기 비전 시스템에서 촬영된 영상데이터를 처리하는 것을 특징으로 하는 딥러닝을 적용한 탁구 로봇 시스템. </div>
</div>
</li> <li class="claim-dependent"> <div class="claim" num="7">
<div class="claim-text">제 3 항에 있어서,<br/>상기 메인 컨트롤러는 raspberry pi의 i2c 통신을 통해서 상기 암 제어기를 제어하는 것을 특징으로 하는 딥러닝을 적용한 탁구 로봇 시스템.</div>
</div>
</li> <li class="claim-dependent"> <div class="claim" num="8">
<div class="claim-text">제 3 항에 있어서, <br/>상기 메인 컨트롤러는 raspberry pi의 gpio pwm 통신을 통해서 상기 레일 제어기를 제어하는 것을 특징으로 하는 딥러닝을 적용한 탁구 로봇 시스템. </div>
</div>
</li> <li class="claim-dependent"> <div class="claim" num="9">
<div class="claim-text">제 2 항에 있어서, <br/>상기 메인 컨트롤러는, 상기 비전 시스템으로부터 촬영된 데이터로부터 탁구공의 전체윤곽을 추출하고, 추출된 전체윤곽의 중심점을 연산하며, 이 중심점을 기준으로 탁구공의 위치시간정보를 연산하는 것을 특징으로 하는 딥러닝을 적용한 탁구 로봇 시스템. </div>
</div>
</li> <li class="claim-dependent"> <div class="claim" num="10">
<div class="claim-text">제 2 항에 있어서, <br/>상기 메인 컨트롤러는, 상기 비전 시스템으로부터 촬영된 데이터에서, 렌즈 왜곡을 보정하기 위해서 fisheye 렌즈왜곡모델을 사용하는 것을 특징으로 하는 딥러닝을 적용한 탁구 로봇 시스템. </div>
</div>
</li> <li class="claim-dependent"> <div class="claim" num="11">
<div class="claim-text">제 2 항에 있어서, <br/>상기 비전 시스템은 적어도 두 개의 카메라를 포함하고, 이들을 시간적으로 동기화 시키기 위해서 serializer 및 deserializer가 사용되는 것을 특징으로 하는 딥러닝을 적용한 탁구 로봇 시스템.<b> </b> </div>
</div>
</li> <li class="claim-dependent"> <div class="claim" num="12">
<div class="claim-text">제 2 항에 있어서, <br/>상기 비전 시스템은 빛에너지를 조사하여 탁구공과의 거리를 감지하는 TOF(time of flight)방식을 이용한 것을 특징으로 하는 딥러닝을 적용한 탁구 로봇 시스템.<b> </b> </div>
</div>
</li> <li class="claim-dependent"> <div class="claim" num="13">
<div class="claim-text">제 2 항에 있어서, <br/>상기 비전 시스템, 상기 메인 컨트롤러, 상기 딥러닝부, 및 상기 로봇 제어기 중에서 적어도 하나는 소켓통신을 이용하여 데이터를 송수신하는 것을 특징으로 하는 딥러닝을 적용한 탁구 로봇 시스템.<b> </b> </div>
</div>
</li> </ol>
</div>
</section>
                </article>
            </search-app>
        </body>
    </html>
    